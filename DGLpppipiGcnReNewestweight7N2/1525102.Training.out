0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 02:47:05 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   31C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b9c80607910> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m6.315s
user	0m2.480s
sys	0m0.870s
[02:47:18] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.6773],
        [-0.4499],
        [ 0.1723],
        ...,
        [-0.0041],
        [ 0.3126],
        [ 0.0627]], device='cuda:0', requires_grad=True) 
node features sum: tensor(51.0271, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0638, -0.0857,  0.0074,  0.1454,  0.0546, -0.0614,  0.0684, -0.1046,
         -0.0069,  0.0995, -0.0546,  0.0211,  0.0092, -0.1210, -0.1341, -0.0660,
         -0.0013,  0.0085,  0.0349,  0.1425,  0.1163,  0.0236,  0.1045,  0.0839,
          0.1180, -0.1505,  0.0991, -0.0264, -0.1104,  0.0757,  0.0168,  0.0227,
         -0.0313,  0.0070,  0.1185, -0.0875,  0.0573,  0.0710,  0.1071,  0.1330,
         -0.1276,  0.0734, -0.1145, -0.0991, -0.0333, -0.0420,  0.1219, -0.1269,
         -0.1522, -0.0227,  0.1054, -0.0657,  0.1438,  0.1519,  0.0816,  0.1030,
          0.0272,  0.0923,  0.0166, -0.0237,  0.0158,  0.0973,  0.0817, -0.0602,
         -0.1360, -0.1333,  0.0204, -0.1397,  0.0748, -0.0754,  0.0371,  0.0623,
          0.0133,  0.0768, -0.0130,  0.0602, -0.0741, -0.1192,  0.0034,  0.0677,
         -0.1388, -0.1271, -0.0488, -0.0610,  0.1429,  0.0973, -0.0241, -0.1086,
         -0.0330, -0.1359, -0.0771, -0.0515,  0.0155,  0.0830,  0.0746,  0.1371,
          0.0460, -0.1028,  0.0820,  0.0579, -0.1327, -0.1508, -0.1243,  0.0608,
          0.0774, -0.0631, -0.0653,  0.1226,  0.0293, -0.0162,  0.0519, -0.0621,
          0.0862, -0.0792,  0.0171, -0.0235,  0.0310,  0.1443, -0.0587,  0.1516,
          0.0255,  0.1238,  0.0927,  0.0279,  0.0652, -0.0230,  0.1043,  0.0754,
          0.0299,  0.0460, -0.1266, -0.0178, -0.1250,  0.1199, -0.1521,  0.0542,
         -0.0866,  0.0834, -0.0624, -0.1272,  0.0549,  0.0833, -0.1340,  0.1435,
         -0.1366, -0.0209,  0.0381,  0.1214, -0.0015, -0.0916,  0.0675, -0.0929,
          0.0568,  0.0920, -0.0141, -0.0284, -0.0262,  0.0916,  0.0832, -0.0922,
         -0.0209, -0.0165, -0.0926, -0.1418,  0.0697,  0.0679,  0.0852, -0.0562,
         -0.1164,  0.0436,  0.1307, -0.1289,  0.0071, -0.0553,  0.0815, -0.1290,
          0.0992, -0.0128,  0.1499, -0.0494,  0.0300,  0.1183,  0.1158, -0.0435,
         -0.0734,  0.0581,  0.0432, -0.1356, -0.0419,  0.1090,  0.0181, -0.0050,
         -0.0184, -0.0925,  0.0333,  0.0503,  0.0739,  0.0633,  0.0383,  0.0035,
         -0.0887,  0.1001,  0.1126,  0.0824, -0.0811, -0.0137,  0.1249,  0.0163,
          0.0357,  0.1496, -0.0207, -0.1106,  0.0708,  0.0275, -0.0300,  0.1184,
          0.1016, -0.0659, -0.1320, -0.0412,  0.0098,  0.1058,  0.0224,  0.0576,
          0.0939, -0.1342, -0.0040, -0.0555,  0.0945,  0.1356,  0.0663, -0.1176,
         -0.0671,  0.1324,  0.1009,  0.0613, -0.0776, -0.0392, -0.0957, -0.1022,
         -0.0929,  0.0928,  0.1211, -0.0181, -0.0631,  0.0152, -0.0729,  0.1438,
         -0.0528,  0.0158, -0.1284, -0.0316, -0.1045, -0.0960, -0.0647,  0.0151]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0638, -0.0857,  0.0074,  0.1454,  0.0546, -0.0614,  0.0684, -0.1046,
         -0.0069,  0.0995, -0.0546,  0.0211,  0.0092, -0.1210, -0.1341, -0.0660,
         -0.0013,  0.0085,  0.0349,  0.1425,  0.1163,  0.0236,  0.1045,  0.0839,
          0.1180, -0.1505,  0.0991, -0.0264, -0.1104,  0.0757,  0.0168,  0.0227,
         -0.0313,  0.0070,  0.1185, -0.0875,  0.0573,  0.0710,  0.1071,  0.1330,
         -0.1276,  0.0734, -0.1145, -0.0991, -0.0333, -0.0420,  0.1219, -0.1269,
         -0.1522, -0.0227,  0.1054, -0.0657,  0.1438,  0.1519,  0.0816,  0.1030,
          0.0272,  0.0923,  0.0166, -0.0237,  0.0158,  0.0973,  0.0817, -0.0602,
         -0.1360, -0.1333,  0.0204, -0.1397,  0.0748, -0.0754,  0.0371,  0.0623,
          0.0133,  0.0768, -0.0130,  0.0602, -0.0741, -0.1192,  0.0034,  0.0677,
         -0.1388, -0.1271, -0.0488, -0.0610,  0.1429,  0.0973, -0.0241, -0.1086,
         -0.0330, -0.1359, -0.0771, -0.0515,  0.0155,  0.0830,  0.0746,  0.1371,
          0.0460, -0.1028,  0.0820,  0.0579, -0.1327, -0.1508, -0.1243,  0.0608,
          0.0774, -0.0631, -0.0653,  0.1226,  0.0293, -0.0162,  0.0519, -0.0621,
          0.0862, -0.0792,  0.0171, -0.0235,  0.0310,  0.1443, -0.0587,  0.1516,
          0.0255,  0.1238,  0.0927,  0.0279,  0.0652, -0.0230,  0.1043,  0.0754,
          0.0299,  0.0460, -0.1266, -0.0178, -0.1250,  0.1199, -0.1521,  0.0542,
         -0.0866,  0.0834, -0.0624, -0.1272,  0.0549,  0.0833, -0.1340,  0.1435,
         -0.1366, -0.0209,  0.0381,  0.1214, -0.0015, -0.0916,  0.0675, -0.0929,
          0.0568,  0.0920, -0.0141, -0.0284, -0.0262,  0.0916,  0.0832, -0.0922,
         -0.0209, -0.0165, -0.0926, -0.1418,  0.0697,  0.0679,  0.0852, -0.0562,
         -0.1164,  0.0436,  0.1307, -0.1289,  0.0071, -0.0553,  0.0815, -0.1290,
          0.0992, -0.0128,  0.1499, -0.0494,  0.0300,  0.1183,  0.1158, -0.0435,
         -0.0734,  0.0581,  0.0432, -0.1356, -0.0419,  0.1090,  0.0181, -0.0050,
         -0.0184, -0.0925,  0.0333,  0.0503,  0.0739,  0.0633,  0.0383,  0.0035,
         -0.0887,  0.1001,  0.1126,  0.0824, -0.0811, -0.0137,  0.1249,  0.0163,
          0.0357,  0.1496, -0.0207, -0.1106,  0.0708,  0.0275, -0.0300,  0.1184,
          0.1016, -0.0659, -0.1320, -0.0412,  0.0098,  0.1058,  0.0224,  0.0576,
          0.0939, -0.1342, -0.0040, -0.0555,  0.0945,  0.1356,  0.0663, -0.1176,
         -0.0671,  0.1324,  0.1009,  0.0613, -0.0776, -0.0392, -0.0957, -0.1022,
         -0.0929,  0.0928,  0.1211, -0.0181, -0.0631,  0.0152, -0.0729,  0.1438,
         -0.0528,  0.0158, -0.1284, -0.0316, -0.1045, -0.0960, -0.0647,  0.0151]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.1016, -0.0353, -0.0303,  ...,  0.1176, -0.0947,  0.0205],
        [-0.0873,  0.0493,  0.0943,  ...,  0.0109,  0.0302, -0.1134],
        [-0.1236,  0.1058,  0.0318,  ...,  0.1181,  0.1024, -0.0418],
        ...,
        [ 0.0915,  0.0552, -0.0384,  ..., -0.0164, -0.0802,  0.0106],
        [-0.1076,  0.1135,  0.0419,  ..., -0.0094,  0.0038, -0.1096],
        [-0.0974,  0.0128,  0.0081,  ...,  0.0176, -0.0880, -0.0981]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1016, -0.0353, -0.0303,  ...,  0.1176, -0.0947,  0.0205],
        [-0.0873,  0.0493,  0.0943,  ...,  0.0109,  0.0302, -0.1134],
        [-0.1236,  0.1058,  0.0318,  ...,  0.1181,  0.1024, -0.0418],
        ...,
        [ 0.0915,  0.0552, -0.0384,  ..., -0.0164, -0.0802,  0.0106],
        [-0.1076,  0.1135,  0.0419,  ..., -0.0094,  0.0038, -0.1096],
        [-0.0974,  0.0128,  0.0081,  ...,  0.0176, -0.0880, -0.0981]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0726,  0.0806, -0.0336,  ...,  0.1407,  0.1513, -0.0722],
        [ 0.0957, -0.1338,  0.0734,  ..., -0.0926,  0.0017, -0.1618],
        [-0.0992, -0.1589, -0.0148,  ...,  0.1334, -0.0846, -0.0940],
        ...,
        [ 0.0468, -0.1448,  0.1555,  ..., -0.1224,  0.1368,  0.0263],
        [-0.1640, -0.1624,  0.0357,  ...,  0.0771, -0.0898,  0.1122],
        [ 0.0339,  0.0920,  0.0823,  ..., -0.0377,  0.1004, -0.1380]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0726,  0.0806, -0.0336,  ...,  0.1407,  0.1513, -0.0722],
        [ 0.0957, -0.1338,  0.0734,  ..., -0.0926,  0.0017, -0.1618],
        [-0.0992, -0.1589, -0.0148,  ...,  0.1334, -0.0846, -0.0940],
        ...,
        [ 0.0468, -0.1448,  0.1555,  ..., -0.1224,  0.1368,  0.0263],
        [-0.1640, -0.1624,  0.0357,  ...,  0.0771, -0.0898,  0.1122],
        [ 0.0339,  0.0920,  0.0823,  ..., -0.0377,  0.1004, -0.1380]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1599, -0.2370, -0.0103,  ...,  0.1593, -0.2442,  0.1981],
        [-0.2314, -0.0536, -0.1761,  ...,  0.2388,  0.0301, -0.1555],
        [-0.2128, -0.2075, -0.1125,  ..., -0.1685,  0.1778, -0.2331],
        ...,
        [ 0.1905,  0.1783, -0.2041,  ..., -0.0224, -0.2188, -0.1352],
        [-0.2056,  0.0796,  0.0384,  ...,  0.1388, -0.2164, -0.0664],
        [-0.0006,  0.1025, -0.0719,  ...,  0.2411, -0.1695, -0.0999]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1599, -0.2370, -0.0103,  ...,  0.1593, -0.2442,  0.1981],
        [-0.2314, -0.0536, -0.1761,  ...,  0.2388,  0.0301, -0.1555],
        [-0.2128, -0.2075, -0.1125,  ..., -0.1685,  0.1778, -0.2331],
        ...,
        [ 0.1905,  0.1783, -0.2041,  ..., -0.0224, -0.2188, -0.1352],
        [-0.2056,  0.0796,  0.0384,  ...,  0.1388, -0.2164, -0.0664],
        [-0.0006,  0.1025, -0.0719,  ...,  0.2411, -0.1695, -0.0999]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2920],
        [ 0.2409],
        [-0.0395],
        [ 0.0242],
        [ 0.1771],
        [-0.2139],
        [-0.1425],
        [-0.3749],
        [ 0.1926],
        [-0.3218],
        [ 0.2002],
        [ 0.3115],
        [ 0.0829],
        [-0.3475],
        [-0.0037],
        [-0.3052],
        [ 0.4169],
        [-0.4056],
        [ 0.1302],
        [-0.2091],
        [ 0.1023],
        [ 0.4025],
        [-0.0350],
        [ 0.3465],
        [-0.1803],
        [ 0.0173],
        [ 0.0936],
        [ 0.1728],
        [-0.2825],
        [-0.0133],
        [-0.0425],
        [ 0.1980]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2920],
        [ 0.2409],
        [-0.0395],
        [ 0.0242],
        [ 0.1771],
        [-0.2139],
        [-0.1425],
        [-0.3749],
        [ 0.1926],
        [-0.3218],
        [ 0.2002],
        [ 0.3115],
        [ 0.0829],
        [-0.3475],
        [-0.0037],
        [-0.3052],
        [ 0.4169],
        [-0.4056],
        [ 0.1302],
        [-0.2091],
        [ 0.1023],
        [ 0.4025],
        [-0.0350],
        [ 0.3465],
        [-0.1803],
        [ 0.0173],
        [ 0.0936],
        [ 0.1728],
        [-0.2825],
        [-0.0133],
        [-0.0425],
        [ 0.1980]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1165,  0.0414,  0.0192, -0.1497, -0.1256,  0.0949,  0.0527, -0.1060,
          0.0126, -0.1021,  0.1129,  0.0368,  0.0220,  0.1223,  0.0062,  0.0558,
          0.0410,  0.0118,  0.1255, -0.0323,  0.1508,  0.0147, -0.1264,  0.0633,
          0.0471, -0.0786, -0.0082,  0.0166,  0.1041,  0.1224, -0.0040, -0.0195,
         -0.0267, -0.0363, -0.1457,  0.0764,  0.0184, -0.0358,  0.1234, -0.0694,
          0.1038, -0.0624,  0.1357, -0.0266, -0.0447, -0.0295,  0.0879,  0.0527,
          0.0290, -0.0183,  0.0179, -0.0475,  0.1092,  0.0950,  0.0572,  0.0441,
          0.0683, -0.1069, -0.1309,  0.0855,  0.0369, -0.0982, -0.0298,  0.0895,
         -0.0882,  0.0886,  0.0089, -0.0033, -0.0548, -0.0154, -0.1027, -0.1020,
          0.0065,  0.0756, -0.1015,  0.0514, -0.1457,  0.0823, -0.1511,  0.0992,
          0.0227,  0.1231, -0.1002,  0.0353,  0.1513,  0.0124,  0.1265,  0.0967,
         -0.0055,  0.0943,  0.1357,  0.0584, -0.0968,  0.0671, -0.1322, -0.0023,
         -0.1285, -0.1470, -0.1384, -0.0402, -0.0650,  0.0580, -0.0086,  0.0685,
          0.1277, -0.1063,  0.0828, -0.0356,  0.0844,  0.0890,  0.0022,  0.0738,
          0.0297,  0.1072, -0.0089, -0.0598, -0.0313,  0.0477,  0.0311,  0.1259,
          0.1124,  0.1398,  0.1496, -0.0132,  0.0507,  0.1139,  0.0113, -0.0087,
          0.1525, -0.0428, -0.0750,  0.0379, -0.1397, -0.0780,  0.0397, -0.0771,
          0.0275, -0.0991, -0.1025, -0.1387,  0.0933,  0.0259,  0.1148, -0.0895,
          0.0448, -0.0135,  0.0287,  0.0600,  0.1272, -0.0615, -0.0058,  0.1286,
          0.0536, -0.0646, -0.0750, -0.1425, -0.1430, -0.1356, -0.0249, -0.0184,
          0.0674, -0.0435, -0.0319, -0.1508, -0.1475,  0.0810, -0.0982,  0.1303,
         -0.0994, -0.0983, -0.0063, -0.0252,  0.1294,  0.0789,  0.0790, -0.0296,
         -0.0538,  0.1360,  0.0811, -0.0833, -0.1022,  0.0840,  0.0109, -0.0042,
         -0.0828,  0.0760, -0.0692, -0.0331,  0.1159,  0.0877, -0.0086, -0.0730,
         -0.1448,  0.0291, -0.0127,  0.0359, -0.0415, -0.1199,  0.0744, -0.1056,
          0.1420,  0.0516,  0.0411,  0.1453,  0.0115,  0.0073, -0.1173,  0.1219,
         -0.1314, -0.0474,  0.1421,  0.0936, -0.0087, -0.0916, -0.0777,  0.0015,
          0.0838, -0.0790,  0.0777,  0.0949, -0.0467,  0.0444, -0.1045,  0.1038,
         -0.1351,  0.1165,  0.1352, -0.1443,  0.0527,  0.0604,  0.0030,  0.0775,
          0.0688,  0.0568,  0.0558,  0.0693, -0.1125, -0.0622, -0.1418,  0.0613,
         -0.0993,  0.0226, -0.1273, -0.1303, -0.1169, -0.0175,  0.1215,  0.1319,
         -0.0558, -0.0962,  0.1284,  0.0133,  0.1038, -0.1494, -0.1162, -0.0980]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1165,  0.0414,  0.0192, -0.1497, -0.1256,  0.0949,  0.0527, -0.1060,
          0.0126, -0.1021,  0.1129,  0.0368,  0.0220,  0.1223,  0.0062,  0.0558,
          0.0410,  0.0118,  0.1255, -0.0323,  0.1508,  0.0147, -0.1264,  0.0633,
          0.0471, -0.0786, -0.0082,  0.0166,  0.1041,  0.1224, -0.0040, -0.0195,
         -0.0267, -0.0363, -0.1457,  0.0764,  0.0184, -0.0358,  0.1234, -0.0694,
          0.1038, -0.0624,  0.1357, -0.0266, -0.0447, -0.0295,  0.0879,  0.0527,
          0.0290, -0.0183,  0.0179, -0.0475,  0.1092,  0.0950,  0.0572,  0.0441,
          0.0683, -0.1069, -0.1309,  0.0855,  0.0369, -0.0982, -0.0298,  0.0895,
         -0.0882,  0.0886,  0.0089, -0.0033, -0.0548, -0.0154, -0.1027, -0.1020,
          0.0065,  0.0756, -0.1015,  0.0514, -0.1457,  0.0823, -0.1511,  0.0992,
          0.0227,  0.1231, -0.1002,  0.0353,  0.1513,  0.0124,  0.1265,  0.0967,
         -0.0055,  0.0943,  0.1357,  0.0584, -0.0968,  0.0671, -0.1322, -0.0023,
         -0.1285, -0.1470, -0.1384, -0.0402, -0.0650,  0.0580, -0.0086,  0.0685,
          0.1277, -0.1063,  0.0828, -0.0356,  0.0844,  0.0890,  0.0022,  0.0738,
          0.0297,  0.1072, -0.0089, -0.0598, -0.0313,  0.0477,  0.0311,  0.1259,
          0.1124,  0.1398,  0.1496, -0.0132,  0.0507,  0.1139,  0.0113, -0.0087,
          0.1525, -0.0428, -0.0750,  0.0379, -0.1397, -0.0780,  0.0397, -0.0771,
          0.0275, -0.0991, -0.1025, -0.1387,  0.0933,  0.0259,  0.1148, -0.0895,
          0.0448, -0.0135,  0.0287,  0.0600,  0.1272, -0.0615, -0.0058,  0.1286,
          0.0536, -0.0646, -0.0750, -0.1425, -0.1430, -0.1356, -0.0249, -0.0184,
          0.0674, -0.0435, -0.0319, -0.1508, -0.1475,  0.0810, -0.0982,  0.1303,
         -0.0994, -0.0983, -0.0063, -0.0252,  0.1294,  0.0789,  0.0790, -0.0296,
         -0.0538,  0.1360,  0.0811, -0.0833, -0.1022,  0.0840,  0.0109, -0.0042,
         -0.0828,  0.0760, -0.0692, -0.0331,  0.1159,  0.0877, -0.0086, -0.0730,
         -0.1448,  0.0291, -0.0127,  0.0359, -0.0415, -0.1199,  0.0744, -0.1056,
          0.1420,  0.0516,  0.0411,  0.1453,  0.0115,  0.0073, -0.1173,  0.1219,
         -0.1314, -0.0474,  0.1421,  0.0936, -0.0087, -0.0916, -0.0777,  0.0015,
          0.0838, -0.0790,  0.0777,  0.0949, -0.0467,  0.0444, -0.1045,  0.1038,
         -0.1351,  0.1165,  0.1352, -0.1443,  0.0527,  0.0604,  0.0030,  0.0775,
          0.0688,  0.0568,  0.0558,  0.0693, -0.1125, -0.0622, -0.1418,  0.0613,
         -0.0993,  0.0226, -0.1273, -0.1303, -0.1169, -0.0175,  0.1215,  0.1319,
         -0.0558, -0.0962,  0.1284,  0.0133,  0.1038, -0.1494, -0.1162, -0.0980]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0874, -0.0320,  0.1247,  ...,  0.0480, -0.0748, -0.0648],
        [ 0.0566, -0.0517, -0.0037,  ..., -0.0994, -0.0689,  0.0046],
        [ 0.1149, -0.0692, -0.0808,  ..., -0.1013,  0.0312, -0.0633],
        ...,
        [-0.0431, -0.0719, -0.0456,  ...,  0.1103,  0.0256, -0.1234],
        [-0.0124,  0.0486,  0.0244,  ...,  0.0569,  0.0447,  0.1246],
        [ 0.0255,  0.0918, -0.0803,  ..., -0.0596,  0.0633,  0.0590]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0874, -0.0320,  0.1247,  ...,  0.0480, -0.0748, -0.0648],
        [ 0.0566, -0.0517, -0.0037,  ..., -0.0994, -0.0689,  0.0046],
        [ 0.1149, -0.0692, -0.0808,  ..., -0.1013,  0.0312, -0.0633],
        ...,
        [-0.0431, -0.0719, -0.0456,  ...,  0.1103,  0.0256, -0.1234],
        [-0.0124,  0.0486,  0.0244,  ...,  0.0569,  0.0447,  0.1246],
        [ 0.0255,  0.0918, -0.0803,  ..., -0.0596,  0.0633,  0.0590]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0207,  0.0430,  0.0712,  ..., -0.0670,  0.0915,  0.0069],
        [-0.1263, -0.1614,  0.0414,  ..., -0.1264, -0.0502,  0.0036],
        [-0.1559, -0.0990, -0.0767,  ...,  0.0717, -0.1115,  0.0231],
        ...,
        [ 0.1002, -0.0325, -0.0280,  ...,  0.0443, -0.1613,  0.0702],
        [-0.0693,  0.0819,  0.1438,  ...,  0.0792,  0.0709, -0.1435],
        [-0.1300,  0.0703, -0.1276,  ..., -0.0371,  0.0187, -0.1025]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0207,  0.0430,  0.0712,  ..., -0.0670,  0.0915,  0.0069],
        [-0.1263, -0.1614,  0.0414,  ..., -0.1264, -0.0502,  0.0036],
        [-0.1559, -0.0990, -0.0767,  ...,  0.0717, -0.1115,  0.0231],
        ...,
        [ 0.1002, -0.0325, -0.0280,  ...,  0.0443, -0.1613,  0.0702],
        [-0.0693,  0.0819,  0.1438,  ...,  0.0792,  0.0709, -0.1435],
        [-0.1300,  0.0703, -0.1276,  ..., -0.0371,  0.0187, -0.1025]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1448, -0.0583, -0.1234,  ...,  0.0923, -0.0795,  0.0961],
        [-0.1008, -0.0713, -0.1216,  ..., -0.1717, -0.0279, -0.0860],
        [ 0.0132,  0.2360,  0.1110,  ...,  0.2314,  0.1910, -0.0701],
        ...,
        [ 0.2494,  0.1409,  0.1759,  ..., -0.1586, -0.0558,  0.0789],
        [ 0.2102,  0.0178, -0.1618,  ..., -0.1564, -0.0681,  0.0783],
        [ 0.0490, -0.1331,  0.0010,  ..., -0.2446, -0.0784, -0.0514]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1448, -0.0583, -0.1234,  ...,  0.0923, -0.0795,  0.0961],
        [-0.1008, -0.0713, -0.1216,  ..., -0.1717, -0.0279, -0.0860],
        [ 0.0132,  0.2360,  0.1110,  ...,  0.2314,  0.1910, -0.0701],
        ...,
        [ 0.2494,  0.1409,  0.1759,  ..., -0.1586, -0.0558,  0.0789],
        [ 0.2102,  0.0178, -0.1618,  ..., -0.1564, -0.0681,  0.0783],
        [ 0.0490, -0.1331,  0.0010,  ..., -0.2446, -0.0784, -0.0514]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2910],
        [ 0.0889],
        [ 0.1798],
        [-0.0458],
        [-0.0985],
        [-0.3672],
        [ 0.2934],
        [ 0.3609],
        [-0.1540],
        [-0.0439],
        [ 0.3630],
        [ 0.1791],
        [-0.1907],
        [ 0.3548],
        [ 0.1950],
        [-0.3310],
        [-0.2120],
        [-0.3689],
        [-0.0274],
        [ 0.3112],
        [-0.3660],
        [-0.0724],
        [ 0.1624],
        [ 0.2757],
        [-0.0355],
        [ 0.2459],
        [-0.3431],
        [-0.1099],
        [-0.2593],
        [ 0.2389],
        [-0.0913],
        [-0.2058]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2910],
        [ 0.0889],
        [ 0.1798],
        [-0.0458],
        [-0.0985],
        [-0.3672],
        [ 0.2934],
        [ 0.3609],
        [-0.1540],
        [-0.0439],
        [ 0.3630],
        [ 0.1791],
        [-0.1907],
        [ 0.3548],
        [ 0.1950],
        [-0.3310],
        [-0.2120],
        [-0.3689],
        [-0.0274],
        [ 0.3112],
        [-0.3660],
        [-0.0724],
        [ 0.1624],
        [ 0.2757],
        [-0.0355],
        [ 0.2459],
        [-0.3431],
        [-0.1099],
        [-0.2593],
        [ 0.2389],
        [-0.0913],
        [-0.2058]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-9.5051, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.7712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(1.8178, device='cuda:0')



h[100].sum tensor(-2.1594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.2162, device='cuda:0')



h[200].sum tensor(2.8665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(2.9419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2841.8101, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.0111e-03, 2.2482e-03, 8.4505e-05,  ..., 1.1203e-03, 0.0000e+00,
         1.5439e-03],
        [5.2759e-03, 1.1731e-02, 4.4092e-04,  ..., 5.8454e-03, 0.0000e+00,
         8.0559e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(13639.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(205.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.4399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29.7416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(275.1122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(22.0082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1573],
        [0.1926],
        [0.2776],
        ...,
        [0.0444],
        [0.0445],
        [0.0352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(3969.1284, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.1573],
        [0.1926],
        [0.2776],
        ...,
        [0.0444],
        [0.0445],
        [0.0352]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(65.4636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.8101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6901, device='cuda:0')



h[100].sum tensor(-5.8372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-5.7899, device='cuda:0')



h[200].sum tensor(-8.9079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-8.8357, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(13961.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0107, 0.0044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(91266.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.2480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2930.6592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(206.1855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-44.8165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0749],
        [-0.0459],
        [-0.0281],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-4400.4238, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.1573],
        [0.1926],
        [0.2776],
        ...,
        [0.0444],
        [0.0445],
        [0.0352]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 2 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0883,  0.0674, -0.0018,  0.0882,  0.0820, -0.1034,  0.0604,  0.1419,
          0.0940, -0.1002,  0.1092, -0.1128, -0.0125,  0.1028,  0.0240,  0.1404,
          0.0018, -0.0907, -0.0107,  0.0697,  0.1203,  0.1269,  0.0403, -0.0161,
         -0.0054,  0.0590,  0.0979, -0.1215,  0.0111,  0.0905,  0.0321,  0.1335,
          0.0123, -0.1244,  0.0528,  0.1193, -0.0336, -0.0039, -0.1002, -0.0835,
         -0.0613, -0.0657, -0.0232,  0.1153, -0.1253,  0.0303,  0.0916,  0.0935,
         -0.1264,  0.1195, -0.0434, -0.1213,  0.1439,  0.0922,  0.1452, -0.1186,
          0.0611,  0.0216,  0.0878, -0.0995,  0.0150, -0.1508,  0.0064, -0.1090,
          0.1364,  0.0190, -0.0662,  0.0224,  0.0176,  0.0822,  0.1455,  0.0975,
         -0.1385,  0.0579,  0.0765, -0.1085,  0.0329,  0.0688,  0.1432, -0.1517,
          0.1014,  0.1512,  0.0031, -0.1269, -0.1237,  0.0827,  0.1178, -0.0119,
          0.1315,  0.1354, -0.0571, -0.0691, -0.1098,  0.0961, -0.0783, -0.0130,
         -0.1367, -0.0879,  0.0832,  0.0146,  0.0280,  0.0373, -0.1217,  0.0355,
          0.0472,  0.1522,  0.0949, -0.1404, -0.0953,  0.1313, -0.0426, -0.1340,
          0.0631,  0.0968, -0.0582, -0.0479, -0.0202, -0.0070, -0.0230,  0.0431,
          0.1374,  0.1142,  0.0841, -0.0164, -0.0944, -0.0172, -0.0226,  0.0067,
          0.0100,  0.1052,  0.1317,  0.0159, -0.1446,  0.0945,  0.1280, -0.0441,
         -0.1302,  0.0433, -0.0644, -0.1285,  0.0906,  0.0370, -0.0550, -0.0420,
          0.0311,  0.0798,  0.0171, -0.0231,  0.0401,  0.1358, -0.0096, -0.0332,
          0.0231, -0.0442,  0.0052, -0.1088, -0.0789,  0.0286, -0.0276, -0.0682,
          0.1241,  0.0647, -0.1003,  0.1186, -0.1248, -0.1413,  0.0072, -0.0934,
          0.0145, -0.0148,  0.1069,  0.0995, -0.0338,  0.0265,  0.1358,  0.0303,
         -0.0630,  0.1180, -0.0240, -0.0834, -0.0365,  0.0207, -0.0359, -0.0162,
          0.1499,  0.0433, -0.0954,  0.0846,  0.0864, -0.0868,  0.0491,  0.0241,
          0.0317, -0.0256,  0.0880,  0.0634, -0.0179,  0.0062, -0.0749,  0.0765,
          0.0986,  0.1168,  0.0588, -0.0416,  0.0741,  0.0998, -0.0469,  0.1218,
          0.0511,  0.1170, -0.1151, -0.0294,  0.0129, -0.1085, -0.0523,  0.0405,
         -0.0602, -0.0967, -0.1257,  0.1151,  0.0328, -0.1131, -0.0177, -0.0262,
          0.1438,  0.0775, -0.0162, -0.1025,  0.0623,  0.1243, -0.0297, -0.0874,
         -0.1024, -0.0253, -0.0589, -0.0957,  0.0206,  0.0511, -0.0085, -0.1249,
         -0.0693,  0.0331,  0.0738,  0.0818, -0.0332,  0.0353,  0.0675, -0.1165,
          0.0835,  0.0462,  0.0290,  0.0351,  0.0078,  0.1403, -0.0352,  0.0566]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0308,  0.0364,  0.0096,  ...,  0.0826, -0.0469, -0.0783],
        [ 0.1077, -0.0010,  0.0199,  ..., -0.0864, -0.0400,  0.0442],
        [ 0.0251, -0.1163, -0.0245,  ...,  0.0057,  0.0858, -0.0510],
        ...,
        [-0.0881,  0.1002,  0.0671,  ..., -0.0897, -0.0198, -0.0308],
        [ 0.1163, -0.1101,  0.0267,  ..., -0.0938, -0.0856,  0.0223],
        [ 0.0411, -0.0002, -0.0904,  ...,  0.0548, -0.0862, -0.0538]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0871, -0.0035,  0.0956,  ..., -0.1694, -0.1624, -0.0779],
        [-0.0595,  0.0238,  0.0591,  ...,  0.0773,  0.1139, -0.0030],
        [-0.0591, -0.1129,  0.0563,  ...,  0.0438, -0.1179,  0.0436],
        ...,
        [-0.0787,  0.0536,  0.0249,  ..., -0.0684, -0.0125, -0.0911],
        [-0.0822,  0.0363,  0.0747,  ...,  0.1421,  0.1716,  0.0799],
        [ 0.0956,  0.0798, -0.1667,  ..., -0.1434,  0.1185,  0.1440]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2218, -0.2492,  0.2297,  ..., -0.0678, -0.1137, -0.0355],
        [ 0.2208, -0.2342,  0.0663,  ..., -0.0032, -0.1755, -0.1004],
        [-0.1873,  0.0219,  0.0924,  ...,  0.0676,  0.0706,  0.1347],
        ...,
        [ 0.1470,  0.0780, -0.1132,  ..., -0.0884,  0.0864,  0.0910],
        [ 0.0318, -0.1743,  0.2046,  ..., -0.1846, -0.0314, -0.0659],
        [-0.0145, -0.2434, -0.1183,  ...,  0.0803, -0.2273,  0.0987]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2646],
        [-0.3907],
        [-0.1107],
        [-0.2225],
        [-0.0674],
        [-0.1198],
        [-0.0291],
        [ 0.0913],
        [-0.0782],
        [-0.1794],
        [-0.1585],
        [ 0.3848],
        [-0.3696],
        [-0.1956],
        [ 0.0873],
        [-0.3551],
        [-0.1469],
        [ 0.1393],
        [ 0.1915],
        [-0.3337],
        [ 0.2439],
        [-0.3753],
        [-0.3932],
        [ 0.3853],
        [ 0.1651],
        [-0.0139],
        [ 0.0189],
        [-0.2288],
        [-0.3926],
        [ 0.0634],
        [ 0.2997],
        [ 0.1917]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0883,  0.0674, -0.0018,  0.0882,  0.0820, -0.1034,  0.0604,  0.1419,
          0.0940, -0.1002,  0.1092, -0.1128, -0.0125,  0.1028,  0.0240,  0.1404,
          0.0018, -0.0907, -0.0107,  0.0697,  0.1203,  0.1269,  0.0403, -0.0161,
         -0.0054,  0.0590,  0.0979, -0.1215,  0.0111,  0.0905,  0.0321,  0.1335,
          0.0123, -0.1244,  0.0528,  0.1193, -0.0336, -0.0039, -0.1002, -0.0835,
         -0.0613, -0.0657, -0.0232,  0.1153, -0.1253,  0.0303,  0.0916,  0.0935,
         -0.1264,  0.1195, -0.0434, -0.1213,  0.1439,  0.0922,  0.1452, -0.1186,
          0.0611,  0.0216,  0.0878, -0.0995,  0.0150, -0.1508,  0.0064, -0.1090,
          0.1364,  0.0190, -0.0662,  0.0224,  0.0176,  0.0822,  0.1455,  0.0975,
         -0.1385,  0.0579,  0.0765, -0.1085,  0.0329,  0.0688,  0.1432, -0.1517,
          0.1014,  0.1512,  0.0031, -0.1269, -0.1237,  0.0827,  0.1178, -0.0119,
          0.1315,  0.1354, -0.0571, -0.0691, -0.1098,  0.0961, -0.0783, -0.0130,
         -0.1367, -0.0879,  0.0832,  0.0146,  0.0280,  0.0373, -0.1217,  0.0355,
          0.0472,  0.1522,  0.0949, -0.1404, -0.0953,  0.1313, -0.0426, -0.1340,
          0.0631,  0.0968, -0.0582, -0.0479, -0.0202, -0.0070, -0.0230,  0.0431,
          0.1374,  0.1142,  0.0841, -0.0164, -0.0944, -0.0172, -0.0226,  0.0067,
          0.0100,  0.1052,  0.1317,  0.0159, -0.1446,  0.0945,  0.1280, -0.0441,
         -0.1302,  0.0433, -0.0644, -0.1285,  0.0906,  0.0370, -0.0550, -0.0420,
          0.0311,  0.0798,  0.0171, -0.0231,  0.0401,  0.1358, -0.0096, -0.0332,
          0.0231, -0.0442,  0.0052, -0.1088, -0.0789,  0.0286, -0.0276, -0.0682,
          0.1241,  0.0647, -0.1003,  0.1186, -0.1248, -0.1413,  0.0072, -0.0934,
          0.0145, -0.0148,  0.1069,  0.0995, -0.0338,  0.0265,  0.1358,  0.0303,
         -0.0630,  0.1180, -0.0240, -0.0834, -0.0365,  0.0207, -0.0359, -0.0162,
          0.1499,  0.0433, -0.0954,  0.0846,  0.0864, -0.0868,  0.0491,  0.0241,
          0.0317, -0.0256,  0.0880,  0.0634, -0.0179,  0.0062, -0.0749,  0.0765,
          0.0986,  0.1168,  0.0588, -0.0416,  0.0741,  0.0998, -0.0469,  0.1218,
          0.0511,  0.1170, -0.1151, -0.0294,  0.0129, -0.1085, -0.0523,  0.0405,
         -0.0602, -0.0967, -0.1257,  0.1151,  0.0328, -0.1131, -0.0177, -0.0262,
          0.1438,  0.0775, -0.0162, -0.1025,  0.0623,  0.1243, -0.0297, -0.0874,
         -0.1024, -0.0253, -0.0589, -0.0957,  0.0206,  0.0511, -0.0085, -0.1249,
         -0.0693,  0.0331,  0.0738,  0.0818, -0.0332,  0.0353,  0.0675, -0.1165,
          0.0835,  0.0462,  0.0290,  0.0351,  0.0078,  0.1403, -0.0352,  0.0566]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0308,  0.0364,  0.0096,  ...,  0.0826, -0.0469, -0.0783],
        [ 0.1077, -0.0010,  0.0199,  ..., -0.0864, -0.0400,  0.0442],
        [ 0.0251, -0.1163, -0.0245,  ...,  0.0057,  0.0858, -0.0510],
        ...,
        [-0.0881,  0.1002,  0.0671,  ..., -0.0897, -0.0198, -0.0308],
        [ 0.1163, -0.1101,  0.0267,  ..., -0.0938, -0.0856,  0.0223],
        [ 0.0411, -0.0002, -0.0904,  ...,  0.0548, -0.0862, -0.0538]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0871, -0.0035,  0.0956,  ..., -0.1694, -0.1624, -0.0779],
        [-0.0595,  0.0238,  0.0591,  ...,  0.0773,  0.1139, -0.0030],
        [-0.0591, -0.1129,  0.0563,  ...,  0.0438, -0.1179,  0.0436],
        ...,
        [-0.0787,  0.0536,  0.0249,  ..., -0.0684, -0.0125, -0.0911],
        [-0.0822,  0.0363,  0.0747,  ...,  0.1421,  0.1716,  0.0799],
        [ 0.0956,  0.0798, -0.1667,  ..., -0.1434,  0.1185,  0.1440]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2218, -0.2492,  0.2297,  ..., -0.0678, -0.1137, -0.0355],
        [ 0.2208, -0.2342,  0.0663,  ..., -0.0032, -0.1755, -0.1004],
        [-0.1873,  0.0219,  0.0924,  ...,  0.0676,  0.0706,  0.1347],
        ...,
        [ 0.1470,  0.0780, -0.1132,  ..., -0.0884,  0.0864,  0.0910],
        [ 0.0318, -0.1743,  0.2046,  ..., -0.1846, -0.0314, -0.0659],
        [-0.0145, -0.2434, -0.1183,  ...,  0.0803, -0.2273,  0.0987]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2646],
        [-0.3907],
        [-0.1107],
        [-0.2225],
        [-0.0674],
        [-0.1198],
        [-0.0291],
        [ 0.0913],
        [-0.0782],
        [-0.1794],
        [-0.1585],
        [ 0.3848],
        [-0.3696],
        [-0.1956],
        [ 0.0873],
        [-0.3551],
        [-0.1469],
        [ 0.1393],
        [ 0.1915],
        [-0.3337],
        [ 0.2439],
        [-0.3753],
        [-0.3932],
        [ 0.3853],
        [ 0.1651],
        [-0.0139],
        [ 0.0189],
        [-0.2288],
        [-0.3926],
        [ 0.0634],
        [ 0.2997],
        [ 0.1917]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0130,  0.0099, -0.0003,  ...,  0.0206, -0.0052,  0.0083],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(783.7446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2984, device='cuda:0')



h[100].sum tensor(8.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.7577, device='cuda:0')



h[200].sum tensor(28.8298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0490, 0.0374, 0.0000,  ..., 0.0778, 0.0000, 0.0314],
        [0.0403, 0.0308, 0.0000,  ..., 0.0640, 0.0000, 0.0258],
        [0.0094, 0.0072, 0.0000,  ..., 0.0150, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32273.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1322, 0.0673, 0.3960,  ..., 0.0384, 0.1136, 0.0000],
        [0.1133, 0.0576, 0.3393,  ..., 0.0329, 0.0973, 0.0000],
        [0.0909, 0.0463, 0.2724,  ..., 0.0264, 0.0781, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(132374.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1841.1555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.6233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(436.4839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-348.9108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[8.1124e-01],
        [8.7780e-01],
        [9.6862e-01],
        ...,
        [9.7458e-07],
        [1.2805e-07],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(15119.2080, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(76.0742, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365922.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        ...,
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04],
        [-1.0000e-04, -1.0000e-04,  0.0000e+00,  ...,  1.0000e-04,
          0.0000e+00,  1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(698.5181, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2335, device='cuda:0')



h[100].sum tensor(10.9919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.5741, device='cuda:0')



h[200].sum tensor(23.1672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3645, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32764.7402, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0009,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0013,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0009,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0002, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0009,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145016.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1731.1235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.1777, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(69.5006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(450.8704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(117.2939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-331.1340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0433],
        [-0.0473],
        [-0.0507],
        ...,
        [-0.0146],
        [-0.0145],
        [-0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-12861.7939, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365922.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365924.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04],
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04],
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04],
        ...,
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04],
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04],
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(426.7708, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.9294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6937, device='cuda:0')



h[100].sum tensor(11.9712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.9911, device='cuda:0')



h[200].sum tensor(13.6600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9697, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0430, 0.0330, 0.0000,  ..., 0.0700, 0.0000, 0.0286],
        [0.0285, 0.0219, 0.0000,  ..., 0.0467, 0.0000, 0.0192],
        [0.0235, 0.0181, 0.0000,  ..., 0.0388, 0.0000, 0.0160],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26129.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0445e-01, 5.1839e-02, 3.3289e-01,  ..., 3.5192e-02, 8.4433e-02,
         0.0000e+00],
        [8.3637e-02, 4.1717e-02, 2.6687e-01,  ..., 2.8035e-02, 6.7282e-02,
         0.0000e+00],
        [6.3350e-02, 3.1855e-02, 2.0247e-01,  ..., 2.1032e-02, 5.0514e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.3881e-04, 1.7416e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3881e-04, 1.7416e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3881e-04, 1.7416e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(126434.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1328.9491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(251.5814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(366.5642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(89.6199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.6263, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0441],
        [-0.0508],
        [-0.0620],
        ...,
        [-0.0301],
        [-0.0299],
        [-0.0299]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-29207.3574, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365924.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365924.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6256e-03,  3.5986e-03, -8.9496e-05,  ...,  7.8373e-03,
         -1.9074e-03,  3.2453e-03],
        [ 1.1464e-02,  8.8157e-03, -2.1696e-04,  ...,  1.8736e-02,
         -4.6238e-03,  7.6523e-03],
        [ 8.3411e-03,  6.4332e-03, -1.5875e-04,  ...,  1.3759e-02,
         -3.3833e-03,  5.6398e-03],
        ...,
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04],
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04],
        [-1.7600e-04, -6.4538e-05,  0.0000e+00,  ...,  1.8499e-04,
          0.0000e+00,  1.5098e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(477.5164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.5862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5574, device='cuda:0')



h[100].sum tensor(12.5020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5734, device='cuda:0')



h[200].sum tensor(15.5086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0409, 0.0314, 0.0000,  ..., 0.0670, 0.0000, 0.0274],
        [0.0261, 0.0202, 0.0000,  ..., 0.0435, 0.0000, 0.0179],
        [0.0220, 0.0170, 0.0000,  ..., 0.0366, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26706.2598, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.0444e-02, 3.5289e-02, 2.2425e-01,  ..., 2.3128e-02, 5.5624e-02,
         0.0000e+00],
        [6.2868e-02, 3.1599e-02, 2.0005e-01,  ..., 2.0438e-02, 4.9211e-02,
         0.0000e+00],
        [5.3946e-02, 2.7271e-02, 1.7195e-01,  ..., 1.7463e-02, 4.2058e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.3881e-04, 1.7416e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3881e-04, 1.7416e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3881e-04, 1.7416e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(123115.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1286.3579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.8917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(374.6431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(84.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-249.7190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0549],
        [-0.0590],
        [-0.0665],
        ...,
        [-0.0301],
        [-0.0300],
        [-0.0300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30158.0117, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9999],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365924.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9999],
        [0.9998],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365921.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6991e-04,  4.8523e-06,  0.0000e+00,  ...,  2.7204e-04,
          0.0000e+00,  1.0541e-04],
        [-1.6991e-04,  4.8523e-06,  0.0000e+00,  ...,  2.7204e-04,
          0.0000e+00,  1.0541e-04],
        [-1.6991e-04,  4.8523e-06,  0.0000e+00,  ...,  2.7204e-04,
          0.0000e+00,  1.0541e-04],
        ...,
        [-1.6991e-04,  4.8523e-06,  0.0000e+00,  ...,  2.7204e-04,
          0.0000e+00,  1.0541e-04],
        [-1.6991e-04,  4.8523e-06,  0.0000e+00,  ...,  2.7204e-04,
          0.0000e+00,  1.0541e-04],
        [-1.6991e-04,  4.8523e-06,  0.0000e+00,  ...,  2.7204e-04,
          0.0000e+00,  1.0541e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(378.8408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(11.4405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8991, device='cuda:0')



h[100].sum tensor(15.3500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.6053, device='cuda:0')



h[200].sum tensor(14.9952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.9414e-05, 0.0000e+00,  ..., 1.0884e-03, 0.0000e+00,
         4.2175e-04],
        [0.0000e+00, 1.9414e-05, 0.0000e+00,  ..., 1.0884e-03, 0.0000e+00,
         4.2175e-04],
        [0.0000e+00, 1.9405e-05, 0.0000e+00,  ..., 1.0879e-03, 0.0000e+00,
         4.2155e-04],
        ...,
        [0.0000e+00, 1.9408e-05, 0.0000e+00,  ..., 1.0881e-03, 0.0000e+00,
         4.2161e-04],
        [0.0000e+00, 1.9408e-05, 0.0000e+00,  ..., 1.0881e-03, 0.0000e+00,
         4.2161e-04],
        [0.0000e+00, 1.9408e-05, 0.0000e+00,  ..., 1.0881e-03, 0.0000e+00,
         4.2161e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24228.7832, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0014, 0.0022,  ..., 0.0000, 0.0005, 0.0000],
        [0.0039, 0.0029, 0.0119,  ..., 0.0005, 0.0029, 0.0000],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(113396.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1171.3263, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(229.6928, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(344.3174, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31.7895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.5828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0712],
        [-0.0739],
        [-0.0776],
        ...,
        [-0.0403],
        [-0.0397],
        [-0.0387]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-27833.2227, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9999],
        [0.9998],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365921.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365920.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.0258e-03,  6.3197e-03, -1.3584e-04,  ...,  1.3406e-02,
         -3.2281e-03,  5.3190e-03],
        [-1.3690e-04,  8.5958e-05,  0.0000e+00,  ...,  3.6280e-04,
          0.0000e+00,  4.3675e-05],
        [-1.3690e-04,  8.5958e-05,  0.0000e+00,  ...,  3.6280e-04,
          0.0000e+00,  4.3675e-05],
        ...,
        [-1.3690e-04,  8.5958e-05,  0.0000e+00,  ...,  3.6280e-04,
          0.0000e+00,  4.3675e-05],
        [-1.3690e-04,  8.5958e-05,  0.0000e+00,  ...,  3.6280e-04,
          0.0000e+00,  4.3675e-05],
        [-1.3690e-04,  8.5958e-05,  0.0000e+00,  ...,  3.6280e-04,
          0.0000e+00,  4.3675e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(584.3248, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.3554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.7523, device='cuda:0')



h[100].sum tensor(20.6945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.1249, device='cuda:0')



h[200].sum tensor(25.4384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0302, 0.0236, 0.0000,  ..., 0.0501, 0.0000, 0.0199],
        [0.0080, 0.0066, 0.0000,  ..., 0.0145, 0.0000, 0.0055],
        [0.0000, 0.0003, 0.0000,  ..., 0.0015, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0015, 0.0000, 0.0002],
        [0.0000, 0.0003, 0.0000,  ..., 0.0015, 0.0000, 0.0002],
        [0.0000, 0.0003, 0.0000,  ..., 0.0015, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36016.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0638, 0.0312, 0.2017,  ..., 0.0210, 0.0518, 0.0000],
        [0.0288, 0.0150, 0.0896,  ..., 0.0082, 0.0238, 0.0000],
        [0.0082, 0.0054, 0.0245,  ..., 0.0017, 0.0075, 0.0000],
        ...,
        [0.0002, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0002, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0002, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174637.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2054.1577, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.2643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(504.4795, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38.9242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-357.2736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0461],
        [-0.0448],
        [-0.0446],
        ...,
        [-0.0392],
        [-0.0390],
        [-0.0390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-20843.5508, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365920.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365920.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9188e-05,  1.5797e-04,  0.0000e+00,  ...,  4.5689e-04,
          0.0000e+00, -2.0374e-05],
        [-8.9188e-05,  1.5797e-04,  0.0000e+00,  ...,  4.5689e-04,
          0.0000e+00, -2.0374e-05],
        [-8.9188e-05,  1.5797e-04,  0.0000e+00,  ...,  4.5689e-04,
          0.0000e+00, -2.0374e-05],
        ...,
        [-8.9188e-05,  1.5797e-04,  0.0000e+00,  ...,  4.5689e-04,
          0.0000e+00, -2.0374e-05],
        [-8.9188e-05,  1.5797e-04,  0.0000e+00,  ...,  4.5689e-04,
          0.0000e+00, -2.0374e-05],
        [-8.9188e-05,  1.5797e-04,  0.0000e+00,  ...,  4.5689e-04,
          0.0000e+00, -2.0374e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(483.3582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.2140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2544, device='cuda:0')



h[100].sum tensor(20.1189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6468, device='cuda:0')



h[200].sum tensor(24.5184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32190.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1284e-02, 7.6599e-03, 3.3297e-02,  ..., 2.4419e-03, 1.0780e-02,
         0.0000e+00],
        [2.5463e-03, 3.6068e-03, 6.3554e-03,  ..., 5.9459e-05, 4.0454e-03,
         0.0000e+00],
        [1.9900e-04, 2.5229e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3282e-03,
         0.0000e+00],
        ...,
        [1.9914e-04, 2.4992e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3052e-03,
         0.0000e+00],
        [1.9914e-04, 2.4992e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3052e-03,
         0.0000e+00],
        [1.9914e-04, 2.4992e-03, 0.0000e+00,  ..., 0.0000e+00, 2.3052e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153446.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1693.8840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.7657, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(453.1700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(13.1457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-310.5465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0128],
        [-0.0173],
        [-0.0215],
        ...,
        [-0.0398],
        [-0.0397],
        [-0.0396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-16685.0898, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9997],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365920.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365923.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4967e-02,  1.1677e-02, -2.2002e-04,  ...,  2.4569e-02,
         -5.9038e-03,  9.6181e-03],
        [-3.8110e-05,  1.9851e-04,  0.0000e+00,  ...,  5.4646e-04,
          0.0000e+00, -8.8546e-05],
        [-3.8110e-05,  1.9851e-04,  0.0000e+00,  ...,  5.4646e-04,
          0.0000e+00, -8.8546e-05],
        ...,
        [-3.8110e-05,  1.9851e-04,  0.0000e+00,  ...,  5.4646e-04,
          0.0000e+00, -8.8546e-05],
        [-3.8110e-05,  1.9851e-04,  0.0000e+00,  ...,  5.4646e-04,
          0.0000e+00, -8.8546e-05],
        [-3.8110e-05,  1.9851e-04,  0.0000e+00,  ...,  5.4646e-04,
          0.0000e+00, -8.8546e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(325.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.1072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9349, device='cuda:0')



h[100].sum tensor(18.2157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7123, device='cuda:0')



h[200].sum tensor(21.3259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0269, 0.0214, 0.0000,  ..., 0.0453, 0.0000, 0.0173],
        [0.0150, 0.0123, 0.0000,  ..., 0.0262, 0.0000, 0.0096],
        [0.0000, 0.0008, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0022, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0022, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26347.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.9185e-02, 4.4751e-02, 2.8472e-01,  ..., 3.0621e-02, 7.4185e-02,
         0.0000e+00],
        [4.4629e-02, 2.3993e-02, 1.4151e-01,  ..., 1.3761e-02, 3.8348e-02,
         0.0000e+00],
        [1.4913e-02, 1.0181e-02, 4.5305e-02,  ..., 2.5981e-03, 1.4197e-02,
         0.0000e+00],
        ...,
        [2.6287e-05, 3.1676e-03, 0.0000e+00,  ..., 0.0000e+00, 2.6269e-03,
         0.0000e+00],
        [2.6287e-05, 3.1676e-03, 0.0000e+00,  ..., 0.0000e+00, 2.6269e-03,
         0.0000e+00],
        [2.6287e-05, 3.1676e-03, 0.0000e+00,  ..., 0.0000e+00, 2.6269e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(131491.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1357.6395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.3668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(370.2809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.2155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.0291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0188],
        [ 0.0244],
        [ 0.0303],
        ...,
        [-0.0408],
        [-0.0406],
        [-0.0405]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-11692.4775, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365923.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1947e-05,  2.1599e-04,  0.0000e+00,  ...,  6.2099e-04,
          0.0000e+00, -1.5743e-04],
        [ 1.1947e-05,  2.1599e-04,  0.0000e+00,  ...,  6.2099e-04,
          0.0000e+00, -1.5743e-04],
        [ 1.1947e-05,  2.1599e-04,  0.0000e+00,  ...,  6.2099e-04,
          0.0000e+00, -1.5743e-04],
        ...,
        [ 1.1947e-05,  2.1599e-04,  0.0000e+00,  ...,  6.2099e-04,
          0.0000e+00, -1.5743e-04],
        [ 1.1947e-05,  2.1599e-04,  0.0000e+00,  ...,  6.2099e-04,
          0.0000e+00, -1.5743e-04],
        [ 1.1947e-05,  2.1599e-04,  0.0000e+00,  ...,  6.2099e-04,
          0.0000e+00, -1.5743e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(220.2804, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.5974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.3733, device='cuda:0')



h[100].sum tensor(16.5416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.0437, device='cuda:0')



h[200].sum tensor(19.1116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.1333e-02, 1.7160e-02, 0.0000e+00,  ..., 3.6585e-02, 0.0000e+00,
         1.3456e-02],
        [1.0691e-02, 9.0124e-03, 0.0000e+00,  ..., 1.9536e-02, 0.0000e+00,
         6.7282e-03],
        [1.8564e-02, 1.5039e-02, 0.0000e+00,  ..., 3.2147e-02, 0.0000e+00,
         1.1822e-02],
        ...,
        [4.7801e-05, 8.6419e-04, 0.0000e+00,  ..., 2.4846e-03, 0.0000e+00,
         0.0000e+00],
        [4.7801e-05, 8.6419e-04, 0.0000e+00,  ..., 2.4846e-03, 0.0000e+00,
         0.0000e+00],
        [4.7801e-05, 8.6419e-04, 0.0000e+00,  ..., 2.4846e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22960.1855, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.1747e-02, 2.3006e-02, 1.2772e-01,  ..., 1.1691e-02, 3.4690e-02,
         0.0000e+00],
        [3.7443e-02, 2.0939e-02, 1.1505e-01,  ..., 1.0250e-02, 3.1616e-02,
         0.0000e+00],
        [4.5164e-02, 2.4478e-02, 1.4081e-01,  ..., 1.3330e-02, 3.8170e-02,
         0.0000e+00],
        ...,
        [2.0906e-05, 3.3152e-03, 0.0000e+00,  ..., 0.0000e+00, 2.6235e-03,
         0.0000e+00],
        [2.0907e-05, 3.3152e-03, 0.0000e+00,  ..., 0.0000e+00, 2.6235e-03,
         0.0000e+00],
        [2.0907e-05, 3.3152e-03, 0.0000e+00,  ..., 0.0000e+00, 2.6235e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(120776.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1096.5400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(209.9565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(323.4149, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.0780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0520],
        [ 0.0534],
        [ 0.0539],
        ...,
        [-0.0457],
        [-0.0455],
        [-0.0454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-15312.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365939.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5897e-05,  2.2843e-04,  0.0000e+00,  ...,  6.8350e-04,
          0.0000e+00, -2.2090e-04],
        [ 6.0213e-03,  4.8065e-03, -7.6039e-05,  ...,  1.0262e-02,
         -2.3383e-03,  3.6454e-03],
        [ 4.5897e-05,  2.2843e-04,  0.0000e+00,  ...,  6.8350e-04,
          0.0000e+00, -2.2090e-04],
        ...,
        [ 4.5897e-05,  2.2843e-04,  0.0000e+00,  ...,  6.8350e-04,
          0.0000e+00, -2.2090e-04],
        [ 4.5897e-05,  2.2843e-04,  0.0000e+00,  ...,  6.8350e-04,
          0.0000e+00, -2.2090e-04],
        [ 4.5897e-05,  2.2843e-04,  0.0000e+00,  ...,  6.8350e-04,
          0.0000e+00, -2.2090e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(170.7087, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.4228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-6.6198, device='cuda:0')



h[100].sum tensor(15.4935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.7910, device='cuda:0')



h[200].sum tensor(18.1920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7384, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0055, 0.0000,  ..., 0.0123, 0.0000, 0.0036],
        [0.0086, 0.0074, 0.0000,  ..., 0.0163, 0.0000, 0.0052],
        [0.0290, 0.0230, 0.0000,  ..., 0.0490, 0.0000, 0.0178],
        ...,
        [0.0002, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0002, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0002, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22623.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0184, 0.0125, 0.0506,  ..., 0.0026, 0.0148, 0.0000],
        [0.0270, 0.0167, 0.0763,  ..., 0.0051, 0.0211, 0.0000],
        [0.0416, 0.0237, 0.1195,  ..., 0.0099, 0.0317, 0.0000],
        ...,
        [0.0004, 0.0038, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0004, 0.0038, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0004, 0.0038, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(125066.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1125.1112, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(210.2637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.9075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.5392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0796],
        [ 0.0966],
        [ 0.1103],
        ...,
        [-0.0553],
        [-0.0550],
        [-0.0548]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-16480.6328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9997],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365939.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365950.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5155e-05,  2.3007e-04,  0.0000e+00,  ...,  7.3557e-04,
          0.0000e+00, -2.8093e-04],
        [ 1.9810e-02,  1.5369e-02, -2.3255e-04,  ...,  3.2404e-02,
         -7.7053e-03,  1.2495e-02],
        [ 2.0068e-02,  1.5566e-02, -2.3558e-04,  ...,  3.2817e-02,
         -7.8059e-03,  1.2662e-02],
        ...,
        [ 6.5155e-05,  2.3007e-04,  0.0000e+00,  ...,  7.3557e-04,
          0.0000e+00, -2.8093e-04],
        [ 6.5155e-05,  2.3007e-04,  0.0000e+00,  ...,  7.3557e-04,
          0.0000e+00, -2.8093e-04],
        [ 6.5155e-05,  2.3007e-04,  0.0000e+00,  ...,  7.3557e-04,
          0.0000e+00, -2.8093e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(338.2426, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5388, device='cuda:0')



h[100].sum tensor(16.6732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5177, device='cuda:0')



h[200].sum tensor(24.4612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0639, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0262, 0.0208, 0.0000,  ..., 0.0446, 0.0000, 0.0162],
        [0.0364, 0.0286, 0.0000,  ..., 0.0609, 0.0000, 0.0225],
        [0.0759, 0.0589, 0.0000,  ..., 0.1242, 0.0000, 0.0478],
        ...,
        [0.0003, 0.0009, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0003, 0.0009, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0003, 0.0009, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29819.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0637, 0.0340, 0.1914,  ..., 0.0181, 0.0499, 0.0000],
        [0.0987, 0.0501, 0.3020,  ..., 0.0311, 0.0778, 0.0000],
        [0.1483, 0.0729, 0.4601,  ..., 0.0497, 0.1177, 0.0000],
        ...,
        [0.0008, 0.0045, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0008, 0.0045, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0008, 0.0045, 0.0000,  ..., 0.0000, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155896.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1512.3591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.7073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(411.6650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.8367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1469],
        [ 0.1479],
        [ 0.1496],
        ...,
        [-0.0680],
        [-0.0677],
        [-0.0676]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-21972.2754, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365950.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(526.5164, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365962.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.0504e-03,  5.5697e-03, -7.5681e-05,  ...,  1.1975e-02,
         -2.7154e-03,  4.1813e-03],
        [ 8.5211e-03,  6.6980e-03, -9.1632e-05,  ...,  1.4335e-02,
         -3.2877e-03,  5.1329e-03],
        [ 1.3974e-02,  1.0882e-02, -1.5078e-04,  ...,  2.3085e-02,
         -5.4099e-03,  8.6618e-03],
        ...,
        [ 7.2890e-05,  2.1677e-04,  0.0000e+00,  ...,  7.7868e-04,
          0.0000e+00, -3.3400e-04],
        [ 7.2890e-05,  2.1677e-04,  0.0000e+00,  ...,  7.7868e-04,
          0.0000e+00, -3.3400e-04],
        [ 7.2890e-05,  2.1677e-04,  0.0000e+00,  ...,  7.7868e-04,
          0.0000e+00, -3.3400e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(567.5911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.7157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.4058, device='cuda:0')



h[100].sum tensor(18.4548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0787, device='cuda:0')



h[200].sum tensor(32.4807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4952, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0367, 0.0288, 0.0000,  ..., 0.0615, 0.0000, 0.0222],
        [0.0395, 0.0309, 0.0000,  ..., 0.0660, 0.0000, 0.0240],
        [0.0274, 0.0217, 0.0000,  ..., 0.0466, 0.0000, 0.0162],
        ...,
        [0.0003, 0.0009, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0003, 0.0009, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0003, 0.0009, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38684.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0563, 0.0318, 0.1624,  ..., 0.0141, 0.0422, 0.0000],
        [0.0717, 0.0389, 0.2096,  ..., 0.0194, 0.0540, 0.0000],
        [0.0874, 0.0463, 0.2580,  ..., 0.0249, 0.0662, 0.0000],
        ...,
        [0.0010, 0.0055, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0010, 0.0055, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0010, 0.0055, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189855.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1910.5707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(354.1096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(537.3332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-343.0276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1207],
        [ 0.1365],
        [ 0.1512],
        ...,
        [-0.0825],
        [-0.0822],
        [-0.0820]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30313.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [0.9998],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365962.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [0.9999],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365974.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.9447e-05,  2.0737e-04,  0.0000e+00,  ...,  8.1192e-04,
          0.0000e+00, -3.8548e-04],
        [ 7.9447e-05,  2.0737e-04,  0.0000e+00,  ...,  8.1192e-04,
          0.0000e+00, -3.8548e-04],
        [ 7.9447e-05,  2.0737e-04,  0.0000e+00,  ...,  8.1192e-04,
          0.0000e+00, -3.8548e-04],
        ...,
        [ 7.9447e-05,  2.0737e-04,  0.0000e+00,  ...,  8.1192e-04,
          0.0000e+00, -3.8548e-04],
        [ 7.9447e-05,  2.0737e-04,  0.0000e+00,  ...,  8.1192e-04,
          0.0000e+00, -3.8548e-04],
        [ 7.9447e-05,  2.0737e-04,  0.0000e+00,  ...,  8.1192e-04,
          0.0000e+00, -3.8548e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(873.6374, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.8371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6740, device='cuda:0')



h[100].sum tensor(21.2861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.8289, device='cuda:0')



h[200].sum tensor(42.8796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0008, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0003, 0.0008, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0100, 0.0083, 0.0000,  ..., 0.0188, 0.0000, 0.0059],
        ...,
        [0.0003, 0.0008, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0003, 0.0008, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0003, 0.0008, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50814.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0070, 0.0027,  ..., 0.0000, 0.0013, 0.0000],
        [0.0074, 0.0093, 0.0158,  ..., 0.0000, 0.0049, 0.0000],
        [0.0210, 0.0158, 0.0543,  ..., 0.0027, 0.0149, 0.0000],
        ...,
        [0.0014, 0.0064, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0014, 0.0064, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0014, 0.0064, 0.0000,  ..., 0.0000, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(245125.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2662.2073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(461.4726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(704.4595, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-467.8376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0616],
        [-0.0124],
        [ 0.0432],
        ...,
        [-0.0946],
        [-0.0942],
        [-0.0940]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-34677.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [0.9999],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365974.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0000],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365983.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9899e-02,  1.5414e-02, -1.7934e-04,  ...,  3.2678e-02,
         -7.6761e-03,  1.2434e-02],
        [ 2.8900e-02,  2.2323e-02, -2.6074e-04,  ...,  4.7132e-02,
         -1.1160e-02,  1.8263e-02],
        [ 2.0679e-02,  1.6012e-02, -1.8639e-04,  ...,  3.3930e-02,
         -7.9779e-03,  1.2939e-02],
        ...,
        [ 6.7239e-05,  1.9074e-04,  0.0000e+00,  ...,  8.3050e-04,
          0.0000e+00, -4.1018e-04],
        [ 6.7239e-05,  1.9074e-04,  0.0000e+00,  ...,  8.3050e-04,
          0.0000e+00, -4.1018e-04],
        [ 6.7239e-05,  1.9074e-04,  0.0000e+00,  ...,  8.3050e-04,
          0.0000e+00, -4.1018e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(564.9478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.6777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6587, device='cuda:0')



h[100].sum tensor(18.1785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.8349, device='cuda:0')



h[200].sum tensor(31.0105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0892, 0.0690, 0.0000,  ..., 0.1461, 0.0000, 0.0559],
        [0.1041, 0.0805, 0.0000,  ..., 0.1701, 0.0000, 0.0656],
        [0.0863, 0.0668, 0.0000,  ..., 0.1414, 0.0000, 0.0541],
        ...,
        [0.0003, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0003, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0003, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41124.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5789e-01, 8.0491e-02, 4.9259e-01,  ..., 5.3698e-02, 1.2421e-01,
         0.0000e+00],
        [1.9170e-01, 9.6058e-02, 6.0114e-01,  ..., 6.6547e-02, 1.5140e-01,
         0.0000e+00],
        [1.8541e-01, 9.3320e-02, 5.7945e-01,  ..., 6.3834e-02, 1.4591e-01,
         0.0000e+00],
        ...,
        [1.0865e-03, 7.5588e-03, 0.0000e+00,  ..., 0.0000e+00, 1.0708e-05,
         0.0000e+00],
        [1.0864e-03, 7.5587e-03, 0.0000e+00,  ..., 0.0000e+00, 1.0710e-05,
         0.0000e+00],
        [1.0864e-03, 7.5586e-03, 0.0000e+00,  ..., 0.0000e+00, 1.0711e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(213401.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2026.5172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.3510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(577.7020, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-356.3342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0935],
        [ 0.1038],
        [ 0.1116],
        ...,
        [-0.1071],
        [-0.1066],
        [-0.1065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-47918., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0000],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365983.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0000],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365992.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.2750e-05,  1.9243e-04,  0.0000e+00,  ...,  8.3453e-04,
          0.0000e+00, -4.2765e-04],
        [ 5.2750e-05,  1.9243e-04,  0.0000e+00,  ...,  8.3453e-04,
          0.0000e+00, -4.2765e-04],
        [ 5.2750e-05,  1.9243e-04,  0.0000e+00,  ...,  8.3453e-04,
          0.0000e+00, -4.2765e-04],
        ...,
        [ 5.2750e-05,  1.9243e-04,  0.0000e+00,  ...,  8.3453e-04,
          0.0000e+00, -4.2765e-04],
        [ 5.2750e-05,  1.9243e-04,  0.0000e+00,  ...,  8.3453e-04,
          0.0000e+00, -4.2765e-04],
        [ 5.2750e-05,  1.9243e-04,  0.0000e+00,  ...,  8.3453e-04,
          0.0000e+00, -4.2765e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(540.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.2337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9536, device='cuda:0')



h[100].sum tensor(18.3488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.7269, device='cuda:0')



h[200].sum tensor(29.1517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0002, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0002, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0002, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0002, 0.0008, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41211.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0007, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(222883.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1990.7864, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.1085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23.7452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(577.9105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.4379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1784],
        [-0.1716],
        [-0.1517],
        ...,
        [-0.1011],
        [-0.1145],
        [-0.1152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53575.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0000],
        [0.9996],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365992.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0001],
        [0.9996],
        ...,
        [0.9994],
        [0.9993],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366001.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7494e-03,  6.8982e-03, -6.3842e-05,  ...,  1.4822e-02,
         -3.3529e-03,  5.2034e-03],
        [ 4.1746e-05,  2.1495e-04,  0.0000e+00,  ...,  8.2792e-04,
          0.0000e+00, -4.4609e-04],
        [ 1.4857e-02,  1.1585e-02, -1.0862e-04,  ...,  2.4636e-02,
         -5.7045e-03,  9.1657e-03],
        ...,
        [ 4.1746e-05,  2.1495e-04,  0.0000e+00,  ...,  8.2792e-04,
          0.0000e+00, -4.4609e-04],
        [ 4.1746e-05,  2.1495e-04,  0.0000e+00,  ...,  8.2792e-04,
          0.0000e+00, -4.4609e-04],
        [ 4.1746e-05,  2.1495e-04,  0.0000e+00,  ...,  8.2792e-04,
          0.0000e+00, -4.4609e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(466.5869, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7537, device='cuda:0')



h[100].sum tensor(18.4470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.1397, device='cuda:0')



h[200].sum tensor(25.6902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3110, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0063, 0.0000,  ..., 0.0147, 0.0000, 0.0042],
        [0.0379, 0.0298, 0.0000,  ..., 0.0640, 0.0000, 0.0227],
        [0.0213, 0.0170, 0.0000,  ..., 0.0372, 0.0000, 0.0123],
        ...,
        [0.0002, 0.0009, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0002, 0.0009, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0002, 0.0009, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38952.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0253, 0.0213, 0.0706,  ..., 0.0044, 0.0177, 0.0000],
        [0.0512, 0.0346, 0.1493,  ..., 0.0128, 0.0368, 0.0000],
        [0.0528, 0.0357, 0.1518,  ..., 0.0127, 0.0373, 0.0000],
        ...,
        [0.0004, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0004, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215209.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1851.6626, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.7556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63.2316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(538.7223, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-330.4306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0111],
        [ 0.0547],
        [ 0.0833],
        ...,
        [-0.1210],
        [-0.1204],
        [-0.1203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50943.2305, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0001],
        [0.9996],
        ...,
        [0.9994],
        [0.9993],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366001.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0002],
        [0.9996],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366010.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7564e-03,  6.1752e-03, -5.0212e-05,  ...,  1.3202e-02,
         -2.9602e-03,  4.5303e-03],
        [ 4.7822e-05,  2.5856e-04,  0.0000e+00,  ...,  8.1056e-04,
          0.0000e+00, -4.7431e-04],
        [ 7.7564e-03,  6.1752e-03, -5.0212e-05,  ...,  1.3202e-02,
         -2.9602e-03,  4.5303e-03],
        ...,
        [ 4.7822e-05,  2.5856e-04,  0.0000e+00,  ...,  8.1056e-04,
          0.0000e+00, -4.7431e-04],
        [ 4.7822e-05,  2.5856e-04,  0.0000e+00,  ...,  8.1056e-04,
          0.0000e+00, -4.7431e-04],
        [ 4.7822e-05,  2.5856e-04,  0.0000e+00,  ...,  8.1056e-04,
          0.0000e+00, -4.7431e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(310.3069, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.1833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0620, device='cuda:0')



h[100].sum tensor(18.1758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0924, device='cuda:0')



h[200].sum tensor(19.8660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0065, 0.0059, 0.0000,  ..., 0.0134, 0.0000, 0.0036],
        [0.0282, 0.0225, 0.0000,  ..., 0.0483, 0.0000, 0.0163],
        [0.0065, 0.0059, 0.0000,  ..., 0.0134, 0.0000, 0.0036],
        ...,
        [0.0002, 0.0010, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31615.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0174, 0.0170, 0.0462,  ..., 0.0019, 0.0122, 0.0000],
        [0.0296, 0.0234, 0.0825,  ..., 0.0056, 0.0210, 0.0000],
        [0.0174, 0.0171, 0.0461,  ..., 0.0019, 0.0122, 0.0000],
        ...,
        [0.0005, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0005, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183463.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1338.8389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(286.6963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(164.4735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(423.1967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.0342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1137],
        [-0.0967],
        [-0.0995],
        ...,
        [-0.1210],
        [-0.1205],
        [-0.1203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56217.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0002],
        [0.9996],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366010.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0002],
        [0.9996],
        ...,
        [0.9993],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366018.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6768e-02,  1.3131e-02, -9.5657e-05,  ...,  2.7657e-02,
         -6.3998e-03,  1.0353e-02],
        [ 1.5507e-02,  1.2163e-02, -8.8437e-05,  ...,  2.5629e-02,
         -5.9168e-03,  9.5338e-03],
        [ 2.3857e-02,  1.8572e-02, -1.3624e-04,  ...,  3.9054e-02,
         -9.1147e-03,  1.4958e-02],
        ...,
        [ 5.7705e-05,  3.0402e-04,  0.0000e+00,  ...,  7.8911e-04,
          0.0000e+00, -5.0263e-04],
        [ 5.7705e-05,  3.0402e-04,  0.0000e+00,  ...,  7.8911e-04,
          0.0000e+00, -5.0263e-04],
        [ 5.7705e-05,  3.0402e-04,  0.0000e+00,  ...,  7.8911e-04,
          0.0000e+00, -5.0263e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(437.2058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8908, device='cuda:0')



h[100].sum tensor(20.9285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.5598, device='cuda:0')



h[200].sum tensor(23.9065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2147, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0668, 0.0523, 0.0000,  ..., 0.1102, 0.0000, 0.0413],
        [0.0866, 0.0675, 0.0000,  ..., 0.1421, 0.0000, 0.0541],
        [0.1043, 0.0811, 0.0000,  ..., 0.1705, 0.0000, 0.0656],
        ...,
        [0.0002, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35080.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4568e-01, 7.9853e-02, 4.6409e-01,  ..., 5.2858e-02, 1.1411e-01,
         0.0000e+00],
        [1.7361e-01, 9.3213e-02, 5.5881e-01,  ..., 6.4993e-02, 1.3699e-01,
         0.0000e+00],
        [2.0528e-01, 1.0837e-01, 6.6642e-01,  ..., 7.8753e-02, 1.6301e-01,
         0.0000e+00],
        ...,
        [6.8959e-04, 7.5973e-03, 0.0000e+00,  ..., 0.0000e+00, 4.0530e-04,
         0.0000e+00],
        [6.8959e-04, 7.5971e-03, 0.0000e+00,  ..., 0.0000e+00, 4.0530e-04,
         0.0000e+00],
        [6.8960e-04, 7.5970e-03, 0.0000e+00,  ..., 0.0000e+00, 4.0530e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193485.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1500.3214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.6636, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(191.5082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(454.3291, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.6890, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0085],
        [-0.0068],
        [-0.0199],
        ...,
        [-0.1205],
        [-0.1199],
        [-0.1198]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-49311.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0002],
        [0.9996],
        ...,
        [0.9993],
        [0.9992],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366018.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9996],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366028.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9591e-03,  4.8671e-03, -2.9212e-05,  ...,  1.0233e-02,
         -2.2471e-03,  3.2887e-03],
        [ 5.5733e-03,  4.5710e-03, -2.7296e-05,  ...,  9.6122e-03,
         -2.0997e-03,  3.0380e-03],
        [ 7.5504e-05,  3.5063e-04,  0.0000e+00,  ...,  7.7122e-04,
          0.0000e+00, -5.3532e-04],
        ...,
        [ 7.5504e-05,  3.5063e-04,  0.0000e+00,  ...,  7.7122e-04,
          0.0000e+00, -5.3532e-04],
        [ 7.5504e-05,  3.5063e-04,  0.0000e+00,  ...,  7.7122e-04,
          0.0000e+00, -5.3532e-04],
        [ 7.5504e-05,  3.5063e-04,  0.0000e+00,  ...,  7.7122e-04,
          0.0000e+00, -5.3532e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(849.2098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.7073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0063, device='cuda:0')



h[100].sum tensor(26.6043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(50.8431, device='cuda:0')



h[200].sum tensor(37.7509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0459, 0.0364, 0.0000,  ..., 0.0763, 0.0000, 0.0275],
        [0.0107, 0.0094, 0.0000,  ..., 0.0198, 0.0000, 0.0057],
        [0.0228, 0.0187, 0.0000,  ..., 0.0392, 0.0000, 0.0130],
        ...,
        [0.0003, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53182.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0840, 0.0492, 0.2574,  ..., 0.0270, 0.0653, 0.0000],
        [0.0509, 0.0329, 0.1488,  ..., 0.0137, 0.0389, 0.0000],
        [0.0560, 0.0356, 0.1640,  ..., 0.0153, 0.0427, 0.0000],
        ...,
        [0.0009, 0.0070, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0009, 0.0070, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0009, 0.0070, 0.0000,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(278010.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2723.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.2837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(214.6047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(687.4943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-488.2219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0967],
        [ 0.1182],
        [ 0.1322],
        ...,
        [-0.0951],
        [-0.0943],
        [-0.0976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33049.3711, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9996],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366028.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9997],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366036.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3567e-05,  3.6955e-04,  0.0000e+00,  ...,  7.5423e-04,
          0.0000e+00, -5.5298e-04],
        [ 7.3567e-05,  3.6955e-04,  0.0000e+00,  ...,  7.5423e-04,
          0.0000e+00, -5.5298e-04],
        [ 7.3567e-05,  3.6955e-04,  0.0000e+00,  ...,  7.5423e-04,
          0.0000e+00, -5.5298e-04],
        ...,
        [ 7.3567e-05,  3.6955e-04,  0.0000e+00,  ...,  7.5423e-04,
          0.0000e+00, -5.5298e-04],
        [ 7.3567e-05,  3.6955e-04,  0.0000e+00,  ...,  7.5423e-04,
          0.0000e+00, -5.5298e-04],
        [ 7.3567e-05,  3.6955e-04,  0.0000e+00,  ...,  7.5423e-04,
          0.0000e+00, -5.5298e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(595.4340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.4459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8028, device='cuda:0')



h[100].sum tensor(25.2848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.2760, device='cuda:0')



h[200].sum tensor(28.3705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0003, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0003, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0003, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0003, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41345.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0071, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0008, 0.0071, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0008, 0.0071, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        ...,
        [0.0008, 0.0071, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0008, 0.0071, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0008, 0.0071, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(226557.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2028.2581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(358.4690, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(253.3655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(519.6330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-360.6028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1654],
        [-0.1675],
        [-0.1636],
        ...,
        [-0.1143],
        [-0.1095],
        [-0.1003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-29086.3711, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0003],
        [0.9997],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366036.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0004],
        [0.9997],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366045.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.4042e-03,  6.0077e-03, -2.6050e-05,  ...,  1.2560e-02,
         -2.7913e-03,  4.2245e-03],
        [ 7.7577e-03,  6.2790e-03, -2.7303e-05,  ...,  1.3129e-02,
         -2.9256e-03,  4.4546e-03],
        [ 7.4042e-03,  6.0077e-03, -2.6050e-05,  ...,  1.2560e-02,
         -2.7913e-03,  4.2245e-03],
        ...,
        [ 5.5343e-05,  3.6640e-04,  0.0000e+00,  ...,  7.3754e-04,
          0.0000e+00, -5.5812e-04],
        [ 5.5343e-05,  3.6640e-04,  0.0000e+00,  ...,  7.3754e-04,
          0.0000e+00, -5.5812e-04],
        [ 5.5343e-05,  3.6640e-04,  0.0000e+00,  ...,  7.3754e-04,
          0.0000e+00, -5.5812e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(748.3533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.0974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1097, device='cuda:0')



h[100].sum tensor(27.8935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.1728, device='cuda:0')



h[200].sum tensor(32.4015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6853, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0342, 0.0276, 0.0000,  ..., 0.0577, 0.0000, 0.0199],
        [0.0332, 0.0268, 0.0000,  ..., 0.0561, 0.0000, 0.0193],
        [0.0139, 0.0120, 0.0000,  ..., 0.0250, 0.0000, 0.0078],
        ...,
        [0.0002, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0002, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0002, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48261.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0489, 0.0337, 0.1415,  ..., 0.0136, 0.0375, 0.0000],
        [0.0496, 0.0341, 0.1429,  ..., 0.0136, 0.0378, 0.0000],
        [0.0389, 0.0285, 0.1092,  ..., 0.0097, 0.0297, 0.0000],
        ...,
        [0.0006, 0.0076, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0006, 0.0076, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0006, 0.0076, 0.0000,  ..., 0.0000, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(258858.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2331.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(416.1846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(355.2163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(609.9344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-434.5626, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0999],
        [ 0.1159],
        [ 0.1265],
        ...,
        [-0.1182],
        [-0.1168],
        [-0.1174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-39385.0547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0004],
        [0.9997],
        ...,
        [0.9992],
        [0.9991],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366045.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(573.9026, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366053.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.5908e-03,  4.6215e-03, -1.6048e-05,  ...,  9.6661e-03,
         -2.1055e-03,  3.0571e-03],
        [ 3.1909e-05,  3.5396e-04,  0.0000e+00,  ...,  7.2118e-04,
          0.0000e+00, -5.6256e-04],
        [ 3.1909e-05,  3.5396e-04,  0.0000e+00,  ...,  7.2118e-04,
          0.0000e+00, -5.6256e-04],
        ...,
        [ 3.1909e-05,  3.5396e-04,  0.0000e+00,  ...,  7.2118e-04,
          0.0000e+00, -5.6256e-04],
        [ 3.1909e-05,  3.5396e-04,  0.0000e+00,  ...,  7.2118e-04,
          0.0000e+00, -5.6256e-04],
        [ 3.1909e-05,  3.5396e-04,  0.0000e+00,  ...,  7.2118e-04,
          0.0000e+00, -5.6256e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(461.1009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5040, device='cuda:0')



h[100].sum tensor(26.0348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.4035, device='cuda:0')



h[200].sum tensor(21.6555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0216, 0.0179, 0.0000,  ..., 0.0374, 0.0000, 0.0128],
        [0.0057, 0.0057, 0.0000,  ..., 0.0118, 0.0000, 0.0031],
        [0.0001, 0.0014, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0014, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0001, 0.0014, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0001, 0.0014, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35384.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0565, 0.0386, 0.1729,  ..., 0.0195, 0.0439, 0.0000],
        [0.0289, 0.0238, 0.0844,  ..., 0.0089, 0.0228, 0.0000],
        [0.0094, 0.0132, 0.0262,  ..., 0.0025, 0.0081, 0.0000],
        ...,
        [0.0003, 0.0082, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0003, 0.0082, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0003, 0.0082, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202635.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1467.5935, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.9000, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(485.9803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(432.4801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.5841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0318],
        [ 0.0185],
        [-0.0108],
        ...,
        [-0.1334],
        [-0.1328],
        [-0.1326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-52049.9258, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366053.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366062.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1233e-05,  3.5230e-04,  0.0000e+00,  ...,  7.1864e-04,
          0.0000e+00, -5.7485e-04],
        [ 2.1233e-05,  3.5230e-04,  0.0000e+00,  ...,  7.1864e-04,
          0.0000e+00, -5.7485e-04],
        [ 2.1233e-05,  3.5230e-04,  0.0000e+00,  ...,  7.1864e-04,
          0.0000e+00, -5.7485e-04],
        ...,
        [ 2.1233e-05,  3.5230e-04,  0.0000e+00,  ...,  7.1864e-04,
          0.0000e+00, -5.7485e-04],
        [ 2.1233e-05,  3.5230e-04,  0.0000e+00,  ...,  7.1864e-04,
          0.0000e+00, -5.7485e-04],
        [ 2.1233e-05,  3.5230e-04,  0.0000e+00,  ...,  7.1864e-04,
          0.0000e+00, -5.7485e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(378.8217, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.0601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9669, device='cuda:0')



h[100].sum tensor(26.2427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.8080, device='cuda:0')



h[200].sum tensor(18.1649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.4990e-05, 1.4102e-03, 0.0000e+00,  ..., 2.8765e-03, 0.0000e+00,
         0.0000e+00],
        [8.5020e-05, 1.4107e-03, 0.0000e+00,  ..., 2.8775e-03, 0.0000e+00,
         0.0000e+00],
        [1.3465e-02, 1.1683e-02, 0.0000e+00,  ..., 2.4409e-02, 0.0000e+00,
         8.1405e-03],
        ...,
        [8.5073e-05, 1.4115e-03, 0.0000e+00,  ..., 2.8793e-03, 0.0000e+00,
         0.0000e+00],
        [8.5071e-05, 1.4115e-03, 0.0000e+00,  ..., 2.8792e-03, 0.0000e+00,
         0.0000e+00],
        [8.5070e-05, 1.4115e-03, 0.0000e+00,  ..., 2.8792e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31308.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0103, 0.0048,  ..., 0.0003, 0.0034, 0.0000],
        [0.0090, 0.0134, 0.0242,  ..., 0.0027, 0.0079, 0.0000],
        [0.0272, 0.0232, 0.0822,  ..., 0.0096, 0.0219, 0.0000],
        ...,
        [0.0003, 0.0086, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0003, 0.0086, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0003, 0.0086, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184902.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1133.1959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.4997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(678.5450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(373.1788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.4472, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0046],
        [-0.0078],
        [-0.0192],
        ...,
        [-0.1400],
        [-0.1394],
        [-0.1392]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62918.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366062.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366072.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2617e-05,  3.6223e-04,  0.0000e+00,  ...,  7.2855e-04,
          0.0000e+00, -5.9564e-04],
        [ 2.2617e-05,  3.6223e-04,  0.0000e+00,  ...,  7.2855e-04,
          0.0000e+00, -5.9564e-04],
        [ 2.2617e-05,  3.6223e-04,  0.0000e+00,  ...,  7.2855e-04,
          0.0000e+00, -5.9564e-04],
        ...,
        [ 2.2617e-05,  3.6223e-04,  0.0000e+00,  ...,  7.2855e-04,
          0.0000e+00, -5.9564e-04],
        [ 2.2617e-05,  3.6223e-04,  0.0000e+00,  ...,  7.2855e-04,
          0.0000e+00, -5.9564e-04],
        [ 2.2617e-05,  3.6223e-04,  0.0000e+00,  ...,  7.2855e-04,
          0.0000e+00, -5.9564e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(657.9122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0332, device='cuda:0')



h[100].sum tensor(30.1153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.9649, device='cuda:0')



h[200].sum tensor(27.1171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.0531e-05, 1.4499e-03, 0.0000e+00,  ..., 2.9162e-03, 0.0000e+00,
         0.0000e+00],
        [9.0564e-05, 1.4505e-03, 0.0000e+00,  ..., 2.9173e-03, 0.0000e+00,
         0.0000e+00],
        [9.0501e-05, 1.4494e-03, 0.0000e+00,  ..., 2.9153e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [9.0629e-05, 1.4515e-03, 0.0000e+00,  ..., 2.9194e-03, 0.0000e+00,
         0.0000e+00],
        [9.0627e-05, 1.4514e-03, 0.0000e+00,  ..., 2.9193e-03, 0.0000e+00,
         0.0000e+00],
        [9.0626e-05, 1.4514e-03, 0.0000e+00,  ..., 2.9193e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43433.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0164, 0.0385,  ..., 0.0041, 0.0119, 0.0000],
        [0.0041, 0.0109, 0.0087,  ..., 0.0007, 0.0043, 0.0000],
        [0.0012, 0.0094, 0.0007,  ..., 0.0000, 0.0022, 0.0000],
        ...,
        [0.0004, 0.0088, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0004, 0.0088, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0004, 0.0088, 0.0000,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(241816.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1983.8271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(359.9852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(593.8494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(538.9315, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-381.4055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0342],
        [-0.0387],
        [-0.0261],
        ...,
        [-0.1426],
        [-0.1419],
        [-0.1417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-49545.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366072.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366082.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8043e-03,  4.0407e-03, -5.4519e-06,  ...,  8.4404e-03,
         -1.7958e-03,  2.5003e-03],
        [ 6.1094e-03,  5.0432e-03, -6.9396e-06,  ...,  1.0542e-02,
         -2.2859e-03,  3.3508e-03],
        [ 2.1699e-05,  3.6695e-04,  0.0000e+00,  ...,  7.4039e-04,
          0.0000e+00, -6.1614e-04],
        ...,
        [ 7.8732e-03,  6.3981e-03, -8.9502e-06,  ...,  1.3381e-02,
         -2.9482e-03,  4.5001e-03],
        [ 2.1699e-05,  3.6695e-04,  0.0000e+00,  ...,  7.4039e-04,
          0.0000e+00, -6.1614e-04],
        [ 2.1699e-05,  3.6695e-04,  0.0000e+00,  ...,  7.4039e-04,
          0.0000e+00, -6.1614e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(578.7228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7046, device='cuda:0')



h[100].sum tensor(30.4033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.9928, device='cuda:0')



h[200].sum tensor(24.0967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3055, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0454, 0.0363, 0.0000,  ..., 0.0759, 0.0000, 0.0271],
        [0.0183, 0.0154, 0.0000,  ..., 0.0322, 0.0000, 0.0106],
        [0.0062, 0.0061, 0.0000,  ..., 0.0128, 0.0000, 0.0034],
        ...,
        [0.0570, 0.0452, 0.0000,  ..., 0.0946, 0.0000, 0.0352],
        [0.0505, 0.0402, 0.0000,  ..., 0.0841, 0.0000, 0.0310],
        [0.0363, 0.0293, 0.0000,  ..., 0.0612, 0.0000, 0.0223]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38280.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0757, 0.0508, 0.2286,  ..., 0.0266, 0.0587, 0.0000],
        [0.0508, 0.0370, 0.1494,  ..., 0.0170, 0.0397, 0.0000],
        [0.0262, 0.0235, 0.0722,  ..., 0.0078, 0.0210, 0.0000],
        ...,
        [0.1693, 0.0994, 0.5574,  ..., 0.0721, 0.1347, 0.0000],
        [0.1366, 0.0822, 0.4448,  ..., 0.0569, 0.1085, 0.0000],
        [0.0955, 0.0604, 0.3065,  ..., 0.0387, 0.0759, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217693.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1575.6394, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.4417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(756.4035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(467.5760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-323.4150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0528],
        [ 0.0451],
        [ 0.0286],
        ...,
        [-0.2789],
        [-0.1995],
        [-0.1253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-60826.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366082.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366093.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6045e-05,  3.7447e-04,  0.0000e+00,  ...,  7.5471e-04,
          0.0000e+00, -6.4158e-04],
        [ 9.3027e-03,  7.5023e-03, -5.9061e-06,  ...,  1.5692e-02,
         -3.4730e-03,  5.4035e-03],
        [ 9.3027e-03,  7.5023e-03, -5.9061e-06,  ...,  1.5692e-02,
         -3.4730e-03,  5.4035e-03],
        ...,
        [ 2.6045e-05,  3.7447e-04,  0.0000e+00,  ...,  7.5471e-04,
          0.0000e+00, -6.4158e-04],
        [ 2.6045e-05,  3.7447e-04,  0.0000e+00,  ...,  7.5471e-04,
          0.0000e+00, -6.4158e-04],
        [ 2.6045e-05,  3.7447e-04,  0.0000e+00,  ...,  7.5471e-04,
          0.0000e+00, -6.4158e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(487.5863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0793, device='cuda:0')



h[100].sum tensor(30.5794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1337, device='cuda:0')



h[200].sum tensor(20.8715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0145, 0.0000,  ..., 0.0302, 0.0000, 0.0097],
        [0.0245, 0.0202, 0.0000,  ..., 0.0423, 0.0000, 0.0140],
        [0.0356, 0.0287, 0.0000,  ..., 0.0601, 0.0000, 0.0212],
        ...,
        [0.0001, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0001, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0001, 0.0015, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35429.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0593, 0.0415, 0.1774,  ..., 0.0209, 0.0470, 0.0000],
        [0.0838, 0.0547, 0.2582,  ..., 0.0313, 0.0663, 0.0000],
        [0.1133, 0.0704, 0.3587,  ..., 0.0449, 0.0898, 0.0000],
        ...,
        [0.0009, 0.0094, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0009, 0.0094, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0009, 0.0094, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208047.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1411.3351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.9025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(967.0980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(427.4236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.7902, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1826],
        [-0.2512],
        [-0.3182],
        ...,
        [-0.1435],
        [-0.1428],
        [-0.1426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66838.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366093.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366104.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9078e-05,  3.8027e-04,  0.0000e+00,  ...,  7.6515e-04,
          0.0000e+00, -6.6776e-04],
        [ 2.9078e-05,  3.8027e-04,  0.0000e+00,  ...,  7.6515e-04,
          0.0000e+00, -6.6776e-04],
        [ 5.8224e-03,  4.8330e-03, -1.0087e-06,  ...,  1.0095e-02,
         -2.1624e-03,  3.1072e-03],
        ...,
        [ 2.9078e-05,  3.8027e-04,  0.0000e+00,  ...,  7.6515e-04,
          0.0000e+00, -6.6776e-04],
        [ 2.9078e-05,  3.8027e-04,  0.0000e+00,  ...,  7.6515e-04,
          0.0000e+00, -6.6776e-04],
        [ 2.9078e-05,  3.8027e-04,  0.0000e+00,  ...,  7.6515e-04,
          0.0000e+00, -6.6776e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(524.2696, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5844, device='cuda:0')



h[100].sum tensor(31.8961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6439, device='cuda:0')



h[200].sum tensor(22.0128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0015, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0059, 0.0060, 0.0000,  ..., 0.0124, 0.0000, 0.0031],
        [0.0134, 0.0117, 0.0000,  ..., 0.0244, 0.0000, 0.0073],
        ...,
        [0.0001, 0.0015, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0001, 0.0015, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0001, 0.0015, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38232.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0140, 0.0154,  ..., 0.0012, 0.0082, 0.0000],
        [0.0250, 0.0229, 0.0622,  ..., 0.0062, 0.0204, 0.0000],
        [0.0464, 0.0348, 0.1280,  ..., 0.0139, 0.0368, 0.0000],
        ...,
        [0.0012, 0.0097, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0012, 0.0097, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0012, 0.0097, 0.0000,  ..., 0.0000, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225228.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1772.5920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.6533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(891.3890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(468.0019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-321.7512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0395],
        [ 0.0436],
        [ 0.0380],
        ...,
        [-0.1410],
        [-0.1403],
        [-0.1401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-51913.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366104.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366115.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6213e-02,  1.2826e-02,  4.0051e-06,  ...,  2.6835e-02,
         -6.0234e-03,  9.8530e-03],
        [ 1.6786e-02,  1.3266e-02,  4.1468e-06,  ...,  2.7758e-02,
         -6.2366e-03,  1.0226e-02],
        [ 1.4176e-02,  1.1260e-02,  3.5011e-06,  ...,  2.3555e-02,
         -5.2654e-03,  8.5258e-03],
        ...,
        [ 2.7199e-05,  3.8116e-04,  0.0000e+00,  ...,  7.6523e-04,
          0.0000e+00, -6.9335e-04],
        [ 2.7199e-05,  3.8116e-04,  0.0000e+00,  ...,  7.6523e-04,
          0.0000e+00, -6.9335e-04],
        [ 2.7199e-05,  3.8116e-04,  0.0000e+00,  ...,  7.6523e-04,
          0.0000e+00, -6.9335e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(440.0705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0172, device='cuda:0')



h[100].sum tensor(31.8206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.9584, device='cuda:0')



h[200].sum tensor(19.2215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.1104e-02, 4.8421e-02, 1.5093e-05,  ..., 1.0131e-01, 0.0000e+00,
         3.6968e-02],
        [6.5782e-02, 5.2018e-02, 1.6250e-05,  ..., 1.0884e-01, 0.0000e+00,
         4.0014e-02],
        [8.1081e-02, 6.3780e-02, 2.0036e-05,  ..., 1.3348e-01, 0.0000e+00,
         4.9985e-02],
        ...,
        [1.0904e-04, 1.5281e-03, 0.0000e+00,  ..., 3.0678e-03, 0.0000e+00,
         0.0000e+00],
        [1.0903e-04, 1.5280e-03, 0.0000e+00,  ..., 3.0677e-03, 0.0000e+00,
         0.0000e+00],
        [1.0903e-04, 1.5280e-03, 0.0000e+00,  ..., 3.0676e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33275.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1183, 0.0736, 0.3652,  ..., 0.0452, 0.0939, 0.0000],
        [0.1287, 0.0793, 0.3993,  ..., 0.0496, 0.1020, 0.0000],
        [0.1396, 0.0851, 0.4358,  ..., 0.0545, 0.1107, 0.0000],
        ...,
        [0.0015, 0.0101, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0015, 0.0101, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0015, 0.0101, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202616.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1400.4177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(255.9621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1144.4637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(398.9008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-263.3185, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0312],
        [ 0.0276],
        [ 0.0304],
        ...,
        [-0.1454],
        [-0.1446],
        [-0.1444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62048.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0005],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366115.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366127.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5589e-05,  3.7284e-04, -5.4813e-05,  ...,  7.6797e-04,
          0.0000e+00, -7.0878e-04],
        [ 6.2427e-03,  5.1619e-03, -5.7163e-05,  ...,  1.0799e-02,
         -2.3104e-03,  3.3485e-03],
        [ 1.5589e-05,  3.7284e-04, -5.4813e-05,  ...,  7.6797e-04,
          0.0000e+00, -7.0878e-04],
        ...,
        [ 1.5589e-05,  3.7284e-04, -5.4813e-05,  ...,  7.6797e-04,
          0.0000e+00, -7.0878e-04],
        [ 1.5589e-05,  3.7284e-04, -5.4813e-05,  ...,  7.6797e-04,
          0.0000e+00, -7.0878e-04],
        [ 1.5589e-05,  3.7284e-04, -5.4813e-05,  ...,  7.6797e-04,
          0.0000e+00, -7.0878e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(538.7480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.6921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4036, device='cuda:0')



h[100].sum tensor(33.3987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1033, device='cuda:0')



h[200].sum tensor(22.4962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1604, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.7098e-02, 2.2284e-02, 0.0000e+00,  ..., 4.6625e-02, 0.0000e+00,
         1.4779e-02],
        [5.1518e-03, 5.4068e-03, 0.0000e+00,  ..., 1.1273e-02, 0.0000e+00,
         2.6065e-03],
        [6.2922e-03, 6.2829e-03, 0.0000e+00,  ..., 1.3108e-02, 0.0000e+00,
         3.3500e-03],
        ...,
        [6.2502e-05, 1.4949e-03, 0.0000e+00,  ..., 3.0791e-03, 0.0000e+00,
         0.0000e+00],
        [6.2500e-05, 1.4948e-03, 0.0000e+00,  ..., 3.0790e-03, 0.0000e+00,
         0.0000e+00],
        [6.2499e-05, 1.4948e-03, 0.0000e+00,  ..., 3.0790e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37269.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0517, 0.0386, 0.1377,  ..., 0.0147, 0.0406, 0.0000],
        [0.0320, 0.0276, 0.0797,  ..., 0.0083, 0.0256, 0.0000],
        [0.0197, 0.0208, 0.0433,  ..., 0.0043, 0.0163, 0.0000],
        ...,
        [0.0017, 0.0107, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0017, 0.0107, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0017, 0.0107, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221510.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1697.0911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.2574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1186.2557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(457.0497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-304.5831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0878],
        [ 0.0876],
        [ 0.0855],
        ...,
        [-0.1520],
        [-0.1516],
        [-0.1513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61084.6484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366127.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366138.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2775e-06,  3.6203e-04, -1.0475e-04,  ...,  7.6375e-04,
          0.0000e+00, -7.2265e-04],
        [ 1.2775e-06,  3.6203e-04, -1.0475e-04,  ...,  7.6375e-04,
          0.0000e+00, -7.2265e-04],
        [ 1.2775e-06,  3.6203e-04, -1.0475e-04,  ...,  7.6375e-04,
          0.0000e+00, -7.2265e-04],
        ...,
        [ 1.2775e-06,  3.6203e-04, -1.0475e-04,  ...,  7.6375e-04,
          0.0000e+00, -7.2265e-04],
        [ 1.2775e-06,  3.6203e-04, -1.0475e-04,  ...,  7.6375e-04,
          0.0000e+00, -7.2265e-04],
        [ 1.2775e-06,  3.6203e-04, -1.0475e-04,  ...,  7.6375e-04,
          0.0000e+00, -7.2265e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(673.5142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.9455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3363, device='cuda:0')



h[100].sum tensor(35.0994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.8815, device='cuda:0')



h[200].sum tensor(26.9539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.1126e-06, 1.4489e-03, 0.0000e+00,  ..., 3.0566e-03, 0.0000e+00,
         0.0000e+00],
        [5.1147e-06, 1.4495e-03, 0.0000e+00,  ..., 3.0579e-03, 0.0000e+00,
         0.0000e+00],
        [1.0888e-02, 9.8201e-03, 0.0000e+00,  ..., 2.0588e-02, 0.0000e+00,
         5.6448e-03],
        ...,
        [5.1225e-06, 1.4517e-03, 0.0000e+00,  ..., 3.0626e-03, 0.0000e+00,
         0.0000e+00],
        [5.1223e-06, 1.4517e-03, 0.0000e+00,  ..., 3.0625e-03, 0.0000e+00,
         0.0000e+00],
        [7.8305e-03, 7.4714e-03, 0.0000e+00,  ..., 1.5669e-02, 0.0000e+00,
         4.3742e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42144.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5676e-03, 1.2886e-02, 3.4777e-03,  ..., 3.2367e-04, 4.3360e-03,
         0.0000e+00],
        [1.6613e-02, 1.9564e-02, 3.7413e-02,  ..., 4.3363e-03, 1.3633e-02,
         0.0000e+00],
        [5.1937e-02, 3.8967e-02, 1.4540e-01,  ..., 1.7485e-02, 4.1249e-02,
         0.0000e+00],
        ...,
        [2.8358e-03, 1.1883e-02, 1.0242e-03,  ..., 7.7756e-05, 3.0539e-03,
         0.0000e+00],
        [1.2385e-02, 1.7170e-02, 2.8002e-02,  ..., 3.3672e-03, 1.0453e-02,
         0.0000e+00],
        [3.2976e-02, 2.8610e-02, 8.5834e-02,  ..., 1.0089e-02, 2.6364e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(243537.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2058.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.6557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1179.9602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(526.4106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.2852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0510],
        [-0.0917],
        [-0.1702],
        ...,
        [-0.0762],
        [-0.0240],
        [ 0.0139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55253.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366138.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366150.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3345e-05,  3.4395e-04, -1.5026e-04,  ...,  7.5531e-04,
          0.0000e+00, -7.3115e-04],
        [-2.3345e-05,  3.4395e-04, -1.5026e-04,  ...,  7.5531e-04,
          0.0000e+00, -7.3115e-04],
        [-2.3345e-05,  3.4395e-04, -1.5026e-04,  ...,  7.5531e-04,
          0.0000e+00, -7.3115e-04],
        ...,
        [-2.3345e-05,  3.4395e-04, -1.5026e-04,  ...,  7.5531e-04,
          0.0000e+00, -7.3115e-04],
        [-2.3345e-05,  3.4395e-04, -1.5026e-04,  ...,  7.5531e-04,
          0.0000e+00, -7.3115e-04],
        [-2.3345e-05,  3.4395e-04, -1.5026e-04,  ...,  7.5531e-04,
          0.0000e+00, -7.3115e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(420.9760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.5237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3959, device='cuda:0')



h[100].sum tensor(32.7206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.1008, device='cuda:0')



h[200].sum tensor(18.4132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32150.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6613e-03, 1.2703e-02, 3.3656e-04,  ..., 6.8458e-05, 2.4637e-03,
         0.0000e+00],
        [1.8345e-03, 1.2235e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8325e-03,
         0.0000e+00],
        [1.8300e-03, 1.2260e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8336e-03,
         0.0000e+00],
        ...,
        [1.8423e-03, 1.2206e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8323e-03,
         0.0000e+00],
        [1.8423e-03, 1.2206e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8322e-03,
         0.0000e+00],
        [1.8423e-03, 1.2205e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8322e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203983.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1338.2406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.7206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1519.3059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(389.0458, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.8710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0754],
        [-0.1444],
        [-0.2041],
        ...,
        [-0.1742],
        [-0.1733],
        [-0.1730]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78706.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0004],
        [0.9997],
        ...,
        [0.9993],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366150.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(544.3008, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9997],
        ...,
        [0.9994],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366161.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.4171e-05,  3.2572e-04, -1.9171e-04,  ...,  7.4792e-04,
          0.0000e+00, -7.3996e-04],
        [-4.4171e-05,  3.2572e-04, -1.9171e-04,  ...,  7.4792e-04,
          0.0000e+00, -7.3996e-04],
        [-4.4171e-05,  3.2572e-04, -1.9171e-04,  ...,  7.4792e-04,
          0.0000e+00, -7.3996e-04],
        ...,
        [-4.4171e-05,  3.2572e-04, -1.9171e-04,  ...,  7.4792e-04,
          0.0000e+00, -7.3996e-04],
        [-4.4171e-05,  3.2572e-04, -1.9171e-04,  ...,  7.4792e-04,
          0.0000e+00, -7.3996e-04],
        [-4.4171e-05,  3.2572e-04, -1.9171e-04,  ...,  7.4792e-04,
          0.0000e+00, -7.3996e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(566.5015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5594, device='cuda:0')



h[100].sum tensor(34.1585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.5692, device='cuda:0')



h[200].sum tensor(22.8375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37949.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8522e-03, 1.3529e-02, 0.0000e+00,  ..., 6.9781e-05, 2.2484e-03,
         0.0000e+00],
        [2.2005e-03, 1.3197e-02, 0.0000e+00,  ..., 9.5681e-05, 1.6807e-03,
         0.0000e+00],
        [1.8823e-03, 1.3061e-02, 0.0000e+00,  ..., 1.1798e-04, 1.4072e-03,
         0.0000e+00],
        ...,
        [1.8946e-03, 1.3007e-02, 0.0000e+00,  ..., 9.1875e-05, 1.4087e-03,
         0.0000e+00],
        [1.8945e-03, 1.3006e-02, 0.0000e+00,  ..., 9.1840e-05, 1.4087e-03,
         0.0000e+00],
        [1.8945e-03, 1.3006e-02, 0.0000e+00,  ..., 9.1792e-05, 1.4086e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230776.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1777.5792, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(286.6472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1471.6995, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(473.4937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.6456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0223],
        [-0.0613],
        [-0.1054],
        ...,
        [-0.1846],
        [-0.1835],
        [-0.1830]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74907.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9997],
        ...,
        [0.9994],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366161.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9997],
        ...,
        [0.9994],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366173.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9130e-05,  3.1282e-04, -2.2949e-04,  ...,  7.4243e-04,
          0.0000e+00, -7.5157e-04],
        [-5.9130e-05,  3.1282e-04, -2.2949e-04,  ...,  7.4243e-04,
          0.0000e+00, -7.5157e-04],
        [ 6.7467e-03,  5.5527e-03, -2.4560e-04,  ...,  1.1711e-02,
         -2.4947e-03,  3.6824e-03],
        ...,
        [-5.9130e-05,  3.1282e-04, -2.2949e-04,  ...,  7.4243e-04,
          0.0000e+00, -7.5157e-04],
        [-5.9130e-05,  3.1282e-04, -2.2949e-04,  ...,  7.4243e-04,
          0.0000e+00, -7.5157e-04],
        [-5.9130e-05,  3.1282e-04, -2.2949e-04,  ...,  7.4243e-04,
          0.0000e+00, -7.5157e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(672.4445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7418, device='cuda:0')



h[100].sum tensor(35.1819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.1039, device='cuda:0')



h[200].sum tensor(26.0079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3096, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0119, 0.0105, 0.0000,  ..., 0.0224, 0.0000, 0.0063],
        [0.0300, 0.0244, 0.0000,  ..., 0.0515, 0.0000, 0.0174],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42703.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9664e-02, 2.3646e-02, 4.1822e-02,  ..., 5.1757e-03, 1.4984e-02,
         0.0000e+00],
        [4.7166e-02, 3.9278e-02, 1.2287e-01,  ..., 1.4932e-02, 3.6372e-02,
         0.0000e+00],
        [8.4963e-02, 6.0419e-02, 2.4336e-01,  ..., 3.0272e-02, 6.6136e-02,
         0.0000e+00],
        ...,
        [1.9986e-03, 1.3569e-02, 0.0000e+00,  ..., 1.9031e-04, 1.2003e-03,
         0.0000e+00],
        [1.9985e-03, 1.3568e-02, 0.0000e+00,  ..., 1.9027e-04, 1.2002e-03,
         0.0000e+00],
        [1.9985e-03, 1.3568e-02, 0.0000e+00,  ..., 1.9021e-04, 1.2002e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(256409.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2065.1814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.8575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1593.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(539.1138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.6543, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0363],
        [ 0.0502],
        [ 0.0461],
        ...,
        [-0.1763],
        [-0.1886],
        [-0.1922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80923.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9997],
        ...,
        [0.9994],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366173.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9997],
        ...,
        [0.9994],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366185.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6842e-05,  3.1371e-04, -2.6390e-04,  ...,  7.4378e-04,
          0.0000e+00, -7.6686e-04],
        [-6.6842e-05,  3.1371e-04, -2.6390e-04,  ...,  7.4378e-04,
          0.0000e+00, -7.6686e-04],
        [-6.6842e-05,  3.1371e-04, -2.6390e-04,  ...,  7.4378e-04,
          0.0000e+00, -7.6686e-04],
        ...,
        [-6.6842e-05,  3.1371e-04, -2.6390e-04,  ...,  7.4378e-04,
          0.0000e+00, -7.6686e-04],
        [-6.6842e-05,  3.1371e-04, -2.6390e-04,  ...,  7.4378e-04,
          0.0000e+00, -7.6686e-04],
        [-6.6842e-05,  3.1371e-04, -2.6390e-04,  ...,  7.4378e-04,
          0.0000e+00, -7.6686e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(444.1714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.0584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3518, device='cuda:0')



h[100].sum tensor(32.8491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.9690, device='cuda:0')



h[200].sum tensor(18.4922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33028.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0140, 0.0000,  ..., 0.0002, 0.0013, 0.0000],
        [0.0023, 0.0140, 0.0000,  ..., 0.0002, 0.0013, 0.0000],
        [0.0023, 0.0140, 0.0000,  ..., 0.0002, 0.0013, 0.0000],
        ...,
        [0.0023, 0.0139, 0.0000,  ..., 0.0002, 0.0013, 0.0000],
        [0.0023, 0.0139, 0.0000,  ..., 0.0002, 0.0013, 0.0000],
        [0.0023, 0.0139, 0.0000,  ..., 0.0002, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214445.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1541.5728, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.9681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1676.7881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(407.3352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.5093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2843],
        [-0.3005],
        [-0.3136],
        ...,
        [-0.1990],
        [-0.1980],
        [-0.1976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82217.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9997],
        ...,
        [0.9994],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366185.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366196.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5072e-02,  1.9691e-02, -3.7363e-04,  ...,  4.1285e-02,
         -9.1594e-03,  1.5596e-02],
        [ 2.6762e-02,  2.0993e-02, -3.7890e-04,  ...,  4.4010e-02,
         -9.7751e-03,  1.6696e-02],
        [ 2.1361e-02,  1.6831e-02, -3.6206e-04,  ...,  3.5303e-02,
         -7.8074e-03,  1.3178e-02],
        ...,
        [-7.0831e-05,  3.2026e-04, -2.9524e-04,  ...,  7.5664e-04,
          0.0000e+00, -7.8017e-04],
        [-7.0831e-05,  3.2026e-04, -2.9524e-04,  ...,  7.5664e-04,
          0.0000e+00, -7.8017e-04],
        [-7.0831e-05,  3.2026e-04, -2.9524e-04,  ...,  7.5664e-04,
          0.0000e+00, -7.8017e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(906.0347, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.8589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8387, device='cuda:0')



h[100].sum tensor(36.9950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(44.3627, device='cuda:0')



h[200].sum tensor(33.4566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1045, 0.0820, 0.0000,  ..., 0.1720, 0.0000, 0.0651],
        [0.1077, 0.0845, 0.0000,  ..., 0.1771, 0.0000, 0.0672],
        [0.1180, 0.0924, 0.0000,  ..., 0.1937, 0.0000, 0.0739],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52143.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1641e-01, 1.3319e-01, 6.8109e-01,  ..., 8.9296e-02, 1.7260e-01,
         0.0000e+00],
        [2.3485e-01, 1.4317e-01, 7.4426e-01,  ..., 9.8059e-02, 1.8761e-01,
         0.0000e+00],
        [2.4609e-01, 1.4919e-01, 7.8363e-01,  ..., 1.0364e-01, 1.9686e-01,
         0.0000e+00],
        ...,
        [2.6719e-03, 1.4311e-02, 0.0000e+00,  ..., 9.3911e-05, 1.7043e-03,
         0.0000e+00],
        [2.6718e-03, 1.4311e-02, 0.0000e+00,  ..., 9.3872e-05, 1.7042e-03,
         0.0000e+00],
        [2.6718e-03, 1.4310e-02, 0.0000e+00,  ..., 9.3820e-05, 1.7042e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(305894.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2811.7476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(406.2981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1613.0863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(669.4581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.1045, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1270],
        [-0.1591],
        [-0.1659],
        ...,
        [-0.2002],
        [-0.1993],
        [-0.1990]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82133.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366196.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9992],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366207.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5675e-03,  5.4385e-03, -3.4666e-04,  ...,  1.1475e-02,
         -2.4118e-03,  3.5310e-03],
        [-7.3830e-05,  3.1968e-04, -3.2379e-04,  ...,  7.6815e-04,
          0.0000e+00, -7.9315e-04],
        [-7.3830e-05,  3.1968e-04, -3.2379e-04,  ...,  7.6815e-04,
          0.0000e+00, -7.9315e-04],
        ...,
        [-7.3830e-05,  3.1968e-04, -3.2379e-04,  ...,  7.6815e-04,
          0.0000e+00, -7.9315e-04],
        [-7.3830e-05,  3.1968e-04, -3.2379e-04,  ...,  7.6815e-04,
          0.0000e+00, -7.9315e-04],
        [-7.3830e-05,  3.1968e-04, -3.2379e-04,  ...,  7.6815e-04,
          0.0000e+00, -7.9315e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(480.8860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.1618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6047, device='cuda:0')



h[100].sum tensor(32.6288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.7251, device='cuda:0')



h[200].sum tensor(19.5773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0055, 0.0000,  ..., 0.0118, 0.0000, 0.0027],
        [0.0066, 0.0064, 0.0000,  ..., 0.0138, 0.0000, 0.0035],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34096.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8350e-02, 2.4138e-02, 3.1433e-02,  ..., 3.7304e-03, 1.3928e-02,
         0.0000e+00],
        [1.4853e-02, 2.2066e-02, 2.2356e-02,  ..., 2.9357e-03, 1.1216e-02,
         0.0000e+00],
        [6.6116e-03, 1.7195e-02, 5.0223e-03,  ..., 9.2174e-04, 4.7869e-03,
         0.0000e+00],
        ...,
        [2.8549e-03, 1.4907e-02, 0.0000e+00,  ..., 5.7259e-05, 1.8504e-03,
         0.0000e+00],
        [2.8548e-03, 1.4907e-02, 0.0000e+00,  ..., 5.7221e-05, 1.8503e-03,
         0.0000e+00],
        [2.8547e-03, 1.4906e-02, 0.0000e+00,  ..., 5.7171e-05, 1.8503e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221983.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1751.0121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.0663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1580.7762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(424.9445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.0527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1465],
        [-0.1547],
        [-0.1513],
        ...,
        [-0.2042],
        [-0.2033],
        [-0.2030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80064.9609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9992],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366207.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366218.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2078e-02,  9.6970e-03, -3.9522e-04,  ...,  2.0365e-02,
         -4.3983e-03,  7.1014e-03],
        [ 1.3819e-02,  1.1039e-02, -4.0173e-04,  ...,  2.3171e-02,
         -5.0284e-03,  8.2344e-03],
        [ 3.8828e-02,  3.0323e-02, -4.9522e-04,  ...,  6.3494e-02,
         -1.4082e-02,  2.4512e-02],
        ...,
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04],
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04],
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(796.3691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.8967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8309, device='cuda:0')



h[100].sum tensor(35.0629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.3599, device='cuda:0')



h[200].sum tensor(29.6333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0946, 0.0745, 0.0000,  ..., 0.1561, 0.0000, 0.0585],
        [0.1073, 0.0843, 0.0000,  ..., 0.1766, 0.0000, 0.0668],
        [0.0651, 0.0518, 0.0000,  ..., 0.1086, 0.0000, 0.0394],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47588.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2015, 0.1270, 0.6186,  ..., 0.0789, 0.1611, 0.0000],
        [0.2152, 0.1344, 0.6648,  ..., 0.0852, 0.1723, 0.0000],
        [0.1801, 0.1153, 0.5476,  ..., 0.0694, 0.1438, 0.0000],
        ...,
        [0.0031, 0.0154, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0031, 0.0154, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0031, 0.0154, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(285071.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2647.6553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(361.7278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1477.9971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(608.7777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.5211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0479],
        [-0.0559],
        [-0.0414],
        ...,
        [-0.2044],
        [-0.2034],
        [-0.2031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76341.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366218.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366218.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04],
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04],
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04],
        ...,
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04],
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04],
        [-7.1734e-05,  3.2847e-04, -3.4980e-04,  ...,  7.7495e-04,
          0.0000e+00, -8.0682e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(752.9579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4895, device='cuda:0')



h[100].sum tensor(34.6548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.3394, device='cuda:0')



h[200].sum tensor(28.2400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0164, 0.0140, 0.0000,  ..., 0.0297, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44855.1055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0183, 0.0086,  ..., 0.0012, 0.0061, 0.0000],
        [0.0136, 0.0216, 0.0224,  ..., 0.0025, 0.0105, 0.0000],
        [0.0355, 0.0345, 0.0794,  ..., 0.0085, 0.0278, 0.0000],
        ...,
        [0.0031, 0.0154, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0031, 0.0154, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0031, 0.0154, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(271098.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2540.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(337.5343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1421.0037, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(573.3666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.9944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0204],
        [ 0.0548],
        [ 0.0991],
        ...,
        [-0.2044],
        [-0.2034],
        [-0.2031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69401.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0003],
        [0.9996],
        ...,
        [0.9995],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366218.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0003],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366228.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7636e-03,  5.6008e-03, -4.0088e-04,  ...,  1.1801e-02,
         -2.4667e-03,  3.6369e-03],
        [ 4.3261e-03,  3.7206e-03, -3.9111e-04,  ...,  7.8701e-03,
         -1.5871e-03,  2.0507e-03],
        [ 1.1162e-02,  8.9933e-03, -4.1850e-04,  ...,  1.8893e-02,
         -4.0539e-03,  6.4990e-03],
        ...,
        [-7.1871e-05,  3.2812e-04, -3.7348e-04,  ...,  7.7805e-04,
          0.0000e+00, -8.1141e-04],
        [-7.1871e-05,  3.2812e-04, -3.7348e-04,  ...,  7.7805e-04,
          0.0000e+00, -8.1141e-04],
        [-7.1871e-05,  3.2812e-04, -3.7348e-04,  ...,  7.7805e-04,
          0.0000e+00, -8.1141e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(507.2079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.1915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5619, device='cuda:0')



h[100].sum tensor(31.7168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5973, device='cuda:0')



h[200].sum tensor(19.8708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9550, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0118, 0.0000,  ..., 0.0250, 0.0000, 0.0072],
        [0.0370, 0.0301, 0.0000,  ..., 0.0633, 0.0000, 0.0210],
        [0.0223, 0.0187, 0.0000,  ..., 0.0394, 0.0000, 0.0122],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35043.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0454, 0.0413, 0.1088,  ..., 0.0116, 0.0357, 0.0000],
        [0.0656, 0.0532, 0.1678,  ..., 0.0181, 0.0517, 0.0000],
        [0.0597, 0.0498, 0.1499,  ..., 0.0160, 0.0472, 0.0000],
        ...,
        [0.0032, 0.0163, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0032, 0.0163, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0032, 0.0163, 0.0000,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(229472.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1800.6006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.9249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1618.3820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(437.1709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.2197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1109],
        [ 0.1190],
        [ 0.1159],
        ...,
        [-0.1855],
        [-0.1708],
        [-0.1597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89222.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0003],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366228.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366239.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1086e-05,  3.3030e-04, -3.9504e-04,  ...,  7.7623e-04,
          0.0000e+00, -8.1254e-04],
        [-7.1086e-05,  3.3030e-04, -3.9504e-04,  ...,  7.7623e-04,
          0.0000e+00, -8.1254e-04],
        [-7.1086e-05,  3.3030e-04, -3.9504e-04,  ...,  7.7623e-04,
          0.0000e+00, -8.1254e-04],
        ...,
        [-7.1086e-05,  3.3030e-04, -3.9504e-04,  ...,  7.7623e-04,
          0.0000e+00, -8.1254e-04],
        [-7.1086e-05,  3.3030e-04, -3.9504e-04,  ...,  7.7623e-04,
          0.0000e+00, -8.1254e-04],
        [-7.1086e-05,  3.3030e-04, -3.9504e-04,  ...,  7.7623e-04,
          0.0000e+00, -8.1254e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1091.1091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.4250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7256, device='cuda:0')



h[100].sum tensor(36.5174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(50.0040, device='cuda:0')



h[200].sum tensor(37.9213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0035, 0.0041, 0.0000,  ..., 0.0089, 0.0000, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54209.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.9752e-03, 1.7248e-02, 0.0000e+00,  ..., 0.0000e+00, 2.5503e-03,
         0.0000e+00],
        [2.9764e-03, 1.7255e-02, 0.0000e+00,  ..., 0.0000e+00, 2.5513e-03,
         0.0000e+00],
        [4.2203e-03, 1.8026e-02, 1.2602e-03,  ..., 1.6959e-04, 3.5406e-03,
         0.0000e+00],
        ...,
        [5.3716e-03, 1.8684e-02, 8.3328e-04,  ..., 7.5400e-05, 4.4642e-03,
         0.0000e+00],
        [8.8662e-03, 2.0789e-02, 6.4141e-03,  ..., 5.2545e-04, 7.2629e-03,
         0.0000e+00],
        [1.4577e-02, 2.4228e-02, 1.7312e-02,  ..., 1.2092e-03, 1.1836e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(304605., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2816.3367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.4207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1421.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(703.1712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-471.5656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2515],
        [-0.1913],
        [-0.1027],
        ...,
        [-0.1421],
        [-0.0887],
        [-0.0407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80358.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366239.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366250.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.2009e-05,  3.2117e-04, -4.1467e-04,  ...,  7.8007e-04,
          0.0000e+00, -8.0919e-04],
        [-7.2009e-05,  3.2117e-04, -4.1467e-04,  ...,  7.8007e-04,
          0.0000e+00, -8.0919e-04],
        [-7.2009e-05,  3.2117e-04, -4.1467e-04,  ...,  7.8007e-04,
          0.0000e+00, -8.0919e-04],
        ...,
        [-7.2009e-05,  3.2117e-04, -4.1467e-04,  ...,  7.8007e-04,
          0.0000e+00, -8.0919e-04],
        [-7.2009e-05,  3.2117e-04, -4.1467e-04,  ...,  7.8007e-04,
          0.0000e+00, -8.0919e-04],
        [-7.2009e-05,  3.2117e-04, -4.1467e-04,  ...,  7.8007e-04,
          0.0000e+00, -8.0919e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(721.1715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.5059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4740, device='cuda:0')



h[100].sum tensor(32.5258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.3034, device='cuda:0')



h[200].sum tensor(25.3913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2798, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0116, 0.0000,  ..., 0.0246, 0.0000, 0.0079],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41543.9492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0383, 0.0391, 0.0955,  ..., 0.0115, 0.0307, 0.0000],
        [0.0218, 0.0294, 0.0462,  ..., 0.0058, 0.0176, 0.0000],
        [0.0138, 0.0249, 0.0211,  ..., 0.0028, 0.0112, 0.0000],
        ...,
        [0.0028, 0.0183, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0028, 0.0183, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0028, 0.0183, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(257994.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2125.7183, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(304.6586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1468.2849, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(535.1677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-332.9204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0833],
        [ 0.0792],
        [ 0.0793],
        ...,
        [-0.2214],
        [-0.2204],
        [-0.2201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86870.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0003],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366250.2812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(516.2386, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366260.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5454e-03,  3.8782e-03, -4.5416e-04,  ...,  8.2394e-03,
         -1.6516e-03,  2.2048e-03],
        [ 4.5454e-03,  3.8782e-03, -4.5416e-04,  ...,  8.2394e-03,
         -1.6516e-03,  2.2048e-03],
        [-7.4642e-05,  3.1089e-04, -4.3254e-04,  ...,  7.8598e-04,
          0.0000e+00, -8.0049e-04],
        ...,
        [-7.4642e-05,  3.1089e-04, -4.3254e-04,  ...,  7.8598e-04,
          0.0000e+00, -8.0049e-04],
        [-7.4642e-05,  3.1089e-04, -4.3254e-04,  ...,  7.8598e-04,
          0.0000e+00, -8.0049e-04],
        [-7.4642e-05,  3.1089e-04, -4.3254e-04,  ...,  7.8598e-04,
          0.0000e+00, -8.0049e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(659.6013, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2401, device='cuda:0')



h[100].sum tensor(31.5038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.6144, device='cuda:0')



h[200].sum tensor(22.6387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0077, 0.0000,  ..., 0.0167, 0.0000, 0.0039],
        [0.0083, 0.0077, 0.0000,  ..., 0.0167, 0.0000, 0.0039],
        [0.0082, 0.0077, 0.0000,  ..., 0.0167, 0.0000, 0.0039],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38845.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0365, 0.0579,  ..., 0.0045, 0.0245, 0.0000],
        [0.0240, 0.0326, 0.0413,  ..., 0.0031, 0.0194, 0.0000],
        [0.0184, 0.0292, 0.0279,  ..., 0.0021, 0.0150, 0.0000],
        ...,
        [0.0025, 0.0194, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0025, 0.0194, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0025, 0.0194, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(246618.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1907.4539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(278.0267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1401.6918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(503.4300, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-302.6839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0831],
        [ 0.0263],
        [-0.0588],
        ...,
        [-0.2297],
        [-0.2287],
        [-0.2284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86993.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9996],
        ...,
        [0.9996],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366260.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9996],
        ...,
        [0.9996],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366270.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.2674e-05,  3.0752e-04, -4.4881e-04,  ...,  7.9099e-04,
          0.0000e+00, -7.9489e-04],
        [ 2.0837e-02,  1.6458e-02, -5.5053e-04,  ...,  3.4530e-02,
         -7.4515e-03,  1.2805e-02],
        [-7.2674e-05,  3.0752e-04, -4.4881e-04,  ...,  7.9099e-04,
          0.0000e+00, -7.9489e-04],
        ...,
        [-7.2674e-05,  3.0752e-04, -4.4881e-04,  ...,  7.9099e-04,
          0.0000e+00, -7.9489e-04],
        [-7.2674e-05,  3.0752e-04, -4.4881e-04,  ...,  7.9099e-04,
          0.0000e+00, -7.9489e-04],
        [-7.2674e-05,  3.0752e-04, -4.4881e-04,  ...,  7.9099e-04,
          0.0000e+00, -7.9489e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(948.0538, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.1151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9767, device='cuda:0')



h[100].sum tensor(33.9615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.7857, device='cuda:0')



h[200].sum tensor(31.0997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5589, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0327, 0.0266, 0.0000,  ..., 0.0561, 0.0000, 0.0198],
        [0.0408, 0.0328, 0.0000,  ..., 0.0692, 0.0000, 0.0250],
        [0.0876, 0.0691, 0.0000,  ..., 0.1450, 0.0000, 0.0540],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49743.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0582, 0.1867,  ..., 0.0229, 0.0540, 0.0000],
        [0.0888, 0.0706, 0.2533,  ..., 0.0311, 0.0710, 0.0000],
        [0.1078, 0.0816, 0.3143,  ..., 0.0391, 0.0863, 0.0000],
        ...,
        [0.0025, 0.0201, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0025, 0.0201, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0025, 0.0201, 0.0000,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(296720.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2576.5376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(373.1304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1374.0632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(658.0895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-420.5980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0043],
        [ 0.0057],
        [-0.0014],
        ...,
        [-0.2348],
        [-0.2338],
        [-0.2334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86480.3203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9996],
        ...,
        [0.9996],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366270.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366281.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4060e-02,  2.6678e-02, -6.3535e-04,  ...,  5.5865e-02,
         -1.2123e-02,  2.1398e-02],
        [ 3.2199e-02,  2.5240e-02, -6.2599e-04,  ...,  5.2862e-02,
         -1.1461e-02,  2.0188e-02],
        [ 3.2577e-02,  2.5532e-02, -6.2789e-04,  ...,  5.3472e-02,
         -1.1596e-02,  2.0434e-02],
        ...,
        [-6.4939e-05,  3.1186e-04, -4.6361e-04,  ...,  7.9462e-04,
          0.0000e+00, -7.9297e-04],
        [-6.4939e-05,  3.1186e-04, -4.6361e-04,  ...,  7.9462e-04,
          0.0000e+00, -7.9297e-04],
        [-6.4939e-05,  3.1186e-04, -4.6361e-04,  ...,  7.9462e-04,
          0.0000e+00, -7.9297e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(812.3341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.0961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.9846, device='cuda:0')



h[100].sum tensor(32.5991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.8300, device='cuda:0')



h[200].sum tensor(26.3381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1287, 0.1009, 0.0000,  ..., 0.2113, 0.0000, 0.0807],
        [0.1402, 0.1098, 0.0000,  ..., 0.2299, 0.0000, 0.0882],
        [0.1374, 0.1076, 0.0000,  ..., 0.2253, 0.0000, 0.0863],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43944.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2763, 0.1782, 0.8537,  ..., 0.1086, 0.2219, 0.0000],
        [0.2851, 0.1831, 0.8831,  ..., 0.1126, 0.2291, 0.0000],
        [0.2705, 0.1751, 0.8342,  ..., 0.1059, 0.2172, 0.0000],
        ...,
        [0.0025, 0.0206, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0025, 0.0206, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0025, 0.0206, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(272689.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2304.0964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.2406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1338.5233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(583.2009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.1423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0948],
        [-0.0930],
        [-0.0762],
        ...,
        [-0.2357],
        [-0.2347],
        [-0.2343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-81769.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366281.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366292.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4180e-05,  3.2492e-04, -4.7708e-04,  ...,  7.9246e-04,
          0.0000e+00, -7.9177e-04],
        [-5.4180e-05,  3.2492e-04, -4.7708e-04,  ...,  7.9246e-04,
          0.0000e+00, -7.9177e-04],
        [-5.4180e-05,  3.2492e-04, -4.7708e-04,  ...,  7.9246e-04,
          0.0000e+00, -7.9177e-04],
        ...,
        [-5.4180e-05,  3.2492e-04, -4.7708e-04,  ...,  7.9246e-04,
          0.0000e+00, -7.9177e-04],
        [-5.4180e-05,  3.2492e-04, -4.7708e-04,  ...,  7.9246e-04,
          0.0000e+00, -7.9177e-04],
        [-5.4180e-05,  3.2492e-04, -4.7708e-04,  ...,  7.9246e-04,
          0.0000e+00, -7.9177e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(947.2981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8019, device='cuda:0')



h[100].sum tensor(33.8807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.2630, device='cuda:0')



h[200].sum tensor(30.1072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47776.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0128, 0.0270, 0.0199,  ..., 0.0017, 0.0117, 0.0000],
        [0.0050, 0.0223, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0038, 0.0216, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        ...,
        [0.0026, 0.0208, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0026, 0.0208, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0026, 0.0208, 0.0000,  ..., 0.0000, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(289220., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2405.3765, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.7965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1438.0442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(633.4899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.0718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0357],
        [-0.0213],
        [-0.0895],
        ...,
        [-0.2327],
        [-0.2317],
        [-0.2313]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90029.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0004],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366292.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0005],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366303.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2444e-05,  3.3619e-04, -4.8933e-04,  ...,  7.7965e-04,
          0.0000e+00, -7.9267e-04],
        [-4.2444e-05,  3.3619e-04, -4.8933e-04,  ...,  7.7965e-04,
          0.0000e+00, -7.9267e-04],
        [ 9.1124e-03,  7.4130e-03, -5.3808e-04,  ...,  1.5557e-02,
         -3.2317e-03,  5.1593e-03],
        ...,
        [-4.2444e-05,  3.3619e-04, -4.8933e-04,  ...,  7.7965e-04,
          0.0000e+00, -7.9267e-04],
        [-4.2444e-05,  3.3619e-04, -4.8933e-04,  ...,  7.7965e-04,
          0.0000e+00, -7.9267e-04],
        [-4.2444e-05,  3.3619e-04, -4.8933e-04,  ...,  7.7965e-04,
          0.0000e+00, -7.9267e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(664.7846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.6621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3353, device='cuda:0')



h[100].sum tensor(31.4418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.9093, device='cuda:0')



h[200].sum tensor(20.6327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0137, 0.0000,  ..., 0.0289, 0.0000, 0.0096],
        [0.0091, 0.0084, 0.0000,  ..., 0.0179, 0.0000, 0.0052],
        [0.0268, 0.0221, 0.0000,  ..., 0.0465, 0.0000, 0.0159],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37566.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0556, 0.0525, 0.1458,  ..., 0.0158, 0.0456, 0.0000],
        [0.0526, 0.0509, 0.1359,  ..., 0.0145, 0.0432, 0.0000],
        [0.0716, 0.0619, 0.1951,  ..., 0.0219, 0.0583, 0.0000],
        ...,
        [0.0026, 0.0209, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0026, 0.0209, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0026, 0.0209, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(249143.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1858.4097, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.4394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1497.1853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(495.3269, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-297.6851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0752],
        [ 0.0721],
        [ 0.0648],
        ...,
        [-0.2321],
        [-0.2311],
        [-0.2308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87895.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0005],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366303.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0005],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366314.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1295e-02,  9.0983e-03, -5.6223e-04,  ...,  1.9047e-02,
         -3.9859e-03,  6.5700e-03],
        [ 5.4573e-03,  4.5846e-03, -5.3040e-04,  ...,  9.6229e-03,
         -1.9315e-03,  2.7748e-03],
        [-3.1673e-05,  3.4074e-04, -5.0048e-04,  ...,  7.6185e-04,
          0.0000e+00, -7.9353e-04],
        ...,
        [-3.1673e-05,  3.4074e-04, -5.0048e-04,  ...,  7.6185e-04,
          0.0000e+00, -7.9353e-04],
        [-3.1673e-05,  3.4074e-04, -5.0048e-04,  ...,  7.6185e-04,
          0.0000e+00, -7.9353e-04],
        [-3.1673e-05,  3.4074e-04, -5.0048e-04,  ...,  7.6185e-04,
          0.0000e+00, -7.9353e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1061.5330, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.6810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6852, device='cuda:0')



h[100].sum tensor(35.2614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.9039, device='cuda:0')



h[200].sum tensor(32.2216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0484, 0.0388, 0.0000,  ..., 0.0813, 0.0000, 0.0283],
        [0.0290, 0.0238, 0.0000,  ..., 0.0500, 0.0000, 0.0173],
        [0.0055, 0.0056, 0.0000,  ..., 0.0119, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50721.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1058, 0.0825, 0.2939,  ..., 0.0321, 0.0853, 0.0000],
        [0.0791, 0.0666, 0.2141,  ..., 0.0230, 0.0643, 0.0000],
        [0.0511, 0.0502, 0.1286,  ..., 0.0128, 0.0422, 0.0000],
        ...,
        [0.0026, 0.0210, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0026, 0.0210, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0026, 0.0210, 0.0000,  ..., 0.0000, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(299662.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2631.3225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.0710, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1429.3611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(680.3613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.2791, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0145],
        [ 0.0464],
        [ 0.0728],
        ...,
        [-0.2346],
        [-0.2336],
        [-0.2333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74119.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0005],
        [0.9997],
        ...,
        [0.9997],
        [0.9993],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366314.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366325.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1885e-05,  3.3452e-04, -5.1061e-04,  ...,  7.4119e-04,
          0.0000e+00, -7.9195e-04],
        [-2.1885e-05,  3.3452e-04, -5.1061e-04,  ...,  7.4119e-04,
          0.0000e+00, -7.9195e-04],
        [-2.1885e-05,  3.3452e-04, -5.1061e-04,  ...,  7.4119e-04,
          0.0000e+00, -7.9195e-04],
        ...,
        [-2.1885e-05,  3.3452e-04, -5.1061e-04,  ...,  7.4119e-04,
          0.0000e+00, -7.9195e-04],
        [-2.1885e-05,  3.3452e-04, -5.1061e-04,  ...,  7.4119e-04,
          0.0000e+00, -7.9195e-04],
        [-2.1885e-05,  3.3452e-04, -5.1061e-04,  ...,  7.4119e-04,
          0.0000e+00, -7.9195e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(701.3168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.6950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4740, device='cuda:0')



h[100].sum tensor(32.1488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.3242, device='cuda:0')



h[200].sum tensor(20.0000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37160.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0384, 0.0425, 0.1006,  ..., 0.0113, 0.0326, 0.0000],
        [0.0125, 0.0272, 0.0233,  ..., 0.0022, 0.0121, 0.0000],
        [0.0049, 0.0227, 0.0012,  ..., 0.0000, 0.0063, 0.0000],
        ...,
        [0.0025, 0.0212, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0025, 0.0212, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0025, 0.0212, 0.0000,  ..., 0.0000, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(243218.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1727.4557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(255.4341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1675.2584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(494.1737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.1177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0618],
        [ 0.0642],
        [ 0.0718],
        ...,
        [-0.2393],
        [-0.2387],
        [-0.2386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86029.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366325.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366337.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0892e-05,  3.2258e-04, -5.1983e-04,  ...,  7.1628e-04,
          0.0000e+00, -7.9315e-04],
        [-1.0892e-05,  3.2258e-04, -5.1983e-04,  ...,  7.1628e-04,
          0.0000e+00, -7.9315e-04],
        [ 6.4061e-03,  5.2859e-03, -5.5623e-04,  ...,  1.1077e-02,
         -2.2439e-03,  3.3779e-03],
        ...,
        [-1.0892e-05,  3.2258e-04, -5.1983e-04,  ...,  7.1628e-04,
          0.0000e+00, -7.9315e-04],
        [-1.0892e-05,  3.2258e-04, -5.1983e-04,  ...,  7.1628e-04,
          0.0000e+00, -7.9315e-04],
        [-1.0892e-05,  3.2258e-04, -5.1983e-04,  ...,  7.1628e-04,
          0.0000e+00, -7.9315e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(825.6319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.3407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2586, device='cuda:0')



h[100].sum tensor(33.4966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6594, device='cuda:0')



h[200].sum tensor(23.0561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0124, 0.0109, 0.0000,  ..., 0.0229, 0.0000, 0.0073],
        [0.0064, 0.0063, 0.0000,  ..., 0.0132, 0.0000, 0.0034],
        [0.0141, 0.0122, 0.0000,  ..., 0.0257, 0.0000, 0.0076],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41056.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0392, 0.0721,  ..., 0.0064, 0.0271, 0.0000],
        [0.0337, 0.0403, 0.0743,  ..., 0.0060, 0.0283, 0.0000],
        [0.0479, 0.0491, 0.1132,  ..., 0.0097, 0.0393, 0.0000],
        ...,
        [0.0024, 0.0213, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0024, 0.0213, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0024, 0.0213, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(260162.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1922.0769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.1429, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1825.5464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(549.3819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.7618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1014],
        [ 0.1121],
        [ 0.1230],
        ...,
        [-0.2512],
        [-0.2502],
        [-0.2499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91238.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366337.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366349.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        ...,
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(748.2096, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.4971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0222, device='cuda:0')



h[100].sum tensor(33.2822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9630, device='cuda:0')



h[200].sum tensor(19.9632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.8176e-06, 1.2324e-03, 0.0000e+00,  ..., 2.7662e-03, 0.0000e+00,
         0.0000e+00],
        [9.8234e-06, 1.2331e-03, 0.0000e+00,  ..., 2.7678e-03, 0.0000e+00,
         0.0000e+00],
        [9.7066e-03, 8.7338e-03, 0.0000e+00,  ..., 1.8423e-02, 0.0000e+00,
         5.5069e-03],
        ...,
        [9.8613e-06, 1.2379e-03, 0.0000e+00,  ..., 2.7785e-03, 0.0000e+00,
         0.0000e+00],
        [9.8607e-06, 1.2378e-03, 0.0000e+00,  ..., 2.7784e-03, 0.0000e+00,
         0.0000e+00],
        [9.8606e-06, 1.2378e-03, 0.0000e+00,  ..., 2.7783e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38328.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0055, 0.0232, 0.0010,  ..., 0.0000, 0.0063, 0.0000],
        [0.0097, 0.0258, 0.0144,  ..., 0.0012, 0.0095, 0.0000],
        [0.0250, 0.0351, 0.0540,  ..., 0.0053, 0.0213, 0.0000],
        ...,
        [0.0023, 0.0213, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0023, 0.0213, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0023, 0.0213, 0.0000,  ..., 0.0000, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(251786.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1808.5785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(268.7266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1972.1794, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(515.4913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-306.2427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0112],
        [-0.0401],
        [-0.0485],
        ...,
        [-0.2637],
        [-0.2626],
        [-0.2623]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89452.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366349.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366349.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        [ 1.2912e-02,  1.0295e-02, -6.0266e-04,  ...,  2.1536e-02,
         -4.5001e-03,  7.5954e-03],
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        ...,
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04],
        [ 2.4534e-06,  3.0797e-04, -5.2821e-04,  ...,  6.9127e-04,
          0.0000e+00, -7.9509e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(730.5317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.0094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8102, device='cuda:0')



h[100].sum tensor(33.1229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3293, device='cuda:0')



h[200].sum tensor(19.4176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8423e-02, 1.5477e-02, 0.0000e+00,  ..., 3.2497e-02, 0.0000e+00,
         1.0376e-02],
        [1.0563e-02, 9.3970e-03, 0.0000e+00,  ..., 1.9808e-02, 0.0000e+00,
         6.0629e-03],
        [6.0524e-02, 4.8046e-02, 0.0000e+00,  ..., 1.0048e-01, 0.0000e+00,
         3.6148e-02],
        ...,
        [9.8613e-06, 1.2379e-03, 0.0000e+00,  ..., 2.7785e-03, 0.0000e+00,
         0.0000e+00],
        [9.8607e-06, 1.2378e-03, 0.0000e+00,  ..., 2.7784e-03, 0.0000e+00,
         0.0000e+00],
        [9.8606e-06, 1.2378e-03, 0.0000e+00,  ..., 2.7783e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39253.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0457, 0.0476, 0.1102,  ..., 0.0103, 0.0375, 0.0000],
        [0.0456, 0.0474, 0.1135,  ..., 0.0115, 0.0375, 0.0000],
        [0.0784, 0.0668, 0.2137,  ..., 0.0235, 0.0633, 0.0000],
        ...,
        [0.0023, 0.0213, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0023, 0.0213, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0023, 0.0213, 0.0000,  ..., 0.0000, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(261498.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1970.0118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.0737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1919.1782, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(528.7287, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-316.6079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0970],
        [ 0.0778],
        [ 0.0556],
        ...,
        [-0.2635],
        [-0.2625],
        [-0.2622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84698.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0005],
        [0.9998],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366349.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(506.7813, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366362.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 7.9492e-03,  6.4296e-03, -5.8225e-04,  ...,  1.3474e-02,
         -2.7554e-03,  4.3519e-03],
        ...,
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1718.7338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(46.6542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.3566, device='cuda:0')



h[100].sum tensor(42.8633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(69.8285, device='cuda:0')



h[200].sum tensor(49.3261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.6051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.7409e-05, 1.1772e-03, 0.0000e+00,  ..., 2.6799e-03, 0.0000e+00,
         0.0000e+00],
        [1.2339e-02, 1.0665e-02, 0.0000e+00,  ..., 2.2480e-02, 0.0000e+00,
         6.3637e-03],
        [2.8586e-02, 2.3235e-02, 0.0000e+00,  ..., 4.8713e-02, 0.0000e+00,
         1.6122e-02],
        ...,
        [7.7763e-05, 1.1826e-03, 0.0000e+00,  ..., 2.6921e-03, 0.0000e+00,
         0.0000e+00],
        [7.7759e-05, 1.1825e-03, 0.0000e+00,  ..., 2.6920e-03, 0.0000e+00,
         0.0000e+00],
        [7.7758e-05, 1.1825e-03, 0.0000e+00,  ..., 2.6920e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74989.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.0308, 0.0318,  ..., 0.0026, 0.0160, 0.0000],
        [0.0433, 0.0457, 0.1007,  ..., 0.0092, 0.0352, 0.0000],
        [0.0757, 0.0652, 0.1954,  ..., 0.0197, 0.0604, 0.0000],
        ...,
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(416636.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4216.3340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(598.0451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1889.4446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1026.1145, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-704.0206, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0791],
        [ 0.0820],
        [ 0.0842],
        ...,
        [-0.2740],
        [-0.2728],
        [-0.2725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72629.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366362.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366362.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        ...,
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04],
        [ 1.9344e-05,  2.9418e-04, -5.3584e-04,  ...,  6.6969e-04,
          0.0000e+00, -8.0124e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(916.8350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2308, device='cuda:0')



h[100].sum tensor(35.6589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.5659, device='cuda:0')



h[200].sum tensor(24.6570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.7409e-05, 1.1772e-03, 0.0000e+00,  ..., 2.6799e-03, 0.0000e+00,
         0.0000e+00],
        [7.7456e-05, 1.1779e-03, 0.0000e+00,  ..., 2.6815e-03, 0.0000e+00,
         0.0000e+00],
        [7.7416e-05, 1.1773e-03, 0.0000e+00,  ..., 2.6801e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [7.7763e-05, 1.1826e-03, 0.0000e+00,  ..., 2.6921e-03, 0.0000e+00,
         0.0000e+00],
        [7.7759e-05, 1.1825e-03, 0.0000e+00,  ..., 2.6920e-03, 0.0000e+00,
         0.0000e+00],
        [7.7758e-05, 1.1825e-03, 0.0000e+00,  ..., 2.6920e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47958.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0257, 0.0086,  ..., 0.0002, 0.0094, 0.0000],
        [0.0079, 0.0245, 0.0050,  ..., 0.0001, 0.0078, 0.0000],
        [0.0041, 0.0222, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        ...,
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(308703.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2519.8943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.4398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2255.4822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(648.0295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.2777, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0287],
        [ 0.0075],
        [-0.0252],
        ...,
        [-0.2740],
        [-0.2728],
        [-0.2725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101885.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9998],
        [0.9994],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366362.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9999],
        [0.9994],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366375.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6006e-02,  1.2639e-02, -6.3753e-04,  ...,  2.6455e-02,
         -5.5346e-03,  9.5799e-03],
        [ 8.0430e-03,  6.4763e-03, -5.9031e-04,  ...,  1.3597e-02,
         -2.7766e-03,  4.4063e-03],
        [ 8.0323e-03,  6.4680e-03, -5.9024e-04,  ...,  1.3580e-02,
         -2.7729e-03,  4.3994e-03],
        ...,
        [ 2.6570e-05,  2.7249e-04, -5.4276e-04,  ...,  6.5264e-04,
          0.0000e+00, -8.0204e-04],
        [ 2.6570e-05,  2.7249e-04, -5.4276e-04,  ...,  6.5264e-04,
          0.0000e+00, -8.0204e-04],
        [ 2.6570e-05,  2.7249e-04, -5.4276e-04,  ...,  6.5264e-04,
          0.0000e+00, -8.0204e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(852.2898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.0080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4005, device='cuda:0')



h[100].sum tensor(35.7634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.0835, device='cuda:0')



h[200].sum tensor(22.2617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0423, 0.0337, 0.0000,  ..., 0.0707, 0.0000, 0.0242],
        [0.0438, 0.0349, 0.0000,  ..., 0.0731, 0.0000, 0.0251],
        [0.0147, 0.0124, 0.0000,  ..., 0.0261, 0.0000, 0.0079],
        ...,
        [0.0001, 0.0011, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0001, 0.0011, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0001, 0.0011, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43704.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0746, 0.0647, 0.1926,  ..., 0.0199, 0.0590, 0.0000],
        [0.0673, 0.0603, 0.1716,  ..., 0.0176, 0.0534, 0.0000],
        [0.0429, 0.0458, 0.0992,  ..., 0.0094, 0.0344, 0.0000],
        ...,
        [0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(282732.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2273.7881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(317.6132, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2315.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(597.1510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.9272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1232],
        [ 0.1184],
        [ 0.1100],
        ...,
        [-0.2857],
        [-0.2850],
        [-0.2852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89551.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9999],
        [0.9994],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366375.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9999],
        [0.9994],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366375.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6433e-02,  1.2970e-02, -6.4007e-04,  ...,  2.7146e-02,
         -5.6827e-03,  9.8577e-03],
        [ 1.1397e-02,  9.0716e-03, -6.1020e-04,  ...,  1.9013e-02,
         -3.9382e-03,  6.5853e-03],
        [ 1.6098e-02,  1.2710e-02, -6.3808e-04,  ...,  2.6604e-02,
         -5.5664e-03,  9.6395e-03],
        ...,
        [ 2.6570e-05,  2.7249e-04, -5.4276e-04,  ...,  6.5264e-04,
          0.0000e+00, -8.0204e-04],
        [ 2.6570e-05,  2.7249e-04, -5.4276e-04,  ...,  6.5264e-04,
          0.0000e+00, -8.0204e-04],
        [ 2.6570e-05,  2.7249e-04, -5.4276e-04,  ...,  6.5264e-04,
          0.0000e+00, -8.0204e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(826.2147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.2932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1117, device='cuda:0')



h[100].sum tensor(35.5299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2201, device='cuda:0')



h[200].sum tensor(21.4621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0408, 0.0326, 0.0000,  ..., 0.0684, 0.0000, 0.0232],
        [0.0458, 0.0364, 0.0000,  ..., 0.0764, 0.0000, 0.0265],
        [0.0286, 0.0231, 0.0000,  ..., 0.0486, 0.0000, 0.0161],
        ...,
        [0.0001, 0.0011, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0001, 0.0011, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0001, 0.0011, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41922.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1007, 0.0801, 0.2659,  ..., 0.0275, 0.0793, 0.0000],
        [0.0979, 0.0784, 0.2593,  ..., 0.0271, 0.0772, 0.0000],
        [0.0820, 0.0690, 0.2122,  ..., 0.0217, 0.0649, 0.0000],
        ...,
        [0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(272160.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2120.3450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.8161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2350.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(573.0809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-343.9134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1095],
        [ 0.1155],
        [ 0.1213],
        ...,
        [-0.2888],
        [-0.2876],
        [-0.2872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92972.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9999],
        [0.9994],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366375.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9999],
        [0.9995],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366387.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.8047e-05,  2.5673e-04, -5.4906e-04,  ...,  6.4344e-04,
          0.0000e+00, -8.0136e-04],
        [ 3.8047e-05,  2.5673e-04, -5.4906e-04,  ...,  6.4344e-04,
          0.0000e+00, -8.0136e-04],
        [ 3.8047e-05,  2.5673e-04, -5.4906e-04,  ...,  6.4344e-04,
          0.0000e+00, -8.0136e-04],
        ...,
        [ 3.8047e-05,  2.5673e-04, -5.4906e-04,  ...,  6.4344e-04,
          0.0000e+00, -8.0136e-04],
        [ 3.8047e-05,  2.5673e-04, -5.4906e-04,  ...,  6.4344e-04,
          0.0000e+00, -8.0136e-04],
        [ 3.8047e-05,  2.5673e-04, -5.4906e-04,  ...,  6.4344e-04,
          0.0000e+00, -8.0136e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(733.7162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5369, device='cuda:0')



h[100].sum tensor(35.4637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5122, device='cuda:0')



h[200].sum tensor(18.2885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38397.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0024, 0.0212, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0024, 0.0213, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        ...,
        [0.0024, 0.0213, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0024, 0.0213, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0024, 0.0213, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259956.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1837.5591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(269.1952, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2645.9126, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(524.8306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.9718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3244],
        [-0.3034],
        [-0.2580],
        ...,
        [-0.2968],
        [-0.2955],
        [-0.2951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105934.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [0.9999],
        ...,
        [0.9999],
        [0.9995],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366387.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [1.0000],
        ...,
        [0.9999],
        [0.9995],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366400.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8007e-05,  2.5096e-04, -5.5478e-04,  ...,  6.4277e-04,
          0.0000e+00, -8.0357e-04],
        [ 5.8007e-05,  2.5096e-04, -5.5478e-04,  ...,  6.4277e-04,
          0.0000e+00, -8.0357e-04],
        [ 5.8007e-05,  2.5096e-04, -5.5478e-04,  ...,  6.4277e-04,
          0.0000e+00, -8.0357e-04],
        ...,
        [ 5.8007e-05,  2.5096e-04, -5.5478e-04,  ...,  6.4277e-04,
          0.0000e+00, -8.0357e-04],
        [ 5.8007e-05,  2.5096e-04, -5.5478e-04,  ...,  6.4277e-04,
          0.0000e+00, -8.0357e-04],
        [ 5.8007e-05,  2.5096e-04, -5.5478e-04,  ...,  6.4277e-04,
          0.0000e+00, -8.0357e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(890.1678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7214, device='cuda:0')



h[100].sum tensor(37.7345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.0432, device='cuda:0')



h[200].sum tensor(22.9233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3074, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41641.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0211, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0023, 0.0211, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0079, 0.0245, 0.0113,  ..., 0.0012, 0.0072, 0.0000],
        ...,
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0024, 0.0211, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266125.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1878.5573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.7794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2811.5576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(571.3040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-341.4289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0421],
        [-0.0665],
        [-0.0631],
        ...,
        [-0.2989],
        [-0.2976],
        [-0.2972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107616.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0006],
        [1.0000],
        ...,
        [0.9999],
        [0.9995],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366400.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [1.0000],
        ...,
        [1.0000],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366413.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.2853e-05,  2.5080e-04, -5.5997e-04,  ...,  6.4849e-04,
          0.0000e+00, -8.0749e-04],
        [ 8.2853e-05,  2.5080e-04, -5.5997e-04,  ...,  6.4849e-04,
          0.0000e+00, -8.0749e-04],
        [ 8.2853e-05,  2.5080e-04, -5.5997e-04,  ...,  6.4849e-04,
          0.0000e+00, -8.0749e-04],
        ...,
        [ 8.2853e-05,  2.5080e-04, -5.5997e-04,  ...,  6.4849e-04,
          0.0000e+00, -8.0749e-04],
        [ 8.2853e-05,  2.5080e-04, -5.5997e-04,  ...,  6.4849e-04,
          0.0000e+00, -8.0749e-04],
        [ 8.2853e-05,  2.5080e-04, -5.5997e-04,  ...,  6.4849e-04,
          0.0000e+00, -8.0749e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1207.0176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.7273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8548, device='cuda:0')



h[100].sum tensor(41.4467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.4005, device='cuda:0')



h[200].sum tensor(32.5699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7684, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0003, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0003, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0003, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0003, 0.0010, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54502.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0294, 0.0249,  ..., 0.0012, 0.0140, 0.0000],
        [0.0120, 0.0266, 0.0136,  ..., 0.0004, 0.0103, 0.0000],
        [0.0086, 0.0247, 0.0070,  ..., 0.0002, 0.0077, 0.0000],
        ...,
        [0.0024, 0.0210, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0024, 0.0210, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0024, 0.0210, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324824.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2885.9973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.5908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2591.0525, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(756.6580, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-487.3373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1095],
        [ 0.0815],
        [ 0.0408],
        ...,
        [-0.2970],
        [-0.2958],
        [-0.2955]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83182.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [1.0000],
        ...,
        [1.0000],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366413.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [1.0001],
        ...,
        [1.0000],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366425.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2249e-02,  1.7397e-02, -7.0160e-04,  ...,  3.6453e-02,
         -7.5769e-03,  1.3590e-02],
        [ 6.2790e-03,  5.0290e-03, -6.0293e-04,  ...,  1.0657e-02,
         -2.1161e-03,  3.2188e-03],
        [ 1.9485e-02,  1.5256e-02, -6.8453e-04,  ...,  3.1988e-02,
         -6.6318e-03,  1.1795e-02],
        ...,
        [ 9.0495e-05,  2.3655e-04, -5.6470e-04,  ...,  6.6054e-04,
          0.0000e+00, -8.0004e-04],
        [ 9.0495e-05,  2.3655e-04, -5.6470e-04,  ...,  6.6054e-04,
          0.0000e+00, -8.0004e-04],
        [ 9.0495e-05,  2.3655e-04, -5.6470e-04,  ...,  6.6054e-04,
          0.0000e+00, -8.0004e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1534.5751, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(42.5052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1195, device='cuda:0')



h[100].sum tensor(45.1801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(60.1506, device='cuda:0')



h[200].sum tensor(42.3644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.2441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0334, 0.0265, 0.0000,  ..., 0.0560, 0.0000, 0.0190],
        [0.0817, 0.0639, 0.0000,  ..., 0.1340, 0.0000, 0.0496],
        [0.0521, 0.0410, 0.0000,  ..., 0.0861, 0.0000, 0.0304],
        ...,
        [0.0004, 0.0010, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0010, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0010, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69055.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1035, 0.0814, 0.2809,  ..., 0.0301, 0.0815, 0.0000],
        [0.1538, 0.1109, 0.4350,  ..., 0.0492, 0.1212, 0.0000],
        [0.1585, 0.1138, 0.4481,  ..., 0.0504, 0.1248, 0.0000],
        ...,
        [0.0021, 0.0214, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0060, 0.0238, 0.0059,  ..., 0.0005, 0.0058, 0.0000],
        [0.0177, 0.0306, 0.0369,  ..., 0.0038, 0.0149, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405512.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3876.6831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(534.6197, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2551.4983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(959.7795, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-645.2753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1031],
        [ 0.1044],
        [ 0.1065],
        ...,
        [-0.1714],
        [-0.0834],
        [-0.0100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83505.5703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [1.0001],
        ...,
        [1.0000],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366425.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [1.0001],
        ...,
        [1.0000],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366437.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4105e-05,  2.1866e-04, -5.6899e-04,  ...,  6.7371e-04,
          0.0000e+00, -7.8758e-04],
        [ 9.4105e-05,  2.1866e-04, -5.6899e-04,  ...,  6.7371e-04,
          0.0000e+00, -7.8758e-04],
        [ 9.4105e-05,  2.1866e-04, -5.6899e-04,  ...,  6.7371e-04,
          0.0000e+00, -7.8758e-04],
        ...,
        [ 9.4105e-05,  2.1866e-04, -5.6899e-04,  ...,  6.7371e-04,
          0.0000e+00, -7.8758e-04],
        [ 9.4105e-05,  2.1866e-04, -5.6899e-04,  ...,  6.7371e-04,
          0.0000e+00, -7.8758e-04],
        [ 9.4105e-05,  2.1866e-04, -5.6899e-04,  ...,  6.7371e-04,
          0.0000e+00, -7.8758e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1153.2146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.8970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6755, device='cuda:0')



h[100].sum tensor(42.6987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.8748, device='cuda:0')



h[200].sum tensor(30.5918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0009, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51362.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0229, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0020, 0.0221, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0017, 0.0220, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0017, 0.0220, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0017, 0.0220, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0017, 0.0220, 0.0000,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(316145.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2570.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.9032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2578.1904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(722.9978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.1598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1460],
        [-0.2488],
        [-0.3315],
        ...,
        [-0.3102],
        [-0.3089],
        [-0.3085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-85234.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0007],
        [1.0001],
        ...,
        [1.0000],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366437.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [1.0001],
        ...,
        [1.0001],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366450.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.1172e-05,  1.9606e-04, -5.7288e-04,  ...,  6.8900e-04,
          0.0000e+00, -7.7243e-04],
        [ 9.1172e-05,  1.9606e-04, -5.7288e-04,  ...,  6.8900e-04,
          0.0000e+00, -7.7243e-04],
        [ 9.1172e-05,  1.9606e-04, -5.7288e-04,  ...,  6.8900e-04,
          0.0000e+00, -7.7243e-04],
        ...,
        [ 9.1172e-05,  1.9606e-04, -5.7288e-04,  ...,  6.8900e-04,
          0.0000e+00, -7.7243e-04],
        [ 9.1172e-05,  1.9606e-04, -5.7288e-04,  ...,  6.8900e-04,
          0.0000e+00, -7.7243e-04],
        [ 9.1172e-05,  1.9606e-04, -5.7288e-04,  ...,  6.8900e-04,
          0.0000e+00, -7.7243e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(856.6536, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4820, device='cuda:0')



h[100].sum tensor(40.9052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.3376, device='cuda:0')



h[200].sum tensor(21.4318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1691, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0008, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0004, 0.0008, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0004, 0.0008, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0008, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0004, 0.0008, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0004, 0.0008, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41895.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0227, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0013, 0.0227, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0013, 0.0228, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        ...,
        [0.0013, 0.0228, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0013, 0.0228, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0013, 0.0228, 0.0000,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276821.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1881.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.5422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2690.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(599.1827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.0105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4367],
        [-0.4169],
        [-0.3796],
        ...,
        [-0.3171],
        [-0.3158],
        [-0.3154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-98768.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [1.0001],
        ...,
        [1.0001],
        [0.9996],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366450.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(443.7822, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [1.0002],
        ...,
        [1.0001],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366462.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.1189e-05,  1.7628e-04, -5.7642e-04,  ...,  7.0612e-04,
          0.0000e+00, -7.5988e-04],
        [ 9.1189e-05,  1.7628e-04, -5.7642e-04,  ...,  7.0612e-04,
          0.0000e+00, -7.5988e-04],
        [ 9.1189e-05,  1.7628e-04, -5.7642e-04,  ...,  7.0612e-04,
          0.0000e+00, -7.5988e-04],
        ...,
        [ 9.1189e-05,  1.7628e-04, -5.7642e-04,  ...,  7.0612e-04,
          0.0000e+00, -7.5988e-04],
        [ 9.1189e-05,  1.7628e-04, -5.7642e-04,  ...,  7.0612e-04,
          0.0000e+00, -7.5988e-04],
        [ 9.1189e-05,  1.7628e-04, -5.7642e-04,  ...,  7.0612e-04,
          0.0000e+00, -7.5988e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1028.6862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6016, device='cuda:0')



h[100].sum tensor(43.1979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.6746, device='cuda:0')



h[200].sum tensor(26.4558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4055, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0007, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0066, 0.0055, 0.0000,  ..., 0.0129, 0.0000, 0.0033],
        ...,
        [0.0004, 0.0007, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45462.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0240, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0087, 0.0281, 0.0150,  ..., 0.0010, 0.0080, 0.0000],
        [0.0229, 0.0367, 0.0478,  ..., 0.0037, 0.0186, 0.0000],
        ...,
        [0.0009, 0.0235, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0009, 0.0235, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0009, 0.0235, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(286193.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1910.9410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.9624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2670.1489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(652.7726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.3226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0987],
        [-0.0147],
        [ 0.0533],
        ...,
        [-0.3219],
        [-0.3206],
        [-0.3202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106845.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [1.0002],
        ...,
        [1.0001],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366462.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [1.0002],
        ...,
        [1.0001],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366475.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4912e-02,  1.1635e-02, -6.7372e-04,  ...,  2.4674e-02,
         -5.0045e-03,  8.8826e-03],
        [ 1.9677e-02,  1.5326e-02, -7.0397e-04,  ...,  3.2376e-02,
         -6.6135e-03,  1.1980e-02],
        [ 9.7702e-03,  7.6531e-03, -6.4108e-04,  ...,  1.6363e-02,
         -3.2683e-03,  5.5404e-03],
        ...,
        [ 9.1069e-05,  1.5688e-04, -5.7963e-04,  ...,  7.1779e-04,
          0.0000e+00, -7.5112e-04],
        [ 9.1069e-05,  1.5688e-04, -5.7963e-04,  ...,  7.1779e-04,
          0.0000e+00, -7.5112e-04],
        [ 9.1069e-05,  1.5688e-04, -5.7963e-04,  ...,  7.1779e-04,
          0.0000e+00, -7.5112e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(712.3782, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0306, device='cuda:0')



h[100].sum tensor(41.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.0089, device='cuda:0')



h[200].sum tensor(16.9520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8957, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0890, 0.0693, 0.0000,  ..., 0.1461, 0.0000, 0.0546],
        [0.0744, 0.0580, 0.0000,  ..., 0.1225, 0.0000, 0.0451],
        [0.0621, 0.0484, 0.0000,  ..., 0.1026, 0.0000, 0.0379],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36155.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2183, 0.1551, 0.6611,  ..., 0.0815, 0.1720, 0.0000],
        [0.1982, 0.1431, 0.5970,  ..., 0.0732, 0.1560, 0.0000],
        [0.1563, 0.1179, 0.4671,  ..., 0.0570, 0.1232, 0.0000],
        ...,
        [0.0007, 0.0240, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0007, 0.0240, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0007, 0.0240, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255413.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1479.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(225.6223, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2636.3052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(531.5166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.1687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1241],
        [-0.1158],
        [-0.0938],
        ...,
        [-0.3269],
        [-0.3257],
        [-0.3253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-99952.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0008],
        [1.0002],
        ...,
        [1.0001],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366475.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0002],
        ...,
        [1.0001],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366487.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8331e-03,  3.8106e-03, -6.1279e-04,  ...,  8.3875e-03,
         -1.5951e-03,  2.3333e-03],
        [ 1.1721e-02,  9.1451e-03, -6.5674e-04,  ...,  1.9522e-02,
         -3.9134e-03,  6.8107e-03],
        [ 9.4039e-05,  1.4021e-04, -5.8255e-04,  ...,  7.2663e-04,
          0.0000e+00, -7.4733e-04],
        ...,
        [ 9.4039e-05,  1.4021e-04, -5.8255e-04,  ...,  7.2663e-04,
          0.0000e+00, -7.4733e-04],
        [ 9.4039e-05,  1.4021e-04, -5.8255e-04,  ...,  7.2663e-04,
          0.0000e+00, -7.4733e-04],
        [ 9.4039e-05,  1.4021e-04, -5.8255e-04,  ...,  7.2663e-04,
          0.0000e+00, -7.4733e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1378.3641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.2657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.2256, device='cuda:0')



h[100].sum tensor(47.6654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(51.4989, device='cuda:0')



h[200].sum tensor(36.9451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0420, 0.0328, 0.0000,  ..., 0.0702, 0.0000, 0.0241],
        [0.0186, 0.0147, 0.0000,  ..., 0.0324, 0.0000, 0.0096],
        [0.0159, 0.0126, 0.0000,  ..., 0.0280, 0.0000, 0.0086],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54062.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0668, 0.0644, 0.1740,  ..., 0.0171, 0.0512, 0.0000],
        [0.0518, 0.0552, 0.1292,  ..., 0.0119, 0.0398, 0.0000],
        [0.0385, 0.0472, 0.0939,  ..., 0.0086, 0.0302, 0.0000],
        ...,
        [0.0007, 0.0244, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0007, 0.0244, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0007, 0.0244, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(317142.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2168.6414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(381.9958, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2764.9385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(775.7122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-478.6010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0734],
        [ 0.0741],
        [ 0.0621],
        ...,
        [-0.3280],
        [-0.3170],
        [-0.2960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121407.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0002],
        ...,
        [1.0001],
        [0.9996],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366487.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0001],
        [0.9997],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366500.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(822.0504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.2866, device='cuda:0')



h[100].sum tensor(43.4450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.7638, device='cuda:0')



h[200].sum tensor(20.9128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0358, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0005, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0005, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37982.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.2181e-03, 2.5731e-02, 1.1013e-03,  ..., 2.8301e-05, 3.6302e-03,
         0.0000e+00],
        [1.2060e-03, 2.4593e-02, 0.0000e+00,  ..., 0.0000e+00, 2.1759e-03,
         0.0000e+00],
        [9.2306e-04, 2.4501e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9801e-03,
         0.0000e+00],
        ...,
        [9.3583e-04, 2.4536e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9802e-03,
         0.0000e+00],
        [9.3582e-04, 2.4535e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9801e-03,
         0.0000e+00],
        [9.3585e-04, 2.4534e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9800e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259790.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1531.6412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(234.1843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2718.4104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(561.5721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-310.3492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1208],
        [-0.1649],
        [-0.1958],
        ...,
        [-0.3325],
        [-0.3313],
        [-0.3309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104595.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0001],
        [0.9997],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366500.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0001],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366513.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0287,  0.0223, -0.0008,  ...,  0.0470, -0.0096,  0.0178],
        [ 0.0136,  0.0106, -0.0007,  ...,  0.0225, -0.0045,  0.0080],
        ...,
        [ 0.0104,  0.0081, -0.0007,  ...,  0.0174, -0.0034,  0.0059],
        [ 0.0073,  0.0057, -0.0006,  ...,  0.0124, -0.0024,  0.0039],
        [ 0.0104,  0.0081, -0.0007,  ...,  0.0174, -0.0034,  0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(982.3521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5207, device='cuda:0')



h[100].sum tensor(45.4181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.4431, device='cuda:0')



h[200].sum tensor(26.1001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0964, 0.0749, 0.0000,  ..., 0.1581, 0.0000, 0.0593],
        [0.0374, 0.0291, 0.0000,  ..., 0.0625, 0.0000, 0.0217],
        [0.0973, 0.0755, 0.0000,  ..., 0.1594, 0.0000, 0.0598],
        ...,
        [0.0162, 0.0127, 0.0000,  ..., 0.0284, 0.0000, 0.0087],
        [0.0441, 0.0343, 0.0000,  ..., 0.0734, 0.0000, 0.0252],
        [0.0354, 0.0276, 0.0000,  ..., 0.0593, 0.0000, 0.0196]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41459.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1741, 0.1283, 0.5106,  ..., 0.0613, 0.1368, 0.0000],
        [0.1345, 0.1046, 0.3852,  ..., 0.0449, 0.1050, 0.0000],
        [0.1792, 0.1313, 0.5253,  ..., 0.0630, 0.1406, 0.0000],
        ...,
        [0.0388, 0.0473, 0.0939,  ..., 0.0091, 0.0300, 0.0000],
        [0.0619, 0.0612, 0.1596,  ..., 0.0162, 0.0475, 0.0000],
        [0.0678, 0.0647, 0.1761,  ..., 0.0179, 0.0521, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(269103.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1645.1204, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.3297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2871.4014, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(607.5615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-347.8228, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0040],
        [ 0.0125],
        [ 0.0027],
        ...,
        [-0.0094],
        [ 0.0256],
        [ 0.0332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109961.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0001],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366513.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0002],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366526.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(834.0609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3766, device='cuda:0')



h[100].sum tensor(44.6371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0329, device='cuda:0')



h[200].sum tensor(21.8150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38874.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1654e-02, 3.6562e-02, 4.8349e-02,  ..., 4.9522e-03, 1.6942e-02,
         0.0000e+00],
        [8.9456e-03, 2.8884e-02, 1.3103e-02,  ..., 1.1905e-03, 7.3794e-03,
         0.0000e+00],
        [5.0062e-03, 2.6495e-02, 1.9421e-03,  ..., 9.8486e-05, 4.5035e-03,
         0.0000e+00],
        ...,
        [1.6762e-03, 2.4708e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9171e-03,
         0.0000e+00],
        [1.6762e-03, 2.4707e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9170e-03,
         0.0000e+00],
        [1.6762e-03, 2.4705e-02, 0.0000e+00,  ..., 0.0000e+00, 1.9169e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266781.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1680.0491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.7176, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3036.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(574.6907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-321.8602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0507],
        [ 0.0432],
        [ 0.0528],
        ...,
        [-0.3385],
        [-0.3372],
        [-0.3368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110250.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0002],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366526.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0002],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366539.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0007,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(941.2215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.0585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8476, device='cuda:0')



h[100].sum tensor(46.0406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.4307, device='cuda:0')



h[200].sum tensor(24.7636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0004, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0004, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43003.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0126, 0.0312, 0.0266,  ..., 0.0032, 0.0098, 0.0000],
        [0.0049, 0.0266, 0.0051,  ..., 0.0006, 0.0038, 0.0000],
        [0.0058, 0.0272, 0.0047,  ..., 0.0005, 0.0045, 0.0000],
        ...,
        [0.0019, 0.0249, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0019, 0.0249, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0019, 0.0249, 0.0000,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(284519.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2114.9048, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.3371, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3025.8682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(637.3741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.5133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0600],
        [-0.0584],
        [-0.0310],
        ...,
        [-0.3479],
        [-0.3471],
        [-0.3470]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94303.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0009],
        [1.0003],
        ...,
        [1.0002],
        [0.9997],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366539.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0010],
        [1.0003],
        ...,
        [1.0002],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366552.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.9035e-05,  6.2096e-05, -5.9355e-04,  ...,  6.8245e-04,
          0.0000e+00, -7.7492e-04],
        [ 8.9035e-05,  6.2096e-05, -5.9355e-04,  ...,  6.8245e-04,
          0.0000e+00, -7.7492e-04],
        [ 8.9035e-05,  6.2096e-05, -5.9355e-04,  ...,  6.8245e-04,
          0.0000e+00, -7.7492e-04],
        ...,
        [ 8.9035e-05,  6.2096e-05, -5.9355e-04,  ...,  6.8245e-04,
          0.0000e+00, -7.7492e-04],
        [ 8.9035e-05,  6.2096e-05, -5.9355e-04,  ...,  6.8245e-04,
          0.0000e+00, -7.7492e-04],
        [ 8.9035e-05,  6.2096e-05, -5.9355e-04,  ...,  6.8245e-04,
          0.0000e+00, -7.7492e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(688.5946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.6360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.5254, device='cuda:0')



h[100].sum tensor(44.3176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.4983, device='cuda:0')



h[200].sum tensor(16.6678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0002, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0045, 0.0034, 0.0000,  ..., 0.0094, 0.0000, 0.0019],
        [0.0085, 0.0066, 0.0000,  ..., 0.0160, 0.0000, 0.0038],
        ...,
        [0.0004, 0.0003, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0003, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0004, 0.0003, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33222.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5380e-02, 3.3267e-02, 2.4282e-02,  ..., 2.0269e-03, 1.0422e-02,
         0.0000e+00],
        [2.4619e-02, 3.8597e-02, 4.4396e-02,  ..., 3.5011e-03, 1.7197e-02,
         0.0000e+00],
        [3.2668e-02, 4.3364e-02, 6.4540e-02,  ..., 5.1755e-03, 2.3098e-02,
         0.0000e+00],
        ...,
        [1.9509e-03, 2.5704e-02, 0.0000e+00,  ..., 6.2424e-05, 6.6282e-04,
         0.0000e+00],
        [1.9509e-03, 2.5703e-02, 0.0000e+00,  ..., 6.2369e-05, 6.6279e-04,
         0.0000e+00],
        [1.9509e-03, 2.5702e-02, 0.0000e+00,  ..., 6.2293e-05, 6.6276e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(246121.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1324.9930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.1098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3457.4094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(500.0136, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.1666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0712],
        [ 0.0993],
        [ 0.1160],
        ...,
        [-0.3689],
        [-0.3675],
        [-0.3671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136994.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0010],
        [1.0003],
        ...,
        [1.0002],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366552.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [1.0004],
        ...,
        [1.0002],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366565.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1307e-02,  8.7500e-03, -6.6846e-04,  ...,  1.8855e-02,
         -3.7089e-03,  6.5176e-03],
        [ 4.7078e-03,  3.6309e-03, -6.2544e-04,  ...,  8.1814e-03,
         -1.5317e-03,  2.2381e-03],
        [ 6.6642e-03,  5.1484e-03, -6.3819e-04,  ...,  1.1346e-02,
         -2.1771e-03,  3.5068e-03],
        ...,
        [ 6.4740e-05,  2.9422e-05, -5.9518e-04,  ...,  6.7187e-04,
          0.0000e+00, -7.7274e-04],
        [ 6.4740e-05,  2.9422e-05, -5.9518e-04,  ...,  6.7187e-04,
          0.0000e+00, -7.7274e-04],
        [ 6.4740e-05,  2.9422e-05, -5.9518e-04,  ...,  6.7187e-04,
          0.0000e+00, -7.7274e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(939.8566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9639, device='cuda:0')



h[100].sum tensor(46.8549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.7783, device='cuda:0')



h[200].sum tensor(23.4203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0189, 0.0146, 0.0000,  ..., 0.0328, 0.0000, 0.0098],
        [0.0423, 0.0327, 0.0000,  ..., 0.0707, 0.0000, 0.0242],
        [0.0189, 0.0146, 0.0000,  ..., 0.0328, 0.0000, 0.0105],
        ...,
        [0.0003, 0.0001, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0003, 0.0001, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0003, 0.0001, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41387.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0554, 0.0577, 0.1257,  ..., 0.0120, 0.0394, 0.0000],
        [0.0710, 0.0673, 0.1699,  ..., 0.0170, 0.0512, 0.0000],
        [0.0576, 0.0593, 0.1354,  ..., 0.0139, 0.0414, 0.0000],
        ...,
        [0.0021, 0.0263, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0021, 0.0263, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0021, 0.0263, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(281023.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1834.2006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(255.2551, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3497.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(615.0009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-341.2278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1298],
        [ 0.1339],
        [ 0.1302],
        ...,
        [-0.3837],
        [-0.3825],
        [-0.3823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147174.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [1.0004],
        ...,
        [1.0002],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366565.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [1.0004],
        ...,
        [1.0002],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366578.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4007e-05,  7.7194e-06, -5.9665e-04,  ...,  6.6542e-04,
          0.0000e+00, -7.6655e-04],
        [ 4.4007e-05,  7.7194e-06, -5.9665e-04,  ...,  6.6542e-04,
          0.0000e+00, -7.6655e-04],
        [ 4.4007e-05,  7.7194e-06, -5.9665e-04,  ...,  6.6542e-04,
          0.0000e+00, -7.6655e-04],
        ...,
        [ 4.4007e-05,  7.7194e-06, -5.9665e-04,  ...,  6.6542e-04,
          0.0000e+00, -7.6655e-04],
        [ 4.4007e-05,  7.7194e-06, -5.9665e-04,  ...,  6.6542e-04,
          0.0000e+00, -7.6655e-04],
        [ 4.4007e-05,  7.7194e-06, -5.9665e-04,  ...,  6.6542e-04,
          0.0000e+00, -7.6655e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(898.7035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4094, device='cuda:0')



h[100].sum tensor(46.9676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1207, device='cuda:0')



h[200].sum tensor(21.5249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1610, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7610e-04, 3.0891e-05, 0.0000e+00,  ..., 2.6629e-03, 0.0000e+00,
         0.0000e+00],
        [1.7625e-04, 3.0916e-05, 0.0000e+00,  ..., 2.6650e-03, 0.0000e+00,
         0.0000e+00],
        [1.7620e-04, 3.0908e-05, 0.0000e+00,  ..., 2.6643e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7730e-04, 3.1101e-05, 0.0000e+00,  ..., 2.6810e-03, 0.0000e+00,
         0.0000e+00],
        [1.7730e-04, 3.1101e-05, 0.0000e+00,  ..., 2.6810e-03, 0.0000e+00,
         0.0000e+00],
        [1.7730e-04, 3.1101e-05, 0.0000e+00,  ..., 2.6809e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42252.2852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0121, 0.0327, 0.0169,  ..., 0.0019, 0.0070, 0.0000],
        [0.0048, 0.0284, 0.0023,  ..., 0.0006, 0.0018, 0.0000],
        [0.0021, 0.0269, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0270, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0021, 0.0270, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0021, 0.0270, 0.0000,  ..., 0.0003, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291443.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2219.0513, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.6133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3249.3723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(636.9548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.0245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0068],
        [-0.0877],
        [-0.1849],
        ...,
        [-0.3958],
        [-0.3943],
        [-0.3938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120161.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0010],
        [1.0004],
        ...,
        [1.0002],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366578.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(550.6137, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [1.0004],
        ...,
        [1.0003],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366590.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6665e-02,  1.2911e-02, -7.0689e-04,  ...,  2.7566e-02,
         -5.4496e-03,  1.0018e-02],
        [ 2.2225e-02,  1.7225e-02, -7.4330e-04,  ...,  3.6560e-02,
         -7.2716e-03,  1.3622e-02],
        [ 2.8631e-02,  2.2197e-02, -7.8526e-04,  ...,  4.6925e-02,
         -9.3713e-03,  1.7775e-02],
        ...,
        [ 3.7325e-05,  8.2422e-06, -5.9799e-04,  ...,  6.6487e-04,
          0.0000e+00, -7.6209e-04],
        [ 3.7325e-05,  8.2422e-06, -5.9799e-04,  ...,  6.6487e-04,
          0.0000e+00, -7.6209e-04],
        [ 3.7325e-05,  8.2422e-06, -5.9799e-04,  ...,  6.6487e-04,
          0.0000e+00, -7.6209e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1217.5742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.5000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4501, device='cuda:0')



h[100].sum tensor(49.9148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.2010, device='cuda:0')



h[200].sum tensor(30.3568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.4820e-02, 5.7976e-02, 0.0000e+00,  ..., 1.2347e-01, 0.0000e+00,
         4.5359e-02],
        [8.1909e-02, 6.3476e-02, 0.0000e+00,  ..., 1.3494e-01, 0.0000e+00,
         4.9951e-02],
        [9.7321e-02, 7.5436e-02, 0.0000e+00,  ..., 1.5987e-01, 0.0000e+00,
         5.9944e-02],
        ...,
        [1.5040e-04, 3.3212e-05, 0.0000e+00,  ..., 2.6791e-03, 0.0000e+00,
         0.0000e+00],
        [1.5040e-04, 3.3212e-05, 0.0000e+00,  ..., 2.6791e-03, 0.0000e+00,
         0.0000e+00],
        [1.5040e-04, 3.3212e-05, 0.0000e+00,  ..., 2.6791e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50582.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4265e-01, 1.1231e-01, 3.8817e-01,  ..., 4.5246e-02, 1.0790e-01,
         0.0000e+00],
        [1.6927e-01, 1.2838e-01, 4.6792e-01,  ..., 5.5204e-02, 1.2898e-01,
         0.0000e+00],
        [1.9278e-01, 1.4254e-01, 5.3714e-01,  ..., 6.3618e-02, 1.4749e-01,
         0.0000e+00],
        ...,
        [2.0925e-03, 2.7506e-02, 0.0000e+00,  ..., 1.5612e-04, 0.0000e+00,
         0.0000e+00],
        [2.0925e-03, 2.7506e-02, 0.0000e+00,  ..., 1.5608e-04, 0.0000e+00,
         0.0000e+00],
        [2.0925e-03, 2.7505e-02, 0.0000e+00,  ..., 1.5601e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(319041.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2615.5205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.8896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3146.5657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(751.5075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.9067, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0587],
        [ 0.0725],
        [ 0.0866],
        ...,
        [-0.4011],
        [-0.3995],
        [-0.3991]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119664.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [1.0004],
        ...,
        [1.0003],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366590.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [1.0005],
        ...,
        [1.0003],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366603.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5639e-05,  1.9950e-05, -5.9921e-04,  ...,  6.6233e-04,
          0.0000e+00, -7.5670e-04],
        [ 3.5639e-05,  1.9950e-05, -5.9921e-04,  ...,  6.6233e-04,
          0.0000e+00, -7.5670e-04],
        [ 3.5639e-05,  1.9950e-05, -5.9921e-04,  ...,  6.6233e-04,
          0.0000e+00, -7.5670e-04],
        ...,
        [ 3.5639e-05,  1.9950e-05, -5.9921e-04,  ...,  6.6233e-04,
          0.0000e+00, -7.5670e-04],
        [ 3.5639e-05,  1.9950e-05, -5.9921e-04,  ...,  6.6233e-04,
          0.0000e+00, -7.5670e-04],
        [ 3.5639e-05,  1.9950e-05, -5.9921e-04,  ...,  6.6233e-04,
          0.0000e+00, -7.5670e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(995.6772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3225, device='cuda:0')



h[100].sum tensor(48.2551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.8506, device='cuda:0')



h[200].sum tensor(23.3471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.4534e-02, 1.1250e-02, 0.0000e+00,  ..., 2.5937e-02, 0.0000e+00,
         7.8154e-03],
        [1.4274e-04, 7.9903e-05, 0.0000e+00,  ..., 2.6528e-03, 0.0000e+00,
         0.0000e+00],
        [1.4270e-04, 7.9881e-05, 0.0000e+00,  ..., 2.6521e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.4363e-04, 8.0398e-05, 0.0000e+00,  ..., 2.6692e-03, 0.0000e+00,
         0.0000e+00],
        [1.4363e-04, 8.0398e-05, 0.0000e+00,  ..., 2.6692e-03, 0.0000e+00,
         0.0000e+00],
        [1.4363e-04, 8.0398e-05, 0.0000e+00,  ..., 2.6692e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44173.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0401, 0.0504, 0.0843,  ..., 0.0068, 0.0285, 0.0000],
        [0.0193, 0.0381, 0.0342,  ..., 0.0026, 0.0134, 0.0000],
        [0.0092, 0.0322, 0.0109,  ..., 0.0008, 0.0061, 0.0000],
        ...,
        [0.0019, 0.0281, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0019, 0.0281, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0019, 0.0281, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297920.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2262.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.5347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3068.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(663.2625, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.1332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0900],
        [ 0.0335],
        [-0.0477],
        ...,
        [-0.4030],
        [-0.4015],
        [-0.4005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115533.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0011],
        [1.0005],
        ...,
        [1.0003],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366603.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0005],
        ...,
        [1.0003],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366616.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.6102e-05,  3.3936e-05, -6.0031e-04,  ...,  6.5653e-04,
          0.0000e+00, -7.5411e-04],
        [ 9.2912e-03,  7.2188e-03, -6.6116e-04,  ...,  1.5635e-02,
         -3.0135e-03,  5.2457e-03],
        [ 6.7289e-03,  5.2296e-03, -6.4431e-04,  ...,  1.1488e-02,
         -2.1792e-03,  3.5846e-03],
        ...,
        [ 3.6102e-05,  3.3936e-05, -6.0031e-04,  ...,  6.5653e-04,
          0.0000e+00, -7.5411e-04],
        [ 3.6102e-05,  3.3936e-05, -6.0031e-04,  ...,  6.5653e-04,
          0.0000e+00, -7.5411e-04],
        [ 3.6102e-05,  3.3936e-05, -6.0031e-04,  ...,  6.5653e-04,
          0.0000e+00, -7.5411e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1014.2828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.2504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3530, device='cuda:0')



h[100].sum tensor(48.5676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.9415, device='cuda:0')



h[200].sum tensor(23.4827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0221, 0.0172, 0.0000,  ..., 0.0381, 0.0000, 0.0127],
        [0.0199, 0.0155, 0.0000,  ..., 0.0346, 0.0000, 0.0113],
        [0.0460, 0.0357, 0.0000,  ..., 0.0768, 0.0000, 0.0267],
        ...,
        [0.0001, 0.0001, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0001, 0.0001, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0001, 0.0001, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45660.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0513, 0.0581, 0.1236,  ..., 0.0120, 0.0385, 0.0000],
        [0.0556, 0.0606, 0.1332,  ..., 0.0124, 0.0415, 0.0000],
        [0.0788, 0.0747, 0.1964,  ..., 0.0189, 0.0589, 0.0000],
        ...,
        [0.0018, 0.0285, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0018, 0.0285, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0018, 0.0285, 0.0000,  ..., 0.0000, 0.0015, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(307005.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2338.1873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(285.9664, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3029.4631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(682.3237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.1453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0714],
        [ 0.0773],
        [ 0.0859],
        ...,
        [-0.4027],
        [-0.4013],
        [-0.4008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113118.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0005],
        ...,
        [1.0003],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366616.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0005],
        ...,
        [1.0003],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366629.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.1028e-05,  5.0121e-05, -6.0131e-04,  ...,  6.4723e-04,
          0.0000e+00, -7.5046e-04],
        [ 4.4183e-03,  3.4489e-03, -6.3014e-04,  ...,  7.7322e-03,
         -1.4206e-03,  2.0874e-03],
        [ 4.4183e-03,  3.4489e-03, -6.3014e-04,  ...,  7.7322e-03,
         -1.4206e-03,  2.0874e-03],
        ...,
        [ 4.1028e-05,  5.0121e-05, -6.0131e-04,  ...,  6.4723e-04,
          0.0000e+00, -7.5046e-04],
        [ 4.1028e-05,  5.0121e-05, -6.0131e-04,  ...,  6.4723e-04,
          0.0000e+00, -7.5046e-04],
        [ 4.1028e-05,  5.0121e-05, -6.0131e-04,  ...,  6.4723e-04,
          0.0000e+00, -7.5046e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1070.3927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.4784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.9948, device='cuda:0')



h[100].sum tensor(49.1887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.8604, device='cuda:0')



h[200].sum tensor(24.8454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0081, 0.0064, 0.0000,  ..., 0.0155, 0.0000, 0.0037],
        [0.0081, 0.0064, 0.0000,  ..., 0.0155, 0.0000, 0.0037],
        [0.0081, 0.0064, 0.0000,  ..., 0.0155, 0.0000, 0.0037],
        ...,
        [0.0002, 0.0002, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44847.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0183, 0.0373, 0.0265,  ..., 0.0003, 0.0136, 0.0000],
        [0.0270, 0.0423, 0.0483,  ..., 0.0019, 0.0200, 0.0000],
        [0.0449, 0.0533, 0.0994,  ..., 0.0074, 0.0337, 0.0000],
        ...,
        [0.0018, 0.0286, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0018, 0.0286, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0018, 0.0286, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(299001.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2250.4248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(275.3314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3007.1853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(667.4987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.9226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0830],
        [ 0.0044],
        [ 0.0495],
        ...,
        [-0.4017],
        [-0.4003],
        [-0.3998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104961.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0005],
        ...,
        [1.0003],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366629.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0006],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366642.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4483e-05,  6.0461e-05, -6.0222e-04,  ...,  6.3481e-04,
          0.0000e+00, -7.4672e-04],
        [ 4.4483e-05,  6.0461e-05, -6.0222e-04,  ...,  6.3481e-04,
          0.0000e+00, -7.4672e-04],
        [ 4.4483e-05,  6.0461e-05, -6.0222e-04,  ...,  6.3481e-04,
          0.0000e+00, -7.4672e-04],
        ...,
        [ 4.4483e-05,  6.0461e-05, -6.0222e-04,  ...,  6.3481e-04,
          0.0000e+00, -7.4672e-04],
        [ 4.4483e-05,  6.0461e-05, -6.0222e-04,  ...,  6.3481e-04,
          0.0000e+00, -7.4672e-04],
        [ 4.4483e-05,  6.0461e-05, -6.0222e-04,  ...,  6.3481e-04,
          0.0000e+00, -7.4672e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(883.8152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.4390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1953, device='cuda:0')



h[100].sum tensor(47.6683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.4908, device='cuda:0')



h[200].sum tensor(19.2802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0002, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0002, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0026, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0026, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37879.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0287, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0020, 0.0287, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0020, 0.0288, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        ...,
        [0.0020, 0.0289, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0020, 0.0289, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0020, 0.0289, 0.0000,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(269950.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1612.4240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.9098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3342.7278, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(561.8456, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-312.2169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5398],
        [-0.5271],
        [-0.5017],
        ...,
        [-0.4021],
        [-0.4007],
        [-0.4002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139018.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0012],
        [1.0006],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366642.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0006],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366655.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4077e-05,  6.5113e-05, -6.0304e-04,  ...,  6.1534e-04,
          0.0000e+00, -7.4518e-04],
        [ 4.4077e-05,  6.5113e-05, -6.0304e-04,  ...,  6.1534e-04,
          0.0000e+00, -7.4518e-04],
        [ 4.4077e-05,  6.5113e-05, -6.0304e-04,  ...,  6.1534e-04,
          0.0000e+00, -7.4518e-04],
        ...,
        [ 4.4077e-05,  6.5113e-05, -6.0304e-04,  ...,  6.1534e-04,
          0.0000e+00, -7.4518e-04],
        [ 4.4077e-05,  6.5113e-05, -6.0304e-04,  ...,  6.1534e-04,
          0.0000e+00, -7.4518e-04],
        [ 4.4077e-05,  6.5113e-05, -6.0304e-04,  ...,  6.1534e-04,
          0.0000e+00, -7.4518e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1022.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9728, device='cuda:0')



h[100].sum tensor(48.9052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.8048, device='cuda:0')



h[200].sum tensor(23.1425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0142, 0.0000,  ..., 0.0314, 0.0000, 0.0101],
        [0.0002, 0.0003, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0003, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46329.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5852e-02, 5.4368e-02, 1.0390e-01,  ..., 7.4680e-03, 3.5092e-02,
         0.0000e+00],
        [1.9471e-02, 3.8754e-02, 3.5097e-02,  ..., 1.8991e-03, 1.5623e-02,
         0.0000e+00],
        [8.5506e-03, 3.2500e-02, 8.6503e-03,  ..., 3.8570e-05, 7.6362e-03,
         0.0000e+00],
        ...,
        [2.1371e-03, 2.9202e-02, 0.0000e+00,  ..., 0.0000e+00, 3.0762e-03,
         0.0000e+00],
        [2.1371e-03, 2.9203e-02, 0.0000e+00,  ..., 0.0000e+00, 3.0762e-03,
         0.0000e+00],
        [2.1371e-03, 2.9202e-02, 0.0000e+00,  ..., 0.0000e+00, 3.0762e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322345.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2490.5054, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.0091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3226.2761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(678.8349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.1957, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0579],
        [-0.0218],
        [-0.1249],
        ...,
        [-0.4053],
        [-0.4038],
        [-0.4033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116132.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0006],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366655.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0007],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366668.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2887e-05,  6.7391e-05, -6.0378e-04,  ...,  5.8767e-04,
          0.0000e+00, -7.4907e-04],
        [ 4.2887e-05,  6.7391e-05, -6.0378e-04,  ...,  5.8767e-04,
          0.0000e+00, -7.4907e-04],
        [ 4.2887e-05,  6.7391e-05, -6.0378e-04,  ...,  5.8767e-04,
          0.0000e+00, -7.4907e-04],
        ...,
        [ 4.2887e-05,  6.7391e-05, -6.0378e-04,  ...,  5.8767e-04,
          0.0000e+00, -7.4907e-04],
        [ 4.2887e-05,  6.7391e-05, -6.0378e-04,  ...,  5.8767e-04,
          0.0000e+00, -7.4907e-04],
        [ 4.2887e-05,  6.7391e-05, -6.0378e-04,  ...,  5.8767e-04,
          0.0000e+00, -7.4907e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(936.8137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7788, device='cuda:0')



h[100].sum tensor(48.2851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.2352, device='cuda:0')



h[200].sum tensor(20.7258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0907, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0088, 0.0070, 0.0000,  ..., 0.0163, 0.0000, 0.0048],
        [0.0002, 0.0003, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0003, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41619.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0257, 0.0432, 0.0551,  ..., 0.0043, 0.0206, 0.0000],
        [0.0085, 0.0329, 0.0121,  ..., 0.0003, 0.0078, 0.0000],
        [0.0035, 0.0300, 0.0011,  ..., 0.0000, 0.0041, 0.0000],
        ...,
        [0.0023, 0.0294, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0023, 0.0294, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0023, 0.0294, 0.0000,  ..., 0.0000, 0.0033, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(295193.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2223.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(247.4582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3318.7192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(613.8052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-359.0043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0578],
        [-0.1695],
        [-0.2960],
        ...,
        [-0.4108],
        [-0.4094],
        [-0.4090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110567.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0007],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366668.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0007],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366681.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.8356e-05,  5.7468e-05, -6.0446e-04,  ...,  5.6818e-04,
          0.0000e+00, -7.4857e-04],
        [ 2.8356e-05,  5.7468e-05, -6.0446e-04,  ...,  5.6818e-04,
          0.0000e+00, -7.4857e-04],
        [ 2.8356e-05,  5.7468e-05, -6.0446e-04,  ...,  5.6818e-04,
          0.0000e+00, -7.4857e-04],
        ...,
        [ 2.8356e-05,  5.7468e-05, -6.0446e-04,  ...,  5.6818e-04,
          0.0000e+00, -7.4857e-04],
        [ 2.8356e-05,  5.7468e-05, -6.0446e-04,  ...,  5.6818e-04,
          0.0000e+00, -7.4857e-04],
        [ 2.8356e-05,  5.7468e-05, -6.0446e-04,  ...,  5.6818e-04,
          0.0000e+00, -7.4857e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(852.7173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6657, device='cuda:0')



h[100].sum tensor(47.7156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.9074, device='cuda:0')



h[200].sum tensor(18.3200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0002, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0001, 0.0002, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0197, 0.0154, 0.0000,  ..., 0.0340, 0.0000, 0.0112],
        ...,
        [0.0001, 0.0002, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0001, 0.0002, 0.0000,  ..., 0.0023, 0.0000, 0.0000],
        [0.0001, 0.0002, 0.0000,  ..., 0.0023, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38132.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.0320, 0.0067,  ..., 0.0002, 0.0060, 0.0000],
        [0.0193, 0.0399, 0.0401,  ..., 0.0032, 0.0157, 0.0000],
        [0.0519, 0.0594, 0.1255,  ..., 0.0108, 0.0399, 0.0000],
        ...,
        [0.0024, 0.0300, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0024, 0.0300, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0024, 0.0300, 0.0000,  ..., 0.0000, 0.0033, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(278893.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1954.8884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.5700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3488.8994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(564.1987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-319.2463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1868],
        [-0.0816],
        [-0.0028],
        ...,
        [-0.4226],
        [-0.4212],
        [-0.4207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120469.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0007],
        ...,
        [1.0004],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366681.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0008],
        ...,
        [1.0004],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366694.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6842e-03,  4.4496e-03, -6.4263e-04,  ...,  9.7257e-03,
         -1.8098e-03,  2.9277e-03],
        [ 1.5715e-05,  4.5423e-05, -6.0507e-04,  ...,  5.4740e-04,
          0.0000e+00, -7.4805e-04],
        [ 1.5715e-05,  4.5423e-05, -6.0507e-04,  ...,  5.4740e-04,
          0.0000e+00, -7.4805e-04],
        ...,
        [ 1.5715e-05,  4.5423e-05, -6.0507e-04,  ...,  5.4740e-04,
          0.0000e+00, -7.4805e-04],
        [ 1.5715e-05,  4.5423e-05, -6.0507e-04,  ...,  5.4740e-04,
          0.0000e+00, -7.4805e-04],
        [ 1.5715e-05,  4.5423e-05, -6.0507e-04,  ...,  5.4740e-04,
          0.0000e+00, -7.4805e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(853.8245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6126, device='cuda:0')



h[100].sum tensor(47.9529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.7488, device='cuda:0')



h[200].sum tensor(18.3573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8747e-02, 1.4699e-02, 0.0000e+00,  ..., 3.2444e-02, 0.0000e+00,
         1.0619e-02],
        [5.7388e-03, 4.5918e-03, 0.0000e+00,  ..., 1.1383e-02, 0.0000e+00,
         2.9315e-03],
        [3.0988e-02, 2.4209e-02, 0.0000e+00,  ..., 5.2265e-02, 0.0000e+00,
         1.8555e-02],
        ...,
        [6.3394e-05, 1.8324e-04, 0.0000e+00,  ..., 2.2082e-03, 0.0000e+00,
         0.0000e+00],
        [6.3395e-05, 1.8324e-04, 0.0000e+00,  ..., 2.2083e-03, 0.0000e+00,
         0.0000e+00],
        [6.3396e-05, 1.8324e-04, 0.0000e+00,  ..., 2.2083e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38058.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0680, 0.0695, 0.1684,  ..., 0.0152, 0.0513, 0.0000],
        [0.0561, 0.0625, 0.1381,  ..., 0.0127, 0.0428, 0.0000],
        [0.0799, 0.0774, 0.2108,  ..., 0.0220, 0.0616, 0.0000],
        ...,
        [0.0026, 0.0306, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0026, 0.0306, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0026, 0.0306, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(278551.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1998.2249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.5381, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3645.7817, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(564.6860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.0004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0625],
        [ 0.0493],
        [ 0.0348],
        ...,
        [-0.4317],
        [-0.4304],
        [-0.4303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124320.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0008],
        ...,
        [1.0004],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366694.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0008],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366707.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2620e-06,  3.0072e-05, -6.0562e-04,  ...,  5.2845e-04,
          0.0000e+00, -7.4818e-04],
        [ 1.2620e-06,  3.0072e-05, -6.0562e-04,  ...,  5.2845e-04,
          0.0000e+00, -7.4818e-04],
        [ 1.2620e-06,  3.0072e-05, -6.0562e-04,  ...,  5.2845e-04,
          0.0000e+00, -7.4818e-04],
        ...,
        [ 1.2620e-06,  3.0072e-05, -6.0562e-04,  ...,  5.2845e-04,
          0.0000e+00, -7.4818e-04],
        [ 1.2620e-06,  3.0072e-05, -6.0562e-04,  ...,  5.2845e-04,
          0.0000e+00, -7.4818e-04],
        [ 1.2620e-06,  3.0072e-05, -6.0562e-04,  ...,  5.2845e-04,
          0.0000e+00, -7.4818e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(886.3774, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.6546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0842, device='cuda:0')



h[100].sum tensor(48.4757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1587, device='cuda:0')



h[200].sum tensor(19.3356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.0506e-06, 1.2036e-04, 0.0000e+00,  ..., 2.1150e-03, 0.0000e+00,
         0.0000e+00],
        [5.0556e-06, 1.2047e-04, 0.0000e+00,  ..., 2.1171e-03, 0.0000e+00,
         0.0000e+00],
        [5.0547e-06, 1.2045e-04, 0.0000e+00,  ..., 2.1167e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.0915e-06, 1.2133e-04, 0.0000e+00,  ..., 2.1321e-03, 0.0000e+00,
         0.0000e+00],
        [5.0916e-06, 1.2133e-04, 0.0000e+00,  ..., 2.1321e-03, 0.0000e+00,
         0.0000e+00],
        [5.0916e-06, 1.2133e-04, 0.0000e+00,  ..., 2.1321e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39696.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0309, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0036, 0.0314, 0.0001,  ..., 0.0000, 0.0035, 0.0000],
        [0.0110, 0.0358, 0.0167,  ..., 0.0007, 0.0088, 0.0000],
        ...,
        [0.0028, 0.0311, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0028, 0.0311, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0028, 0.0311, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(288856.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2044.4670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.6287, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3922.3020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(581.9071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-331.0387, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4019],
        [-0.2763],
        [-0.1203],
        ...,
        [-0.4497],
        [-0.4482],
        [-0.4477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146073.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0008],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366707.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(545.1025, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0009],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366720.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3885e-06,  2.5369e-05, -6.0613e-04,  ...,  5.1336e-04,
          0.0000e+00, -7.5704e-04],
        [ 1.1903e-02,  9.2721e-03, -6.8512e-04,  ...,  1.9781e-02,
         -3.7742e-03,  6.9583e-03],
        [ 6.0187e-03,  4.6997e-03, -6.4606e-04,  ...,  1.0253e-02,
         -1.9079e-03,  3.1432e-03],
        ...,
        [ 3.3885e-06,  2.5369e-05, -6.0613e-04,  ...,  5.1336e-04,
          0.0000e+00, -7.5704e-04],
        [ 3.3885e-06,  2.5369e-05, -6.0613e-04,  ...,  5.1336e-04,
          0.0000e+00, -7.5704e-04],
        [ 3.3885e-06,  2.5369e-05, -6.0613e-04,  ...,  5.1336e-04,
          0.0000e+00, -7.5704e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1045.4098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0597, device='cuda:0')



h[100].sum tensor(50.1330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.0648, device='cuda:0')



h[200].sum tensor(24.0971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.6835e-02, 1.3173e-02, 0.0000e+00,  ..., 2.9292e-02, 0.0000e+00,
         9.3917e-03],
        [1.9203e-02, 1.5013e-02, 0.0000e+00,  ..., 3.3128e-02, 0.0000e+00,
         1.0167e-02],
        [6.9152e-02, 5.3828e-02, 0.0000e+00,  ..., 1.1401e-01, 0.0000e+00,
         4.1796e-02],
        ...,
        [1.3673e-05, 1.0237e-04, 0.0000e+00,  ..., 2.0715e-03, 0.0000e+00,
         0.0000e+00],
        [1.3673e-05, 1.0237e-04, 0.0000e+00,  ..., 2.0715e-03, 0.0000e+00,
         0.0000e+00],
        [1.3674e-05, 1.0237e-04, 0.0000e+00,  ..., 2.0716e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41895.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0476, 0.0571, 0.1086,  ..., 0.0089, 0.0356, 0.0000],
        [0.0790, 0.0760, 0.1959,  ..., 0.0184, 0.0590, 0.0000],
        [0.1386, 0.1129, 0.3729,  ..., 0.0402, 0.1053, 0.0000],
        ...,
        [0.0032, 0.0311, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0032, 0.0311, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0032, 0.0311, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291501.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2303.6372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.1603, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4000.6006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(614.5446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-359.3137, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0565],
        [ 0.0777],
        [ 0.0822],
        ...,
        [-0.4574],
        [-0.4558],
        [-0.4553]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128471.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0009],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366720.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0009],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366733.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.8964e-06,  2.0404e-05, -6.0658e-04,  ...,  5.1107e-04,
          0.0000e+00, -7.6620e-04],
        [ 7.8964e-06,  2.0404e-05, -6.0658e-04,  ...,  5.1107e-04,
          0.0000e+00, -7.6620e-04],
        [ 7.8964e-06,  2.0404e-05, -6.0658e-04,  ...,  5.1107e-04,
          0.0000e+00, -7.6620e-04],
        ...,
        [ 7.8964e-06,  2.0404e-05, -6.0658e-04,  ...,  5.1107e-04,
          0.0000e+00, -7.6620e-04],
        [ 7.8964e-06,  2.0404e-05, -6.0658e-04,  ...,  5.1107e-04,
          0.0000e+00, -7.6620e-04],
        [ 7.8964e-06,  2.0404e-05, -6.0658e-04,  ...,  5.1107e-04,
          0.0000e+00, -7.6620e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1196.4026, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.6709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0701, device='cuda:0')



h[100].sum tensor(51.7457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.0752, device='cuda:0')



h[200].sum tensor(28.6348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4578, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.1604e-05, 8.1663e-05, 0.0000e+00,  ..., 2.0455e-03, 0.0000e+00,
         0.0000e+00],
        [3.1637e-05, 8.1747e-05, 0.0000e+00,  ..., 2.0476e-03, 0.0000e+00,
         0.0000e+00],
        [3.1632e-05, 8.1734e-05, 0.0000e+00,  ..., 2.0473e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.1868e-05, 8.2344e-05, 0.0000e+00,  ..., 2.0626e-03, 0.0000e+00,
         0.0000e+00],
        [3.1869e-05, 8.2346e-05, 0.0000e+00,  ..., 2.0626e-03, 0.0000e+00,
         0.0000e+00],
        [3.1869e-05, 8.2347e-05, 0.0000e+00,  ..., 2.0626e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50930.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0309, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0035, 0.0309, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0035, 0.0310, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        ...,
        [0.0035, 0.0312, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0035, 0.0312, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0035, 0.0312, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(338927.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3116.9263, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.9313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3978.0044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(737.9833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.1336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3530],
        [-0.4101],
        [-0.4592],
        ...,
        [-0.4642],
        [-0.4626],
        [-0.4622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121999.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0009],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366733.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366746.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4435e-05,  1.8464e-05, -6.0699e-04,  ...,  5.2725e-04,
          0.0000e+00, -7.7134e-04],
        [ 1.4435e-05,  1.8464e-05, -6.0699e-04,  ...,  5.2725e-04,
          0.0000e+00, -7.7134e-04],
        [ 1.4435e-05,  1.8464e-05, -6.0699e-04,  ...,  5.2725e-04,
          0.0000e+00, -7.7134e-04],
        ...,
        [ 1.4435e-05,  1.8464e-05, -6.0699e-04,  ...,  5.2725e-04,
          0.0000e+00, -7.7134e-04],
        [ 1.4435e-05,  1.8464e-05, -6.0699e-04,  ...,  5.2725e-04,
          0.0000e+00, -7.7134e-04],
        [ 1.4435e-05,  1.8464e-05, -6.0699e-04,  ...,  5.2725e-04,
          0.0000e+00, -7.7134e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1381.4128, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.3368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2753, device='cuda:0')



h[100].sum tensor(53.5459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.6680, device='cuda:0')



h[200].sum tensor(34.1714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.3491e-03, 3.4092e-03, 0.0000e+00,  ..., 9.0591e-03, 0.0000e+00,
         2.0099e-03],
        [8.6507e-03, 6.7524e-03, 0.0000e+00,  ..., 1.6027e-02, 0.0000e+00,
         4.0245e-03],
        [4.3537e-03, 3.4127e-03, 0.0000e+00,  ..., 9.0682e-03, 0.0000e+00,
         2.0120e-03],
        ...,
        [5.8263e-05, 7.4527e-05, 0.0000e+00,  ..., 2.1281e-03, 0.0000e+00,
         0.0000e+00],
        [5.8265e-05, 7.4530e-05, 0.0000e+00,  ..., 2.1282e-03, 0.0000e+00,
         0.0000e+00],
        [5.8266e-05, 7.4531e-05, 0.0000e+00,  ..., 2.1282e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53816.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0254, 0.0426, 0.0398,  ..., 0.0010, 0.0180, 0.0000],
        [0.0286, 0.0443, 0.0470,  ..., 0.0014, 0.0203, 0.0000],
        [0.0275, 0.0439, 0.0451,  ..., 0.0015, 0.0196, 0.0000],
        ...,
        [0.0038, 0.0313, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0038, 0.0313, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0038, 0.0313, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347424.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3193.2915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.6988, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4076.8491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(773.2056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-490.3370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1373],
        [ 0.1394],
        [ 0.1371],
        ...,
        [-0.4689],
        [-0.4673],
        [-0.4668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135143.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0016],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366746.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366758.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04],
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04],
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04],
        ...,
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04],
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04],
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(991.5441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2794, device='cuda:0')



h[100].sum tensor(50.7250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.7318, device='cuda:0')



h[200].sum tensor(23.2815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.7511e-05, 3.2128e-05, 0.0000e+00,  ..., 2.2479e-03, 0.0000e+00,
         0.0000e+00],
        [2.7541e-05, 3.2162e-05, 0.0000e+00,  ..., 2.2503e-03, 0.0000e+00,
         0.0000e+00],
        [2.7537e-05, 3.2157e-05, 0.0000e+00,  ..., 2.2500e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.7748e-05, 3.2403e-05, 0.0000e+00,  ..., 2.2672e-03, 0.0000e+00,
         0.0000e+00],
        [2.7749e-05, 3.2405e-05, 0.0000e+00,  ..., 2.2673e-03, 0.0000e+00,
         0.0000e+00],
        [2.7749e-05, 3.2405e-05, 0.0000e+00,  ..., 2.2673e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42474.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0365, 0.0510, 0.0786,  ..., 0.0074, 0.0270, 0.0000],
        [0.0238, 0.0434, 0.0452,  ..., 0.0037, 0.0174, 0.0000],
        [0.0180, 0.0401, 0.0290,  ..., 0.0026, 0.0133, 0.0000],
        ...,
        [0.0039, 0.0319, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0039, 0.0319, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0039, 0.0319, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(303760.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2614.3154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(234.4076, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4058.8721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(618.5413, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.1350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0483],
        [ 0.0433],
        [ 0.0419],
        ...,
        [-0.4731],
        [-0.4711],
        [-0.4681]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136765.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366758.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366758.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3632e-02,  2.6144e-02, -8.3099e-04,  ...,  5.5017e-02,
         -1.0559e-02,  2.1023e-02],
        [ 2.4151e-02,  1.8774e-02, -7.6794e-04,  ...,  3.9662e-02,
         -7.5817e-03,  1.4877e-02],
        [ 1.2745e-02,  9.9092e-03, -6.9209e-04,  ...,  2.1191e-02,
         -4.0001e-03,  7.4844e-03],
        ...,
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04],
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04],
        [ 6.8735e-06,  8.0269e-06, -6.0737e-04,  ...,  5.6162e-04,
          0.0000e+00, -7.7238e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(978.0244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9912, device='cuda:0')



h[100].sum tensor(50.6145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.8703, device='cuda:0')



h[200].sum tensor(22.9021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0308e-01, 8.0135e-02, 0.0000e+00,  ..., 1.6915e-01, 0.0000e+00,
         6.3707e-02],
        [9.4796e-02, 7.3693e-02, 0.0000e+00,  ..., 1.5573e-01, 0.0000e+00,
         5.8332e-02],
        [7.8421e-02, 6.0965e-02, 0.0000e+00,  ..., 1.2921e-01, 0.0000e+00,
         4.7718e-02],
        ...,
        [2.7748e-05, 3.2403e-05, 0.0000e+00,  ..., 2.2672e-03, 0.0000e+00,
         0.0000e+00],
        [2.7749e-05, 3.2405e-05, 0.0000e+00,  ..., 2.2673e-03, 0.0000e+00,
         0.0000e+00],
        [2.7749e-05, 3.2405e-05, 0.0000e+00,  ..., 2.2673e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40451.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1974, 0.1498, 0.5534,  ..., 0.0660, 0.1524, 0.0000],
        [0.1738, 0.1352, 0.4820,  ..., 0.0567, 0.1337, 0.0000],
        [0.1433, 0.1163, 0.3871,  ..., 0.0439, 0.1093, 0.0000],
        ...,
        [0.0039, 0.0319, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0039, 0.0319, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0039, 0.0319, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291451.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2368.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.9548, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4133.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(587.8279, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-346.4819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0420],
        [ 0.0572],
        [ 0.0742],
        ...,
        [-0.4795],
        [-0.4779],
        [-0.4774]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145476.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366758.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366771.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.7771e-02,  2.9357e-02, -8.5906e-04,  ...,  6.1776e-02,
         -1.1823e-02,  2.3708e-02],
        [ 1.1504e-02,  8.9386e-03, -6.8428e-04,  ...,  1.9232e-02,
         -3.6016e-03,  6.6835e-03],
        [-3.1610e-06, -6.6462e-06, -6.0771e-04,  ...,  5.9404e-04,
          0.0000e+00, -7.7479e-04],
        ...,
        [-3.1610e-06, -6.6462e-06, -6.0771e-04,  ...,  5.9404e-04,
          0.0000e+00, -7.7479e-04],
        [-3.1610e-06, -6.6462e-06, -6.0771e-04,  ...,  5.9404e-04,
          0.0000e+00, -7.7479e-04],
        [-3.1610e-06, -6.6462e-06, -6.0771e-04,  ...,  5.9404e-04,
          0.0000e+00, -7.7479e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(975.3806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.2680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0097, device='cuda:0')



h[100].sum tensor(50.9264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9257, device='cuda:0')



h[200].sum tensor(22.7832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1165, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0634, 0.0493, 0.0000,  ..., 0.1051, 0.0000, 0.0380],
        [0.0753, 0.0585, 0.0000,  ..., 0.1244, 0.0000, 0.0465],
        [0.0301, 0.0234, 0.0000,  ..., 0.0512, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41482.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1476, 0.1195, 0.3979,  ..., 0.0454, 0.1122, 0.0000],
        [0.1472, 0.1193, 0.3985,  ..., 0.0458, 0.1122, 0.0000],
        [0.1101, 0.0965, 0.2885,  ..., 0.0320, 0.0832, 0.0000],
        ...,
        [0.0041, 0.0324, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0041, 0.0324, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0041, 0.0324, 0.0000,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(299706.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2564.3831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(219.1338, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4051.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(605.6707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-360.4865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0285],
        [ 0.0412],
        [ 0.0538],
        ...,
        [-0.4907],
        [-0.4890],
        [-0.4885]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141362.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0010],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366771.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0011],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366783.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2288e-06, -1.5267e-05, -6.0801e-04,  ...,  6.2240e-04,
          0.0000e+00, -7.7843e-04],
        [-6.2288e-06, -1.5267e-05, -6.0801e-04,  ...,  6.2240e-04,
          0.0000e+00, -7.7843e-04],
        [-6.2288e-06, -1.5267e-05, -6.0801e-04,  ...,  6.2240e-04,
          0.0000e+00, -7.7843e-04],
        ...,
        [-6.2288e-06, -1.5267e-05, -6.0801e-04,  ...,  6.2240e-04,
          0.0000e+00, -7.7843e-04],
        [-6.2288e-06, -1.5267e-05, -6.0801e-04,  ...,  6.2240e-04,
          0.0000e+00, -7.7843e-04],
        [-6.2288e-06, -1.5267e-05, -6.0801e-04,  ...,  6.2240e-04,
          0.0000e+00, -7.7843e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1400.4042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.5192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5141, device='cuda:0')



h[100].sum tensor(54.8312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.3819, device='cuda:0')



h[200].sum tensor(34.8013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54257.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0324, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0042, 0.0324, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0042, 0.0325, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        ...,
        [0.0043, 0.0327, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0043, 0.0327, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0043, 0.0327, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355901.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3473.1069, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.0187, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3829.2778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(784.0158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-502.7014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5896],
        [-0.6179],
        [-0.6306],
        ...,
        [-0.4915],
        [-0.4897],
        [-0.4891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125515.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0011],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366783.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0011],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366796.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3658e-02,  1.0604e-02, -6.9934e-04,  ...,  2.2805e-02,
         -4.2504e-03,  8.0882e-03],
        [ 2.8474e-02,  2.2122e-02, -7.9801e-04,  ...,  4.6808e-02,
         -8.8569e-03,  1.7692e-02],
        [ 2.2749e-02,  1.7671e-02, -7.5988e-04,  ...,  3.7532e-02,
         -7.0767e-03,  1.3980e-02],
        ...,
        [-1.2410e-05, -2.4633e-05, -6.0829e-04,  ...,  6.5638e-04,
          0.0000e+00, -7.7289e-04],
        [-1.2410e-05, -2.4633e-05, -6.0829e-04,  ...,  6.5638e-04,
          0.0000e+00, -7.7289e-04],
        [-1.2410e-05, -2.4633e-05, -6.0829e-04,  ...,  6.5638e-04,
          0.0000e+00, -7.7289e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1299.0964, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.4198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6950, device='cuda:0')



h[100].sum tensor(54.5535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.9436, device='cuda:0')



h[200].sum tensor(31.8298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0700, 0.0544, 0.0000,  ..., 0.1161, 0.0000, 0.0423],
        [0.0744, 0.0578, 0.0000,  ..., 0.1233, 0.0000, 0.0452],
        [0.0920, 0.0715, 0.0000,  ..., 0.1518, 0.0000, 0.0566],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51520.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1223, 0.1050, 0.3239,  ..., 0.0369, 0.0924, 0.0000],
        [0.1483, 0.1210, 0.3988,  ..., 0.0459, 0.1124, 0.0000],
        [0.1666, 0.1324, 0.4527,  ..., 0.0527, 0.1266, 0.0000],
        ...,
        [0.0043, 0.0333, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0043, 0.0333, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0043, 0.0333, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(340708.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3294.6021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.9847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3679.1113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(750.0351, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.1954, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0338],
        [ 0.0192],
        [ 0.0038],
        ...,
        [-0.4998],
        [-0.4981],
        [-0.4976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121381.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0011],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366796.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0012],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366808.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3286e-03,  4.1229e-03, -6.4410e-04,  ...,  9.3250e-03,
         -1.6540e-03,  2.6798e-03],
        [ 1.7677e-02,  1.3724e-02, -7.2638e-04,  ...,  2.9333e-02,
         -5.4805e-03,  1.0683e-02],
        [-9.1380e-06, -2.7245e-05, -6.0854e-04,  ...,  6.7678e-04,
          0.0000e+00, -7.7946e-04],
        ...,
        [-9.1380e-06, -2.7245e-05, -6.0854e-04,  ...,  6.7678e-04,
          0.0000e+00, -7.7946e-04],
        [-9.1380e-06, -2.7245e-05, -6.0854e-04,  ...,  6.7678e-04,
          0.0000e+00, -7.7946e-04],
        [-9.1380e-06, -2.7245e-05, -6.0854e-04,  ...,  6.7678e-04,
          0.0000e+00, -7.7946e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1611.8403, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6057, device='cuda:0')



h[100].sum tensor(57.4522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(52.6352, device='cuda:0')



h[200].sum tensor(40.6986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0567, 0.0440, 0.0000,  ..., 0.0947, 0.0000, 0.0337],
        [0.0233, 0.0180, 0.0000,  ..., 0.0405, 0.0000, 0.0128],
        [0.0382, 0.0296, 0.0000,  ..., 0.0646, 0.0000, 0.0216],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59379.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1208, 0.1034, 0.3113,  ..., 0.0340, 0.0904, 0.0000],
        [0.0953, 0.0877, 0.2367,  ..., 0.0248, 0.0706, 0.0000],
        [0.0842, 0.0809, 0.2057,  ..., 0.0213, 0.0623, 0.0000],
        ...,
        [0.0045, 0.0335, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0045, 0.0335, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0045, 0.0335, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370628., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3776.9551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.6324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3508.7190, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(858.1650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-562.6743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0711],
        [ 0.0698],
        [ 0.0631],
        ...,
        [-0.5027],
        [-0.5011],
        [-0.5006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119556.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0012],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366808.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0012],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366821.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [ 5.6748e-03,  4.3878e-03, -6.4664e-04,  ...,  9.8979e-03,
         -1.7551e-03,  2.9036e-03],
        ...,
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1543.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.1078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4847, device='cuda:0')



h[100].sum tensor(57.2934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(49.2838, device='cuda:0')



h[200].sum tensor(38.5886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0057, 0.0044, 0.0000,  ..., 0.0120, 0.0000, 0.0029],
        [0.0192, 0.0149, 0.0000,  ..., 0.0339, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58848.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0391, 0.0189,  ..., 0.0015, 0.0097, 0.0000],
        [0.0312, 0.0491, 0.0582,  ..., 0.0049, 0.0222, 0.0000],
        [0.0540, 0.0627, 0.1178,  ..., 0.0107, 0.0389, 0.0000],
        ...,
        [0.0045, 0.0339, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0045, 0.0339, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0045, 0.0339, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375075.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3791.5146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.7043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3504.1531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(850.4290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-555.7209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0999],
        [ 0.0142],
        [ 0.0797],
        ...,
        [-0.5093],
        [-0.5076],
        [-0.5071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126183.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0012],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366821.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0012],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366821.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        ...,
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04],
        [-8.0044e-06, -3.0832e-05, -6.0877e-04,  ...,  6.8978e-04,
          0.0000e+00, -7.7940e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1111.6677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.4325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0276, device='cuda:0')



h[100].sum tensor(53.8139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9688, device='cuda:0')



h[200].sum tensor(26.6426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47738.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0358, 0.0022,  ..., 0.0000, 0.0058, 0.0000],
        [0.0071, 0.0349, 0.0005,  ..., 0.0000, 0.0046, 0.0000],
        [0.0134, 0.0386, 0.0144,  ..., 0.0009, 0.0092, 0.0000],
        ...,
        [0.0045, 0.0339, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0045, 0.0339, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0045, 0.0339, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(340124.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3299.6272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.6781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3566.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(697.2504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.1951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0054],
        [-0.0152],
        [ 0.0048],
        ...,
        [-0.5093],
        [-0.5076],
        [-0.5071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124187.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0019],
        [1.0012],
        ...,
        [1.0005],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366821.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0019],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366834.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.2422e-06, -3.0702e-05, -6.0898e-04,  ...,  6.8346e-04,
          0.0000e+00, -7.8029e-04],
        [-3.2422e-06, -3.0702e-05, -6.0898e-04,  ...,  6.8346e-04,
          0.0000e+00, -7.8029e-04],
        [-3.2422e-06, -3.0702e-05, -6.0898e-04,  ...,  6.8346e-04,
          0.0000e+00, -7.8029e-04],
        ...,
        [-3.2422e-06, -3.0702e-05, -6.0898e-04,  ...,  6.8346e-04,
          0.0000e+00, -7.8029e-04],
        [-3.2422e-06, -3.0702e-05, -6.0898e-04,  ...,  6.8346e-04,
          0.0000e+00, -7.8029e-04],
        [-3.2422e-06, -3.0702e-05, -6.0898e-04,  ...,  6.8346e-04,
          0.0000e+00, -7.8029e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1007.5579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8018, device='cuda:0')



h[100].sum tensor(53.4264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3041, device='cuda:0')



h[200].sum tensor(23.4485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0933, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40702.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0151, 0.0404, 0.0254,  ..., 0.0030, 0.0108, 0.0000],
        [0.0111, 0.0378, 0.0104,  ..., 0.0007, 0.0076, 0.0000],
        [0.0206, 0.0433, 0.0337,  ..., 0.0029, 0.0145, 0.0000],
        ...,
        [0.0044, 0.0342, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0044, 0.0342, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0044, 0.0342, 0.0000,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(296806.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2509.0713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.8782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3835.7280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(595.2146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-356.0376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0212],
        [-0.0148],
        [ 0.0108],
        ...,
        [-0.5149],
        [-0.5121],
        [-0.5049]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150084.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0019],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366834.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0020],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366846.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2104e-03,  4.7986e-03, -6.5054e-04,  ...,  1.0725e-02,
         -1.9036e-03,  3.2361e-03],
        [ 4.9401e-03,  3.8107e-03, -6.4207e-04,  ...,  8.6670e-03,
         -1.5138e-03,  2.4127e-03],
        [ 1.1145e-02,  8.6358e-03, -6.8345e-04,  ...,  1.8721e-02,
         -3.4174e-03,  6.4343e-03],
        ...,
        [ 5.5314e-06, -2.6560e-05, -6.0916e-04,  ...,  6.7100e-04,
          0.0000e+00, -7.8551e-04],
        [ 5.5314e-06, -2.6560e-05, -6.0916e-04,  ...,  6.7100e-04,
          0.0000e+00, -7.8551e-04],
        [ 5.5314e-06, -2.6560e-05, -6.0916e-04,  ...,  6.7100e-04,
          0.0000e+00, -7.8551e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(896.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.1464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2255, device='cuda:0')



h[100].sum tensor(52.7844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.5914, device='cuda:0')



h[200].sum tensor(20.3351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8518e-02, 1.4330e-02, 0.0000e+00,  ..., 3.2657e-02, 0.0000e+00,
         1.0415e-02],
        [4.0513e-02, 3.1381e-02, 0.0000e+00,  ..., 6.8301e-02, 0.0000e+00,
         2.3095e-02],
        [1.8541e-02, 1.4321e-02, 0.0000e+00,  ..., 3.2698e-02, 0.0000e+00,
         9.6416e-03],
        ...,
        [2.2351e-05, 0.0000e+00, 0.0000e+00,  ..., 2.7114e-03, 0.0000e+00,
         0.0000e+00],
        [2.2353e-05, 0.0000e+00, 0.0000e+00,  ..., 2.7116e-03, 0.0000e+00,
         0.0000e+00],
        [2.2354e-05, 0.0000e+00, 0.0000e+00,  ..., 2.7117e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37602.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0521, 0.0618, 0.1145,  ..., 0.0109, 0.0377, 0.0000],
        [0.0715, 0.0736, 0.1668,  ..., 0.0165, 0.0519, 0.0000],
        [0.0567, 0.0644, 0.1250,  ..., 0.0117, 0.0408, 0.0000],
        ...,
        [0.0044, 0.0342, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0044, 0.0342, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0044, 0.0342, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287649.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2378.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(167.7342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3888.3091, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(552.8146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.6416, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0917],
        [ 0.1006],
        [ 0.0875],
        ...,
        [-0.5153],
        [-0.5036],
        [-0.4667]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142538.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0020],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366846.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0020],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366859.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5149e-05, -2.0739e-05, -6.0933e-04,  ...,  6.5343e-04,
          0.0000e+00, -7.9371e-04],
        [ 1.5149e-05, -2.0739e-05, -6.0933e-04,  ...,  6.5343e-04,
          0.0000e+00, -7.9371e-04],
        [ 6.8281e-03,  5.2776e-03, -6.5477e-04,  ...,  1.1693e-02,
         -2.0831e-03,  3.6216e-03],
        ...,
        [ 1.5149e-05, -2.0739e-05, -6.0933e-04,  ...,  6.5343e-04,
          0.0000e+00, -7.9371e-04],
        [ 1.5149e-05, -2.0739e-05, -6.0933e-04,  ...,  6.5343e-04,
          0.0000e+00, -7.9371e-04],
        [ 1.5149e-05, -2.0739e-05, -6.0933e-04,  ...,  6.5343e-04,
          0.0000e+00, -7.9371e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1282.3921, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.7866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1592, device='cuda:0')



h[100].sum tensor(56.1815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.3415, device='cuda:0')



h[200].sum tensor(30.8580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.0067e-02, 1.5538e-02, 0.0000e+00,  ..., 3.5032e-02, 0.0000e+00,
         1.2171e-02],
        [6.8882e-03, 5.2889e-03, 0.0000e+00,  ..., 1.3682e-02, 0.0000e+00,
         3.6293e-03],
        [1.1877e-02, 9.1481e-03, 0.0000e+00,  ..., 2.1766e-02, 0.0000e+00,
         6.0677e-03],
        ...,
        [6.1224e-05, 0.0000e+00, 0.0000e+00,  ..., 2.6408e-03, 0.0000e+00,
         0.0000e+00],
        [6.1229e-05, 0.0000e+00, 0.0000e+00,  ..., 2.6410e-03, 0.0000e+00,
         0.0000e+00],
        [6.1232e-05, 0.0000e+00, 0.0000e+00,  ..., 2.6411e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48929.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0540, 0.0641, 0.1335,  ..., 0.0164, 0.0409, 0.0000],
        [0.0368, 0.0532, 0.0799,  ..., 0.0088, 0.0272, 0.0000],
        [0.0448, 0.0576, 0.0978,  ..., 0.0099, 0.0327, 0.0000],
        ...,
        [0.0043, 0.0341, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0043, 0.0341, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0043, 0.0341, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334052.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2994.6804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(269.5601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3867.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(706.6230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.0683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0438],
        [ 0.0622],
        [ 0.0819],
        ...,
        [-0.5272],
        [-0.5255],
        [-0.5250]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146301.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0020],
        [1.0013],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366859.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0014],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366872.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0162e-05, -1.5336e-05, -6.0948e-04,  ...,  6.3907e-04,
          0.0000e+00, -8.0144e-04],
        [ 2.0162e-05, -1.5336e-05, -6.0948e-04,  ...,  6.3907e-04,
          0.0000e+00, -8.0144e-04],
        [ 7.1346e-03,  5.5179e-03, -6.5695e-04,  ...,  1.2167e-02,
         -2.1680e-03,  3.8090e-03],
        ...,
        [ 2.0162e-05, -1.5336e-05, -6.0948e-04,  ...,  6.3907e-04,
          0.0000e+00, -8.0144e-04],
        [ 2.0162e-05, -1.5336e-05, -6.0948e-04,  ...,  6.3907e-04,
          0.0000e+00, -8.0144e-04],
        [ 2.0162e-05, -1.5336e-05, -6.0948e-04,  ...,  6.3907e-04,
          0.0000e+00, -8.0144e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1368.8590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.9424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0046, device='cuda:0')



h[100].sum tensor(57.1980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.8690, device='cuda:0')



h[200].sum tensor(33.0649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.0709e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5582e-03, 0.0000e+00,
         0.0000e+00],
        [7.2109e-03, 5.5300e-03, 0.0000e+00,  ..., 1.4115e-02, 0.0000e+00,
         3.8173e-03],
        [5.9025e-03, 4.5124e-03, 0.0000e+00,  ..., 1.1994e-02, 0.0000e+00,
         2.9694e-03],
        ...,
        [8.1493e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5831e-03, 0.0000e+00,
         0.0000e+00],
        [8.1500e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5833e-03, 0.0000e+00,
         0.0000e+00],
        [8.1504e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5834e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53400.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0098, 0.0366, 0.0079,  ..., 0.0005, 0.0071, 0.0000],
        [0.0195, 0.0422, 0.0289,  ..., 0.0022, 0.0140, 0.0000],
        [0.0272, 0.0466, 0.0479,  ..., 0.0037, 0.0196, 0.0000],
        ...,
        [0.0043, 0.0340, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0043, 0.0340, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0043, 0.0340, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(358724., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3389.9663, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(309.5099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3786.8638, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(769.6159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-491.9186, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1892],
        [-0.0370],
        [ 0.0678],
        ...,
        [-0.5334],
        [-0.5317],
        [-0.5312]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135438.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0014],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366872.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0014],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366884.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6933e-05, -1.1876e-05, -6.0962e-04,  ...,  6.3979e-04,
          0.0000e+00, -7.9799e-04],
        [ 1.6933e-05, -1.1876e-05, -6.0962e-04,  ...,  6.3979e-04,
          0.0000e+00, -7.9799e-04],
        [ 1.6933e-05, -1.1876e-05, -6.0962e-04,  ...,  6.3979e-04,
          0.0000e+00, -7.9799e-04],
        ...,
        [ 1.6933e-05, -1.1876e-05, -6.0962e-04,  ...,  6.3979e-04,
          0.0000e+00, -7.9799e-04],
        [ 1.6933e-05, -1.1876e-05, -6.0962e-04,  ...,  6.3979e-04,
          0.0000e+00, -7.9799e-04],
        [ 1.6933e-05, -1.1876e-05, -6.0962e-04,  ...,  6.3979e-04,
          0.0000e+00, -7.9799e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1607.0547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.3975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.6487, device='cuda:0')



h[100].sum tensor(59.5093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(49.7739, device='cuda:0')



h[200].sum tensor(39.2519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8569, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.7786e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5611e-03, 0.0000e+00,
         0.0000e+00],
        [6.7877e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5646e-03, 0.0000e+00,
         0.0000e+00],
        [6.7873e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5644e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8453e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5863e-03, 0.0000e+00,
         0.0000e+00],
        [6.8460e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5866e-03, 0.0000e+00,
         0.0000e+00],
        [6.8463e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5867e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58350.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.0818e-03, 3.4038e-02, 0.0000e+00,  ..., 2.0232e-05, 3.1044e-03,
         0.0000e+00],
        [4.0874e-03, 3.4085e-02, 0.0000e+00,  ..., 2.0435e-05, 3.1089e-03,
         0.0000e+00],
        [4.0966e-03, 3.4177e-02, 0.0000e+00,  ..., 3.6027e-05, 3.1244e-03,
         0.0000e+00],
        ...,
        [4.1262e-03, 3.4415e-02, 0.0000e+00,  ..., 1.0905e-05, 3.1421e-03,
         0.0000e+00],
        [4.1267e-03, 3.4420e-02, 0.0000e+00,  ..., 1.1002e-05, 3.1426e-03,
         0.0000e+00],
        [4.1270e-03, 3.4422e-02, 0.0000e+00,  ..., 1.1076e-05, 3.1429e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(370884.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3543.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.5867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3743.6323, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(839.6324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-545.2881, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6869],
        [-0.6728],
        [-0.6377],
        ...,
        [-0.5363],
        [-0.5364],
        [-0.5365]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-126519.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0014],
        ...,
        [1.0006],
        [1.0000],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366884.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0015],
        ...,
        [1.0006],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366896.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.8570e-03,  4.5425e-03, -6.4880e-04,  ...,  1.0131e-02,
         -1.7711e-03,  3.0000e-03],
        [ 5.7710e-06, -8.6882e-06, -6.0975e-04,  ...,  6.4824e-04,
          0.0000e+00, -7.9194e-04],
        [ 5.7710e-06, -8.6882e-06, -6.0975e-04,  ...,  6.4824e-04,
          0.0000e+00, -7.9194e-04],
        ...,
        [ 5.7710e-06, -8.6882e-06, -6.0975e-04,  ...,  6.4824e-04,
          0.0000e+00, -7.9194e-04],
        [ 5.7710e-06, -8.6882e-06, -6.0975e-04,  ...,  6.4824e-04,
          0.0000e+00, -7.9194e-04],
        [ 5.7710e-06, -8.6882e-06, -6.0975e-04,  ...,  6.4824e-04,
          0.0000e+00, -7.9194e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1134.4487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.3111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8855, device='cuda:0')



h[100].sum tensor(56.1578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.5440, device='cuda:0')



h[200].sum tensor(25.8983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2141, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.9579e-02, 1.5194e-02, 0.0000e+00,  ..., 3.4289e-02, 0.0000e+00,
         1.1089e-02],
        [5.8849e-03, 4.5507e-03, 0.0000e+00,  ..., 1.2098e-02, 0.0000e+00,
         3.0054e-03],
        [2.3132e-05, 0.0000e+00, 0.0000e+00,  ..., 2.5984e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.3332e-05, 0.0000e+00, 0.0000e+00,  ..., 2.6208e-03, 0.0000e+00,
         0.0000e+00],
        [2.3334e-05, 0.0000e+00, 0.0000e+00,  ..., 2.6211e-03, 0.0000e+00,
         0.0000e+00],
        [2.3336e-05, 0.0000e+00, 0.0000e+00,  ..., 2.6212e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44046.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.4151e-02, 6.4188e-02, 1.2382e-01,  ..., 1.2840e-02, 3.9768e-02,
         0.0000e+00],
        [3.0446e-02, 5.0138e-02, 6.0619e-02,  ..., 6.2769e-03, 2.2300e-02,
         0.0000e+00],
        [1.4198e-02, 4.0810e-02, 2.1123e-02,  ..., 2.4167e-03, 1.0512e-02,
         0.0000e+00],
        ...,
        [3.9415e-03, 3.5014e-02, 0.0000e+00,  ..., 1.1133e-04, 2.9915e-03,
         0.0000e+00],
        [3.9420e-03, 3.5019e-02, 0.0000e+00,  ..., 1.1146e-04, 2.9920e-03,
         0.0000e+00],
        [3.9423e-03, 3.5022e-02, 0.0000e+00,  ..., 1.1154e-04, 2.9923e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313382.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2580.0876, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(225.3524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3896.5271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(640.4951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.3199, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0716],
        [-0.0028],
        [-0.1123],
        ...,
        [-0.5551],
        [-0.5533],
        [-0.5528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147085.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0015],
        ...,
        [1.0006],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366896.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0022],
        [1.0015],
        ...,
        [1.0006],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366909., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4019e-02,  2.6458e-02, -8.3688e-04,  ...,  5.5776e-02,
         -1.0260e-02,  2.1248e-02],
        [ 6.9618e-06,  4.6648e-07, -6.0986e-04,  ...,  6.5133e-04,
          0.0000e+00, -7.9200e-04],
        [ 2.0165e-02,  1.5681e-02, -7.4441e-04,  ...,  3.3322e-02,
         -6.0810e-03,  1.2270e-02],
        ...,
        [ 6.9618e-06,  4.6648e-07, -6.0986e-04,  ...,  6.5133e-04,
          0.0000e+00, -7.9200e-04],
        [ 6.9618e-06,  4.6648e-07, -6.0986e-04,  ...,  6.5133e-04,
          0.0000e+00, -7.9200e-04],
        [ 6.9618e-06,  4.6648e-07, -6.0986e-04,  ...,  6.5133e-04,
          0.0000e+00, -7.9200e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1265.7192, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.3126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2439, device='cuda:0')



h[100].sum tensor(57.5107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.6051, device='cuda:0')



h[200].sum tensor(29.2375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.9902e-02, 4.6577e-02, 0.0000e+00,  ..., 9.9648e-02, 0.0000e+00,
         3.6421e-02],
        [8.7302e-02, 6.7891e-02, 0.0000e+00,  ..., 1.4406e-01, 0.0000e+00,
         5.3379e-02],
        [1.6524e-02, 1.2834e-02, 0.0000e+00,  ..., 2.9347e-02, 0.0000e+00,
         9.8959e-03],
        ...,
        [2.8150e-05, 1.8862e-06, 0.0000e+00,  ..., 2.6337e-03, 0.0000e+00,
         0.0000e+00],
        [2.8153e-05, 1.8864e-06, 0.0000e+00,  ..., 2.6340e-03, 0.0000e+00,
         0.0000e+00],
        [2.8155e-05, 1.8865e-06, 0.0000e+00,  ..., 2.6341e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48230.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5594e-01, 1.2861e-01, 4.3501e-01,  ..., 5.5460e-02, 1.2056e-01,
         0.0000e+00],
        [1.2699e-01, 1.1080e-01, 3.4977e-01,  ..., 4.4598e-02, 9.7953e-02,
         0.0000e+00],
        [6.1374e-02, 7.0333e-02, 1.5801e-01,  ..., 2.0320e-02, 4.7179e-02,
         0.0000e+00],
        ...,
        [3.9258e-03, 3.5149e-02, 0.0000e+00,  ..., 1.1817e-04, 3.1393e-03,
         0.0000e+00],
        [3.9263e-03, 3.5154e-02, 0.0000e+00,  ..., 1.1830e-04, 3.1399e-03,
         0.0000e+00],
        [3.9266e-03, 3.5157e-02, 0.0000e+00,  ..., 1.1839e-04, 3.1402e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(336861.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2873.6611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.3083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3809.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(698.2305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-430.8929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0181],
        [ 0.0096],
        [-0.0171],
        ...,
        [-0.5621],
        [-0.5603],
        [-0.5598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157762.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0022],
        [1.0015],
        ...,
        [1.0006],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366909., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0022],
        [1.0015],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366921.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6772e-06,  1.1905e-05, -6.0997e-04,  ...,  6.5882e-04,
          0.0000e+00, -7.9012e-04],
        [ 9.6772e-06,  1.1905e-05, -6.0997e-04,  ...,  6.5882e-04,
          0.0000e+00, -7.9012e-04],
        [ 9.5460e-03,  7.4307e-03, -6.7362e-04,  ...,  1.6116e-02,
         -2.8671e-03,  5.3891e-03],
        ...,
        [ 9.6772e-06,  1.1905e-05, -6.0997e-04,  ...,  6.5882e-04,
          0.0000e+00, -7.9012e-04],
        [ 9.6772e-06,  1.1905e-05, -6.0997e-04,  ...,  6.5882e-04,
          0.0000e+00, -7.9012e-04],
        [ 9.6772e-06,  1.1905e-05, -6.0997e-04,  ...,  6.5882e-04,
          0.0000e+00, -7.9012e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1449.9553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2294, device='cuda:0')



h[100].sum tensor(59.2225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.5411, device='cuda:0')



h[200].sum tensor(34.0528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.8741e-05, 4.7658e-05, 0.0000e+00,  ..., 2.6374e-03, 0.0000e+00,
         0.0000e+00],
        [9.5984e-03, 7.4846e-03, 0.0000e+00,  ..., 1.8136e-02, 0.0000e+00,
         5.4022e-03],
        [3.3252e-02, 2.5886e-02, 0.0000e+00,  ..., 5.6475e-02, 0.0000e+00,
         1.9938e-02],
        ...,
        [3.9135e-05, 4.8144e-05, 0.0000e+00,  ..., 2.6643e-03, 0.0000e+00,
         0.0000e+00],
        [3.9140e-05, 4.8149e-05, 0.0000e+00,  ..., 2.6646e-03, 0.0000e+00,
         0.0000e+00],
        [3.9142e-05, 4.8152e-05, 0.0000e+00,  ..., 2.6648e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55614.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2580e-02, 4.6149e-02, 4.4599e-02,  ..., 5.2943e-03, 1.7101e-02,
         0.0000e+00],
        [5.7851e-02, 6.7961e-02, 1.4373e-01,  ..., 1.7440e-02, 4.4038e-02,
         0.0000e+00],
        [1.1705e-01, 1.0481e-01, 3.1741e-01,  ..., 3.9789e-02, 8.9931e-02,
         0.0000e+00],
        ...,
        [3.9626e-03, 3.5379e-02, 0.0000e+00,  ..., 6.6801e-05, 3.2899e-03,
         0.0000e+00],
        [3.9632e-03, 3.5384e-02, 0.0000e+00,  ..., 6.6926e-05, 3.2905e-03,
         0.0000e+00],
        [3.9636e-03, 3.5387e-02, 0.0000e+00,  ..., 6.7019e-05, 3.2909e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367655.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3354.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.0422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3638.1392, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(800.2588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-511.0497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0228],
        [-0.0067],
        [-0.0040],
        ...,
        [-0.5673],
        [-0.5656],
        [-0.5650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145164.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0022],
        [1.0015],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366921.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0022],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366932.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4657e-05,  2.4319e-05, -6.1006e-04,  ...,  6.6777e-04,
          0.0000e+00, -7.9061e-04],
        [ 1.4657e-05,  2.4319e-05, -6.1006e-04,  ...,  6.6777e-04,
          0.0000e+00, -7.9061e-04],
        [ 1.4657e-05,  2.4319e-05, -6.1006e-04,  ...,  6.6777e-04,
          0.0000e+00, -7.9061e-04],
        ...,
        [ 1.4657e-05,  2.4319e-05, -6.1006e-04,  ...,  6.6777e-04,
          0.0000e+00, -7.9061e-04],
        [ 1.4657e-05,  2.4319e-05, -6.1006e-04,  ...,  6.6777e-04,
          0.0000e+00, -7.9061e-04],
        [ 1.4657e-05,  2.4319e-05, -6.1006e-04,  ...,  6.6777e-04,
          0.0000e+00, -7.9061e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(936.5043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.0282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0187, device='cuda:0')



h[100].sum tensor(55.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.9731, device='cuda:0')



h[200].sum tensor(20.0975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8944, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.8678e-05, 9.7357e-05, 0.0000e+00,  ..., 2.6733e-03, 0.0000e+00,
         0.0000e+00],
        [5.8763e-05, 9.7498e-05, 0.0000e+00,  ..., 2.6772e-03, 0.0000e+00,
         0.0000e+00],
        [2.0752e-02, 1.6197e-02, 0.0000e+00,  ..., 3.6220e-02, 0.0000e+00,
         1.2615e-02],
        ...,
        [5.9283e-05, 9.8360e-05, 0.0000e+00,  ..., 2.7009e-03, 0.0000e+00,
         0.0000e+00],
        [5.9290e-05, 9.8372e-05, 0.0000e+00,  ..., 2.7012e-03, 0.0000e+00,
         0.0000e+00],
        [5.9294e-05, 9.8378e-05, 0.0000e+00,  ..., 2.7014e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37621.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0066, 0.0367, 0.0056,  ..., 0.0007, 0.0054, 0.0000],
        [0.0152, 0.0420, 0.0283,  ..., 0.0037, 0.0120, 0.0000],
        [0.0423, 0.0589, 0.1028,  ..., 0.0132, 0.0329, 0.0000],
        ...,
        [0.0040, 0.0355, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0040, 0.0355, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0040, 0.0355, 0.0000,  ..., 0.0000, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(291001.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2187.0083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(170.9579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3808.1885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(547.9303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-315.4230, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4099],
        [-0.2159],
        [-0.0545],
        ...,
        [-0.4872],
        [-0.5468],
        [-0.5626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160609.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0022],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366932.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(977.4152, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0023],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366944.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0593e-05,  3.3949e-05, -6.1014e-04,  ...,  6.7359e-04,
          0.0000e+00, -7.9331e-04],
        [ 2.0593e-05,  3.3949e-05, -6.1014e-04,  ...,  6.7359e-04,
          0.0000e+00, -7.9331e-04],
        [ 5.8089e-03,  4.5376e-03, -6.4879e-04,  ...,  1.0057e-02,
         -1.7284e-03,  2.9564e-03],
        ...,
        [ 2.0593e-05,  3.3949e-05, -6.1014e-04,  ...,  6.7359e-04,
          0.0000e+00, -7.9331e-04],
        [ 2.0593e-05,  3.3949e-05, -6.1014e-04,  ...,  6.7359e-04,
          0.0000e+00, -7.9331e-04],
        [ 2.0593e-05,  3.3949e-05, -6.1014e-04,  ...,  6.7359e-04,
          0.0000e+00, -7.9331e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(991.2454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5287, device='cuda:0')



h[100].sum tensor(56.0346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.4978, device='cuda:0')



h[200].sum tensor(21.4973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.2445e-05, 1.3591e-04, 0.0000e+00,  ..., 2.6967e-03, 0.0000e+00,
         0.0000e+00],
        [5.8855e-03, 4.6512e-03, 0.0000e+00,  ..., 1.2107e-02, 0.0000e+00,
         2.9639e-03],
        [4.8206e-03, 3.8226e-03, 0.0000e+00,  ..., 1.0381e-02, 0.0000e+00,
         2.2740e-03],
        ...,
        [8.3303e-05, 1.3733e-04, 0.0000e+00,  ..., 2.7248e-03, 0.0000e+00,
         0.0000e+00],
        [8.3313e-05, 1.3734e-04, 0.0000e+00,  ..., 2.7251e-03, 0.0000e+00,
         0.0000e+00],
        [8.3319e-05, 1.3735e-04, 0.0000e+00,  ..., 2.7253e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38982.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0077, 0.0371, 0.0048,  ..., 0.0003, 0.0062, 0.0000],
        [0.0154, 0.0414, 0.0198,  ..., 0.0016, 0.0117, 0.0000],
        [0.0195, 0.0437, 0.0291,  ..., 0.0021, 0.0146, 0.0000],
        ...,
        [0.0042, 0.0356, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0042, 0.0356, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0046, 0.0358, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(296630.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2339.5090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(182.8732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3734.7449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(567.7733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-331.6295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3394],
        [-0.2090],
        [-0.0627],
        ...,
        [-0.5602],
        [-0.5323],
        [-0.4942]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-153603.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0023],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366944.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366956.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2977e-03,  4.9195e-03, -6.5214e-04,  ...,  1.0857e-02,
         -1.8683e-03,  3.2735e-03],
        [ 1.9437e-05,  3.4339e-05, -6.1022e-04,  ...,  6.7967e-04,
          0.0000e+00, -7.9326e-04],
        [ 6.2977e-03,  4.9195e-03, -6.5214e-04,  ...,  1.0857e-02,
         -1.8683e-03,  3.2735e-03],
        ...,
        [ 1.9437e-05,  3.4339e-05, -6.1022e-04,  ...,  6.7967e-04,
          0.0000e+00, -7.9326e-04],
        [ 1.9437e-05,  3.4339e-05, -6.1022e-04,  ...,  6.7967e-04,
          0.0000e+00, -7.9326e-04],
        [ 1.9437e-05,  3.4339e-05, -6.1022e-04,  ...,  6.7967e-04,
          0.0000e+00, -7.9326e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1595.3313, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.7319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0841, device='cuda:0')



h[100].sum tensor(61.1293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.0861, device='cuda:0')



h[200].sum tensor(37.4150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.9412e-02, 1.5182e-02, 0.0000e+00,  ..., 3.4064e-02, 0.0000e+00,
         1.0936e-02],
        [3.2131e-02, 2.5078e-02, 0.0000e+00,  ..., 5.4686e-02, 0.0000e+00,
         1.7582e-02],
        [9.8112e-03, 7.7113e-03, 0.0000e+00,  ..., 1.8504e-02, 0.0000e+00,
         5.5096e-03],
        ...,
        [7.8635e-05, 1.3893e-04, 0.0000e+00,  ..., 2.7498e-03, 0.0000e+00,
         0.0000e+00],
        [7.8645e-05, 1.3894e-04, 0.0000e+00,  ..., 2.7501e-03, 0.0000e+00,
         0.0000e+00],
        [7.8650e-05, 1.3895e-04, 0.0000e+00,  ..., 2.7503e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59201.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0596, 0.0678, 0.1352,  ..., 0.0132, 0.0442, 0.0000],
        [0.0599, 0.0677, 0.1340,  ..., 0.0125, 0.0442, 0.0000],
        [0.0380, 0.0549, 0.0775,  ..., 0.0070, 0.0282, 0.0000],
        ...,
        [0.0043, 0.0358, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0043, 0.0358, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0043, 0.0358, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381372.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3803.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.9515, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3302.8198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(855.4263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-553.3456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1278],
        [ 0.1189],
        [ 0.0836],
        ...,
        [-0.5837],
        [-0.5818],
        [-0.5812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113517.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0016],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366956.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0017],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366968.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.5988e-07,  1.8401e-05, -6.1029e-04,  ...,  7.0252e-04,
          0.0000e+00, -7.7815e-04],
        [ 7.5988e-07,  1.8401e-05, -6.1029e-04,  ...,  7.0252e-04,
          0.0000e+00, -7.7815e-04],
        [ 7.5988e-07,  1.8401e-05, -6.1029e-04,  ...,  7.0252e-04,
          0.0000e+00, -7.7815e-04],
        ...,
        [ 7.5988e-07,  1.8401e-05, -6.1029e-04,  ...,  7.0252e-04,
          0.0000e+00, -7.7815e-04],
        [ 7.5988e-07,  1.8401e-05, -6.1029e-04,  ...,  7.0252e-04,
          0.0000e+00, -7.7815e-04],
        [ 7.5988e-07,  1.8401e-05, -6.1029e-04,  ...,  7.0252e-04,
          0.0000e+00, -7.7815e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1366.1461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0289, device='cuda:0')



h[100].sum tensor(59.8329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.9519, device='cuda:0')



h[200].sum tensor(30.4544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4532, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.0423e-06, 7.3674e-05, 0.0000e+00,  ..., 2.8127e-03, 0.0000e+00,
         0.0000e+00],
        [3.0469e-06, 7.3784e-05, 0.0000e+00,  ..., 2.8169e-03, 0.0000e+00,
         0.0000e+00],
        [5.3072e-03, 4.2010e-03, 0.0000e+00,  ..., 1.1417e-02, 0.0000e+00,
         2.6565e-03],
        ...,
        [3.0747e-06, 7.4457e-05, 0.0000e+00,  ..., 2.8426e-03, 0.0000e+00,
         0.0000e+00],
        [3.0751e-06, 7.4466e-05, 0.0000e+00,  ..., 2.8430e-03, 0.0000e+00,
         0.0000e+00],
        [3.0753e-06, 7.4472e-05, 0.0000e+00,  ..., 2.8432e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50004.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0962e-02, 3.9623e-02, 5.7815e-03,  ..., 2.0898e-04, 7.8643e-03,
         0.0000e+00],
        [1.2572e-02, 4.0906e-02, 1.1570e-02,  ..., 1.1442e-03, 9.1415e-03,
         0.0000e+00],
        [2.1727e-02, 4.6575e-02, 3.6240e-02,  ..., 3.8814e-03, 1.5971e-02,
         0.0000e+00],
        ...,
        [5.2234e-03, 3.7396e-02, 5.2431e-04,  ..., 4.4097e-05, 3.8360e-03,
         0.0000e+00],
        [4.2352e-03, 3.6817e-02, 0.0000e+00,  ..., 1.9973e-05, 3.1128e-03,
         0.0000e+00],
        [4.2356e-03, 3.6821e-02, 0.0000e+00,  ..., 2.0073e-05, 3.1132e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342883.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3026.6108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(276.1225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3586.1616, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(725.8430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.9200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0697],
        [-0.0589],
        [-0.0162],
        ...,
        [-0.5362],
        [-0.5770],
        [-0.5921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156188.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0017],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366968.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0024],
        [1.0017],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366979.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1379e-05,  4.1277e-06, -6.1035e-04,  ...,  7.5397e-04,
          0.0000e+00, -7.5735e-04],
        [ 8.7856e-03,  6.8571e-03, -6.6916e-04,  ...,  1.5038e-02,
         -2.6033e-03,  4.9488e-03],
        [ 7.5429e-03,  5.8901e-03, -6.6086e-04,  ...,  1.3023e-02,
         -2.2360e-03,  4.1436e-03],
        ...,
        [-2.1379e-05,  4.1277e-06, -6.1035e-04,  ...,  7.5397e-04,
          0.0000e+00, -7.5735e-04],
        [-2.1379e-05,  4.1277e-06, -6.1035e-04,  ...,  7.5397e-04,
          0.0000e+00, -7.5735e-04],
        [-2.1379e-05,  4.1277e-06, -6.1035e-04,  ...,  7.5397e-04,
          0.0000e+00, -7.5735e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(931.4794, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.9540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.5126, device='cuda:0')



h[100].sum tensor(56.9152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.4601, device='cuda:0')



h[200].sum tensor(18.2207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.7953e-03, 6.8770e-03, 0.0000e+00,  ..., 1.7319e-02, 0.0000e+00,
         4.9543e-03],
        [2.0947e-02, 1.6349e-02, 0.0000e+00,  ..., 3.7067e-02, 0.0000e+00,
         1.2081e-02],
        [5.1783e-02, 4.0377e-02, 0.0000e+00,  ..., 8.7150e-02, 0.0000e+00,
         3.0570e-02],
        ...,
        [0.0000e+00, 1.6704e-05, 0.0000e+00,  ..., 3.0512e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6706e-05, 0.0000e+00,  ..., 3.0516e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6707e-05, 0.0000e+00,  ..., 3.0518e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36940.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.4885e-02, 6.2613e-02, 1.0222e-01,  ..., 1.2224e-02, 3.3197e-02,
         0.0000e+00],
        [6.3887e-02, 7.4380e-02, 1.5443e-01,  ..., 1.8279e-02, 4.7541e-02,
         0.0000e+00],
        [9.6991e-02, 9.5053e-02, 2.4692e-01,  ..., 2.9406e-02, 7.2678e-02,
         0.0000e+00],
        ...,
        [4.2052e-03, 3.7895e-02, 0.0000e+00,  ..., 2.4430e-05, 2.7189e-03,
         0.0000e+00],
        [4.2060e-03, 3.7902e-02, 0.0000e+00,  ..., 2.4569e-05, 2.7196e-03,
         0.0000e+00],
        [4.2064e-03, 3.7905e-02, 0.0000e+00,  ..., 2.4668e-05, 2.7199e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(292292.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2193.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(154.8690, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3584.3132, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(546.4743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-306.2736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0209],
        [ 0.0299],
        [ 0.0375],
        ...,
        [-0.6071],
        [-0.6047],
        [-0.6003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177390.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0024],
        [1.0017],
        ...,
        [1.0007],
        [1.0001],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366979.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0024],
        [1.0018],
        ...,
        [1.0007],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366991.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1254e-02,  8.7912e-03, -6.8579e-04,  ...,  1.9104e-02,
         -3.3253e-03,  6.5653e-03],
        [ 1.9127e-02,  1.4918e-02, -7.3837e-04,  ...,  3.1876e-02,
         -5.6446e-03,  1.1666e-02],
        [ 1.9913e-02,  1.5529e-02, -7.4362e-04,  ...,  3.3150e-02,
         -5.8760e-03,  1.2175e-02],
        ...,
        [-3.3375e-05,  7.6437e-06, -6.1041e-04,  ...,  7.9265e-04,
          0.0000e+00, -7.4817e-04],
        [-3.3375e-05,  7.6437e-06, -6.1041e-04,  ...,  7.9265e-04,
          0.0000e+00, -7.4817e-04],
        [-3.3375e-05,  7.6437e-06, -6.1041e-04,  ...,  7.9265e-04,
          0.0000e+00, -7.4817e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1127.3456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.8737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8252, device='cuda:0')



h[100].sum tensor(58.5863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3740, device='cuda:0')



h[200].sum tensor(23.4459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.9487e-02, 4.6425e-02, 0.0000e+00,  ..., 9.9893e-02, 0.0000e+00,
         3.5634e-02],
        [7.9400e-02, 6.1921e-02, 0.0000e+00,  ..., 1.3220e-01, 0.0000e+00,
         4.8531e-02],
        [7.0148e-02, 5.4721e-02, 0.0000e+00,  ..., 1.1719e-01, 0.0000e+00,
         4.2537e-02],
        ...,
        [0.0000e+00, 3.0937e-05, 0.0000e+00,  ..., 3.2082e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0941e-05, 0.0000e+00,  ..., 3.2086e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0944e-05, 0.0000e+00,  ..., 3.2088e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43451.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1464, 0.1268, 0.3886,  ..., 0.0474, 0.1107, 0.0000],
        [0.1544, 0.1323, 0.4156,  ..., 0.0519, 0.1173, 0.0000],
        [0.1308, 0.1175, 0.3486,  ..., 0.0435, 0.0993, 0.0000],
        ...,
        [0.0043, 0.0383, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0043, 0.0383, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0043, 0.0383, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324106.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2694.9043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(210.4201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3274.4072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(639.2902, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-379.1475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0529],
        [ 0.0430],
        [ 0.0295],
        ...,
        [-0.6113],
        [-0.6095],
        [-0.6090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170869.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0024],
        [1.0018],
        ...,
        [1.0007],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366991.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0025],
        [1.0018],
        ...,
        [1.0007],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367002.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4544e-02,  1.1368e-02, -7.0784e-04,  ...,  2.4468e-02,
         -4.2809e-03,  8.6984e-03],
        [ 8.5234e-03,  6.6822e-03, -6.6763e-04,  ...,  1.4699e-02,
         -2.5132e-03,  4.7977e-03],
        [ 3.0256e-02,  2.3594e-02, -8.1277e-04,  ...,  4.9958e-02,
         -8.8934e-03,  1.8877e-02],
        ...,
        [-3.7053e-05,  2.0384e-05, -6.1046e-04,  ...,  8.1067e-04,
          0.0000e+00, -7.4832e-04],
        [-3.7053e-05,  2.0384e-05, -6.1046e-04,  ...,  8.1067e-04,
          0.0000e+00, -7.4832e-04],
        [-3.7053e-05,  2.0384e-05, -6.1046e-04,  ...,  8.1067e-04,
          0.0000e+00, -7.4832e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1183.1150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.8179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3223, device='cuda:0')



h[100].sum tensor(58.9361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8603, device='cuda:0')



h[200].sum tensor(25.1906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.2900e-02, 3.3582e-02, 0.0000e+00,  ..., 7.3090e-02, 0.0000e+00,
         2.4894e-02],
        [7.5723e-02, 5.9125e-02, 0.0000e+00,  ..., 1.2635e-01, 0.0000e+00,
         4.6153e-02],
        [3.3271e-02, 2.6060e-02, 0.0000e+00,  ..., 5.7412e-02, 0.0000e+00,
         1.9377e-02],
        ...,
        [0.0000e+00, 8.2514e-05, 0.0000e+00,  ..., 3.2816e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.2525e-05, 0.0000e+00,  ..., 3.2820e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.2532e-05, 0.0000e+00,  ..., 3.2822e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43854.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0789, 0.0845, 0.1972,  ..., 0.0233, 0.0594, 0.0000],
        [0.1065, 0.1022, 0.2792,  ..., 0.0345, 0.0810, 0.0000],
        [0.0871, 0.0900, 0.2221,  ..., 0.0268, 0.0659, 0.0000],
        ...,
        [0.0044, 0.0383, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0045, 0.0383, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0045, 0.0383, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(319361.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2720.9751, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.5414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3096.9282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(646.6365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.6498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0468],
        [-0.0193],
        [-0.0020],
        ...,
        [-0.6133],
        [-0.6115],
        [-0.6110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154035.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0025],
        [1.0018],
        ...,
        [1.0007],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367002.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0025],
        [1.0019],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367014.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.0929e-05,  4.6364e-05, -6.1050e-04,  ...,  8.0125e-04,
          0.0000e+00, -7.5848e-04],
        [-3.0929e-05,  4.6364e-05, -6.1050e-04,  ...,  8.0125e-04,
          0.0000e+00, -7.5848e-04],
        [-3.0929e-05,  4.6364e-05, -6.1050e-04,  ...,  8.0125e-04,
          0.0000e+00, -7.5848e-04],
        ...,
        [-3.0929e-05,  4.6364e-05, -6.1050e-04,  ...,  8.0125e-04,
          0.0000e+00, -7.5848e-04],
        [-3.0929e-05,  4.6364e-05, -6.1050e-04,  ...,  8.0125e-04,
          0.0000e+00, -7.5848e-04],
        [-3.0929e-05,  4.6364e-05, -6.1050e-04,  ...,  8.0125e-04,
          0.0000e+00, -7.5848e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1119.3059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4176, device='cuda:0')



h[100].sum tensor(58.1344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1553, device='cuda:0')



h[200].sum tensor(24.0471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0055, 0.0000,  ..., 0.0143, 0.0000, 0.0037],
        [0.0000, 0.0002, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41612.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0250, 0.0493, 0.0422,  ..., 0.0031, 0.0185, 0.0000],
        [0.0122, 0.0418, 0.0132,  ..., 0.0007, 0.0091, 0.0000],
        [0.0063, 0.0384, 0.0003,  ..., 0.0000, 0.0047, 0.0000],
        ...,
        [0.0046, 0.0378, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0046, 0.0378, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0046, 0.0378, 0.0000,  ..., 0.0000, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(308737.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2616.8391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(196.8160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3100.3391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(612.5683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.3112, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0051],
        [-0.0724],
        [-0.1553],
        ...,
        [-0.6130],
        [-0.6113],
        [-0.6108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145267.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0025],
        [1.0019],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367014.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0026],
        [1.0019],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367026.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7364e-03,  3.7726e-03, -6.4230e-04,  ...,  8.4786e-03,
         -1.3862e-03,  2.3030e-03],
        [ 4.7364e-03,  3.7726e-03, -6.4230e-04,  ...,  8.4786e-03,
         -1.3862e-03,  2.3030e-03],
        [-1.8220e-05,  7.1865e-05, -6.1055e-04,  ...,  7.6605e-04,
          0.0000e+00, -7.7570e-04],
        ...,
        [-1.8220e-05,  7.1865e-05, -6.1055e-04,  ...,  7.6605e-04,
          0.0000e+00, -7.7570e-04],
        [-1.8220e-05,  7.1865e-05, -6.1055e-04,  ...,  7.6605e-04,
          0.0000e+00, -7.7570e-04],
        [-1.8220e-05,  7.1865e-05, -6.1055e-04,  ...,  7.6605e-04,
          0.0000e+00, -7.7570e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1870.8317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.5548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.5716, device='cuda:0')



h[100].sum tensor(63.5967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.5228, device='cuda:0')



h[200].sum tensor(44.3675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0302, 0.0239, 0.0000,  ..., 0.0522, 0.0000, 0.0165],
        [0.0179, 0.0142, 0.0000,  ..., 0.0322, 0.0000, 0.0093],
        [0.0357, 0.0282, 0.0000,  ..., 0.0612, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66662.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0774, 0.0799, 0.1773,  ..., 0.0164, 0.0574, 0.0000],
        [0.0704, 0.0757, 0.1588,  ..., 0.0144, 0.0522, 0.0000],
        [0.0807, 0.0825, 0.1890,  ..., 0.0184, 0.0601, 0.0000],
        ...,
        [0.0049, 0.0371, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0049, 0.0371, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0049, 0.0371, 0.0000,  ..., 0.0000, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421746.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4224.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(424.2008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3109.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(951.4506, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-636.1978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1300],
        [ 0.1287],
        [ 0.1195],
        ...,
        [-0.6155],
        [-0.6138],
        [-0.6133]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146350.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0026],
        [1.0019],
        ...,
        [1.0006],
        [1.0000],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367026.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0020],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367038.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.8208e-06,  8.7010e-05, -6.1058e-04,  ...,  7.2775e-04,
          0.0000e+00, -7.9026e-04],
        [-7.8208e-06,  8.7010e-05, -6.1058e-04,  ...,  7.2775e-04,
          0.0000e+00, -7.9026e-04],
        [-7.8208e-06,  8.7010e-05, -6.1058e-04,  ...,  7.2775e-04,
          0.0000e+00, -7.9026e-04],
        ...,
        [-7.8208e-06,  8.7010e-05, -6.1058e-04,  ...,  7.2775e-04,
          0.0000e+00, -7.9026e-04],
        [-7.8208e-06,  8.7010e-05, -6.1058e-04,  ...,  7.2775e-04,
          0.0000e+00, -7.9026e-04],
        [-7.8208e-06,  8.7010e-05, -6.1058e-04,  ...,  7.2775e-04,
          0.0000e+00, -7.9026e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1376.0188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5896, device='cuda:0')



h[100].sum tensor(59.6435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.6385, device='cuda:0')



h[200].sum tensor(31.3079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0211, 0.0168, 0.0000,  ..., 0.0372, 0.0000, 0.0129],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49737.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0145, 0.0420, 0.0175,  ..., 0.0012, 0.0110, 0.0000],
        [0.0248, 0.0484, 0.0485,  ..., 0.0051, 0.0189, 0.0000],
        [0.0539, 0.0666, 0.1288,  ..., 0.0149, 0.0412, 0.0000],
        ...,
        [0.0050, 0.0367, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0050, 0.0367, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0050, 0.0367, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343716.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3218.9004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(275.2993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3318.1045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(718.3664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-455.7825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0667],
        [ 0.0664],
        [ 0.0729],
        ...,
        [-0.5540],
        [-0.5746],
        [-0.5934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132728.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0020],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367038.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0020],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367050.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8294e-06,  8.1333e-05, -6.1062e-04,  ...,  6.9598e-04,
          0.0000e+00, -7.9782e-04],
        [-4.8294e-06,  8.1333e-05, -6.1062e-04,  ...,  6.9598e-04,
          0.0000e+00, -7.9782e-04],
        [-4.8294e-06,  8.1333e-05, -6.1062e-04,  ...,  6.9598e-04,
          0.0000e+00, -7.9782e-04],
        ...,
        [ 1.2841e-02,  1.0081e-02, -6.9640e-04,  ...,  2.1529e-02,
         -3.7194e-03,  7.5183e-03],
        [-4.8294e-06,  8.1333e-05, -6.1062e-04,  ...,  6.9598e-04,
          0.0000e+00, -7.9782e-04],
        [-4.8294e-06,  8.1333e-05, -6.1062e-04,  ...,  6.9598e-04,
          0.0000e+00, -7.9782e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1587.0509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.5854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0756, device='cuda:0')



h[100].sum tensor(61.3330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.0710, device='cuda:0')



h[200].sum tensor(36.3514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6815, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        ...,
        [0.0210, 0.0167, 0.0000,  ..., 0.0369, 0.0000, 0.0112],
        [0.0183, 0.0146, 0.0000,  ..., 0.0325, 0.0000, 0.0102],
        [0.0000, 0.0003, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58701.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0082, 0.0383, 0.0055,  ..., 0.0002, 0.0060, 0.0000],
        [0.0048, 0.0364, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0048, 0.0365, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        ...,
        [0.0665, 0.0736, 0.1521,  ..., 0.0148, 0.0493, 0.0000],
        [0.0495, 0.0636, 0.1093,  ..., 0.0107, 0.0369, 0.0000],
        [0.0199, 0.0458, 0.0340,  ..., 0.0031, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391203.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3865.8464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.1740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3401.8159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(842.8011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-551.2604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1814],
        [-0.3146],
        [-0.4012],
        ...,
        [ 0.0650],
        [-0.0126],
        [-0.1700]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132871.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0020],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367050.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(525.6329, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0028],
        [1.0021],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367062.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7448e-03,  3.7691e-03, -6.4241e-04,  ...,  8.4023e-03,
         -1.3722e-03,  2.2866e-03],
        [ 6.0879e-03,  4.8147e-03, -6.5138e-04,  ...,  1.0581e-02,
         -1.7598e-03,  3.1563e-03],
        [-1.0275e-05,  6.7598e-05, -6.1065e-04,  ...,  6.8999e-04,
          0.0000e+00, -7.9243e-04],
        ...,
        [-1.0275e-05,  6.7598e-05, -6.1065e-04,  ...,  6.8999e-04,
          0.0000e+00, -7.9243e-04],
        [-1.0275e-05,  6.7598e-05, -6.1065e-04,  ...,  6.8999e-04,
          0.0000e+00, -7.9243e-04],
        [-1.0275e-05,  6.7598e-05, -6.1065e-04,  ...,  6.8999e-04,
          0.0000e+00, -7.9243e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1356.5679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.1372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6601, device='cuda:0')



h[100].sum tensor(59.8097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.8493, device='cuda:0')



h[200].sum tensor(29.5116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0472, 0.0370, 0.0000,  ..., 0.0793, 0.0000, 0.0274],
        [0.0194, 0.0154, 0.0000,  ..., 0.0342, 0.0000, 0.0110],
        [0.0061, 0.0050, 0.0000,  ..., 0.0127, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49132.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0874, 0.0874, 0.2128,  ..., 0.0223, 0.0651, 0.0000],
        [0.0600, 0.0706, 0.1388,  ..., 0.0140, 0.0445, 0.0000],
        [0.0329, 0.0542, 0.0663,  ..., 0.0063, 0.0243, 0.0000],
        ...,
        [0.0046, 0.0374, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0046, 0.0375, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0046, 0.0375, 0.0000,  ..., 0.0000, 0.0032, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(346096.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3130.4280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.5430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3560.2236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(714.1301, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.7404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0861],
        [ 0.0620],
        [-0.0103],
        ...,
        [-0.6539],
        [-0.6520],
        [-0.6514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146389.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0028],
        [1.0021],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367062.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0029],
        [1.0021],
        ...,
        [1.0006],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367073.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9390e-05,  5.3186e-05, -6.1068e-04,  ...,  6.9821e-04,
          0.0000e+00, -7.7730e-04],
        [-1.9390e-05,  5.3186e-05, -6.1068e-04,  ...,  6.9821e-04,
          0.0000e+00, -7.7730e-04],
        [ 4.7441e-03,  3.7613e-03, -6.4249e-04,  ...,  8.4256e-03,
         -1.3701e-03,  2.3083e-03],
        ...,
        [-1.9390e-05,  5.3186e-05, -6.1068e-04,  ...,  6.9821e-04,
          0.0000e+00, -7.7730e-04],
        [-1.9390e-05,  5.3186e-05, -6.1068e-04,  ...,  6.9821e-04,
          0.0000e+00, -7.7730e-04],
        [-1.9390e-05,  5.3186e-05, -6.1068e-04,  ...,  6.9821e-04,
          0.0000e+00, -7.7730e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1643.4585, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.5000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9019, device='cuda:0')



h[100].sum tensor(62.2728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.5412, device='cuda:0')



h[200].sum tensor(36.1079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0032, 0.0000,  ..., 0.0091, 0.0000, 0.0017],
        [0.0200, 0.0158, 0.0000,  ..., 0.0353, 0.0000, 0.0107],
        [0.0346, 0.0272, 0.0000,  ..., 0.0591, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0028, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0028, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59945.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0309, 0.0535, 0.0584,  ..., 0.0046, 0.0223, 0.0000],
        [0.0580, 0.0702, 0.1300,  ..., 0.0117, 0.0423, 0.0000],
        [0.0790, 0.0837, 0.1884,  ..., 0.0187, 0.0580, 0.0000],
        ...,
        [0.0042, 0.0385, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0042, 0.0385, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0042, 0.0385, 0.0000,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(395923.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3788.1528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(359.9730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3410.5447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(871.7084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-562.2429, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0444],
        [ 0.0442],
        [ 0.0775],
        ...,
        [-0.6727],
        [-0.6707],
        [-0.6702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144659.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0029],
        [1.0021],
        ...,
        [1.0006],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367073.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0029],
        [1.0022],
        ...,
        [1.0006],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367084.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0689e-05,  4.9253e-05, -6.1070e-04,  ...,  7.1451e-04,
          0.0000e+00, -7.6565e-04],
        [-2.0689e-05,  4.9253e-05, -6.1070e-04,  ...,  7.1451e-04,
          0.0000e+00, -7.6565e-04],
        [-2.0689e-05,  4.9253e-05, -6.1070e-04,  ...,  7.1451e-04,
          0.0000e+00, -7.6565e-04],
        ...,
        [-2.0689e-05,  4.9253e-05, -6.1070e-04,  ...,  7.1451e-04,
          0.0000e+00, -7.6565e-04],
        [-2.0689e-05,  4.9253e-05, -6.1070e-04,  ...,  7.1451e-04,
          0.0000e+00, -7.6565e-04],
        [-2.0689e-05,  4.9253e-05, -6.1070e-04,  ...,  7.1451e-04,
          0.0000e+00, -7.6565e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1157.3242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.8315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0240, device='cuda:0')



h[100].sum tensor(58.6499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9684, device='cuda:0')



h[200].sum tensor(22.7467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43913.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0387, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0039, 0.0387, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0039, 0.0388, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        ...,
        [0.0039, 0.0392, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0039, 0.0392, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0039, 0.0392, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(328653.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2564.0662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.5426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3752.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(647.5607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.9218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7823],
        [-0.6983],
        [-0.5764],
        ...,
        [-0.6853],
        [-0.6833],
        [-0.6827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180935.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0029],
        [1.0022],
        ...,
        [1.0006],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367084.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0030],
        [1.0022],
        ...,
        [1.0006],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367095.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6214e-05,  5.9576e-05, -6.1072e-04,  ...,  7.3809e-04,
          0.0000e+00, -7.6244e-04],
        [-1.6214e-05,  5.9576e-05, -6.1072e-04,  ...,  7.3809e-04,
          0.0000e+00, -7.6244e-04],
        [ 5.7691e-03,  4.5633e-03, -6.4937e-04,  ...,  1.0126e-02,
         -1.6528e-03,  2.9868e-03],
        ...,
        [-1.6214e-05,  5.9576e-05, -6.1072e-04,  ...,  7.3809e-04,
          0.0000e+00, -7.6244e-04],
        [-1.6214e-05,  5.9576e-05, -6.1072e-04,  ...,  7.3809e-04,
          0.0000e+00, -7.6244e-04],
        [-1.6214e-05,  5.9576e-05, -6.1072e-04,  ...,  7.3809e-04,
          0.0000e+00, -7.6244e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1358.9414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3575, device='cuda:0')



h[100].sum tensor(60.0706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.9448, device='cuda:0')



h[200].sum tensor(27.9642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0058, 0.0048, 0.0000,  ..., 0.0124, 0.0000, 0.0030],
        [0.0086, 0.0070, 0.0000,  ..., 0.0170, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50028.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0094, 0.0422, 0.0102,  ..., 0.0002, 0.0068, 0.0000],
        [0.0230, 0.0504, 0.0407,  ..., 0.0028, 0.0167, 0.0000],
        [0.0355, 0.0581, 0.0727,  ..., 0.0054, 0.0259, 0.0000],
        ...,
        [0.0038, 0.0394, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0038, 0.0394, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0038, 0.0394, 0.0000,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353264.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3033.1401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(269.5139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3439.7759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(739.0079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-453.7044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4231],
        [-0.2184],
        [-0.0586],
        ...,
        [-0.6905],
        [-0.6885],
        [-0.6879]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147304.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0030],
        [1.0022],
        ...,
        [1.0006],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367095.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0031],
        [1.0023],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367107.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2105e-07,  8.2164e-05, -6.1075e-04,  ...,  7.6431e-04,
          0.0000e+00, -7.7127e-04],
        [ 6.2105e-07,  8.2164e-05, -6.1075e-04,  ...,  7.6431e-04,
          0.0000e+00, -7.7127e-04],
        [ 6.2105e-07,  8.2164e-05, -6.1075e-04,  ...,  7.6431e-04,
          0.0000e+00, -7.7127e-04],
        ...,
        [ 6.2105e-07,  8.2164e-05, -6.1075e-04,  ...,  7.6431e-04,
          0.0000e+00, -7.7127e-04],
        [ 6.2105e-07,  8.2164e-05, -6.1075e-04,  ...,  7.6431e-04,
          0.0000e+00, -7.7127e-04],
        [ 6.2105e-07,  8.2164e-05, -6.1075e-04,  ...,  7.6431e-04,
          0.0000e+00, -7.7127e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1296.8486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4069, device='cuda:0')



h[100].sum tensor(59.3207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.1028, device='cuda:0')



h[200].sum tensor(26.8074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.4875e-06, 3.2909e-04, 0.0000e+00,  ..., 3.0613e-03, 0.0000e+00,
         0.0000e+00],
        [2.4920e-06, 3.2969e-04, 0.0000e+00,  ..., 3.0669e-03, 0.0000e+00,
         0.0000e+00],
        [1.0866e-02, 8.7875e-03, 0.0000e+00,  ..., 2.0698e-02, 0.0000e+00,
         5.4925e-03],
        ...,
        [2.5173e-06, 3.3303e-04, 0.0000e+00,  ..., 3.0979e-03, 0.0000e+00,
         0.0000e+00],
        [2.5176e-06, 3.3307e-04, 0.0000e+00,  ..., 3.0983e-03, 0.0000e+00,
         0.0000e+00],
        [2.5178e-06, 3.3309e-04, 0.0000e+00,  ..., 3.0986e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46016.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0398, 0.0030,  ..., 0.0000, 0.0049, 0.0000],
        [0.0138, 0.0443, 0.0198,  ..., 0.0009, 0.0103, 0.0000],
        [0.0325, 0.0556, 0.0643,  ..., 0.0045, 0.0240, 0.0000],
        ...,
        [0.0040, 0.0389, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0040, 0.0389, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0040, 0.0389, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(333388.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2717.6057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.0906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3464.3120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(679.3536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.9292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4452],
        [-0.2467],
        [-0.0693],
        ...,
        [-0.6698],
        [-0.6639],
        [-0.6616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151899.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0031],
        [1.0023],
        ...,
        [1.0006],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367107.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0032],
        [1.0024],
        ...,
        [1.0007],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367118.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0966e-02,  2.4196e-02, -8.1744e-04,  ...,  5.1001e-02,
         -8.7782e-03,  1.9263e-02],
        [ 7.7750e-03,  6.1400e-03, -6.6254e-04,  ...,  1.3362e-02,
         -2.1989e-03,  4.2377e-03],
        [ 2.5172e-02,  1.9684e-02, -7.7874e-04,  ...,  4.1596e-02,
         -7.1343e-03,  1.5508e-02],
        ...,
        [ 2.4043e-05,  1.0534e-04, -6.1076e-04,  ...,  7.8293e-04,
          0.0000e+00, -7.8398e-04],
        [ 2.4043e-05,  1.0534e-04, -6.1076e-04,  ...,  7.8293e-04,
          0.0000e+00, -7.8398e-04],
        [ 2.4043e-05,  1.0534e-04, -6.1076e-04,  ...,  7.8293e-04,
          0.0000e+00, -7.8398e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1889.9841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.3494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.2221, device='cuda:0')



h[100].sum tensor(63.7020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(54.4778, device='cuda:0')



h[200].sum tensor(42.6249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.5646e-02, 4.3671e-02, 0.0000e+00,  ..., 9.3291e-02, 0.0000e+00,
         3.2849e-02],
        [1.2514e-01, 9.7776e-02, 0.0000e+00,  ..., 2.0608e-01, 0.0000e+00,
         7.7865e-02],
        [7.9381e-02, 6.2151e-02, 0.0000e+00,  ..., 1.3182e-01, 0.0000e+00,
         4.8221e-02],
        ...,
        [9.7468e-05, 4.2704e-04, 0.0000e+00,  ..., 3.1739e-03, 0.0000e+00,
         0.0000e+00],
        [9.7480e-05, 4.2709e-04, 0.0000e+00,  ..., 3.1743e-03, 0.0000e+00,
         0.0000e+00],
        [9.7487e-05, 4.2712e-04, 0.0000e+00,  ..., 3.1745e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65630.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1559, 0.1336, 0.4248,  ..., 0.0519, 0.1200, 0.0000],
        [0.2071, 0.1670, 0.5815,  ..., 0.0745, 0.1604, 0.0000],
        [0.1903, 0.1563, 0.5303,  ..., 0.0672, 0.1471, 0.0000],
        ...,
        [0.0043, 0.0383, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0043, 0.0383, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0043, 0.0383, 0.0000,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(416505.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4082.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(409.5286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3081.1125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(952.0019, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.7717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0377],
        [ 0.0251],
        [ 0.0139],
        ...,
        [-0.6675],
        [-0.6579],
        [-0.6493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122718.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0032],
        [1.0024],
        ...,
        [1.0007],
        [1.0000],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367118.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0032],
        [1.0024],
        ...,
        [1.0007],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367130.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5698e-05,  1.1618e-04, -6.1078e-04,  ...,  7.9816e-04,
          0.0000e+00, -7.9380e-04],
        [ 1.7918e-02,  1.4039e-02, -7.3022e-04,  ...,  2.9821e-02,
         -5.0555e-03,  1.0790e-02],
        [ 3.5698e-05,  1.1618e-04, -6.1078e-04,  ...,  7.9816e-04,
          0.0000e+00, -7.9380e-04],
        ...,
        [ 3.5698e-05,  1.1618e-04, -6.1078e-04,  ...,  7.9816e-04,
          0.0000e+00, -7.9380e-04],
        [ 3.5698e-05,  1.1618e-04, -6.1078e-04,  ...,  7.9816e-04,
          0.0000e+00, -7.9380e-04],
        [ 3.5698e-05,  1.1618e-04, -6.1078e-04,  ...,  7.9816e-04,
          0.0000e+00, -7.9380e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1234.2896, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.3222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4223, device='cuda:0')



h[100].sum tensor(58.7578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1590, device='cuda:0')



h[200].sum tensor(25.6743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1625, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0144, 0.0000,  ..., 0.0323, 0.0000, 0.0108],
        [0.0148, 0.0119, 0.0000,  ..., 0.0270, 0.0000, 0.0087],
        [0.0653, 0.0512, 0.0000,  ..., 0.1090, 0.0000, 0.0390],
        ...,
        [0.0001, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44893.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0354, 0.0571, 0.0802,  ..., 0.0084, 0.0276, 0.0000],
        [0.0443, 0.0627, 0.1048,  ..., 0.0114, 0.0343, 0.0000],
        [0.0725, 0.0807, 0.1850,  ..., 0.0214, 0.0560, 0.0000],
        ...,
        [0.0045, 0.0380, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0045, 0.0380, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0045, 0.0380, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(330412.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2755.7563, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(225.7266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3441.4888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(660.4244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.1428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2208],
        [-0.1130],
        [-0.0360],
        ...,
        [-0.6794],
        [-0.6776],
        [-0.6771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143916.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0032],
        [1.0024],
        ...,
        [1.0007],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367130.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0033],
        [1.0025],
        ...,
        [1.0007],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367141.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4490e-05,  1.1419e-04, -6.1080e-04,  ...,  8.1012e-04,
          0.0000e+00, -8.0064e-04],
        [ 3.4490e-05,  1.1419e-04, -6.1080e-04,  ...,  8.1012e-04,
          0.0000e+00, -8.0064e-04],
        [ 3.4490e-05,  1.1419e-04, -6.1080e-04,  ...,  8.1012e-04,
          0.0000e+00, -8.0064e-04],
        ...,
        [ 3.4490e-05,  1.1419e-04, -6.1080e-04,  ...,  8.1012e-04,
          0.0000e+00, -8.0064e-04],
        [ 3.4490e-05,  1.1419e-04, -6.1080e-04,  ...,  8.1012e-04,
          0.0000e+00, -8.0064e-04],
        [ 3.4490e-05,  1.1419e-04, -6.1080e-04,  ...,  8.1012e-04,
          0.0000e+00, -8.0064e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1266.2483, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6018, device='cuda:0')



h[100].sum tensor(59.0980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6957, device='cuda:0')



h[200].sum tensor(26.1935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0005, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44895.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0375, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0049, 0.0377, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0058, 0.0382, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        ...,
        [0.0046, 0.0380, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0046, 0.0380, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0046, 0.0380, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(329648.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2789.0354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.7202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3441.2517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(663.6942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.6719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4750],
        [-0.5141],
        [-0.5097],
        ...,
        [-0.6587],
        [-0.6811],
        [-0.6857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137973.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0033],
        [1.0025],
        ...,
        [1.0007],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367141.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0033],
        [1.0025],
        ...,
        [1.0007],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367152.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.8202e-05,  1.0678e-04, -6.1081e-04,  ...,  8.1641e-04,
          0.0000e+00, -8.0500e-04],
        [ 2.8202e-05,  1.0678e-04, -6.1081e-04,  ...,  8.1641e-04,
          0.0000e+00, -8.0500e-04],
        [ 4.7976e-03,  3.8201e-03, -6.4266e-04,  ...,  8.5577e-03,
         -1.3390e-03,  2.2840e-03],
        ...,
        [ 2.8202e-05,  1.0678e-04, -6.1081e-04,  ...,  8.1641e-04,
          0.0000e+00, -8.0500e-04],
        [ 2.8202e-05,  1.0678e-04, -6.1081e-04,  ...,  8.1641e-04,
          0.0000e+00, -8.0500e-04],
        [ 2.8202e-05,  1.0678e-04, -6.1081e-04,  ...,  8.1641e-04,
          0.0000e+00, -8.0500e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1186.3019, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7901, device='cuda:0')



h[100].sum tensor(58.6954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.2690, device='cuda:0')



h[200].sum tensor(23.5255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0088, 0.0072, 0.0000,  ..., 0.0174, 0.0000, 0.0040],
        [0.0125, 0.0100, 0.0000,  ..., 0.0233, 0.0000, 0.0056],
        ...,
        [0.0001, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41901.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0129, 0.0425, 0.0158,  ..., 0.0005, 0.0102, 0.0000],
        [0.0292, 0.0521, 0.0526,  ..., 0.0033, 0.0223, 0.0000],
        [0.0401, 0.0584, 0.0788,  ..., 0.0050, 0.0304, 0.0000],
        ...,
        [0.0044, 0.0382, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0044, 0.0382, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0044, 0.0382, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(317365., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2400.9653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(198.9704, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3681.4456, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(617.1080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.6659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1697],
        [-0.0193],
        [ 0.0731],
        ...,
        [-0.7031],
        [-0.7012],
        [-0.7007]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171305.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0033],
        [1.0025],
        ...,
        [1.0007],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367152.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0034],
        [1.0026],
        ...,
        [1.0008],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367164.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.4211e-05,  9.9297e-05, -6.1082e-04,  ...,  8.1671e-04,
          0.0000e+00, -8.1207e-04],
        [ 2.4211e-05,  9.9297e-05, -6.1082e-04,  ...,  8.1671e-04,
          0.0000e+00, -8.1207e-04],
        [ 5.7690e-03,  4.5720e-03, -6.4919e-04,  ...,  1.0141e-02,
         -1.6072e-03,  2.9082e-03],
        ...,
        [ 2.4211e-05,  9.9297e-05, -6.1082e-04,  ...,  8.1671e-04,
          0.0000e+00, -8.1207e-04],
        [ 1.5412e-02,  1.2080e-02, -7.1359e-04,  ...,  2.5792e-02,
         -4.3049e-03,  9.1527e-03],
        [ 2.4211e-05,  9.9297e-05, -6.1082e-04,  ...,  8.1671e-04,
          0.0000e+00, -8.1207e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1487.0488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3268, device='cuda:0')



h[100].sum tensor(61.1683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.8427, device='cuda:0')



h[200].sum tensor(30.8069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.6991e-05, 3.9780e-04, 0.0000e+00,  ..., 3.2719e-03, 0.0000e+00,
         0.0000e+00],
        [5.8627e-03, 4.8873e-03, 0.0000e+00,  ..., 1.2636e-02, 0.0000e+00,
         2.9186e-03],
        [1.9292e-02, 1.5343e-02, 0.0000e+00,  ..., 3.4433e-02, 0.0000e+00,
         1.0801e-02],
        ...,
        [1.5702e-02, 1.2552e-02, 0.0000e+00,  ..., 2.8640e-02, 0.0000e+00,
         9.2815e-03],
        [1.2840e-02, 1.0323e-02, 0.0000e+00,  ..., 2.3995e-02, 0.0000e+00,
         7.4280e-03],
        [5.6798e-02, 4.4547e-02, 0.0000e+00,  ..., 9.5342e-02, 0.0000e+00,
         3.3423e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52958.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0213, 0.0479, 0.0354,  ..., 0.0029, 0.0163, 0.0000],
        [0.0334, 0.0552, 0.0657,  ..., 0.0059, 0.0253, 0.0000],
        [0.0534, 0.0676, 0.1184,  ..., 0.0113, 0.0404, 0.0000],
        ...,
        [0.0323, 0.0557, 0.0681,  ..., 0.0075, 0.0248, 0.0000],
        [0.0403, 0.0607, 0.0900,  ..., 0.0100, 0.0309, 0.0000],
        [0.0657, 0.0768, 0.1607,  ..., 0.0187, 0.0503, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368253.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3444.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.1228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3331.4253, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(781.9963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-488.6961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0758],
        [ 0.0940],
        [ 0.1108],
        ...,
        [-0.2261],
        [-0.0859],
        [-0.0193]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-124008.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0034],
        [1.0026],
        ...,
        [1.0008],
        [1.0001],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367164.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(556.3777, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0034],
        [1.0026],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367175.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8013e-06,  7.5198e-05, -6.1083e-04,  ...,  8.3344e-04,
          0.0000e+00, -8.1077e-04],
        [-3.8013e-06,  7.5198e-05, -6.1083e-04,  ...,  8.3344e-04,
          0.0000e+00, -8.1077e-04],
        [-3.8013e-06,  7.5198e-05, -6.1083e-04,  ...,  8.3344e-04,
          0.0000e+00, -8.1077e-04],
        ...,
        [-3.8013e-06,  7.5198e-05, -6.1083e-04,  ...,  8.3344e-04,
          0.0000e+00, -8.1077e-04],
        [-3.8013e-06,  7.5198e-05, -6.1083e-04,  ...,  8.3344e-04,
          0.0000e+00, -8.1077e-04],
        [-3.8013e-06,  7.5198e-05, -6.1083e-04,  ...,  8.3344e-04,
          0.0000e+00, -8.1077e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1287.1760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9727, device='cuda:0')



h[100].sum tensor(59.9306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.8047, device='cuda:0')



h[200].sum tensor(24.7596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48128.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.5024e-03, 3.9760e-02, 2.5978e-03,  ..., 8.1852e-05, 5.0508e-03,
         0.0000e+00],
        [4.4323e-03, 3.8567e-02, 0.0000e+00,  ..., 0.0000e+00, 3.4631e-03,
         0.0000e+00],
        [4.2904e-03, 3.8589e-02, 0.0000e+00,  ..., 0.0000e+00, 3.3623e-03,
         0.0000e+00],
        ...,
        [4.3376e-03, 3.9013e-02, 0.0000e+00,  ..., 0.0000e+00, 3.4005e-03,
         0.0000e+00],
        [4.3385e-03, 3.9020e-02, 0.0000e+00,  ..., 0.0000e+00, 3.4014e-03,
         0.0000e+00],
        [4.3389e-03, 3.9024e-02, 0.0000e+00,  ..., 0.0000e+00, 3.4018e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353593.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2961.4380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(250.4831, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3511.3667, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(713.6567, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.0402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5112],
        [-0.6953],
        [-0.8222],
        ...,
        [-0.7392],
        [-0.7371],
        [-0.7366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171723.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0034],
        [1.0026],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367175.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0035],
        [1.0027],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367185.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.9184e-05,  5.5959e-05, -6.1084e-04,  ...,  8.4702e-04,
          0.0000e+00, -8.0715e-04],
        [-2.9184e-05,  5.5959e-05, -6.1084e-04,  ...,  8.4702e-04,
          0.0000e+00, -8.0715e-04],
        [-2.9184e-05,  5.5959e-05, -6.1084e-04,  ...,  8.4702e-04,
          0.0000e+00, -8.0715e-04],
        ...,
        [-2.9184e-05,  5.5959e-05, -6.1084e-04,  ...,  8.4702e-04,
          0.0000e+00, -8.0715e-04],
        [-2.9184e-05,  5.5959e-05, -6.1084e-04,  ...,  8.4702e-04,
          0.0000e+00, -8.0715e-04],
        [-2.9184e-05,  5.5959e-05, -6.1084e-04,  ...,  8.4702e-04,
          0.0000e+00, -8.0715e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1155.1509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.2620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3301, device='cuda:0')



h[100].sum tensor(59.3353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8937, device='cuda:0')



h[200].sum tensor(20.4355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0047, 0.0000,  ..., 0.0128, 0.0000, 0.0029],
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0115, 0.0092, 0.0000,  ..., 0.0222, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43579.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0163, 0.0466, 0.0206,  ..., 0.0012, 0.0123, 0.0000],
        [0.0138, 0.0454, 0.0156,  ..., 0.0012, 0.0105, 0.0000],
        [0.0255, 0.0530, 0.0481,  ..., 0.0052, 0.0194, 0.0000],
        ...,
        [0.0041, 0.0398, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0041, 0.0398, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0041, 0.0398, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(335411.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2610.3743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.2708, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3600.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(655.8444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.3182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5651],
        [-0.5725],
        [-0.5104],
        ...,
        [-0.7580],
        [-0.7556],
        [-0.7549]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177297.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0035],
        [1.0027],
        ...,
        [1.0008],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367185.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0035],
        [1.0028],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367196.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2709e-05,  5.3126e-05, -6.1085e-04,  ...,  8.4506e-04,
          0.0000e+00, -8.0994e-04],
        [-4.2709e-05,  5.3126e-05, -6.1085e-04,  ...,  8.4506e-04,
          0.0000e+00, -8.0994e-04],
        [-4.2709e-05,  5.3126e-05, -6.1085e-04,  ...,  8.4506e-04,
          0.0000e+00, -8.0994e-04],
        ...,
        [-4.2709e-05,  5.3126e-05, -6.1085e-04,  ...,  8.4506e-04,
          0.0000e+00, -8.0994e-04],
        [-4.2709e-05,  5.3126e-05, -6.1085e-04,  ...,  8.4506e-04,
          0.0000e+00, -8.0994e-04],
        [-4.2709e-05,  5.3126e-05, -6.1085e-04,  ...,  8.4506e-04,
          0.0000e+00, -8.0994e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1451.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.7286, device='cuda:0')



h[100].sum tensor(61.7489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.0543, device='cuda:0')



h[200].sum tensor(27.3656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50120.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.8335e-03, 3.9995e-02, 0.0000e+00,  ..., 0.0000e+00, 3.7030e-03,
         0.0000e+00],
        [5.5103e-03, 4.0401e-02, 0.0000e+00,  ..., 0.0000e+00, 4.2462e-03,
         0.0000e+00],
        [9.6603e-03, 4.2864e-02, 6.5972e-03,  ..., 1.8505e-05, 7.4391e-03,
         0.0000e+00],
        ...,
        [3.9717e-03, 4.0083e-02, 0.0000e+00,  ..., 0.0000e+00, 3.0548e-03,
         0.0000e+00],
        [3.9725e-03, 4.0091e-02, 0.0000e+00,  ..., 0.0000e+00, 3.0556e-03,
         0.0000e+00],
        [3.9729e-03, 4.0094e-02, 0.0000e+00,  ..., 0.0000e+00, 3.0560e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355351.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3050.4646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.4481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3352.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(753.3067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.5040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5471],
        [-0.4184],
        [-0.2347],
        ...,
        [-0.7686],
        [-0.7665],
        [-0.7661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145647.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0035],
        [1.0028],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367196.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0036],
        [1.0029],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367207.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0130e-05,  4.2791e-05, -6.1086e-04,  ...,  8.6893e-04,
          0.0000e+00, -8.0409e-04],
        [-6.0130e-05,  4.2791e-05, -6.1086e-04,  ...,  8.6893e-04,
          0.0000e+00, -8.0409e-04],
        [-6.0130e-05,  4.2791e-05, -6.1086e-04,  ...,  8.6893e-04,
          0.0000e+00, -8.0409e-04],
        ...,
        [-6.0130e-05,  4.2791e-05, -6.1086e-04,  ...,  8.6893e-04,
          0.0000e+00, -8.0409e-04],
        [-6.0130e-05,  4.2791e-05, -6.1086e-04,  ...,  8.6893e-04,
          0.0000e+00, -8.0409e-04],
        [-6.0130e-05,  4.2791e-05, -6.1086e-04,  ...,  8.6893e-04,
          0.0000e+00, -8.0409e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1245.9987, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.0406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3224, device='cuda:0')



h[100].sum tensor(60.0941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8605, device='cuda:0')



h[200].sum tensor(21.6546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0500, 0.0392, 0.0000,  ..., 0.0849, 0.0000, 0.0301],
        [0.0081, 0.0065, 0.0000,  ..., 0.0167, 0.0000, 0.0045],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44405.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1041, 0.1048, 0.2699,  ..., 0.0338, 0.0803, 0.0000],
        [0.0657, 0.0800, 0.1606,  ..., 0.0194, 0.0505, 0.0000],
        [0.0483, 0.0689, 0.1125,  ..., 0.0136, 0.0371, 0.0000],
        ...,
        [0.0038, 0.0409, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0038, 0.0409, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0038, 0.0409, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334865.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2574.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.5627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3490.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(673.1865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.7813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0541],
        [ 0.0535],
        [ 0.0495],
        ...,
        [-0.7810],
        [-0.7788],
        [-0.7781]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177084.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0036],
        [1.0029],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367207.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0037],
        [1.0030],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367218.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8367e-05,  4.5800e-05, -6.1087e-04,  ...,  8.7656e-04,
          0.0000e+00, -8.0240e-04],
        [-6.8367e-05,  4.5800e-05, -6.1087e-04,  ...,  8.7656e-04,
          0.0000e+00, -8.0240e-04],
        [-6.8367e-05,  4.5800e-05, -6.1087e-04,  ...,  8.7656e-04,
          0.0000e+00, -8.0240e-04],
        ...,
        [-6.8367e-05,  4.5800e-05, -6.1087e-04,  ...,  8.7656e-04,
          0.0000e+00, -8.0240e-04],
        [-6.8367e-05,  4.5800e-05, -6.1087e-04,  ...,  8.7656e-04,
          0.0000e+00, -8.0240e-04],
        [-6.8367e-05,  4.5800e-05, -6.1087e-04,  ...,  8.7656e-04,
          0.0000e+00, -8.0240e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1265.6619, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.9755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4500, device='cuda:0')



h[100].sum tensor(59.9822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2421, device='cuda:0')



h[200].sum tensor(21.8431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46074.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0437, 0.0068,  ..., 0.0002, 0.0067, 0.0000],
        [0.0049, 0.0415, 0.0001,  ..., 0.0000, 0.0037, 0.0000],
        [0.0037, 0.0409, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        ...,
        [0.0038, 0.0413, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0038, 0.0413, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0038, 0.0413, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344719.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2747.7507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(223.9336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3306.9451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(697.1059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.5043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5029],
        [-0.7096],
        [-0.8637],
        ...,
        [-0.7876],
        [-0.7856],
        [-0.7850]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168352.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0037],
        [1.0030],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367218.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0037],
        [1.0030],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367230.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9472e-05,  5.3230e-05, -6.1088e-04,  ...,  8.6722e-04,
          0.0000e+00, -8.0544e-04],
        [-6.9472e-05,  5.3230e-05, -6.1088e-04,  ...,  8.6722e-04,
          0.0000e+00, -8.0544e-04],
        [-6.9472e-05,  5.3230e-05, -6.1088e-04,  ...,  8.6722e-04,
          0.0000e+00, -8.0544e-04],
        ...,
        [-6.9472e-05,  5.3230e-05, -6.1088e-04,  ...,  8.6722e-04,
          0.0000e+00, -8.0544e-04],
        [-6.9472e-05,  5.3230e-05, -6.1088e-04,  ...,  8.6722e-04,
          0.0000e+00, -8.0544e-04],
        [-6.9472e-05,  5.3230e-05, -6.1088e-04,  ...,  8.6722e-04,
          0.0000e+00, -8.0544e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1308.2124, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.6992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7897, device='cuda:0')



h[100].sum tensor(59.9792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.2575, device='cuda:0')



h[200].sum tensor(22.7398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45639.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0190, 0.0502, 0.0290,  ..., 0.0023, 0.0147, 0.0000],
        [0.0148, 0.0476, 0.0200,  ..., 0.0014, 0.0114, 0.0000],
        [0.0106, 0.0450, 0.0093,  ..., 0.0006, 0.0081, 0.0000],
        ...,
        [0.0039, 0.0414, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0039, 0.0414, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0039, 0.0414, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(337348.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2693.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.2075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3318.0229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(691.5486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.6963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.8992e-02],
        [ 5.5307e-04],
        [-2.4966e-02],
        ...,
        [-7.9088e-01],
        [-7.8936e-01],
        [-7.8960e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161964.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0037],
        [1.0030],
        ...,
        [1.0009],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367230.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0038],
        [1.0031],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367242.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.1555e-05,  7.5891e-05, -6.1088e-04,  ...,  8.4604e-04,
          0.0000e+00, -8.1050e-04],
        [-6.1555e-05,  7.5891e-05, -6.1088e-04,  ...,  8.4604e-04,
          0.0000e+00, -8.1050e-04],
        [-6.1555e-05,  7.5891e-05, -6.1088e-04,  ...,  8.4604e-04,
          0.0000e+00, -8.1050e-04],
        ...,
        [-6.1555e-05,  7.5891e-05, -6.1088e-04,  ...,  8.4604e-04,
          0.0000e+00, -8.1050e-04],
        [-6.1555e-05,  7.5891e-05, -6.1088e-04,  ...,  8.4604e-04,
          0.0000e+00, -8.1050e-04],
        [-6.1555e-05,  7.5891e-05, -6.1088e-04,  ...,  8.4604e-04,
          0.0000e+00, -8.1050e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1300.4554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5156, device='cuda:0')



h[100].sum tensor(59.5599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.4380, device='cuda:0')



h[200].sum tensor(22.4739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1729, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44672.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0040, 0.0404, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0040, 0.0404, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0040, 0.0405, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        ...,
        [0.0041, 0.0410, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0041, 0.0410, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0041, 0.0410, 0.0000,  ..., 0.0000, 0.0034, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334827.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2567.2180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(218.6585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3526.5190, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(667.7069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.1793, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7052],
        [-0.7387],
        [-0.7537],
        ...,
        [-0.7936],
        [-0.7914],
        [-0.7908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188894.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0038],
        [1.0031],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367242.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0039],
        [1.0032],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367254.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1465e-05,  9.9975e-05, -6.1089e-04,  ...,  8.1657e-04,
          0.0000e+00, -8.1661e-04],
        [-5.1465e-05,  9.9975e-05, -6.1089e-04,  ...,  8.1657e-04,
          0.0000e+00, -8.1661e-04],
        [-5.1465e-05,  9.9975e-05, -6.1089e-04,  ...,  8.1657e-04,
          0.0000e+00, -8.1661e-04],
        ...,
        [-5.1465e-05,  9.9975e-05, -6.1089e-04,  ...,  8.1657e-04,
          0.0000e+00, -8.1661e-04],
        [-5.1465e-05,  9.9975e-05, -6.1089e-04,  ...,  8.1657e-04,
          0.0000e+00, -8.1661e-04],
        [-5.1465e-05,  9.9975e-05, -6.1089e-04,  ...,  8.1657e-04,
          0.0000e+00, -8.1661e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1313.0999, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.8022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4955, device='cuda:0')



h[100].sum tensor(59.3934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.3781, device='cuda:0')



h[200].sum tensor(22.7234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45826.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0066, 0.0413, 0.0013,  ..., 0.0000, 0.0055, 0.0000],
        [0.0143, 0.0462, 0.0194,  ..., 0.0015, 0.0115, 0.0000],
        [0.0235, 0.0520, 0.0414,  ..., 0.0040, 0.0186, 0.0000],
        ...,
        [0.0043, 0.0405, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0043, 0.0405, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0043, 0.0405, 0.0000,  ..., 0.0000, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(340883.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2851.2866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(232.1887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3405.8835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(682.7822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.0565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0644],
        [-0.0032],
        [ 0.0461],
        ...,
        [-0.7945],
        [-0.7923],
        [-0.7917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155283.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0039],
        [1.0032],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367254.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0040],
        [1.0032],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367266.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.4272e-05,  1.1844e-04, -6.1089e-04,  ...,  7.9233e-04,
          0.0000e+00, -8.1498e-04],
        [-4.4272e-05,  1.1844e-04, -6.1089e-04,  ...,  7.9233e-04,
          0.0000e+00, -8.1498e-04],
        [-4.4272e-05,  1.1844e-04, -6.1089e-04,  ...,  7.9233e-04,
          0.0000e+00, -8.1498e-04],
        ...,
        [-4.4272e-05,  1.1844e-04, -6.1089e-04,  ...,  7.9233e-04,
          0.0000e+00, -8.1498e-04],
        [-4.4272e-05,  1.1844e-04, -6.1089e-04,  ...,  7.9233e-04,
          0.0000e+00, -8.1498e-04],
        [-4.4272e-05,  1.1844e-04, -6.1089e-04,  ...,  7.9233e-04,
          0.0000e+00, -8.1498e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1328.1353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6027, device='cuda:0')



h[100].sum tensor(59.4561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6985, device='cuda:0')



h[200].sum tensor(22.8465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45220.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0061, 0.0404, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0060, 0.0404, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0070, 0.0413, 0.0016,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0078, 0.0424, 0.0043,  ..., 0.0002, 0.0067, 0.0000],
        [0.0147, 0.0467, 0.0200,  ..., 0.0012, 0.0121, 0.0000],
        [0.0182, 0.0489, 0.0267,  ..., 0.0019, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(335299.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2691.0906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.5551, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3579.9985, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(667.9842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.1187, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4926],
        [-0.4202],
        [-0.3057],
        ...,
        [-0.4999],
        [-0.3339],
        [-0.2326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168999.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0040],
        [1.0032],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367266.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0041],
        [1.0033],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367278., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.0901e-05,  1.2668e-04, -6.1090e-04,  ...,  7.7538e-04,
          0.0000e+00, -8.0898e-04],
        [-4.0901e-05,  1.2668e-04, -6.1090e-04,  ...,  7.7538e-04,
          0.0000e+00, -8.0898e-04],
        [ 1.4421e-02,  1.1388e-02, -7.0744e-04,  ...,  2.4255e-02,
         -3.9062e-03,  8.5518e-03],
        ...,
        [-4.0901e-05,  1.2668e-04, -6.1090e-04,  ...,  7.7538e-04,
          0.0000e+00, -8.0898e-04],
        [-4.0901e-05,  1.2668e-04, -6.1090e-04,  ...,  7.7538e-04,
          0.0000e+00, -8.0898e-04],
        [-4.0901e-05,  1.2668e-04, -6.1090e-04,  ...,  7.7538e-04,
          0.0000e+00, -8.0898e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2032.1357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.5159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.4523, device='cuda:0')



h[100].sum tensor(64.6896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.1662, device='cuda:0')



h[200].sum tensor(40.2219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0581, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0263, 0.0211, 0.0000,  ..., 0.0460, 0.0000, 0.0155],
        [0.0263, 0.0210, 0.0000,  ..., 0.0459, 0.0000, 0.0154],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63776.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0328, 0.0573, 0.0658,  ..., 0.0054, 0.0263, 0.0000],
        [0.0586, 0.0740, 0.1389,  ..., 0.0151, 0.0464, 0.0000],
        [0.0726, 0.0831, 0.1786,  ..., 0.0204, 0.0573, 0.0000],
        ...,
        [0.0043, 0.0403, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0043, 0.0403, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0043, 0.0403, 0.0000,  ..., 0.0000, 0.0042, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(412486.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3859.9045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(395.3963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3355.1768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(925.7963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-606.1489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1184],
        [ 0.1128],
        [ 0.1103],
        ...,
        [-0.8082],
        [-0.8060],
        [-0.8053]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145668.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0041],
        [1.0033],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367278., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(564.1266, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0042],
        [1.0034],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367289.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7851e-03,  3.8861e-03, -6.4311e-04,  ...,  8.5966e-03,
         -1.2987e-03,  2.3254e-03],
        [ 2.1138e-02,  1.6620e-02, -7.5228e-04,  ...,  3.5148e-02,
         -5.7003e-03,  1.2913e-02],
        [ 3.2824e-02,  2.5720e-02, -8.3030e-04,  ...,  5.4122e-02,
         -8.8459e-03,  2.0479e-02],
        ...,
        [-3.9727e-05,  1.2899e-04, -6.1090e-04,  ...,  7.6271e-04,
          0.0000e+00, -7.9843e-04],
        [-3.9727e-05,  1.2899e-04, -6.1090e-04,  ...,  7.6271e-04,
          0.0000e+00, -7.9843e-04],
        [-3.9727e-05,  1.2899e-04, -6.1090e-04,  ...,  7.6271e-04,
          0.0000e+00, -7.9843e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1677.8529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1768, device='cuda:0')



h[100].sum tensor(62.4066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.3837, device='cuda:0')



h[200].sum tensor(30.6867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5812, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0693, 0.0546, 0.0000,  ..., 0.1159, 0.0000, 0.0418],
        [0.0778, 0.0612, 0.0000,  ..., 0.1297, 0.0000, 0.0473],
        [0.1033, 0.0811, 0.0000,  ..., 0.1710, 0.0000, 0.0638],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56235.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1747, 0.1486, 0.4719,  ..., 0.0599, 0.1374, 0.0000],
        [0.1888, 0.1577, 0.5136,  ..., 0.0659, 0.1487, 0.0000],
        [0.2110, 0.1722, 0.5800,  ..., 0.0757, 0.1663, 0.0000],
        ...,
        [0.0040, 0.0407, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0040, 0.0407, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0040, 0.0407, 0.0000,  ..., 0.0000, 0.0042, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387827.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3401.5376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.9521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3512.5811, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(822.0562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-525.9546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0953],
        [ 0.0928],
        [ 0.0859],
        ...,
        [-0.8237],
        [-0.8215],
        [-0.8209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156490.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0042],
        [1.0034],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367289.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0043],
        [1.0035],
        ...,
        [1.0011],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367301.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2105e-02,  9.5815e-03, -6.9196e-04,  ...,  2.0463e-02,
         -3.2568e-03,  7.0738e-03],
        [-3.6513e-05,  1.2708e-04, -6.1091e-04,  ...,  7.4895e-04,
          0.0000e+00, -7.8893e-04],
        [ 3.3223e-02,  2.6026e-02, -8.3295e-04,  ...,  5.4754e-02,
         -8.9215e-03,  2.0750e-02],
        ...,
        [-3.6513e-05,  1.2708e-04, -6.1091e-04,  ...,  7.4895e-04,
          0.0000e+00, -7.8893e-04],
        [-3.6513e-05,  1.2708e-04, -6.1091e-04,  ...,  7.4895e-04,
          0.0000e+00, -7.8893e-04],
        [-3.6513e-05,  1.2708e-04, -6.1091e-04,  ...,  7.4895e-04,
          0.0000e+00, -7.8893e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2013.1582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.4060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6867, device='cuda:0')



h[100].sum tensor(65.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(52.8774, device='cuda:0')



h[200].sum tensor(38.5123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9727, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0082, 0.0000,  ..., 0.0191, 0.0000, 0.0056],
        [0.0732, 0.0576, 0.0000,  ..., 0.1221, 0.0000, 0.0443],
        [0.0682, 0.0537, 0.0000,  ..., 0.1139, 0.0000, 0.0419],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60571.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.4143e-02, 7.2099e-02, 1.2937e-01,  ..., 1.4083e-02, 4.3185e-02,
         0.0000e+00],
        [1.1780e-01, 1.1295e-01, 3.1174e-01,  ..., 3.8670e-02, 9.3043e-02,
         0.0000e+00],
        [1.4720e-01, 1.3196e-01, 3.9742e-01,  ..., 5.0742e-02, 1.1616e-01,
         0.0000e+00],
        ...,
        [3.8596e-03, 4.0999e-02, 0.0000e+00,  ..., 0.0000e+00, 3.9568e-03,
         0.0000e+00],
        [7.6441e-03, 4.3277e-02, 4.4609e-03,  ..., 0.0000e+00, 6.9003e-03,
         0.0000e+00],
        [1.3185e-02, 4.6603e-02, 1.5829e-02,  ..., 2.5167e-04, 1.1209e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390150.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3330.7129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(365.1911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3655.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(880.7441, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-572.1255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0304],
        [ 0.0647],
        [ 0.0668],
        ...,
        [-0.7343],
        [-0.5919],
        [-0.4071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171925.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0043],
        [1.0035],
        ...,
        [1.0011],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367301.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0043],
        [1.0035],
        ...,
        [1.0011],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367313.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.0942e-05,  1.2368e-04, -6.1091e-04,  ...,  7.3977e-04,
          0.0000e+00, -7.8256e-04],
        [-3.0942e-05,  1.2368e-04, -6.1091e-04,  ...,  7.3977e-04,
          0.0000e+00, -7.8256e-04],
        [ 1.1761e-02,  9.3063e-03, -6.8964e-04,  ...,  1.9888e-02,
         -3.1521e-03,  6.8556e-03],
        ...,
        [-3.0942e-05,  1.2368e-04, -6.1091e-04,  ...,  7.3977e-04,
          0.0000e+00, -7.8256e-04],
        [-3.0942e-05,  1.2368e-04, -6.1091e-04,  ...,  7.3977e-04,
          0.0000e+00, -7.8256e-04],
        [-3.0942e-05,  1.2368e-04, -6.1091e-04,  ...,  7.3977e-04,
          0.0000e+00, -7.8256e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1197.5054, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.1184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2854, device='cuda:0')



h[100].sum tensor(59.6413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.7705, device='cuda:0')



h[200].sum tensor(17.7605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0161, 0.0131, 0.0000,  ..., 0.0293, 0.0000, 0.0089],
        [0.0149, 0.0122, 0.0000,  ..., 0.0273, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40550.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0467, 0.0201,  ..., 0.0010, 0.0116, 0.0000],
        [0.0354, 0.0598, 0.0714,  ..., 0.0053, 0.0283, 0.0000],
        [0.0443, 0.0650, 0.0924,  ..., 0.0066, 0.0353, 0.0000],
        ...,
        [0.0038, 0.0413, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0038, 0.0413, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0038, 0.0413, 0.0000,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322521.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2279.5186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(185.9512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3932.4294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(603.7245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-355.0706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2487],
        [-0.0684],
        [ 0.0456],
        ...,
        [-0.8538],
        [-0.8515],
        [-0.8509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193691.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0043],
        [1.0035],
        ...,
        [1.0011],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367313.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0044],
        [1.0036],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367324.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1259e-02,  1.6695e-02, -7.5297e-04,  ...,  3.5287e-02,
         -5.6677e-03,  1.3000e-02],
        [ 2.1412e-02,  1.6815e-02, -7.5399e-04,  ...,  3.5536e-02,
         -5.7086e-03,  1.3099e-02],
        [-1.9213e-05,  1.2647e-04, -6.1091e-04,  ...,  7.3446e-04,
          0.0000e+00, -7.8385e-04],
        ...,
        [-1.9213e-05,  1.2647e-04, -6.1091e-04,  ...,  7.3446e-04,
          0.0000e+00, -7.8385e-04],
        [-1.9213e-05,  1.2647e-04, -6.1091e-04,  ...,  7.3446e-04,
          0.0000e+00, -7.8385e-04],
        [-1.9213e-05,  1.2647e-04, -6.1091e-04,  ...,  7.3446e-04,
          0.0000e+00, -7.8385e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1668.8781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.7820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6020, device='cuda:0')



h[100].sum tensor(63.2699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.6655, device='cuda:0')



h[200].sum tensor(29.3546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0874, 0.0686, 0.0000,  ..., 0.1450, 0.0000, 0.0535],
        [0.0630, 0.0496, 0.0000,  ..., 0.1054, 0.0000, 0.0385],
        [0.0579, 0.0457, 0.0000,  ..., 0.0971, 0.0000, 0.0344],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53925.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1499, 0.1336, 0.4049,  ..., 0.0529, 0.1175, 0.0000],
        [0.1511, 0.1342, 0.4067,  ..., 0.0526, 0.1184, 0.0000],
        [0.1444, 0.1297, 0.3842,  ..., 0.0485, 0.1131, 0.0000],
        ...,
        [0.0040, 0.0411, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0040, 0.0411, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0040, 0.0411, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(378728.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3192.1018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.7031, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3779.7534, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(790.4946, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-499.1852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0305],
        [ 0.0331],
        [ 0.0362],
        ...,
        [-0.8630],
        [-0.8607],
        [-0.8601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174455.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0044],
        [1.0036],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367324.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0037],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367336.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5885e-03,  3.7083e-03, -6.4157e-04,  ...,  8.1938e-03,
         -1.2188e-03,  2.1879e-03],
        [ 6.2723e-03,  5.0194e-03, -6.5281e-04,  ...,  1.0928e-02,
         -1.6657e-03,  3.2786e-03],
        [-3.3987e-06,  1.3267e-04, -6.1092e-04,  ...,  7.3733e-04,
          0.0000e+00, -7.8652e-04],
        ...,
        [-3.3987e-06,  1.3267e-04, -6.1092e-04,  ...,  7.3733e-04,
          0.0000e+00, -7.8652e-04],
        [-3.3987e-06,  1.3267e-04, -6.1092e-04,  ...,  7.3733e-04,
          0.0000e+00, -7.8652e-04],
        [-3.3987e-06,  1.3267e-04, -6.1092e-04,  ...,  7.3733e-04,
          0.0000e+00, -7.8652e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1317.3687, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.2584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3285, device='cuda:0')



h[100].sum tensor(60.8159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8891, device='cuda:0')



h[200].sum tensor(20.5864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0412, 0.0326, 0.0000,  ..., 0.0699, 0.0000, 0.0235],
        [0.0135, 0.0111, 0.0000,  ..., 0.0249, 0.0000, 0.0072],
        [0.0063, 0.0054, 0.0000,  ..., 0.0132, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0030, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43528.7852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0864, 0.0914, 0.2133,  ..., 0.0245, 0.0675, 0.0000],
        [0.0497, 0.0681, 0.1107,  ..., 0.0113, 0.0390, 0.0000],
        [0.0253, 0.0531, 0.0453,  ..., 0.0041, 0.0201, 0.0000],
        ...,
        [0.0043, 0.0408, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0043, 0.0408, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0043, 0.0408, 0.0000,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(336386.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2637.6045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(209.5966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3943.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(645.4005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.8696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0557],
        [ 0.0171],
        [-0.0817],
        ...,
        [-0.8681],
        [-0.8658],
        [-0.8652]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179079.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0037],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367336.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0037],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367348.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1230e-05,  1.4288e-04, -6.1092e-04,  ...,  7.5869e-04,
          0.0000e+00, -7.8560e-04],
        [ 1.1230e-05,  1.4288e-04, -6.1092e-04,  ...,  7.5869e-04,
          0.0000e+00, -7.8560e-04],
        [ 1.1230e-05,  1.4288e-04, -6.1092e-04,  ...,  7.5869e-04,
          0.0000e+00, -7.8560e-04],
        ...,
        [ 1.1230e-05,  1.4288e-04, -6.1092e-04,  ...,  7.5869e-04,
          0.0000e+00, -7.8560e-04],
        [ 1.1230e-05,  1.4288e-04, -6.1092e-04,  ...,  7.5869e-04,
          0.0000e+00, -7.8560e-04],
        [ 1.1230e-05,  1.4288e-04, -6.1092e-04,  ...,  7.5869e-04,
          0.0000e+00, -7.8560e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1401.2533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.2266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2825, device='cuda:0')



h[100].sum tensor(61.4489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.7411, device='cuda:0')



h[200].sum tensor(22.8471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.5019e-05, 5.7279e-04, 0.0000e+00,  ..., 3.0415e-03, 0.0000e+00,
         0.0000e+00],
        [4.5119e-05, 5.7406e-04, 0.0000e+00,  ..., 3.0482e-03, 0.0000e+00,
         0.0000e+00],
        [7.9178e-03, 6.7045e-03, 0.0000e+00,  ..., 1.5834e-02, 0.0000e+00,
         4.3101e-03],
        ...,
        [4.5654e-05, 5.8086e-04, 0.0000e+00,  ..., 3.0844e-03, 0.0000e+00,
         0.0000e+00],
        [4.5661e-05, 5.8095e-04, 0.0000e+00,  ..., 3.0848e-03, 0.0000e+00,
         0.0000e+00],
        [4.5664e-05, 5.8099e-04, 0.0000e+00,  ..., 3.0850e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46106.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0467, 0.0221,  ..., 0.0018, 0.0126, 0.0000],
        [0.0234, 0.0515, 0.0423,  ..., 0.0041, 0.0184, 0.0000],
        [0.0378, 0.0604, 0.0782,  ..., 0.0079, 0.0297, 0.0000],
        ...,
        [0.0047, 0.0405, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0047, 0.0405, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0047, 0.0405, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(351463.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2935.1382, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(231.0050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3784.9060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(679.9553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.5411, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0047],
        [ 0.0272],
        [ 0.0538],
        ...,
        [-0.8691],
        [-0.8669],
        [-0.8663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173350.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0037],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367348.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0045],
        [1.0038],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367359.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04],
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04],
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04],
        ...,
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04],
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04],
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1408.1014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9861, device='cuda:0')



h[100].sum tensor(61.7731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.8552, device='cuda:0')



h[200].sum tensor(22.8273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.4026e-05, 5.8378e-04, 0.0000e+00,  ..., 3.1283e-03, 0.0000e+00,
         0.0000e+00],
        [6.4168e-05, 5.8508e-04, 0.0000e+00,  ..., 3.1353e-03, 0.0000e+00,
         0.0000e+00],
        [6.4176e-05, 5.8515e-04, 0.0000e+00,  ..., 3.1356e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.4936e-05, 5.9208e-04, 0.0000e+00,  ..., 3.1728e-03, 0.0000e+00,
         0.0000e+00],
        [6.4945e-05, 5.9216e-04, 0.0000e+00,  ..., 3.1732e-03, 0.0000e+00,
         0.0000e+00],
        [6.4950e-05, 5.9220e-04, 0.0000e+00,  ..., 3.1735e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45209.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0414, 0.0006,  ..., 0.0000, 0.0064, 0.0000],
        [0.0059, 0.0404, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0050, 0.0401, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        ...,
        [0.0049, 0.0405, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0049, 0.0405, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0049, 0.0405, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342804.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2850.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(220.5570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3713.8259, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(669.1166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.2529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1826],
        [-0.3324],
        [-0.5134],
        ...,
        [-0.8626],
        [-0.8659],
        [-0.8679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174894.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0045],
        [1.0038],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367359.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0045],
        [1.0038],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367359.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0524e-02,  8.3284e-03, -6.8107e-04,  ...,  1.7848e-02,
         -2.7693e-03,  6.0269e-03],
        [ 1.1892e-02,  9.3937e-03, -6.9020e-04,  ...,  2.0070e-02,
         -3.1299e-03,  6.9131e-03],
        [ 1.2163e-02,  9.6041e-03, -6.9201e-04,  ...,  2.0508e-02,
         -3.2011e-03,  7.0882e-03],
        ...,
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04],
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04],
        [ 1.5970e-05,  1.4562e-04, -6.1092e-04,  ...,  7.8032e-04,
          0.0000e+00, -7.8056e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1438.8958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.8710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4263, device='cuda:0')



h[100].sum tensor(61.9932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1710, device='cuda:0')



h[200].sum tensor(23.5879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.8577e-02, 3.0573e-02, 0.0000e+00,  ..., 6.5679e-02, 0.0000e+00,
         2.1820e-02],
        [4.4985e-02, 3.5565e-02, 0.0000e+00,  ..., 7.6095e-02, 0.0000e+00,
         2.5964e-02],
        [5.6812e-02, 4.4774e-02, 0.0000e+00,  ..., 9.5304e-02, 0.0000e+00,
         3.3626e-02],
        ...,
        [6.4936e-05, 5.9208e-04, 0.0000e+00,  ..., 3.1728e-03, 0.0000e+00,
         0.0000e+00],
        [6.4945e-05, 5.9216e-04, 0.0000e+00,  ..., 3.1732e-03, 0.0000e+00,
         0.0000e+00],
        [6.4950e-05, 5.9220e-04, 0.0000e+00,  ..., 3.1735e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46419.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1349, 0.1214, 0.3514,  ..., 0.0442, 0.1055, 0.0000],
        [0.1240, 0.1142, 0.3183,  ..., 0.0391, 0.0970, 0.0000],
        [0.1107, 0.1059, 0.2803,  ..., 0.0339, 0.0866, 0.0000],
        ...,
        [0.0049, 0.0405, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0049, 0.0405, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0049, 0.0405, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349573.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2973.5083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(231.0628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3679.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(686.3677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.9310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0657],
        [ 0.0727],
        [ 0.0700],
        ...,
        [-0.8747],
        [-0.8723],
        [-0.8714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165581.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0045],
        [1.0038],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367359.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0046],
        [1.0039],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367370.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1154e-02,  8.8183e-03, -6.8529e-04,  ...,  1.8901e-02,
         -2.9257e-03,  6.4483e-03],
        [ 4.7113e-03,  3.8010e-03, -6.4228e-04,  ...,  8.4348e-03,
         -1.2337e-03,  2.2738e-03],
        [ 6.4566e-03,  5.1601e-03, -6.5393e-04,  ...,  1.1270e-02,
         -1.6920e-03,  3.4046e-03],
        ...,
        [ 1.3488e-05,  1.4287e-04, -6.1092e-04,  ...,  8.0356e-04,
          0.0000e+00, -7.6984e-04],
        [ 1.3488e-05,  1.4287e-04, -6.1092e-04,  ...,  8.0356e-04,
          0.0000e+00, -7.6984e-04],
        [ 1.3488e-05,  1.4287e-04, -6.1092e-04,  ...,  8.0356e-04,
          0.0000e+00, -7.6984e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1366.8905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.8565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3857, device='cuda:0')



h[100].sum tensor(61.9052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0600, device='cuda:0')



h[200].sum tensor(21.3771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.8522e-02, 1.4954e-02, 0.0000e+00,  ..., 3.3221e-02, 0.0000e+00,
         9.6506e-03],
        [4.1449e-02, 3.2808e-02, 0.0000e+00,  ..., 7.0472e-02, 0.0000e+00,
         2.3726e-02],
        [1.8572e-02, 1.4994e-02, 0.0000e+00,  ..., 3.3310e-02, 0.0000e+00,
         1.0450e-02],
        ...,
        [5.4850e-05, 5.8101e-04, 0.0000e+00,  ..., 3.2678e-03, 0.0000e+00,
         0.0000e+00],
        [5.4858e-05, 5.8109e-04, 0.0000e+00,  ..., 3.2682e-03, 0.0000e+00,
         0.0000e+00],
        [5.4862e-05, 5.8113e-04, 0.0000e+00,  ..., 3.2684e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42988.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0552, 0.0703, 0.1213,  ..., 0.0123, 0.0433, 0.0000],
        [0.0703, 0.0801, 0.1639,  ..., 0.0180, 0.0549, 0.0000],
        [0.0507, 0.0680, 0.1107,  ..., 0.0116, 0.0397, 0.0000],
        ...,
        [0.0050, 0.0407, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0050, 0.0407, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0050, 0.0407, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(335677.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2641.2605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(199.0049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3840.4541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(636.0328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.1989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0842],
        [ 0.0865],
        [ 0.0338],
        ...,
        [-0.8862],
        [-0.8839],
        [-0.8833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202529.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0046],
        [1.0039],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367370.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0046],
        [1.0039],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367381.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.1461e-03,  4.1385e-03, -6.4519e-04,  ...,  9.1586e-03,
         -1.3433e-03,  2.5643e-03],
        [ 1.1616e-02,  9.1769e-03, -6.8838e-04,  ...,  1.9670e-02,
         -3.0364e-03,  6.7567e-03],
        [ 1.2740e-05,  1.4121e-04, -6.1092e-04,  ...,  8.1886e-04,
          0.0000e+00, -7.6186e-04],
        ...,
        [ 1.2740e-05,  1.4121e-04, -6.1092e-04,  ...,  8.1886e-04,
          0.0000e+00, -7.6186e-04],
        [ 1.2740e-05,  1.4121e-04, -6.1092e-04,  ...,  8.1886e-04,
          0.0000e+00, -7.6186e-04],
        [ 1.2740e-05,  1.4121e-04, -6.1092e-04,  ...,  8.1886e-04,
          0.0000e+00, -7.6186e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1929.6161, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.9086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.6799, device='cuda:0')



h[100].sum tensor(66.3717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.8776, device='cuda:0')



h[200].sum tensor(34.7579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7489, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.0409e-02, 3.1993e-02, 0.0000e+00,  ..., 6.8850e-02, 0.0000e+00,
         2.3096e-02],
        [1.8458e-02, 1.4901e-02, 0.0000e+00,  ..., 3.3194e-02, 0.0000e+00,
         9.6307e-03],
        [1.5921e-02, 1.2925e-02, 0.0000e+00,  ..., 2.9073e-02, 0.0000e+00,
         8.7517e-03],
        ...,
        [5.1815e-05, 5.7434e-04, 0.0000e+00,  ..., 3.3305e-03, 0.0000e+00,
         0.0000e+00],
        [5.1822e-05, 5.7441e-04, 0.0000e+00,  ..., 3.3309e-03, 0.0000e+00,
         0.0000e+00],
        [5.1825e-05, 5.7446e-04, 0.0000e+00,  ..., 3.3312e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63711.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0703, 0.0802, 0.1633,  ..., 0.0181, 0.0548, 0.0000],
        [0.0546, 0.0702, 0.1193,  ..., 0.0122, 0.0427, 0.0000],
        [0.0400, 0.0615, 0.0815,  ..., 0.0083, 0.0312, 0.0000],
        ...,
        [0.0051, 0.0408, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0051, 0.0408, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0051, 0.0408, 0.0000,  ..., 0.0000, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434997.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4236.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.8581, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3339.1841, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(928.7575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-603.7003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0818],
        [ 0.0550],
        [-0.0378],
        ...,
        [-0.8970],
        [-0.8943],
        [-0.8933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163161.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0046],
        [1.0039],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367381.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(460.7650, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0047],
        [1.0040],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367392.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3698e-02,  1.8583e-02, -7.6901e-04,  ...,  3.9300e-02,
         -6.1750e-03,  1.4586e-02],
        [ 4.7348e-02,  3.7000e-02, -9.2688e-04,  ...,  7.7725e-02,
         -1.2342e-02,  2.9911e-02],
        [ 2.3838e-02,  1.8693e-02, -7.6995e-04,  ...,  3.9529e-02,
         -6.2117e-03,  1.4677e-02],
        ...,
        [ 1.5459e-05,  1.4163e-04, -6.1093e-04,  ...,  8.2371e-04,
          0.0000e+00, -7.5885e-04],
        [ 1.5459e-05,  1.4163e-04, -6.1093e-04,  ...,  8.2371e-04,
          0.0000e+00, -7.5885e-04],
        [ 1.5459e-05,  1.4163e-04, -6.1093e-04,  ...,  8.2371e-04,
          0.0000e+00, -7.5885e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1364.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1503, device='cuda:0')



h[100].sum tensor(62.7763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.3562, device='cuda:0')



h[200].sum tensor(20.4302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0206, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1589e-01, 9.0765e-02, 0.0000e+00,  ..., 1.9149e-01, 0.0000e+00,
         7.2010e-02],
        [1.0807e-01, 8.4679e-02, 0.0000e+00,  ..., 1.7880e-01, 0.0000e+00,
         6.6938e-02],
        [1.1757e-01, 9.2072e-02, 0.0000e+00,  ..., 1.9422e-01, 0.0000e+00,
         7.3089e-02],
        ...,
        [6.2883e-05, 5.7611e-04, 0.0000e+00,  ..., 3.3507e-03, 0.0000e+00,
         0.0000e+00],
        [6.2891e-05, 5.7619e-04, 0.0000e+00,  ..., 3.3512e-03, 0.0000e+00,
         0.0000e+00],
        [6.2896e-05, 5.7623e-04, 0.0000e+00,  ..., 3.3514e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42679.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1901, 0.1578, 0.5229,  ..., 0.0737, 0.1493, 0.0000],
        [0.2021, 0.1657, 0.5576,  ..., 0.0787, 0.1586, 0.0000],
        [0.2017, 0.1655, 0.5559,  ..., 0.0783, 0.1583, 0.0000],
        ...,
        [0.0053, 0.0408, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0053, 0.0408, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0053, 0.0408, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(337336.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2737.2258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(191.2144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3783.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(635.7259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-377.7915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0351],
        [ 0.0321],
        [ 0.0367],
        ...,
        [-0.9086],
        [-0.9062],
        [-0.9056]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198768.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0047],
        [1.0040],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367392.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0047],
        [1.0040],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367403.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3263e-05,  1.4537e-04, -6.1093e-04,  ...,  8.2012e-04,
          0.0000e+00, -7.6102e-04],
        [ 5.8475e-03,  4.6808e-03, -6.4980e-04,  ...,  1.0283e-02,
         -1.5132e-03,  3.0126e-03],
        [ 2.3263e-05,  1.4537e-04, -6.1093e-04,  ...,  8.2012e-04,
          0.0000e+00, -7.6102e-04],
        ...,
        [ 2.3263e-05,  1.4537e-04, -6.1093e-04,  ...,  8.2012e-04,
          0.0000e+00, -7.6102e-04],
        [ 2.3263e-05,  1.4537e-04, -6.1093e-04,  ...,  8.2012e-04,
          0.0000e+00, -7.6102e-04],
        [ 2.3263e-05,  1.4537e-04, -6.1093e-04,  ...,  8.2012e-04,
          0.0000e+00, -7.6102e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1442.4458, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9164, device='cuda:0')



h[100].sum tensor(63.6106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.6468, device='cuda:0')



h[200].sum tensor(22.0339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.0460e-02, 2.4230e-02, 0.0000e+00,  ..., 5.2625e-02, 0.0000e+00,
         1.6624e-02],
        [4.8700e-03, 4.3038e-03, 0.0000e+00,  ..., 1.1056e-02, 0.0000e+00,
         2.3304e-03],
        [5.9452e-03, 5.1411e-03, 0.0000e+00,  ..., 1.2803e-02, 0.0000e+00,
         3.0268e-03],
        ...,
        [9.4644e-05, 5.9144e-04, 0.0000e+00,  ..., 3.3366e-03, 0.0000e+00,
         0.0000e+00],
        [9.4656e-05, 5.9152e-04, 0.0000e+00,  ..., 3.3370e-03, 0.0000e+00,
         0.0000e+00],
        [9.4663e-05, 5.9156e-04, 0.0000e+00,  ..., 3.3373e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44500.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0500, 0.0666, 0.1050,  ..., 0.0105, 0.0390, 0.0000],
        [0.0274, 0.0530, 0.0455,  ..., 0.0037, 0.0213, 0.0000],
        [0.0247, 0.0517, 0.0392,  ..., 0.0034, 0.0191, 0.0000],
        ...,
        [0.0055, 0.0406, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0055, 0.0407, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0055, 0.0407, 0.0000,  ..., 0.0000, 0.0040, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344061.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2837.3364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(207.0443, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3841.9612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(658.4098, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.0809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2417],
        [-0.3589],
        [-0.4156],
        ...,
        [-0.9179],
        [-0.9155],
        [-0.9148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202073.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0047],
        [1.0040],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367403.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0048],
        [1.0041],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367415.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5702e-05,  1.5230e-04, -6.1093e-04,  ...,  8.0575e-04,
          0.0000e+00, -7.6826e-04],
        [ 1.5023e-02,  1.1823e-02, -7.1096e-04,  ...,  2.5153e-02,
         -3.8797e-03,  8.9408e-03],
        [ 2.2453e-02,  1.7609e-02, -7.6055e-04,  ...,  3.7224e-02,
         -5.8031e-03,  1.3754e-02],
        ...,
        [ 3.5702e-05,  1.5230e-04, -6.1093e-04,  ...,  8.0575e-04,
          0.0000e+00, -7.6826e-04],
        [ 3.5702e-05,  1.5230e-04, -6.1093e-04,  ...,  8.0575e-04,
          0.0000e+00, -7.6826e-04],
        [ 3.5702e-05,  1.5230e-04, -6.1093e-04,  ...,  8.0575e-04,
          0.0000e+00, -7.6826e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1616.4290, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7776, device='cuda:0')



h[100].sum tensor(64.9822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2109, device='cuda:0')



h[200].sum tensor(26.0691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0286, 0.0228, 0.0000,  ..., 0.0494, 0.0000, 0.0161],
        [0.0409, 0.0323, 0.0000,  ..., 0.0694, 0.0000, 0.0241],
        [0.0624, 0.0491, 0.0000,  ..., 0.1043, 0.0000, 0.0372],
        ...,
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49436.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1161, 0.1088, 0.2973,  ..., 0.0386, 0.0908, 0.0000],
        [0.1364, 0.1219, 0.3563,  ..., 0.0474, 0.1066, 0.0000],
        [0.1610, 0.1378, 0.4272,  ..., 0.0576, 0.1258, 0.0000],
        ...,
        [0.0056, 0.0405, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0056, 0.0405, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0056, 0.0405, 0.0000,  ..., 0.0000, 0.0042, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364403.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3257.3523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.2113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3784.9373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(728.9865, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-453.6355, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0442],
        [-0.0343],
        [-0.0261],
        ...,
        [-0.9242],
        [-0.9216],
        [-0.9204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177036.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0048],
        [1.0041],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367415.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0048],
        [1.0041],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367426.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9001e-05,  1.6221e-04, -6.1093e-04,  ...,  7.8853e-04,
          0.0000e+00, -7.7451e-04],
        [ 4.9001e-05,  1.6221e-04, -6.1093e-04,  ...,  7.8853e-04,
          0.0000e+00, -7.7451e-04],
        [ 6.2889e-03,  5.0217e-03, -6.5257e-04,  ...,  1.0925e-02,
         -1.6095e-03,  3.2674e-03],
        ...,
        [ 4.9001e-05,  1.6221e-04, -6.1093e-04,  ...,  7.8853e-04,
          0.0000e+00, -7.7451e-04],
        [ 4.9001e-05,  1.6221e-04, -6.1093e-04,  ...,  7.8853e-04,
          0.0000e+00, -7.7451e-04],
        [ 4.9001e-05,  1.6221e-04, -6.1093e-04,  ...,  7.8853e-04,
          0.0000e+00, -7.7451e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1470.6853, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0663, device='cuda:0')



h[100].sum tensor(64.0116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.0950, device='cuda:0')



h[200].sum tensor(22.3299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0007, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0065, 0.0055, 0.0000,  ..., 0.0134, 0.0000, 0.0033],
        [0.0344, 0.0273, 0.0000,  ..., 0.0587, 0.0000, 0.0206],
        ...,
        [0.0002, 0.0007, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0007, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0007, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45725.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0224, 0.0498, 0.0372,  ..., 0.0037, 0.0175, 0.0000],
        [0.0448, 0.0635, 0.0954,  ..., 0.0107, 0.0351, 0.0000],
        [0.0859, 0.0893, 0.2114,  ..., 0.0265, 0.0674, 0.0000],
        ...,
        [0.0057, 0.0403, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0057, 0.0403, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0057, 0.0403, 0.0000,  ..., 0.0000, 0.0045, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353576.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3079.1050, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.0437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3964.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(672.3959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.5184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0548],
        [ 0.0295],
        [ 0.0643],
        ...,
        [-0.9312],
        [-0.9288],
        [-0.9281]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182173.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0048],
        [1.0041],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367426.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0049],
        [1.0042],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367437.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2837e-05,  1.5595e-04, -6.1093e-04,  ...,  7.8025e-04,
          0.0000e+00, -7.7297e-04],
        [ 4.2837e-05,  1.5595e-04, -6.1093e-04,  ...,  7.8025e-04,
          0.0000e+00, -7.7297e-04],
        [ 4.2837e-05,  1.5595e-04, -6.1093e-04,  ...,  7.8025e-04,
          0.0000e+00, -7.7297e-04],
        ...,
        [ 4.2837e-05,  1.5595e-04, -6.1093e-04,  ...,  7.8025e-04,
          0.0000e+00, -7.7297e-04],
        [ 4.2837e-05,  1.5595e-04, -6.1093e-04,  ...,  7.8025e-04,
          0.0000e+00, -7.7297e-04],
        [ 4.2837e-05,  1.5595e-04, -6.1093e-04,  ...,  7.8025e-04,
          0.0000e+00, -7.7297e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1216.4343, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.4101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.1059, device='cuda:0')



h[100].sum tensor(62.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.2441, device='cuda:0')



h[200].sum tensor(15.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0286, 0.0228, 0.0000,  ..., 0.0493, 0.0000, 0.0169],
        [0.0002, 0.0006, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0002, 0.0006, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0006, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0006, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0002, 0.0006, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39559.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0540, 0.0702, 0.1241,  ..., 0.0153, 0.0422, 0.0000],
        [0.0216, 0.0501, 0.0368,  ..., 0.0043, 0.0169, 0.0000],
        [0.0114, 0.0438, 0.0112,  ..., 0.0010, 0.0089, 0.0000],
        ...,
        [0.0055, 0.0407, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0055, 0.0407, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0055, 0.0407, 0.0000,  ..., 0.0000, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(332773.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2718.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.7336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4083.1794, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(588.7662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-347.9196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0742],
        [-0.2889],
        [-0.5745],
        ...,
        [-0.9484],
        [-0.9460],
        [-0.9453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188329.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0049],
        [1.0042],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367437.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0049],
        [1.0043],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367447.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2269e-02,  9.6783e-03, -6.9262e-04,  ...,  2.0672e-02,
         -3.1347e-03,  7.1679e-03],
        [ 2.4722e-02,  1.9377e-02, -7.7573e-04,  ...,  4.0903e-02,
         -6.3238e-03,  1.5235e-02],
        [ 3.5826e-02,  2.8025e-02, -8.4984e-04,  ...,  5.8943e-02,
         -9.1674e-03,  2.2428e-02],
        ...,
        [ 2.8014e-05,  1.4538e-04, -6.1093e-04,  ...,  7.8600e-04,
          0.0000e+00, -7.6130e-04],
        [ 2.8014e-05,  1.4538e-04, -6.1093e-04,  ...,  7.8600e-04,
          0.0000e+00, -7.6130e-04],
        [ 2.8014e-05,  1.4538e-04, -6.1093e-04,  ...,  7.8600e-04,
          0.0000e+00, -7.6130e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1292.2441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.4227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9540, device='cuda:0')



h[100].sum tensor(63.2815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.7798, device='cuda:0')



h[200].sum tensor(16.9490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8872, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.0370e-02, 6.3086e-02, 0.0000e+00,  ..., 1.3354e-01, 0.0000e+00,
         4.8935e-02],
        [1.1435e-01, 8.9555e-02, 0.0000e+00,  ..., 1.8876e-01, 0.0000e+00,
         7.0942e-02],
        [1.6879e-01, 1.3195e-01, 0.0000e+00,  ..., 2.7720e-01, 0.0000e+00,
         1.0620e-01],
        ...,
        [1.1404e-04, 5.9181e-04, 0.0000e+00,  ..., 3.1996e-03, 0.0000e+00,
         0.0000e+00],
        [1.1405e-04, 5.9188e-04, 0.0000e+00,  ..., 3.2000e-03, 0.0000e+00,
         0.0000e+00],
        [1.1406e-04, 5.9193e-04, 0.0000e+00,  ..., 3.2003e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40055.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2068, 0.1677, 0.5647,  ..., 0.0795, 0.1626, 0.0000],
        [0.2887, 0.2207, 0.8117,  ..., 0.1187, 0.2276, 0.0000],
        [0.3733, 0.2753, 1.0680,  ..., 0.1595, 0.2951, 0.0000],
        ...,
        [0.0053, 0.0415, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0053, 0.0415, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0053, 0.0415, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(332540.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2562.3994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(163.1689, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4146.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(595.0864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-350.1889, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0071],
        [-0.0355],
        [-0.0764],
        ...,
        [-0.9680],
        [-0.9655],
        [-0.9648]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217674., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0049],
        [1.0043],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367447.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0050],
        [1.0043],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367458.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3212e-05,  1.3502e-04, -6.1093e-04,  ...,  7.9109e-04,
          0.0000e+00, -7.5148e-04],
        [ 1.3212e-05,  1.3502e-04, -6.1093e-04,  ...,  7.9109e-04,
          0.0000e+00, -7.5148e-04],
        [ 1.3212e-05,  1.3502e-04, -6.1093e-04,  ...,  7.9109e-04,
          0.0000e+00, -7.5148e-04],
        ...,
        [ 1.3212e-05,  1.3502e-04, -6.1093e-04,  ...,  7.9109e-04,
          0.0000e+00, -7.5148e-04],
        [ 1.3212e-05,  1.3502e-04, -6.1093e-04,  ...,  7.9109e-04,
          0.0000e+00, -7.5148e-04],
        [ 1.3212e-05,  1.3502e-04, -6.1093e-04,  ...,  7.9109e-04,
          0.0000e+00, -7.5148e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1502.6375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.3532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3136, device='cuda:0')



h[100].sum tensor(64.9686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8343, device='cuda:0')



h[200].sum tensor(21.6858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.0301e-03, 5.9752e-03, 0.0000e+00,  ..., 1.4508e-02, 0.0000e+00,
         3.7660e-03],
        [5.3113e-05, 5.4276e-04, 0.0000e+00,  ..., 3.1801e-03, 0.0000e+00,
         0.0000e+00],
        [8.6746e-03, 7.2572e-03, 0.0000e+00,  ..., 1.7188e-02, 0.0000e+00,
         4.8301e-03],
        ...,
        [5.3793e-05, 5.4971e-04, 0.0000e+00,  ..., 3.2208e-03, 0.0000e+00,
         0.0000e+00],
        [5.3799e-05, 5.4977e-04, 0.0000e+00,  ..., 3.2212e-03, 0.0000e+00,
         0.0000e+00],
        [5.3803e-05, 5.4982e-04, 0.0000e+00,  ..., 3.2214e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46917.1758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0496, 0.0236,  ..., 0.0012, 0.0144, 0.0000],
        [0.0147, 0.0473, 0.0133,  ..., 0.0002, 0.0111, 0.0000],
        [0.0260, 0.0544, 0.0441,  ..., 0.0040, 0.0199, 0.0000],
        ...,
        [0.0051, 0.0422, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0051, 0.0422, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0051, 0.0422, 0.0000,  ..., 0.0000, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(365535.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3037.1106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.3292, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3916.8452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(694.6751, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.3394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4923],
        [-0.4029],
        [-0.2324],
        ...,
        [-0.9877],
        [-0.9851],
        [-0.9843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210444.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0050],
        [1.0043],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367458.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0050],
        [1.0044],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367469.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5778e-03,  3.6885e-03, -6.4150e-04,  ...,  8.2338e-03,
         -1.1646e-03,  2.2223e-03],
        [-2.8847e-06,  1.2102e-04, -6.1093e-04,  ...,  7.9096e-04,
          0.0000e+00, -7.4488e-04],
        [-2.8847e-06,  1.2102e-04, -6.1093e-04,  ...,  7.9096e-04,
          0.0000e+00, -7.4488e-04],
        ...,
        [-2.8847e-06,  1.2102e-04, -6.1093e-04,  ...,  7.9096e-04,
          0.0000e+00, -7.4488e-04],
        [-2.8847e-06,  1.2102e-04, -6.1093e-04,  ...,  7.9096e-04,
          0.0000e+00, -7.4488e-04],
        [-2.8847e-06,  1.2102e-04, -6.1093e-04,  ...,  7.9096e-04,
          0.0000e+00, -7.4488e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1832.5375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6609, device='cuda:0')



h[100].sum tensor(67.5132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.8414, device='cuda:0')



h[200].sum tensor(29.4082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0130, 0.0000,  ..., 0.0292, 0.0000, 0.0089],
        [0.0293, 0.0233, 0.0000,  ..., 0.0507, 0.0000, 0.0167],
        [0.0487, 0.0384, 0.0000,  ..., 0.0824, 0.0000, 0.0293],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55513.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0859, 0.0922, 0.2092,  ..., 0.0261, 0.0670, 0.0000],
        [0.1124, 0.1093, 0.2853,  ..., 0.0374, 0.0876, 0.0000],
        [0.1412, 0.1283, 0.3702,  ..., 0.0506, 0.1101, 0.0000],
        ...,
        [0.0049, 0.0428, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0049, 0.0428, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0049, 0.0428, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398836.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3481.1028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.5989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3844.5220, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(815.4081, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-516.0808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0272],
        [ 0.0225],
        [ 0.0179],
        ...,
        [-1.0056],
        [-1.0029],
        [-1.0022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209294.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0050],
        [1.0044],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367469.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0050],
        [1.0045],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367479.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1434e-05,  1.2310e-04, -6.1093e-04,  ...,  7.9675e-04,
          0.0000e+00, -7.4397e-04],
        [-1.1434e-05,  1.2310e-04, -6.1093e-04,  ...,  7.9675e-04,
          0.0000e+00, -7.4397e-04],
        [-1.1434e-05,  1.2310e-04, -6.1093e-04,  ...,  7.9675e-04,
          0.0000e+00, -7.4397e-04],
        ...,
        [-1.1434e-05,  1.2310e-04, -6.1093e-04,  ...,  7.9675e-04,
          0.0000e+00, -7.4397e-04],
        [-1.1434e-05,  1.2310e-04, -6.1093e-04,  ...,  7.9675e-04,
          0.0000e+00, -7.4397e-04],
        [-1.1434e-05,  1.2310e-04, -6.1093e-04,  ...,  7.9675e-04,
          0.0000e+00, -7.4397e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1347.6536, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.9511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3566, device='cuda:0')



h[100].sum tensor(64.1545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.9833, device='cuda:0')



h[200].sum tensor(17.5664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0062, 0.0000,  ..., 0.0151, 0.0000, 0.0032],
        [0.0036, 0.0033, 0.0000,  ..., 0.0092, 0.0000, 0.0016],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42886.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0203, 0.0500, 0.0228,  ..., 0.0000, 0.0162, 0.0000],
        [0.0158, 0.0478, 0.0126,  ..., 0.0000, 0.0124, 0.0000],
        [0.0105, 0.0452, 0.0044,  ..., 0.0000, 0.0079, 0.0000],
        ...,
        [0.0050, 0.0430, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0050, 0.0430, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0050, 0.0430, 0.0000,  ..., 0.0000, 0.0033, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(351248.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2831.6028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(182.5253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3863.3325, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(642.9277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.0847, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7082],
        [-0.7691],
        [-0.8561],
        ...,
        [-1.0185],
        [-1.0158],
        [-1.0150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204891.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0050],
        [1.0045],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367479.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0051],
        [1.0045],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367490.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7476e-03,  4.6246e-03, -6.4938e-04,  ...,  1.0172e-02,
         -1.4543e-03,  2.9842e-03],
        [ 5.5968e-03,  4.5072e-03, -6.4837e-04,  ...,  9.9274e-03,
         -1.4162e-03,  2.8866e-03],
        [ 1.1359e-02,  8.9949e-03, -6.8682e-04,  ...,  1.9290e-02,
         -2.8705e-03,  6.6183e-03],
        ...,
        [-1.4169e-05,  1.3691e-04, -6.1093e-04,  ...,  8.0944e-04,
          0.0000e+00, -7.4748e-04],
        [-1.4169e-05,  1.3691e-04, -6.1093e-04,  ...,  8.0944e-04,
          0.0000e+00, -7.4748e-04],
        [-1.4169e-05,  1.3691e-04, -6.1093e-04,  ...,  8.0944e-04,
          0.0000e+00, -7.4748e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1608.5098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0479, device='cuda:0')



h[100].sum tensor(65.7339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.0296, device='cuda:0')



h[200].sum tensor(23.8516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0152, 0.0000,  ..., 0.0338, 0.0000, 0.0107],
        [0.0390, 0.0310, 0.0000,  ..., 0.0667, 0.0000, 0.0223],
        [0.0188, 0.0153, 0.0000,  ..., 0.0339, 0.0000, 0.0100],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48243.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0568, 0.0734, 0.1253,  ..., 0.0130, 0.0450, 0.0000],
        [0.0795, 0.0875, 0.1863,  ..., 0.0208, 0.0630, 0.0000],
        [0.0718, 0.0826, 0.1646,  ..., 0.0177, 0.0570, 0.0000],
        ...,
        [0.0052, 0.0428, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0052, 0.0428, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0052, 0.0428, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(366559.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3122.9114, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.5131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3700.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(717.4396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.8151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0883],
        [ 0.1049],
        [ 0.1129],
        ...,
        [-1.0186],
        [-1.0161],
        [-1.0156]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192369.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0051],
        [1.0045],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367490.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(498.9272, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0052],
        [1.0046],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367501.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6461e-05,  1.4384e-04, -6.1094e-04,  ...,  8.1691e-04,
          0.0000e+00, -7.5102e-04],
        [-1.6461e-05,  1.4384e-04, -6.1094e-04,  ...,  8.1691e-04,
          0.0000e+00, -7.5102e-04],
        [-1.6461e-05,  1.4384e-04, -6.1094e-04,  ...,  8.1691e-04,
          0.0000e+00, -7.5102e-04],
        ...,
        [-1.6461e-05,  1.4384e-04, -6.1094e-04,  ...,  8.1691e-04,
          0.0000e+00, -7.5102e-04],
        [-1.6461e-05,  1.4384e-04, -6.1094e-04,  ...,  8.1691e-04,
          0.0000e+00, -7.5102e-04],
        [-1.6461e-05,  1.4384e-04, -6.1094e-04,  ...,  8.1691e-04,
          0.0000e+00, -7.5102e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1554.3739, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.5843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4645, device='cuda:0')



h[100].sum tensor(65.2095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2854, device='cuda:0')



h[200].sum tensor(22.5440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1672, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47051.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0052, 0.0419, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0056, 0.0421, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0064, 0.0426, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        ...,
        [0.0053, 0.0427, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0053, 0.0427, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0053, 0.0427, 0.0000,  ..., 0.0000, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367644.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2989.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.6788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3858.3989, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(695.6262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.8608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7582],
        [-0.7753],
        [-0.7696],
        ...,
        [-1.0209],
        [-1.0193],
        [-1.0195]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220897.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0052],
        [1.0046],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367501.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0052],
        [1.0047],
        ...,
        [1.0013],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367513.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2309e-05,  1.5154e-04, -6.1094e-04,  ...,  8.1044e-04,
          0.0000e+00, -7.6087e-04],
        [-1.2309e-05,  1.5154e-04, -6.1094e-04,  ...,  8.1044e-04,
          0.0000e+00, -7.6087e-04],
        [-1.2309e-05,  1.5154e-04, -6.1094e-04,  ...,  8.1044e-04,
          0.0000e+00, -7.6087e-04],
        ...,
        [-1.2309e-05,  1.5154e-04, -6.1094e-04,  ...,  8.1044e-04,
          0.0000e+00, -7.6087e-04],
        [-1.2309e-05,  1.5154e-04, -6.1094e-04,  ...,  8.1044e-04,
          0.0000e+00, -7.6087e-04],
        [-1.2309e-05,  1.5154e-04, -6.1094e-04,  ...,  8.1044e-04,
          0.0000e+00, -7.6087e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1374.7384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.6730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2586, device='cuda:0')



h[100].sum tensor(63.7844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.6903, device='cuda:0')



h[200].sum tensor(18.3730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0035, 0.0000,  ..., 0.0092, 0.0000, 0.0016],
        [0.0073, 0.0063, 0.0000,  ..., 0.0152, 0.0000, 0.0032],
        [0.0036, 0.0035, 0.0000,  ..., 0.0092, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43488.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0206, 0.0496, 0.0250,  ..., 0.0002, 0.0174, 0.0000],
        [0.0248, 0.0519, 0.0349,  ..., 0.0004, 0.0210, 0.0000],
        [0.0197, 0.0493, 0.0226,  ..., 0.0003, 0.0167, 0.0000],
        ...,
        [0.0055, 0.0425, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0055, 0.0425, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0055, 0.0425, 0.0000,  ..., 0.0000, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(357569.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3004.8252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(189.3038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3716.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(649.8732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.1435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2089],
        [-0.1075],
        [-0.0737],
        ...,
        [-1.0269],
        [-1.0243],
        [-1.0236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191220.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0052],
        [1.0047],
        ...,
        [1.0013],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367513.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0053],
        [1.0048],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367524.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.1468e-06,  1.5343e-04, -6.1094e-04,  ...,  7.9915e-04,
          0.0000e+00, -7.6847e-04],
        [-8.1468e-06,  1.5343e-04, -6.1094e-04,  ...,  7.9915e-04,
          0.0000e+00, -7.6847e-04],
        [ 1.2029e-02,  9.5301e-03, -6.9124e-04,  ...,  2.0358e-02,
         -3.0050e-03,  7.0243e-03],
        ...,
        [-8.1468e-06,  1.5343e-04, -6.1094e-04,  ...,  7.9915e-04,
          0.0000e+00, -7.6847e-04],
        [-8.1468e-06,  1.5343e-04, -6.1094e-04,  ...,  7.9915e-04,
          0.0000e+00, -7.6847e-04],
        [-8.1468e-06,  1.5343e-04, -6.1094e-04,  ...,  7.9915e-04,
          0.0000e+00, -7.6847e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1833.5486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1338, device='cuda:0')



h[100].sum tensor(66.9502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.2655, device='cuda:0')



h[200].sum tensor(29.1727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0065, 0.0057, 0.0000,  ..., 0.0138, 0.0000, 0.0035],
        [0.0357, 0.0284, 0.0000,  ..., 0.0612, 0.0000, 0.0208],
        [0.0538, 0.0426, 0.0000,  ..., 0.0907, 0.0000, 0.0325],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53671.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0519, 0.0698, 0.1142,  ..., 0.0124, 0.0419, 0.0000],
        [0.1069, 0.1045, 0.2686,  ..., 0.0341, 0.0854, 0.0000],
        [0.1534, 0.1341, 0.4017,  ..., 0.0541, 0.1219, 0.0000],
        ...,
        [0.0056, 0.0424, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0056, 0.0424, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0056, 0.0424, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389432.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3490.9106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.1668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3666.2815, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(790.5184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-501.9024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0668],
        [ 0.1067],
        [ 0.1215],
        ...,
        [-1.0345],
        [-1.0320],
        [-1.0315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187067.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0053],
        [1.0048],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367524.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0054],
        [1.0049],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367535.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.0007e-03,  6.3873e-03, -6.6438e-04,  ...,  1.3829e-02,
         -1.9926e-03,  4.4248e-03],
        [-1.0620e-05,  1.4650e-04, -6.1094e-04,  ...,  8.1059e-04,
          0.0000e+00, -7.6144e-04],
        [-1.0620e-05,  1.4650e-04, -6.1094e-04,  ...,  8.1059e-04,
          0.0000e+00, -7.6144e-04],
        ...,
        [-1.0620e-05,  1.4650e-04, -6.1094e-04,  ...,  8.1059e-04,
          0.0000e+00, -7.6144e-04],
        [-1.0620e-05,  1.4650e-04, -6.1094e-04,  ...,  8.1059e-04,
          0.0000e+00, -7.6144e-04],
        [-1.0620e-05,  1.4650e-04, -6.1094e-04,  ...,  8.1059e-04,
          0.0000e+00, -7.6144e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1403.5327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.9418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3932, device='cuda:0')



h[100].sum tensor(64.3349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.0928, device='cuda:0')



h[200].sum tensor(18.4255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0256, 0.0205, 0.0000,  ..., 0.0449, 0.0000, 0.0150],
        [0.0080, 0.0069, 0.0000,  ..., 0.0163, 0.0000, 0.0044],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42459.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0787, 0.0876, 0.1904,  ..., 0.0232, 0.0632, 0.0000],
        [0.0402, 0.0635, 0.0836,  ..., 0.0090, 0.0328, 0.0000],
        [0.0169, 0.0491, 0.0224,  ..., 0.0012, 0.0143, 0.0000],
        ...,
        [0.0054, 0.0429, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0054, 0.0429, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0054, 0.0429, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(348694.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2782.7917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(180.9095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3757.4670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(636.0112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.8351, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0851],
        [ 0.0223],
        [-0.0914],
        ...,
        [-1.0503],
        [-1.0477],
        [-1.0470]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204726.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0054],
        [1.0049],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367535.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0054],
        [1.0050],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367546.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5140e-05,  1.3908e-04, -6.1094e-04,  ...,  8.3036e-04,
          0.0000e+00, -7.4916e-04],
        [ 2.1410e-02,  1.6830e-02, -7.5386e-04,  ...,  3.5651e-02,
         -5.3097e-03,  1.3121e-02],
        [ 2.5086e-02,  1.9693e-02, -7.7838e-04,  ...,  4.1626e-02,
         -6.2207e-03,  1.5501e-02],
        ...,
        [-1.5140e-05,  1.3908e-04, -6.1094e-04,  ...,  8.3036e-04,
          0.0000e+00, -7.4916e-04],
        [-1.5140e-05,  1.3908e-04, -6.1094e-04,  ...,  8.3036e-04,
          0.0000e+00, -7.4916e-04],
        [-1.5140e-05,  1.3908e-04, -6.1094e-04,  ...,  8.3036e-04,
          0.0000e+00, -7.4916e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1453.3829, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.5022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6907, device='cuda:0')



h[100].sum tensor(65.1530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.9824, device='cuda:0')



h[200].sum tensor(19.1033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0173, 0.0000,  ..., 0.0382, 0.0000, 0.0132],
        [0.0474, 0.0375, 0.0000,  ..., 0.0805, 0.0000, 0.0292],
        [0.1124, 0.0881, 0.0000,  ..., 0.1861, 0.0000, 0.0698],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44085.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0635, 0.0793, 0.1536,  ..., 0.0195, 0.0515, 0.0000],
        [0.1130, 0.1105, 0.2935,  ..., 0.0400, 0.0908, 0.0000],
        [0.1723, 0.1479, 0.4622,  ..., 0.0650, 0.1380, 0.0000],
        ...,
        [0.0052, 0.0436, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0052, 0.0436, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0052, 0.0436, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359200.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2772.0867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.8528, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3727.1462, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(658.7651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.2198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0681],
        [ 0.1075],
        [ 0.1267],
        ...,
        [-1.0653],
        [-1.0626],
        [-1.0619]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230390.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0054],
        [1.0050],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367546.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0055],
        [1.0051],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367557.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7519e-05,  1.3465e-04, -6.1094e-04,  ...,  8.4377e-04,
          0.0000e+00, -7.3824e-04],
        [-1.7519e-05,  1.3465e-04, -6.1094e-04,  ...,  8.4377e-04,
          0.0000e+00, -7.3824e-04],
        [-1.7519e-05,  1.3465e-04, -6.1094e-04,  ...,  8.4377e-04,
          0.0000e+00, -7.3824e-04],
        ...,
        [-1.7519e-05,  1.3465e-04, -6.1094e-04,  ...,  8.4377e-04,
          0.0000e+00, -7.3824e-04],
        [-1.7519e-05,  1.3465e-04, -6.1094e-04,  ...,  8.4377e-04,
          0.0000e+00, -7.3824e-04],
        [-1.7519e-05,  1.3465e-04, -6.1094e-04,  ...,  8.4377e-04,
          0.0000e+00, -7.3824e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1880.1099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.0942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2135, device='cuda:0')



h[100].sum tensor(68.6469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.5038, device='cuda:0')



h[200].sum tensor(28.6976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52293.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0515, 0.0283,  ..., 0.0016, 0.0166, 0.0000],
        [0.0093, 0.0458, 0.0048,  ..., 0.0000, 0.0089, 0.0000],
        [0.0055, 0.0438, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0049, 0.0442, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0049, 0.0442, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0049, 0.0442, 0.0000,  ..., 0.0000, 0.0051, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380909.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3137.3606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(266.3610, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3415.3157, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(779.0920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-483.4849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1034],
        [-0.3015],
        [-0.5257],
        ...,
        [-1.0727],
        [-1.0703],
        [-1.0701]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196588.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0055],
        [1.0051],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367557.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0056],
        [1.0052],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367568.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.8716e-05,  1.2919e-04, -6.1094e-04,  ...,  8.5337e-04,
          0.0000e+00, -7.3022e-04],
        [-1.8716e-05,  1.2919e-04, -6.1094e-04,  ...,  8.5337e-04,
          0.0000e+00, -7.3022e-04],
        [-1.8716e-05,  1.2919e-04, -6.1094e-04,  ...,  8.5337e-04,
          0.0000e+00, -7.3022e-04],
        ...,
        [-1.8716e-05,  1.2919e-04, -6.1094e-04,  ...,  8.5337e-04,
          0.0000e+00, -7.3022e-04],
        [-1.8716e-05,  1.2919e-04, -6.1094e-04,  ...,  8.5337e-04,
          0.0000e+00, -7.3022e-04],
        [-1.8716e-05,  1.2919e-04, -6.1094e-04,  ...,  8.5337e-04,
          0.0000e+00, -7.3022e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1553.4014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.8103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4802, device='cuda:0')



h[100].sum tensor(66.9099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.3425, device='cuda:0')



h[200].sum tensor(20.4920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45320.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0438, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0066, 0.0452, 0.0034,  ..., 0.0000, 0.0067, 0.0000],
        [0.0094, 0.0471, 0.0087,  ..., 0.0002, 0.0090, 0.0000],
        ...,
        [0.0046, 0.0447, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0046, 0.0447, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0046, 0.0447, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359299.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2727.6057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(204.4213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3438.6990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(684.8188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-406.8044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0283],
        [-0.9311],
        [-0.7353],
        ...,
        [-1.0921],
        [-1.0894],
        [-1.0886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205943.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0056],
        [1.0052],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367568.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0057],
        [1.0053],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367580.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6129e-03,  5.2899e-03, -6.5514e-04,  ...,  1.1623e-02,
         -1.6243e-03,  3.5637e-03],
        [ 1.5630e-02,  1.2315e-02, -7.1529e-04,  ...,  2.6282e-02,
         -3.8348e-03,  9.4028e-03],
        [ 1.9753e-02,  1.5527e-02, -7.4279e-04,  ...,  3.2984e-02,
         -4.8455e-03,  1.2073e-02],
        ...,
        [-1.3255e-05,  1.2764e-04, -6.1094e-04,  ...,  8.5163e-04,
          0.0000e+00, -7.2696e-04],
        [-1.3255e-05,  1.2764e-04, -6.1094e-04,  ...,  8.5163e-04,
          0.0000e+00, -7.2696e-04],
        [-1.3255e-05,  1.2764e-04, -6.1094e-04,  ...,  8.5163e-04,
          0.0000e+00, -7.2696e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2530.3987, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.3364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.3363, device='cuda:0')



h[100].sum tensor(74.0214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(57.8089, device='cuda:0')



h[200].sum tensor(43.2607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.1567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0297, 0.0236, 0.0000,  ..., 0.0517, 0.0000, 0.0178],
        [0.0665, 0.0523, 0.0000,  ..., 0.1115, 0.0000, 0.0401],
        [0.0802, 0.0631, 0.0000,  ..., 0.1340, 0.0000, 0.0491],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71709.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0911, 0.0982, 0.2297,  ..., 0.0287, 0.0748, 0.0000],
        [0.1452, 0.1327, 0.3841,  ..., 0.0520, 0.1177, 0.0000],
        [0.1832, 0.1571, 0.4936,  ..., 0.0690, 0.1476, 0.0000],
        ...,
        [0.0045, 0.0450, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0045, 0.0450, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0045, 0.0450, 0.0000,  ..., 0.0000, 0.0053, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469783., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4182.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(438.7679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3424.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1046.0664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-687.3237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0649],
        [ 0.0469],
        [ 0.0269],
        ...,
        [-1.0903],
        [-1.0945],
        [-1.0959]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220634.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0057],
        [1.0053],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367580.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0058],
        [1.0054],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367591.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8670e-02,  1.4681e-02, -7.3546e-04,  ...,  3.1195e-02,
         -4.5596e-03,  1.1357e-02],
        [ 3.2753e-02,  2.5653e-02, -8.2940e-04,  ...,  5.4088e-02,
         -7.9992e-03,  2.0476e-02],
        [ 1.7719e-02,  1.3940e-02, -7.2912e-04,  ...,  2.9649e-02,
         -4.3273e-03,  1.0741e-02],
        ...,
        [ 2.9630e-07,  1.3571e-04, -6.1094e-04,  ...,  8.4694e-04,
          0.0000e+00, -7.3042e-04],
        [ 2.9630e-07,  1.3571e-04, -6.1094e-04,  ...,  8.4694e-04,
          0.0000e+00, -7.3042e-04],
        [ 2.9630e-07,  1.3571e-04, -6.1094e-04,  ...,  8.4694e-04,
          0.0000e+00, -7.3042e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1557.0735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.8752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1992, device='cuda:0')



h[100].sum tensor(67.6103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.5026, device='cuda:0')



h[200].sum tensor(20.0268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.1767e-02, 7.2037e-02, 0.0000e+00,  ..., 1.5257e-01, 0.0000e+00,
         5.6484e-02],
        [9.5049e-02, 7.4596e-02, 0.0000e+00,  ..., 1.5791e-01, 0.0000e+00,
         5.8600e-02],
        [1.0049e-01, 7.8832e-02, 0.0000e+00,  ..., 1.6675e-01, 0.0000e+00,
         6.2120e-02],
        ...,
        [1.2085e-06, 5.5351e-04, 0.0000e+00,  ..., 3.4544e-03, 0.0000e+00,
         0.0000e+00],
        [1.2086e-06, 5.5357e-04, 0.0000e+00,  ..., 3.4548e-03, 0.0000e+00,
         0.0000e+00],
        [1.2087e-06, 5.5362e-04, 0.0000e+00,  ..., 3.4551e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44539.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1506, 0.1357, 0.4018,  ..., 0.0563, 0.1219, 0.0000],
        [0.1754, 0.1513, 0.4711,  ..., 0.0661, 0.1419, 0.0000],
        [0.1795, 0.1538, 0.4816,  ..., 0.0673, 0.1453, 0.0000],
        ...,
        [0.0048, 0.0450, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0048, 0.0450, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0048, 0.0450, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(356057., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2665.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(196.8779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3589.8071, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(673.5557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-394.9374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0277],
        [ 0.0639],
        [ 0.0853],
        ...,
        [-1.1061],
        [-1.1033],
        [-1.1026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213954.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0058],
        [1.0054],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367591.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0059],
        [1.0055],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367602.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3753e-02,  1.8639e-02, -7.6927e-04,  ...,  3.9426e-02,
         -5.7762e-03,  1.4632e-02],
        [ 4.6190e-02,  3.6119e-02, -9.1890e-04,  ...,  7.5895e-02,
         -1.1235e-02,  2.9156e-02],
        [ 4.5197e-02,  3.5346e-02, -9.1228e-04,  ...,  7.4281e-02,
         -1.0994e-02,  2.8514e-02],
        ...,
        [ 1.4346e-05,  1.4407e-04, -6.1094e-04,  ...,  8.3932e-04,
          0.0000e+00, -7.3560e-04],
        [ 1.4346e-05,  1.4407e-04, -6.1094e-04,  ...,  8.3932e-04,
          0.0000e+00, -7.3560e-04],
        [ 1.4346e-05,  1.4407e-04, -6.1094e-04,  ...,  8.3932e-04,
          0.0000e+00, -7.3560e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1599.6013, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4463, device='cuda:0')



h[100].sum tensor(68.0620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.2412, device='cuda:0')



h[200].sum tensor(20.8798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1722e-01, 9.1857e-02, 0.0000e+00,  ..., 1.9380e-01, 0.0000e+00,
         7.2893e-02],
        [1.5545e-01, 1.2164e-01, 0.0000e+00,  ..., 2.5595e-01, 0.0000e+00,
         9.7632e-02],
        [1.6266e-01, 1.2726e-01, 0.0000e+00,  ..., 2.6767e-01, 0.0000e+00,
         1.0230e-01],
        ...,
        [5.8523e-05, 5.8771e-04, 0.0000e+00,  ..., 3.4238e-03, 0.0000e+00,
         0.0000e+00],
        [5.8529e-05, 5.8778e-04, 0.0000e+00,  ..., 3.4242e-03, 0.0000e+00,
         0.0000e+00],
        [5.8534e-05, 5.8783e-04, 0.0000e+00,  ..., 3.4245e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46547.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2160, 0.1768, 0.5934,  ..., 0.0880, 0.1739, 0.0000],
        [0.2787, 0.2164, 0.7743,  ..., 0.1164, 0.2238, 0.0000],
        [0.2884, 0.2224, 0.8012,  ..., 0.1202, 0.2317, 0.0000],
        ...,
        [0.0050, 0.0450, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0050, 0.0451, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0050, 0.0451, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367878.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2918.7729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.7920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3595.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(702.6788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.7284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0149],
        [ 0.0413],
        [ 0.0614],
        ...,
        [-1.1076],
        [-1.1047],
        [-1.1029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199732.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0059],
        [1.0055],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367602.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0059],
        [1.0055],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367602.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.8068e-03,  7.7733e-03, -6.7625e-04,  ...,  1.6756e-02,
         -2.3827e-03,  5.6036e-03],
        [ 2.3584e-02,  1.8507e-02, -7.6814e-04,  ...,  3.9151e-02,
         -5.7350e-03,  1.4522e-02],
        [ 2.0214e-02,  1.5882e-02, -7.4566e-04,  ...,  3.3673e-02,
         -4.9150e-03,  1.2341e-02],
        ...,
        [ 1.4346e-05,  1.4407e-04, -6.1094e-04,  ...,  8.3932e-04,
          0.0000e+00, -7.3560e-04],
        [ 1.4346e-05,  1.4407e-04, -6.1094e-04,  ...,  8.3932e-04,
          0.0000e+00, -7.3560e-04],
        [ 1.4346e-05,  1.4407e-04, -6.1094e-04,  ...,  8.3932e-04,
          0.0000e+00, -7.3560e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1985.2720, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6314, device='cuda:0')



h[100].sum tensor(70.6825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.7533, device='cuda:0')



h[200].sum tensor(29.9697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.6008e-02, 5.1960e-02, 0.0000e+00,  ..., 1.1057e-01, 0.0000e+00,
         3.9742e-02],
        [6.5681e-02, 5.1706e-02, 0.0000e+00,  ..., 1.1004e-01, 0.0000e+00,
         3.9521e-02],
        [6.8416e-02, 5.3838e-02, 0.0000e+00,  ..., 1.1449e-01, 0.0000e+00,
         4.1292e-02],
        ...,
        [5.8523e-05, 5.8771e-04, 0.0000e+00,  ..., 3.4238e-03, 0.0000e+00,
         0.0000e+00],
        [5.8529e-05, 5.8778e-04, 0.0000e+00,  ..., 3.4242e-03, 0.0000e+00,
         0.0000e+00],
        [5.8534e-05, 5.8783e-04, 0.0000e+00,  ..., 3.4245e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55390.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1090, 0.1090, 0.2792,  ..., 0.0370, 0.0889, 0.0000],
        [0.1216, 0.1170, 0.3141,  ..., 0.0420, 0.0990, 0.0000],
        [0.1206, 0.1164, 0.3112,  ..., 0.0415, 0.0983, 0.0000],
        ...,
        [0.0050, 0.0450, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0050, 0.0451, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0050, 0.0451, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402872., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3443.9751, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.5211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3490.2180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(825.4789, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-512.5818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1006],
        [ 0.1087],
        [ 0.1118],
        ...,
        [-1.1059],
        [-1.1049],
        [-1.1049]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192524.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0059],
        [1.0055],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367602.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0060],
        [1.0056],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367613.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3201e-02,  1.0413e-02, -6.9883e-04,  ...,  2.2247e-02,
         -3.1950e-03,  7.7909e-03],
        [ 1.1896e-02,  9.3964e-03, -6.9013e-04,  ...,  2.0126e-02,
         -2.8787e-03,  6.9462e-03],
        [ 1.2958e-02,  1.0224e-02, -6.9722e-04,  ...,  2.1853e-02,
         -3.1362e-03,  7.6338e-03],
        ...,
        [ 2.1244e-05,  1.4495e-04, -6.1094e-04,  ...,  8.2609e-04,
          0.0000e+00, -7.4015e-04],
        [ 1.4829e-02,  1.1682e-02, -7.0970e-04,  ...,  2.4894e-02,
         -3.5898e-03,  8.8451e-03],
        [ 2.1244e-05,  1.4495e-04, -6.1094e-04,  ...,  8.2609e-04,
          0.0000e+00, -7.4015e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1869.7957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3711, device='cuda:0')



h[100].sum tensor(70.1308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.9855, device='cuda:0')



h[200].sum tensor(26.7699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3798, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0457, 0.0361, 0.0000,  ..., 0.0775, 0.0000, 0.0266],
        [0.0470, 0.0371, 0.0000,  ..., 0.0796, 0.0000, 0.0274],
        [0.0455, 0.0360, 0.0000,  ..., 0.0772, 0.0000, 0.0264],
        ...,
        [0.0152, 0.0124, 0.0000,  ..., 0.0279, 0.0000, 0.0090],
        [0.0124, 0.0102, 0.0000,  ..., 0.0234, 0.0000, 0.0072],
        [0.0550, 0.0434, 0.0000,  ..., 0.0926, 0.0000, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52036.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0942, 0.0990, 0.2313,  ..., 0.0280, 0.0779, 0.0000],
        [0.0939, 0.0990, 0.2312,  ..., 0.0283, 0.0775, 0.0000],
        [0.0925, 0.0982, 0.2272,  ..., 0.0277, 0.0764, 0.0000],
        ...,
        [0.0320, 0.0618, 0.0643,  ..., 0.0068, 0.0276, 0.0000],
        [0.0397, 0.0666, 0.0852,  ..., 0.0093, 0.0338, 0.0000],
        [0.0643, 0.0819, 0.1529,  ..., 0.0188, 0.0535, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385699.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3301.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.5736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3499.8110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(782.8088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-477.4532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1500],
        [ 0.1494],
        [ 0.1482],
        ...,
        [-0.3371],
        [-0.2380],
        [-0.1956]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170413.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0060],
        [1.0056],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367613.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0061],
        [1.0057],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367625.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5501e-05,  1.4366e-04, -6.1094e-04,  ...,  8.1491e-04,
          0.0000e+00, -7.4091e-04],
        [ 2.5501e-05,  1.4366e-04, -6.1094e-04,  ...,  8.1491e-04,
          0.0000e+00, -7.4091e-04],
        [ 2.5501e-05,  1.4366e-04, -6.1094e-04,  ...,  8.1491e-04,
          0.0000e+00, -7.4091e-04],
        ...,
        [ 2.5501e-05,  1.4366e-04, -6.1094e-04,  ...,  8.1491e-04,
          0.0000e+00, -7.4091e-04],
        [ 2.5501e-05,  1.4366e-04, -6.1094e-04,  ...,  8.1491e-04,
          0.0000e+00, -7.4091e-04],
        [ 2.5501e-05,  1.4366e-04, -6.1094e-04,  ...,  8.1491e-04,
          0.0000e+00, -7.4091e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2058.2456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.5102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0878, device='cuda:0')



h[100].sum tensor(71.7935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.1179, device='cuda:0')



h[200].sum tensor(30.7506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5713, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0001, 0.0006, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56687.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0143, 0.0496, 0.0165,  ..., 0.0004, 0.0137, 0.0000],
        [0.0091, 0.0467, 0.0032,  ..., 0.0000, 0.0095, 0.0000],
        [0.0072, 0.0457, 0.0010,  ..., 0.0000, 0.0079, 0.0000],
        ...,
        [0.0052, 0.0454, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0052, 0.0454, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0052, 0.0454, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403454.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3573.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.1839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3502.1897, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(848.3815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-525.0156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1281],
        [-0.3074],
        [-0.5052],
        ...,
        [-1.1317],
        [-1.1289],
        [-1.1282]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173062.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0061],
        [1.0057],
        ...,
        [1.0013],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367625.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0062],
        [1.0058],
        ...,
        [1.0014],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367635.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0405e-07,  1.2508e-04, -6.1094e-04,  ...,  8.3238e-04,
          0.0000e+00, -7.2048e-04],
        [-1.0405e-07,  1.2508e-04, -6.1094e-04,  ...,  8.3238e-04,
          0.0000e+00, -7.2048e-04],
        [ 1.1713e-02,  9.2501e-03, -6.8905e-04,  ...,  1.9871e-02,
         -2.8186e-03,  6.8616e-03],
        ...,
        [-1.0405e-07,  1.2508e-04, -6.1094e-04,  ...,  8.3238e-04,
          0.0000e+00, -7.2048e-04],
        [-1.0405e-07,  1.2508e-04, -6.1094e-04,  ...,  8.3238e-04,
          0.0000e+00, -7.2048e-04],
        [-1.0405e-07,  1.2508e-04, -6.1094e-04,  ...,  8.3238e-04,
          0.0000e+00, -7.2048e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1996.3256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3098, device='cuda:0')



h[100].sum tensor(71.8177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.7917, device='cuda:0')



h[200].sum tensor(28.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4845, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0214, 0.0172, 0.0000,  ..., 0.0382, 0.0000, 0.0124],
        [0.0296, 0.0235, 0.0000,  ..., 0.0514, 0.0000, 0.0170],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0157, 0.0127, 0.0000,  ..., 0.0289, 0.0000, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53292.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0226, 0.0564, 0.0388,  ..., 0.0043, 0.0201, 0.0000],
        [0.0549, 0.0766, 0.1267,  ..., 0.0156, 0.0461, 0.0000],
        [0.0820, 0.0933, 0.1999,  ..., 0.0256, 0.0682, 0.0000],
        ...,
        [0.0101, 0.0495, 0.0086,  ..., 0.0002, 0.0102, 0.0000],
        [0.0207, 0.0557, 0.0337,  ..., 0.0029, 0.0189, 0.0000],
        [0.0453, 0.0707, 0.0967,  ..., 0.0101, 0.0391, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(388857.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3232.6846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(270.9391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3515.5076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(803.1623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-484.1700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1174],
        [-0.0118],
        [ 0.0684],
        ...,
        [-0.6809],
        [-0.3573],
        [-0.0901]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189384.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0062],
        [1.0058],
        ...,
        [1.0014],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367635.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0063],
        [1.0059],
        ...,
        [1.0014],
        [1.0006],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367646.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0663e-02,  8.4318e-03, -6.8218e-04,  ...,  1.8213e-02,
         -2.5615e-03,  6.2106e-03],
        [ 2.8404e-02,  2.2253e-02, -8.0050e-04,  ...,  4.7054e-02,
         -6.8152e-03,  1.7696e-02],
        [ 4.7837e-03,  3.8515e-03, -6.4298e-04,  ...,  8.6552e-03,
         -1.1518e-03,  2.4044e-03],
        ...,
        [-2.0272e-05,  1.0896e-04, -6.1094e-04,  ...,  8.4566e-04,
          0.0000e+00, -7.0571e-04],
        [-2.0272e-05,  1.0896e-04, -6.1094e-04,  ...,  8.4566e-04,
          0.0000e+00, -7.0571e-04],
        [-2.0272e-05,  1.0896e-04, -6.1094e-04,  ...,  8.4566e-04,
          0.0000e+00, -7.0571e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1632.0337, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.4299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3143, device='cuda:0')



h[100].sum tensor(69.7277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8465, device='cuda:0')



h[200].sum tensor(19.8683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1011, 0.0792, 0.0000,  ..., 0.1678, 0.0000, 0.0627],
        [0.0511, 0.0403, 0.0000,  ..., 0.0866, 0.0000, 0.0303],
        [0.0569, 0.0448, 0.0000,  ..., 0.0960, 0.0000, 0.0340],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43937.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1649, 0.1468, 0.4385,  ..., 0.0644, 0.1339, 0.0000],
        [0.1304, 0.1247, 0.3374,  ..., 0.0476, 0.1069, 0.0000],
        [0.1096, 0.1117, 0.2788,  ..., 0.0385, 0.0903, 0.0000],
        ...,
        [0.0047, 0.0472, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0047, 0.0472, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0047, 0.0472, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355170.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2534.5303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(189.5889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3706.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(674.1580, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.3661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0694],
        [ 0.0714],
        [ 0.0621],
        ...,
        [-1.1694],
        [-1.1665],
        [-1.1657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240650.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0063],
        [1.0059],
        ...,
        [1.0014],
        [1.0006],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367646.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0064],
        [1.0060],
        ...,
        [1.0014],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367657.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.3476e-03,  7.4116e-03, -6.7349e-04,  ...,  1.6108e-02,
         -2.2405e-03,  5.3707e-03],
        [ 1.8788e-02,  1.4767e-02, -7.3644e-04,  ...,  3.1457e-02,
         -4.4957e-03,  1.1483e-02],
        [ 9.4092e-03,  7.4596e-03, -6.7390e-04,  ...,  1.6208e-02,
         -2.2552e-03,  5.4106e-03],
        ...,
        [-3.1640e-05,  1.0464e-04, -6.1094e-04,  ...,  8.5982e-04,
          0.0000e+00, -7.0129e-04],
        [-3.1640e-05,  1.0464e-04, -6.1094e-04,  ...,  8.5982e-04,
          0.0000e+00, -7.0129e-04],
        [-3.1640e-05,  1.0464e-04, -6.1094e-04,  ...,  8.5982e-04,
          0.0000e+00, -7.0129e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2120.0862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2624, device='cuda:0')



h[100].sum tensor(73.0645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.6396, device='cuda:0')



h[200].sum tensor(31.1124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0621, 0.0489, 0.0000,  ..., 0.1046, 0.0000, 0.0382],
        [0.0622, 0.0490, 0.0000,  ..., 0.1048, 0.0000, 0.0375],
        [0.0616, 0.0485, 0.0000,  ..., 0.1038, 0.0000, 0.0378],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58992.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1853, 0.1606, 0.5004,  ..., 0.0760, 0.1499, 0.0000],
        [0.2029, 0.1719, 0.5498,  ..., 0.0835, 0.1640, 0.0000],
        [0.1945, 0.1667, 0.5263,  ..., 0.0800, 0.1572, 0.0000],
        ...,
        [0.0046, 0.0474, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0046, 0.0474, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0046, 0.0474, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419091.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3496.0879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(322.4217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3409.1802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(887.3439, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-542.0421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0171],
        [-0.0202],
        [-0.0191],
        ...,
        [-1.1812],
        [-1.1783],
        [-1.1775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214258.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0064],
        [1.0060],
        ...,
        [1.0014],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367657.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0065],
        [1.0061],
        ...,
        [1.0014],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367668.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.7405e-05,  1.1160e-04, -6.1094e-04,  ...,  8.8199e-04,
          0.0000e+00, -7.0241e-04],
        [-3.7405e-05,  1.1160e-04, -6.1094e-04,  ...,  8.8199e-04,
          0.0000e+00, -7.0241e-04],
        [-3.7405e-05,  1.1160e-04, -6.1094e-04,  ...,  8.8199e-04,
          0.0000e+00, -7.0241e-04],
        ...,
        [-3.7405e-05,  1.1160e-04, -6.1094e-04,  ...,  8.8199e-04,
          0.0000e+00, -7.0241e-04],
        [-3.7405e-05,  1.1160e-04, -6.1094e-04,  ...,  8.8199e-04,
          0.0000e+00, -7.0241e-04],
        [-3.7405e-05,  1.1160e-04, -6.1094e-04,  ...,  8.8199e-04,
          0.0000e+00, -7.0241e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1715.3357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.9665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8657, device='cuda:0')



h[100].sum tensor(70.1351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.4950, device='cuda:0')



h[200].sum tensor(21.6867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1004, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47581.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0061, 0.0471, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0048, 0.0465, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0045, 0.0465, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        ...,
        [0.0046, 0.0473, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0046, 0.0473, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0046, 0.0473, 0.0000,  ..., 0.0000, 0.0061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(374885.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2884.8970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.9297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3349.7749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(733.1836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.0597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6318],
        [-0.8734],
        [-1.0700],
        ...,
        [-1.1851],
        [-1.1823],
        [-1.1815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206442.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0065],
        [1.0061],
        ...,
        [1.0014],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367668.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0067],
        [1.0063],
        ...,
        [1.0015],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367679.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.4847e-05,  1.2792e-04, -6.1094e-04,  ...,  8.9398e-04,
          0.0000e+00, -7.1131e-04],
        [-3.4847e-05,  1.2792e-04, -6.1094e-04,  ...,  8.9398e-04,
          0.0000e+00, -7.1131e-04],
        [-3.4847e-05,  1.2792e-04, -6.1094e-04,  ...,  8.9398e-04,
          0.0000e+00, -7.1131e-04],
        ...,
        [-3.4847e-05,  1.2792e-04, -6.1094e-04,  ...,  8.9398e-04,
          0.0000e+00, -7.1131e-04],
        [-3.4847e-05,  1.2792e-04, -6.1094e-04,  ...,  8.9398e-04,
          0.0000e+00, -7.1131e-04],
        [-3.4847e-05,  1.2792e-04, -6.1094e-04,  ...,  8.9398e-04,
          0.0000e+00, -7.1131e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1652.1605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.3771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9144, device='cuda:0')



h[100].sum tensor(69.1841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.6511, device='cuda:0')



h[200].sum tensor(20.5447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44931.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0046, 0.0458, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0047, 0.0459, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0047, 0.0461, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        ...,
        [0.0048, 0.0468, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0048, 0.0468, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0048, 0.0468, 0.0000,  ..., 0.0000, 0.0067, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(362563., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2712.5549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(200.6104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3357.5845, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(695.9913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-398.0507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2372],
        [-1.3009],
        [-1.3488],
        ...,
        [-1.1818],
        [-1.1791],
        [-1.1784]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206159.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0067],
        [1.0063],
        ...,
        [1.0015],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367679.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0068],
        [1.0064],
        ...,
        [1.0015],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367691.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3281e-02,  1.0513e-02, -6.9968e-04,  ...,  2.2534e-02,
         -3.1439e-03,  7.8900e-03],
        [ 2.2339e-02,  1.7570e-02, -7.6008e-04,  ...,  3.7261e-02,
         -5.2836e-03,  1.3752e-02],
        [ 1.5221e-02,  1.2025e-02, -7.1262e-04,  ...,  2.5689e-02,
         -3.6023e-03,  9.1458e-03],
        ...,
        [-2.9595e-05,  1.4242e-04, -6.1094e-04,  ...,  8.9379e-04,
          0.0000e+00, -7.2273e-04],
        [-2.9595e-05,  1.4242e-04, -6.1094e-04,  ...,  8.9379e-04,
          0.0000e+00, -7.2273e-04],
        [-2.9595e-05,  1.4242e-04, -6.1094e-04,  ...,  8.9379e-04,
          0.0000e+00, -7.2273e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2903.6116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(42.1892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.0682, device='cuda:0')



h[100].sum tensor(77.1162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(65.9763, device='cuda:0')



h[200].sum tensor(49.7282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.4614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0368, 0.0293, 0.0000,  ..., 0.0636, 0.0000, 0.0217],
        [0.0541, 0.0428, 0.0000,  ..., 0.0917, 0.0000, 0.0322],
        [0.1037, 0.0814, 0.0000,  ..., 0.1723, 0.0000, 0.0642],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84340.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1118, 0.1109, 0.2814,  ..., 0.0373, 0.0943, 0.0000],
        [0.1549, 0.1383, 0.4037,  ..., 0.0568, 0.1286, 0.0000],
        [0.2154, 0.1769, 0.5780,  ..., 0.0857, 0.1761, 0.0000],
        ...,
        [0.0048, 0.0464, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0048, 0.0464, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0048, 0.0464, 0.0000,  ..., 0.0000, 0.0072, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558829.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5600.3120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(549.2069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3003.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1242.4432, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-828.3758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1466],
        [ 0.1335],
        [ 0.1196],
        ...,
        [-1.1811],
        [-1.1784],
        [-1.1778]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159140.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0068],
        [1.0064],
        ...,
        [1.0015],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367691.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0069],
        [1.0065],
        ...,
        [1.0016],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367702.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1643e-02,  9.2442e-03, -6.8878e-04,  ...,  1.9890e-02,
         -2.7476e-03,  6.8348e-03],
        [ 5.2612e-03,  4.2721e-03, -6.4624e-04,  ...,  9.5132e-03,
         -1.2459e-03,  2.7056e-03],
        [ 2.0277e-02,  1.5971e-02, -7.4634e-04,  ...,  3.3929e-02,
         -4.7794e-03,  1.2422e-02],
        ...,
        [-3.2958e-05,  1.4718e-04, -6.1094e-04,  ...,  9.0475e-04,
          0.0000e+00, -7.2012e-04],
        [-3.2958e-05,  1.4718e-04, -6.1094e-04,  ...,  9.0475e-04,
          0.0000e+00, -7.2012e-04],
        [-3.2958e-05,  1.4718e-04, -6.1094e-04,  ...,  9.0475e-04,
          0.0000e+00, -7.2012e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2091.6975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1967, device='cuda:0')



h[100].sum tensor(71.9503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.4537, device='cuda:0')



h[200].sum tensor(30.5488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4719, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0261, 0.0210, 0.0000,  ..., 0.0462, 0.0000, 0.0141],
        [0.0469, 0.0373, 0.0000,  ..., 0.0802, 0.0000, 0.0276],
        [0.0263, 0.0212, 0.0000,  ..., 0.0466, 0.0000, 0.0149],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59190.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0904, 0.0968, 0.2184,  ..., 0.0255, 0.0786, 0.0000],
        [0.0898, 0.0970, 0.2174,  ..., 0.0256, 0.0779, 0.0000],
        [0.0753, 0.0883, 0.1780,  ..., 0.0199, 0.0661, 0.0000],
        ...,
        [0.0044, 0.0463, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0044, 0.0463, 0.0000,  ..., 0.0000, 0.0076, 0.0000],
        [0.0044, 0.0463, 0.0000,  ..., 0.0000, 0.0076, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(432862.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3740.4868, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.3957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3116.4119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(898.9616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-561.5874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1570],
        [ 0.1517],
        [ 0.1424],
        ...,
        [-1.1889],
        [-1.1863],
        [-1.1857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158842.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0069],
        [1.0065],
        ...,
        [1.0016],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367702.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(1025.7068, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0071],
        [1.0066],
        ...,
        [1.0015],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367714.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6125e-03,  6.8808e-03, -6.6866e-04,  ...,  1.5003e-02,
         -2.0299e-03,  4.8981e-03],
        [ 8.7959e-03,  7.0237e-03, -6.6989e-04,  ...,  1.5301e-02,
         -2.0729e-03,  5.0168e-03],
        [-4.4756e-05,  1.3582e-04, -6.1094e-04,  ...,  9.2247e-04,
          0.0000e+00, -7.0569e-04],
        ...,
        [-4.4756e-05,  1.3582e-04, -6.1094e-04,  ...,  9.2247e-04,
          0.0000e+00, -7.0569e-04],
        [-4.4756e-05,  1.3582e-04, -6.1094e-04,  ...,  9.2247e-04,
          0.0000e+00, -7.0569e-04],
        [-4.4756e-05,  1.3582e-04, -6.1094e-04,  ...,  9.2247e-04,
          0.0000e+00, -7.0569e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2076.1685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.8275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9992, device='cuda:0')



h[100].sum tensor(72.7108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.8632, device='cuda:0')



h[200].sum tensor(29.1516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0478, 0.0379, 0.0000,  ..., 0.0818, 0.0000, 0.0282],
        [0.0159, 0.0130, 0.0000,  ..., 0.0297, 0.0000, 0.0089],
        [0.0089, 0.0075, 0.0000,  ..., 0.0182, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57406.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0768, 0.0911, 0.1886,  ..., 0.0226, 0.0669, 0.0000],
        [0.0446, 0.0712, 0.0999,  ..., 0.0096, 0.0410, 0.0000],
        [0.0251, 0.0593, 0.0470,  ..., 0.0037, 0.0250, 0.0000],
        ...,
        [0.0037, 0.0469, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0037, 0.0469, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0037, 0.0469, 0.0000,  ..., 0.0000, 0.0075, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426743.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3357.5723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.3413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3278.5330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(876.0246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-542.8959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0810],
        [-0.3228],
        [-0.6346],
        ...,
        [-1.2109],
        [-1.2082],
        [-1.2075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184399.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0071],
        [1.0066],
        ...,
        [1.0015],
        [1.0007],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367714.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0072],
        [1.0068],
        ...,
        [1.0015],
        [1.0007],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367725.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.2484e-05,  1.1799e-04, -6.1094e-04,  ...,  9.2496e-04,
          0.0000e+00, -6.9790e-04],
        [ 2.6452e-02,  2.0767e-02, -7.8767e-04,  ...,  4.4040e-02,
         -6.1920e-03,  1.6462e-02],
        [-5.2484e-05,  1.1799e-04, -6.1094e-04,  ...,  9.2496e-04,
          0.0000e+00, -6.9790e-04],
        ...,
        [-5.2484e-05,  1.1799e-04, -6.1094e-04,  ...,  9.2496e-04,
          0.0000e+00, -6.9790e-04],
        [-5.2484e-05,  1.1799e-04, -6.1094e-04,  ...,  9.2496e-04,
          0.0000e+00, -6.9790e-04],
        [-5.2484e-05,  1.1799e-04, -6.1094e-04,  ...,  9.2496e-04,
          0.0000e+00, -6.9790e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2055.1099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6377, device='cuda:0')



h[100].sum tensor(73.3748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.7825, device='cuda:0')



h[200].sum tensor(27.4457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4096, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0602, 0.0475, 0.0000,  ..., 0.1020, 0.0000, 0.0363],
        [0.0217, 0.0175, 0.0000,  ..., 0.0392, 0.0000, 0.0127],
        [0.0629, 0.0496, 0.0000,  ..., 0.1064, 0.0000, 0.0380],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0005, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55928.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0768, 0.0925, 0.1926,  ..., 0.0240, 0.0669, 0.0000],
        [0.0621, 0.0835, 0.1516,  ..., 0.0176, 0.0551, 0.0000],
        [0.0790, 0.0942, 0.1990,  ..., 0.0250, 0.0687, 0.0000],
        ...,
        [0.0029, 0.0475, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0029, 0.0475, 0.0000,  ..., 0.0000, 0.0073, 0.0000],
        [0.0029, 0.0475, 0.0000,  ..., 0.0000, 0.0073, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414146.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2913.6716, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.7051, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3515.8857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(858.5195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-526.5475, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1944],
        [-0.2132],
        [-0.2538],
        ...,
        [-1.2252],
        [-1.2229],
        [-1.2240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213075.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0072],
        [1.0068],
        ...,
        [1.0015],
        [1.0007],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367725.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0073],
        [1.0069],
        ...,
        [1.0015],
        [1.0006],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367736.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7720e-03,  5.4224e-03, -6.5647e-04,  ...,  1.2025e-02,
         -1.5893e-03,  3.7265e-03],
        [ 2.0331e-02,  1.5986e-02, -7.4689e-04,  ...,  3.4084e-02,
         -4.7455e-03,  1.2507e-02],
        [ 2.5083e-02,  1.9687e-02, -7.7857e-04,  ...,  4.1814e-02,
         -5.8515e-03,  1.5584e-02],
        ...,
        [-5.6181e-05,  1.0297e-04, -6.1094e-04,  ...,  9.1638e-04,
          0.0000e+00, -6.9514e-04],
        [-5.6181e-05,  1.0297e-04, -6.1094e-04,  ...,  9.1638e-04,
          0.0000e+00, -6.9514e-04],
        [-5.6181e-05,  1.0297e-04, -6.1094e-04,  ...,  9.1638e-04,
          0.0000e+00, -6.9514e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1877.9874, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.1320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7434, device='cuda:0')



h[100].sum tensor(72.8997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.1192, device='cuda:0')



h[200].sum tensor(22.2605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0296, 0.0236, 0.0000,  ..., 0.0521, 0.0000, 0.0179],
        [0.0700, 0.0551, 0.0000,  ..., 0.1180, 0.0000, 0.0427],
        [0.1160, 0.0910, 0.0000,  ..., 0.1928, 0.0000, 0.0725],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50707.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0917, 0.1024, 0.2367,  ..., 0.0311, 0.0793, 0.0000],
        [0.1526, 0.1410, 0.4108,  ..., 0.0595, 0.1279, 0.0000],
        [0.2185, 0.1831, 0.6016,  ..., 0.0914, 0.1802, 0.0000],
        ...,
        [0.0024, 0.0479, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0024, 0.0479, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0024, 0.0480, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(394157.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2523.1357, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.1677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3613.6565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(792.7906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-473.6320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0461],
        [ 0.0215],
        [-0.0055],
        ...,
        [-1.2629],
        [-1.2599],
        [-1.2592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198405.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0073],
        [1.0069],
        ...,
        [1.0015],
        [1.0006],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367736.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0074],
        [1.0070],
        ...,
        [1.0014],
        [1.0006],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367748.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0362e-02,  8.2076e-03, -6.8039e-04,  ...,  1.7850e-02,
         -2.4152e-03,  6.0446e-03],
        [-5.2456e-05,  9.4614e-05, -6.1094e-04,  ...,  9.0565e-04,
          0.0000e+00, -6.9996e-04],
        [-5.2456e-05,  9.4614e-05, -6.1094e-04,  ...,  9.0565e-04,
          0.0000e+00, -6.9996e-04],
        ...,
        [-5.2456e-05,  9.4614e-05, -6.1094e-04,  ...,  9.0565e-04,
          0.0000e+00, -6.9996e-04],
        [-5.2456e-05,  9.4614e-05, -6.1094e-04,  ...,  9.0565e-04,
          0.0000e+00, -6.9996e-04],
        [-5.2456e-05,  9.4614e-05, -6.1094e-04,  ...,  9.0565e-04,
          0.0000e+00, -6.9996e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1843.0298, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4590, device='cuda:0')



h[100].sum tensor(73.1400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2689, device='cuda:0')



h[200].sum tensor(20.6250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0168, 0.0000,  ..., 0.0379, 0.0000, 0.0122],
        [0.0104, 0.0086, 0.0000,  ..., 0.0207, 0.0000, 0.0061],
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48727.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0540, 0.0789, 0.1310,  ..., 0.0148, 0.0492, 0.0000],
        [0.0313, 0.0647, 0.0671,  ..., 0.0061, 0.0311, 0.0000],
        [0.0136, 0.0536, 0.0181,  ..., 0.0002, 0.0171, 0.0000],
        ...,
        [0.0022, 0.0480, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0022, 0.0480, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0022, 0.0480, 0.0000,  ..., 0.0000, 0.0072, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(385895.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2253.8154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(225.5270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3863.1372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(764.5690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-449.5770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0811],
        [-0.2176],
        [-0.3833],
        ...,
        [-1.2776],
        [-1.2747],
        [-1.2740]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229685.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0074],
        [1.0070],
        ...,
        [1.0014],
        [1.0006],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367748.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0075],
        [1.0071],
        ...,
        [1.0014],
        [1.0005],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367760., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1273e-02,  8.9039e-03, -6.8641e-04,  ...,  1.9309e-02,
         -2.6149e-03,  6.6206e-03],
        [ 4.8870e-03,  3.9296e-03, -6.4383e-04,  ...,  8.9190e-03,
         -1.1395e-03,  2.4851e-03],
        [ 6.3407e-03,  5.0619e-03, -6.5352e-04,  ...,  1.1284e-02,
         -1.4754e-03,  3.4265e-03],
        ...,
        [-4.5312e-05,  8.7661e-05, -6.1094e-04,  ...,  8.9434e-04,
          0.0000e+00, -7.0893e-04],
        [-4.5312e-05,  8.7661e-05, -6.1094e-04,  ...,  8.9434e-04,
          0.0000e+00, -7.0893e-04],
        [-4.5312e-05,  8.7661e-05, -6.1094e-04,  ...,  8.9434e-04,
          0.0000e+00, -7.0893e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1766.8091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.0736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5573, device='cuda:0')



h[100].sum tensor(72.9488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5732, device='cuda:0')



h[200].sum tensor(18.2334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0186, 0.0150, 0.0000,  ..., 0.0341, 0.0000, 0.0100],
        [0.0414, 0.0327, 0.0000,  ..., 0.0712, 0.0000, 0.0240],
        [0.0188, 0.0150, 0.0000,  ..., 0.0343, 0.0000, 0.0108],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46786.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0516, 0.0758, 0.1192,  ..., 0.0117, 0.0489, 0.0000],
        [0.0671, 0.0856, 0.1620,  ..., 0.0182, 0.0614, 0.0000],
        [0.0484, 0.0742, 0.1107,  ..., 0.0106, 0.0462, 0.0000],
        ...,
        [0.0023, 0.0478, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0023, 0.0478, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0023, 0.0478, 0.0000,  ..., 0.0000, 0.0075, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382182.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2146.5366, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(209.7689, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4061.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(739.5496, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.5123, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0270],
        [ 0.0808],
        [ 0.0506],
        ...,
        [-1.2889],
        [-1.2860],
        [-1.2853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240250.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0075],
        [1.0071],
        ...,
        [1.0014],
        [1.0005],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367760., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0076],
        [1.0072],
        ...,
        [1.0013],
        [1.0005],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367771.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6258e-03,  4.4921e-03, -6.4867e-04,  ...,  1.0089e-02,
         -1.3023e-03,  2.9379e-03],
        [-3.2551e-05,  8.4846e-05, -6.1094e-04,  ...,  8.8366e-04,
          0.0000e+00, -7.2552e-04],
        [-3.2551e-05,  8.4846e-05, -6.1094e-04,  ...,  8.8366e-04,
          0.0000e+00, -7.2552e-04],
        ...,
        [-3.2551e-05,  8.4846e-05, -6.1094e-04,  ...,  8.8366e-04,
          0.0000e+00, -7.2552e-04],
        [-3.2551e-05,  8.4846e-05, -6.1094e-04,  ...,  8.8366e-04,
          0.0000e+00, -7.2552e-04],
        [-3.2551e-05,  8.4846e-05, -6.1094e-04,  ...,  8.8366e-04,
          0.0000e+00, -7.2552e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1838.2791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3245, device='cuda:0')



h[100].sum tensor(73.3960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8669, device='cuda:0')



h[200].sum tensor(19.5632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0191, 0.0153, 0.0000,  ..., 0.0348, 0.0000, 0.0110],
        [0.0057, 0.0048, 0.0000,  ..., 0.0128, 0.0000, 0.0030],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49465.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0502, 0.0744, 0.1147,  ..., 0.0119, 0.0476, 0.0000],
        [0.0283, 0.0615, 0.0568,  ..., 0.0046, 0.0296, 0.0000],
        [0.0126, 0.0523, 0.0183,  ..., 0.0005, 0.0166, 0.0000],
        ...,
        [0.0025, 0.0475, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0025, 0.0475, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0025, 0.0475, 0.0000,  ..., 0.0000, 0.0079, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391878.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2415.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.2822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4002.1985, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(780.0508, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-458.4732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0562],
        [-0.0686],
        [-0.2557],
        ...,
        [-1.2589],
        [-1.2769],
        [-1.2824]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217240.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0076],
        [1.0072],
        ...,
        [1.0013],
        [1.0005],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367771.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0077],
        [1.0073],
        ...,
        [1.0013],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367783.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3696e-03,  4.2742e-03, -6.4688e-04,  ...,  9.6469e-03,
         -1.2361e-03,  2.7513e-03],
        [-2.1547e-05,  7.5374e-05, -6.1094e-04,  ...,  8.7601e-04,
          0.0000e+00, -7.3846e-04],
        [-2.1547e-05,  7.5374e-05, -6.1094e-04,  ...,  8.7601e-04,
          0.0000e+00, -7.3846e-04],
        ...,
        [-2.1547e-05,  7.5374e-05, -6.1094e-04,  ...,  8.7601e-04,
          0.0000e+00, -7.3846e-04],
        [-2.1547e-05,  7.5374e-05, -6.1094e-04,  ...,  8.7601e-04,
          0.0000e+00, -7.3846e-04],
        [-2.1547e-05,  7.5374e-05, -6.1094e-04,  ...,  8.7601e-04,
          0.0000e+00, -7.3846e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1861.6355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4318, device='cuda:0')



h[100].sum tensor(73.6425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1875, device='cuda:0')



h[200].sum tensor(19.9166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1635, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0108, 0.0000,  ..., 0.0255, 0.0000, 0.0065],
        [0.0098, 0.0080, 0.0000,  ..., 0.0196, 0.0000, 0.0049],
        [0.0000, 0.0003, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51886.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0392, 0.0663, 0.0809,  ..., 0.0066, 0.0394, 0.0000],
        [0.0291, 0.0608, 0.0549,  ..., 0.0040, 0.0307, 0.0000],
        [0.0132, 0.0520, 0.0169,  ..., 0.0002, 0.0171, 0.0000],
        ...,
        [0.0028, 0.0474, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0028, 0.0474, 0.0000,  ..., 0.0000, 0.0079, 0.0000],
        [0.0028, 0.0474, 0.0000,  ..., 0.0000, 0.0079, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409713.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2707.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.8609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4070.8818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(815.4761, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-483.3373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0527],
        [-0.0690],
        [-0.2894],
        ...,
        [-1.3060],
        [-1.3030],
        [-1.3023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214503.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0077],
        [1.0073],
        ...,
        [1.0013],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367783.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0077],
        [1.0073],
        ...,
        [1.0013],
        [1.0004],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367793.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3828e-02,  1.8623e-02, -7.6991e-04,  ...,  3.9673e-02,
         -5.4467e-03,  1.4691e-02],
        [ 3.8318e-02,  2.9907e-02, -8.6651e-04,  ...,  6.3248e-02,
         -8.7565e-03,  2.4070e-02],
        [ 2.7204e-02,  2.1252e-02, -7.9241e-04,  ...,  4.5166e-02,
         -6.2178e-03,  1.6876e-02],
        ...,
        [-1.7244e-05,  5.2111e-05, -6.1094e-04,  ...,  8.7732e-04,
          0.0000e+00, -7.4318e-04],
        [-1.7244e-05,  5.2111e-05, -6.1094e-04,  ...,  8.7732e-04,
          0.0000e+00, -7.4318e-04],
        [-1.7244e-05,  5.2111e-05, -6.1094e-04,  ...,  8.7732e-04,
          0.0000e+00, -7.4318e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1606.0367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(14.9569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9222, device='cuda:0')



h[100].sum tensor(72.2767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6847, device='cuda:0')



h[200].sum tensor(14.0154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.0331e-01, 8.0718e-02, 0.0000e+00,  ..., 1.7171e-01, 0.0000e+00,
         6.3927e-02],
        [1.3279e-01, 1.0368e-01, 0.0000e+00,  ..., 2.1969e-01, 0.0000e+00,
         8.2997e-02],
        [1.4056e-01, 1.0973e-01, 0.0000e+00,  ..., 2.3233e-01, 0.0000e+00,
         8.8026e-02],
        ...,
        [0.0000e+00, 2.1310e-04, 0.0000e+00,  ..., 3.5876e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1312e-04, 0.0000e+00,  ..., 3.5880e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1312e-04, 0.0000e+00,  ..., 3.5880e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43416.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1924, 0.1645, 0.5217,  ..., 0.0835, 0.1601, 0.0000],
        [0.2416, 0.1955, 0.6617,  ..., 0.1071, 0.1995, 0.0000],
        [0.2492, 0.2002, 0.6821,  ..., 0.1102, 0.2058, 0.0000],
        ...,
        [0.0029, 0.0478, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0029, 0.0478, 0.0000,  ..., 0.0000, 0.0075, 0.0000],
        [0.0029, 0.0478, 0.0000,  ..., 0.0000, 0.0075, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(372422.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2114.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.1560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4283.2979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(701.8386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.1172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0789],
        [ 0.0881],
        [ 0.0997],
        ...,
        [-1.3307],
        [-1.3276],
        [-1.3268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248313.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0077],
        [1.0073],
        ...,
        [1.0013],
        [1.0004],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367793.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0079],
        [1.0075],
        ...,
        [1.0012],
        [1.0003],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367804.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1351e-02,  8.8912e-03, -6.8669e-04,  ...,  1.9381e-02,
         -2.5858e-03,  6.6109e-03],
        [ 5.4758e-03,  4.3159e-03, -6.4753e-04,  ...,  9.8213e-03,
         -1.2489e-03,  2.8085e-03],
        [ 5.8623e-03,  4.6169e-03, -6.5011e-04,  ...,  1.0450e-02,
         -1.3369e-03,  3.0586e-03],
        ...,
        [-1.2565e-05,  4.1654e-05, -6.1094e-04,  ...,  8.9089e-04,
          0.0000e+00, -7.4379e-04],
        [-1.2565e-05,  4.1654e-05, -6.1094e-04,  ...,  8.9089e-04,
          0.0000e+00, -7.4379e-04],
        [-1.2565e-05,  4.1654e-05, -6.1094e-04,  ...,  8.9089e-04,
          0.0000e+00, -7.4379e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1957.2949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5078, device='cuda:0')



h[100].sum tensor(74.6570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.4046, device='cuda:0')



h[200].sum tensor(21.9397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2835, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0148, 0.0000,  ..., 0.0342, 0.0000, 0.0099],
        [0.0395, 0.0310, 0.0000,  ..., 0.0679, 0.0000, 0.0226],
        [0.0189, 0.0149, 0.0000,  ..., 0.0343, 0.0000, 0.0107],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53297.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0511, 0.0753, 0.1143,  ..., 0.0138, 0.0482, 0.0000],
        [0.0650, 0.0841, 0.1520,  ..., 0.0197, 0.0595, 0.0000],
        [0.0468, 0.0732, 0.1036,  ..., 0.0127, 0.0443, 0.0000],
        ...,
        [0.0030, 0.0480, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0030, 0.0480, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0030, 0.0480, 0.0000,  ..., 0.0000, 0.0074, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411216.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2828.8506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(268.2650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3920.9700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(845.6559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-500.1648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0515],
        [ 0.0475],
        [-0.1065],
        ...,
        [-1.3430],
        [-1.3399],
        [-1.3390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209504.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0079],
        [1.0075],
        ...,
        [1.0012],
        [1.0003],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367804.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0080],
        [1.0076],
        ...,
        [1.0012],
        [1.0003],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367814.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8364e-02,  1.4346e-02, -7.3347e-04,  ...,  3.0828e-02,
         -4.1667e-03,  1.1162e-02],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        ...,
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2043.2974, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2740, device='cuda:0')



h[100].sum tensor(75.5036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.6951, device='cuda:0')



h[200].sum tensor(23.7238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0679, 0.0530, 0.0000,  ..., 0.1142, 0.0000, 0.0417],
        [0.0462, 0.0361, 0.0000,  ..., 0.0789, 0.0000, 0.0277],
        [0.0276, 0.0216, 0.0000,  ..., 0.0486, 0.0000, 0.0157],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54639.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1622, 0.1473, 0.4320,  ..., 0.0685, 0.1366, 0.0000],
        [0.1372, 0.1317, 0.3601,  ..., 0.0559, 0.1168, 0.0000],
        [0.1124, 0.1159, 0.2880,  ..., 0.0431, 0.0974, 0.0000],
        ...,
        [0.0029, 0.0485, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0029, 0.0485, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0029, 0.0485, 0.0000,  ..., 0.0000, 0.0074, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(414187.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2867.4409, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.7255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3664.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(865.6048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-515.5510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0805],
        [ 0.0928],
        [ 0.1047],
        ...,
        [-1.3549],
        [-1.3517],
        [-1.3509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205780.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0080],
        [1.0076],
        ...,
        [1.0012],
        [1.0003],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367814.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0080],
        [1.0076],
        ...,
        [1.0012],
        [1.0003],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367814.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        ...,
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04],
        [-1.4400e-05,  3.2823e-05, -6.1094e-04,  ...,  9.1730e-04,
          0.0000e+00, -7.3451e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1951.5479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4948, device='cuda:0')



h[100].sum tensor(74.9047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.3654, device='cuda:0')



h[200].sum tensor(21.6421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0060, 0.0048, 0.0000,  ..., 0.0135, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52425.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0047, 0.0483, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0096, 0.0513, 0.0097,  ..., 0.0002, 0.0133, 0.0000],
        [0.0234, 0.0597, 0.0425,  ..., 0.0043, 0.0251, 0.0000],
        ...,
        [0.0029, 0.0485, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0029, 0.0485, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0029, 0.0485, 0.0000,  ..., 0.0000, 0.0074, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409422.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2699.9065, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.9336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3817.3804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(831.4881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-489.0493, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6832],
        [-0.4110],
        [-0.1524],
        ...,
        [-1.3549],
        [-1.3517],
        [-1.3509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240873.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0080],
        [1.0076],
        ...,
        [1.0012],
        [1.0003],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367814.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0081],
        [1.0077],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367825.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9588e-03,  5.4525e-03, -6.5744e-04,  ...,  1.2294e-02,
         -1.5755e-03,  3.7910e-03],
        [-1.6274e-05,  2.0344e-05, -6.1094e-04,  ...,  9.4040e-04,
          0.0000e+00, -7.2467e-04],
        [ 6.9588e-03,  5.4525e-03, -6.5744e-04,  ...,  1.2294e-02,
         -1.5755e-03,  3.7910e-03],
        ...,
        [-1.6274e-05,  2.0344e-05, -6.1094e-04,  ...,  9.4040e-04,
          0.0000e+00, -7.2467e-04],
        [-1.6274e-05,  2.0344e-05, -6.1094e-04,  ...,  9.4040e-04,
          0.0000e+00, -7.2467e-04],
        [-1.6274e-05,  2.0344e-05, -6.1094e-04,  ...,  9.4040e-04,
          0.0000e+00, -7.2467e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2972.6021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(42.0287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.4076, device='cuda:0')



h[100].sum tensor(81.8945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(64.0015, device='cuda:0')



h[200].sum tensor(44.6648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.3877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.4467e-02, 1.1374e-02, 0.0000e+00,  ..., 2.7380e-02, 0.0000e+00,
         7.9317e-03],
        [2.9888e-02, 2.3409e-02, 0.0000e+00,  ..., 5.2548e-02, 0.0000e+00,
         1.6469e-02],
        [5.7305e-03, 4.5577e-03, 0.0000e+00,  ..., 1.3148e-02, 0.0000e+00,
         2.9893e-03],
        ...,
        [0.0000e+00, 8.3226e-05, 0.0000e+00,  ..., 3.8472e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.3234e-05, 0.0000e+00,  ..., 3.8476e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.3232e-05, 0.0000e+00,  ..., 3.8475e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83733.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0446, 0.0733, 0.0990,  ..., 0.0120, 0.0433, 0.0000],
        [0.0507, 0.0773, 0.1151,  ..., 0.0142, 0.0485, 0.0000],
        [0.0319, 0.0659, 0.0650,  ..., 0.0073, 0.0325, 0.0000],
        ...,
        [0.0027, 0.0491, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0027, 0.0491, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0027, 0.0491, 0.0000,  ..., 0.0000, 0.0072, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560251.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4680.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(539.8582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3671.7563, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1259.4745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-826.1388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0700],
        [ 0.0648],
        [-0.0111],
        ...,
        [-1.3710],
        [-1.3678],
        [-1.3669]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253543.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0081],
        [1.0077],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367825.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0082],
        [1.0078],
        ...,
        [1.0011],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367835.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4104e-03,  4.2433e-03, -6.4709e-04,  ...,  9.7816e-03,
         -1.2201e-03,  2.7857e-03],
        [ 5.4104e-03,  4.2433e-03, -6.4709e-04,  ...,  9.7816e-03,
         -1.2201e-03,  2.7857e-03],
        [-1.2303e-05,  1.9934e-05, -6.1094e-04,  ...,  9.5514e-04,
          0.0000e+00, -7.2367e-04],
        ...,
        [-1.2303e-05,  1.9934e-05, -6.1094e-04,  ...,  9.5514e-04,
          0.0000e+00, -7.2367e-04],
        [-1.2303e-05,  1.9934e-05, -6.1094e-04,  ...,  9.5514e-04,
          0.0000e+00, -7.2367e-04],
        [-1.2303e-05,  1.9934e-05, -6.1094e-04,  ...,  9.5514e-04,
          0.0000e+00, -7.2367e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2017.2968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7981, device='cuda:0')



h[100].sum tensor(75.6995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2724, device='cuda:0')



h[200].sum tensor(23.1005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3159, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7336e-02, 1.3611e-02, 0.0000e+00,  ..., 3.2116e-02, 0.0000e+00,
         9.0625e-03],
        [1.3647e-02, 1.0738e-02, 0.0000e+00,  ..., 2.6125e-02, 0.0000e+00,
         6.6670e-03],
        [9.9110e-03, 7.8189e-03, 0.0000e+00,  ..., 2.0026e-02, 0.0000e+00,
         4.9702e-03],
        ...,
        [0.0000e+00, 8.1561e-05, 0.0000e+00,  ..., 3.9080e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.1569e-05, 0.0000e+00,  ..., 3.9084e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.1566e-05, 0.0000e+00,  ..., 3.9083e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52680.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0478, 0.0751, 0.1052,  ..., 0.0116, 0.0474, 0.0000],
        [0.0408, 0.0708, 0.0864,  ..., 0.0088, 0.0416, 0.0000],
        [0.0309, 0.0650, 0.0609,  ..., 0.0053, 0.0330, 0.0000],
        ...,
        [0.0026, 0.0494, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0026, 0.0494, 0.0000,  ..., 0.0000, 0.0074, 0.0000],
        [0.0026, 0.0494, 0.0000,  ..., 0.0000, 0.0074, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406636.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2646.6106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.7473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3503.0715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(838.2534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-497.3523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0976],
        [ 0.0614],
        [-0.0255],
        ...,
        [-1.3767],
        [-1.3733],
        [-1.3721]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-225778.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0082],
        [1.0078],
        ...,
        [1.0011],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367835.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0084],
        [1.0079],
        ...,
        [1.0011],
        [1.0002],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367845.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1706e-06,  2.8542e-05, -6.1094e-04,  ...,  9.5263e-04,
          0.0000e+00, -7.3018e-04],
        [ 1.4092e-02,  1.1006e-02, -7.0488e-04,  ...,  2.3890e-02,
         -3.1587e-03,  8.3872e-03],
        [-2.1706e-06,  2.8542e-05, -6.1094e-04,  ...,  9.5263e-04,
          0.0000e+00, -7.3018e-04],
        ...,
        [-2.1706e-06,  2.8542e-05, -6.1094e-04,  ...,  9.5263e-04,
          0.0000e+00, -7.3018e-04],
        [-2.1706e-06,  2.8542e-05, -6.1094e-04,  ...,  9.5263e-04,
          0.0000e+00, -7.3018e-04],
        [-2.1706e-06,  2.8542e-05, -6.1094e-04,  ...,  9.5263e-04,
          0.0000e+00, -7.3018e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1840.0635, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7323, device='cuda:0')



h[100].sum tensor(74.3125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0964, device='cuda:0')



h[200].sum tensor(19.2102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0297, 0.0233, 0.0000,  ..., 0.0522, 0.0000, 0.0163],
        [0.0116, 0.0092, 0.0000,  ..., 0.0227, 0.0000, 0.0060],
        [0.0360, 0.0282, 0.0000,  ..., 0.0625, 0.0000, 0.0204],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48399.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0561, 0.0810, 0.1295,  ..., 0.0152, 0.0542, 0.0000],
        [0.0508, 0.0779, 0.1152,  ..., 0.0130, 0.0496, 0.0000],
        [0.0660, 0.0876, 0.1565,  ..., 0.0195, 0.0622, 0.0000],
        ...,
        [0.0025, 0.0495, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0025, 0.0495, 0.0000,  ..., 0.0000, 0.0078, 0.0000],
        [0.0025, 0.0495, 0.0000,  ..., 0.0000, 0.0078, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389845.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2358.0708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.2862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3551.8564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(775.2195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-453.2597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1373],
        [ 0.1376],
        [ 0.1381],
        ...,
        [-1.3782],
        [-1.3741],
        [-1.3712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227834.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0084],
        [1.0079],
        ...,
        [1.0011],
        [1.0002],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367845.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0085],
        [1.0080],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367856.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2885e-02,  1.0070e-02, -6.9675e-04,  ...,  2.1891e-02,
         -2.8745e-03,  7.5879e-03],
        [ 1.1374e-02,  8.8927e-03, -6.8668e-04,  ...,  1.9432e-02,
         -2.5372e-03,  6.6108e-03],
        [ 9.7126e-06,  4.0164e-05, -6.1094e-04,  ...,  9.3873e-04,
          0.0000e+00, -7.3868e-04],
        ...,
        [ 9.7126e-06,  4.0164e-05, -6.1094e-04,  ...,  9.3873e-04,
          0.0000e+00, -7.3868e-04],
        [ 9.7126e-06,  4.0164e-05, -6.1094e-04,  ...,  9.3873e-04,
          0.0000e+00, -7.3868e-04],
        [ 9.7126e-06,  4.0164e-05, -6.1094e-04,  ...,  9.3873e-04,
          0.0000e+00, -7.3868e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2044.7964, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7797, device='cuda:0')



h[100].sum tensor(75.4853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2172, device='cuda:0')



h[200].sum tensor(23.8381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.2086e-02, 4.0703e-02, 0.0000e+00,  ..., 8.8468e-02, 0.0000e+00,
         3.0690e-02],
        [2.2373e-02, 1.7559e-02, 0.0000e+00,  ..., 4.0130e-02, 0.0000e+00,
         1.2954e-02],
        [1.1501e-02, 9.0899e-03, 0.0000e+00,  ..., 2.2439e-02, 0.0000e+00,
         6.6671e-03],
        ...,
        [3.9752e-05, 1.6438e-04, 0.0000e+00,  ..., 3.8420e-03, 0.0000e+00,
         0.0000e+00],
        [3.9756e-05, 1.6440e-04, 0.0000e+00,  ..., 3.8424e-03, 0.0000e+00,
         0.0000e+00],
        [3.9754e-05, 1.6439e-04, 0.0000e+00,  ..., 3.8422e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51464.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0748, 0.0939, 0.1854,  ..., 0.0251, 0.0680, 0.0000],
        [0.0508, 0.0790, 0.1200,  ..., 0.0146, 0.0484, 0.0000],
        [0.0304, 0.0662, 0.0650,  ..., 0.0068, 0.0319, 0.0000],
        ...,
        [0.0023, 0.0496, 0.0000,  ..., 0.0000, 0.0084, 0.0000],
        [0.0023, 0.0496, 0.0000,  ..., 0.0000, 0.0084, 0.0000],
        [0.0023, 0.0496, 0.0000,  ..., 0.0000, 0.0084, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(395231.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2326.9543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.5211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3717.6438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(810.8242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-486.0347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1793],
        [-0.2463],
        [-0.3491],
        ...,
        [-1.3826],
        [-1.3796],
        [-1.3788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242547.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0085],
        [1.0080],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367856.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0086],
        [1.0081],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367866.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.1694e-03,  4.0621e-03, -6.4521e-04,  ...,  9.2857e-03,
         -1.1436e-03,  2.5761e-03],
        [ 1.2574e-02,  9.8300e-03, -6.9455e-04,  ...,  2.1333e-02,
         -2.7903e-03,  7.3633e-03],
        [ 6.7892e-03,  5.3239e-03, -6.5600e-04,  ...,  1.1921e-02,
         -1.5039e-03,  3.6234e-03],
        ...,
        [ 2.7090e-05,  5.6274e-05, -6.1094e-04,  ...,  9.1866e-04,
          0.0000e+00, -7.4856e-04],
        [ 2.7090e-05,  5.6274e-05, -6.1094e-04,  ...,  9.1866e-04,
          0.0000e+00, -7.4856e-04],
        [ 2.7090e-05,  5.6274e-05, -6.1094e-04,  ...,  9.1866e-04,
          0.0000e+00, -7.4856e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1813.4608, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.6757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0537, device='cuda:0')



h[100].sum tensor(73.9308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0674, device='cuda:0')



h[200].sum tensor(18.5617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0098, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0365, 0.0286, 0.0000,  ..., 0.0629, 0.0000, 0.0205],
        [0.0225, 0.0176, 0.0000,  ..., 0.0401, 0.0000, 0.0114],
        [0.0372, 0.0291, 0.0000,  ..., 0.0641, 0.0000, 0.0210],
        ...,
        [0.0001, 0.0002, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0001, 0.0002, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0001, 0.0002, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47158.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.6874e-02, 8.1305e-02, 1.3212e-01,  ..., 1.3716e-02, 5.5876e-02,
         0.0000e+00],
        [5.9049e-02, 8.2360e-02, 1.3638e-01,  ..., 1.3740e-02, 5.8382e-02,
         0.0000e+00],
        [7.2548e-02, 9.1083e-02, 1.7316e-01,  ..., 1.9598e-02, 6.9304e-02,
         0.0000e+00],
        ...,
        [3.0236e-03, 5.0051e-02, 9.2727e-05,  ..., 0.0000e+00, 9.7508e-03,
         0.0000e+00],
        [2.0586e-03, 4.9456e-02, 0.0000e+00,  ..., 0.0000e+00, 8.9491e-03,
         0.0000e+00],
        [2.0586e-03, 4.9454e-02, 0.0000e+00,  ..., 0.0000e+00, 8.9489e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383799.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2166.1284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.8535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3778.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(750.2856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.0732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3076],
        [-0.1459],
        [-0.0591],
        ...,
        [-1.0112],
        [-1.1816],
        [-1.3005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211457.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0086],
        [1.0081],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367866.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0087],
        [1.0082],
        ...,
        [1.0010],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367876.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.7075e-05,  6.8216e-05, -6.1094e-04,  ...,  9.0263e-04,
          0.0000e+00, -7.5379e-04],
        [ 3.7075e-05,  6.8216e-05, -6.1094e-04,  ...,  9.0263e-04,
          0.0000e+00, -7.5379e-04],
        [ 3.7075e-05,  6.8216e-05, -6.1094e-04,  ...,  9.0263e-04,
          0.0000e+00, -7.5379e-04],
        ...,
        [ 3.7075e-05,  6.8216e-05, -6.1094e-04,  ...,  9.0263e-04,
          0.0000e+00, -7.5379e-04],
        [ 3.7075e-05,  6.8216e-05, -6.1094e-04,  ...,  9.0263e-04,
          0.0000e+00, -7.5379e-04],
        [ 3.7075e-05,  6.8216e-05, -6.1094e-04,  ...,  9.0263e-04,
          0.0000e+00, -7.5379e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1716.2058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.7177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9041, device='cuda:0')



h[100].sum tensor(73.4160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6307, device='cuda:0')



h[200].sum tensor(16.3387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8816, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0002, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44908.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0483, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0025, 0.0489, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0037, 0.0497, 0.0008,  ..., 0.0000, 0.0109, 0.0000],
        ...,
        [0.0018, 0.0494, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0018, 0.0494, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0018, 0.0494, 0.0000,  ..., 0.0000, 0.0094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375538.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1959.8824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(196.9194, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3885.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(715.9296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-421.1292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2190],
        [-1.0097],
        [-0.7404],
        ...,
        [-1.3733],
        [-1.3618],
        [-1.3521]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214374.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0087],
        [1.0082],
        ...,
        [1.0010],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367876.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0088],
        [1.0083],
        ...,
        [1.0010],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367886.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5616e-05,  7.1616e-05, -6.1094e-04,  ...,  8.8510e-04,
          0.0000e+00, -7.5547e-04],
        [ 3.5616e-05,  7.1616e-05, -6.1094e-04,  ...,  8.8510e-04,
          0.0000e+00, -7.5547e-04],
        [ 3.5616e-05,  7.1616e-05, -6.1094e-04,  ...,  8.8510e-04,
          0.0000e+00, -7.5547e-04],
        ...,
        [ 3.5616e-05,  7.1616e-05, -6.1094e-04,  ...,  8.8510e-04,
          0.0000e+00, -7.5547e-04],
        [ 3.5616e-05,  7.1616e-05, -6.1094e-04,  ...,  8.8510e-04,
          0.0000e+00, -7.5547e-04],
        [ 3.5616e-05,  7.1616e-05, -6.1094e-04,  ...,  8.8510e-04,
          0.0000e+00, -7.5547e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1822.6997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.5879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8491, device='cuda:0')



h[100].sum tensor(74.3661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.4558, device='cuda:0')



h[200].sum tensor(18.7372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9870, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0001, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46774.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0486, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        [0.0014, 0.0487, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0014, 0.0489, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        ...,
        [0.0014, 0.0497, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0014, 0.0497, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0014, 0.0497, 0.0000,  ..., 0.0000, 0.0095, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382679.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1930.7722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.3554, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4061.7920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(738.5339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.0529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2062],
        [-1.3463],
        [-1.4540],
        ...,
        [-1.3976],
        [-1.3950],
        [-1.3947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218469.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0088],
        [1.0083],
        ...,
        [1.0010],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367886.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0089],
        [1.0084],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367897.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3333e-05,  6.8190e-05, -6.1094e-04,  ...,  8.7231e-04,
          0.0000e+00, -7.5410e-04],
        [ 1.1987e-02,  9.3893e-03, -6.9066e-04,  ...,  2.0335e-02,
         -2.6303e-03,  6.9776e-03],
        [ 1.7865e-02,  1.3969e-02, -7.2982e-04,  ...,  2.9897e-02,
         -3.9225e-03,  1.0776e-02],
        ...,
        [ 2.3333e-05,  6.8190e-05, -6.1094e-04,  ...,  8.7231e-04,
          0.0000e+00, -7.5410e-04],
        [ 2.3333e-05,  6.8190e-05, -6.1094e-04,  ...,  8.7231e-04,
          0.0000e+00, -7.5410e-04],
        [ 2.3333e-05,  6.8190e-05, -6.1094e-04,  ...,  8.7231e-04,
          0.0000e+00, -7.5410e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1730.6589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.2197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.8779, device='cuda:0')



h[100].sum tensor(74.1163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.5521, device='cuda:0')



h[200].sum tensor(16.7744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.3850e-02, 2.6573e-02, 0.0000e+00,  ..., 5.8419e-02, 0.0000e+00,
         1.8783e-02],
        [2.7958e-02, 2.1984e-02, 0.0000e+00,  ..., 4.8849e-02, 0.0000e+00,
         1.6486e-02],
        [2.6877e-02, 2.1142e-02, 0.0000e+00,  ..., 4.7091e-02, 0.0000e+00,
         1.5786e-02],
        ...,
        [9.5551e-05, 2.7925e-04, 0.0000e+00,  ..., 3.5722e-03, 0.0000e+00,
         0.0000e+00],
        [9.5560e-05, 2.7928e-04, 0.0000e+00,  ..., 3.5726e-03, 0.0000e+00,
         0.0000e+00],
        [9.5555e-05, 2.7926e-04, 0.0000e+00,  ..., 3.5724e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44417.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0549, 0.0815, 0.1297,  ..., 0.0112, 0.0552, 0.0000],
        [0.0642, 0.0877, 0.1560,  ..., 0.0159, 0.0625, 0.0000],
        [0.0721, 0.0927, 0.1769,  ..., 0.0190, 0.0690, 0.0000],
        ...,
        [0.0010, 0.0503, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0010, 0.0503, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0010, 0.0503, 0.0000,  ..., 0.0000, 0.0094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375964.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1695.9087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(189.6142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4161.9746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(705.2125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.1010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0900],
        [ 0.1496],
        [ 0.1690],
        ...,
        [-1.4229],
        [-1.4200],
        [-1.4192]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235910.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0089],
        [1.0084],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367897.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0090],
        [1.0085],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367907.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2303e-05,  6.4459e-05, -6.1094e-04,  ...,  8.6262e-04,
          0.0000e+00, -7.5383e-04],
        [ 1.2303e-05,  6.4459e-05, -6.1094e-04,  ...,  8.6262e-04,
          0.0000e+00, -7.5383e-04],
        [ 1.2303e-05,  6.4459e-05, -6.1094e-04,  ...,  8.6262e-04,
          0.0000e+00, -7.5383e-04],
        ...,
        [ 1.2303e-05,  6.4459e-05, -6.1094e-04,  ...,  8.6262e-04,
          0.0000e+00, -7.5383e-04],
        [ 1.2303e-05,  6.4459e-05, -6.1094e-04,  ...,  8.6262e-04,
          0.0000e+00, -7.5383e-04],
        [ 1.2303e-05,  6.4459e-05, -6.1094e-04,  ...,  8.6262e-04,
          0.0000e+00, -7.5383e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1903.4614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.2200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5326, device='cuda:0')



h[100].sum tensor(75.4690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.4992, device='cuda:0')



h[200].sum tensor(20.8563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0632, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[4.9462e-05, 2.5915e-04, 0.0000e+00,  ..., 3.4681e-03, 0.0000e+00,
         0.0000e+00],
        [4.9660e-05, 2.6019e-04, 0.0000e+00,  ..., 3.4820e-03, 0.0000e+00,
         0.0000e+00],
        [4.9671e-05, 2.6025e-04, 0.0000e+00,  ..., 3.4828e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.0388e-05, 2.6401e-04, 0.0000e+00,  ..., 3.5331e-03, 0.0000e+00,
         0.0000e+00],
        [5.0393e-05, 2.6403e-04, 0.0000e+00,  ..., 3.5334e-03, 0.0000e+00,
         0.0000e+00],
        [5.0390e-05, 2.6402e-04, 0.0000e+00,  ..., 3.5332e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50841.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0506, 0.0008,  ..., 0.0000, 0.0107, 0.0000],
        [0.0068, 0.0532, 0.0074,  ..., 0.0000, 0.0146, 0.0000],
        [0.0090, 0.0546, 0.0112,  ..., 0.0000, 0.0165, 0.0000],
        ...,
        [0.0007, 0.0507, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0007, 0.0507, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0007, 0.0507, 0.0000,  ..., 0.0000, 0.0093, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409174.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2104.8110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(244.0229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4141.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(793.3237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-487.3229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4370],
        [-0.4244],
        [-0.3536],
        ...,
        [-1.4432],
        [-1.4401],
        [-1.4393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233640.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0090],
        [1.0085],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367907.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(998.4489, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0091],
        [1.0086],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367917.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6645e-02,  1.3027e-02, -7.2182e-04,  ...,  2.7924e-02,
         -3.6308e-03,  1.0003e-02],
        [ 3.1810e-06,  6.0765e-05, -6.1094e-04,  ...,  8.5355e-04,
          0.0000e+00, -7.5045e-04],
        [ 3.1810e-06,  6.0765e-05, -6.1094e-04,  ...,  8.5355e-04,
          0.0000e+00, -7.5045e-04],
        ...,
        [ 3.1810e-06,  6.0765e-05, -6.1094e-04,  ...,  8.5355e-04,
          0.0000e+00, -7.5045e-04],
        [ 3.1810e-06,  6.0765e-05, -6.1094e-04,  ...,  8.5355e-04,
          0.0000e+00, -7.5045e-04],
        [ 3.1810e-06,  6.0765e-05, -6.1094e-04,  ...,  8.5355e-04,
          0.0000e+00, -7.5045e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1929.6624, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.2767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6971, device='cuda:0')



h[100].sum tensor(76.0535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.9909, device='cuda:0')



h[200].sum tensor(21.4835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0816, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.2082e-02, 4.0814e-02, 0.0000e+00,  ..., 8.8129e-02, 0.0000e+00,
         3.2137e-02],
        [3.0505e-02, 2.4003e-02, 0.0000e+00,  ..., 5.3045e-02, 0.0000e+00,
         1.8190e-02],
        [1.2845e-05, 2.4536e-04, 0.0000e+00,  ..., 3.4465e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.3030e-05, 2.4891e-04, 0.0000e+00,  ..., 3.4964e-03, 0.0000e+00,
         0.0000e+00],
        [1.3032e-05, 2.4894e-04, 0.0000e+00,  ..., 3.4967e-03, 0.0000e+00,
         0.0000e+00],
        [1.3031e-05, 2.4892e-04, 0.0000e+00,  ..., 3.4965e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50382.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1008, 0.1121, 0.2627,  ..., 0.0350, 0.0905, 0.0000],
        [0.0707, 0.0937, 0.1792,  ..., 0.0208, 0.0664, 0.0000],
        [0.0360, 0.0722, 0.0848,  ..., 0.0056, 0.0382, 0.0000],
        ...,
        [0.0004, 0.0511, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        [0.0004, 0.0511, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        [0.0004, 0.0511, 0.0000,  ..., 0.0000, 0.0092, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406241.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2007.9292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(237.7859, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4124.8540, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(789.0190, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-483.1713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0249],
        [ 0.0339],
        [ 0.0508],
        ...,
        [-1.4654],
        [-1.4621],
        [-1.4613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231586.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0091],
        [1.0086],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367917.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0092],
        [1.0087],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367927.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8918e-02,  1.4800e-02, -7.3703e-04,  ...,  3.1633e-02,
         -4.1131e-03,  1.1486e-02],
        [ 8.4952e-03,  6.6791e-03, -6.6758e-04,  ...,  1.4678e-02,
         -1.8477e-03,  4.7497e-03],
        [ 1.0908e-02,  8.5590e-03, -6.8366e-04,  ...,  1.8603e-02,
         -2.3721e-03,  6.3091e-03],
        ...,
        [-5.7386e-06,  5.5614e-05, -6.1094e-04,  ...,  8.4959e-04,
          0.0000e+00, -7.4507e-04],
        [-5.7386e-06,  5.5614e-05, -6.1094e-04,  ...,  8.4959e-04,
          0.0000e+00, -7.4507e-04],
        [-5.7386e-06,  5.5614e-05, -6.1094e-04,  ...,  8.4959e-04,
          0.0000e+00, -7.4507e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2212.0854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.3903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5569, device='cuda:0')



h[100].sum tensor(78.3954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5410, device='cuda:0')



h[200].sum tensor(27.6899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4006, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0547, 0.0429, 0.0000,  ..., 0.0925, 0.0000, 0.0324],
        [0.0551, 0.0432, 0.0000,  ..., 0.0931, 0.0000, 0.0326],
        [0.0176, 0.0139, 0.0000,  ..., 0.0320, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56983.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1529, 0.1451, 0.4026,  ..., 0.0555, 0.1342, 0.0000],
        [0.1238, 0.1271, 0.3216,  ..., 0.0417, 0.1108, 0.0000],
        [0.0802, 0.1001, 0.2023,  ..., 0.0224, 0.0755, 0.0000],
        ...,
        [0.0016, 0.0522, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0045, 0.0538, 0.0025,  ..., 0.0000, 0.0133, 0.0000],
        [0.0059, 0.0545, 0.0047,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(429890., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2300.2026, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.1222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4046.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(884.1218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-555.0947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1611],
        [ 0.1572],
        [ 0.1509],
        ...,
        [-1.2954],
        [-1.1680],
        [-1.0870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226371.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0092],
        [1.0087],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367927.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0093],
        [1.0088],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367937.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5411e-05,  5.3538e-05, -6.1094e-04,  ...,  8.5520e-04,
          0.0000e+00, -7.3097e-04],
        [-1.5411e-05,  5.3538e-05, -6.1094e-04,  ...,  8.5520e-04,
          0.0000e+00, -7.3097e-04],
        [-1.5411e-05,  5.3538e-05, -6.1094e-04,  ...,  8.5520e-04,
          0.0000e+00, -7.3097e-04],
        ...,
        [-1.5411e-05,  5.3538e-05, -6.1094e-04,  ...,  8.5520e-04,
          0.0000e+00, -7.3097e-04],
        [-1.5411e-05,  5.3538e-05, -6.1094e-04,  ...,  8.5520e-04,
          0.0000e+00, -7.3097e-04],
        [-1.5411e-05,  5.3538e-05, -6.1094e-04,  ...,  8.5520e-04,
          0.0000e+00, -7.3097e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1985.8134, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.1738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0868, device='cuda:0')



h[100].sum tensor(77.7010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1561, device='cuda:0')



h[200].sum tensor(22.5481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49521.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1410e-02, 6.3465e-02, 4.9134e-02,  ..., 2.9621e-03, 2.7100e-02,
         0.0000e+00],
        [6.3523e-03, 5.4373e-02, 8.1810e-03,  ..., 0.0000e+00, 1.4630e-02,
         0.0000e+00],
        [1.4844e-03, 5.1588e-02, 0.0000e+00,  ..., 0.0000e+00, 1.0307e-02,
         0.0000e+00],
        ...,
        [2.3235e-05, 5.1711e-02, 0.0000e+00,  ..., 0.0000e+00, 9.1627e-03,
         0.0000e+00],
        [2.3250e-05, 5.1718e-02, 0.0000e+00,  ..., 0.0000e+00, 9.1641e-03,
         0.0000e+00],
        [2.3266e-05, 5.1715e-02, 0.0000e+00,  ..., 0.0000e+00, 9.1637e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(399301.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1663.5812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.2114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4328.5146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(780.1992, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-472.9380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1933],
        [-0.4195],
        [-0.6293],
        ...,
        [-1.5011],
        [-1.4976],
        [-1.4967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264125.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0093],
        [1.0088],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367937.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0094],
        [1.0089],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367947.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9472e-03,  7.8219e-03, -6.7737e-04,  ...,  1.7083e-02,
         -2.1503e-03,  5.7266e-03],
        [ 7.2655e-03,  5.7326e-03, -6.5949e-04,  ...,  1.2718e-02,
         -1.5717e-03,  3.9912e-03],
        [ 5.0608e-03,  4.0150e-03, -6.4480e-04,  ...,  9.1307e-03,
         -1.0960e-03,  2.5645e-03],
        ...,
        [-1.9018e-05,  5.7334e-05, -6.1094e-04,  ...,  8.6407e-04,
          0.0000e+00, -7.2268e-04],
        [-1.9018e-05,  5.7334e-05, -6.1094e-04,  ...,  8.6407e-04,
          0.0000e+00, -7.2268e-04],
        [-1.9018e-05,  5.7334e-05, -6.1094e-04,  ...,  8.6407e-04,
          0.0000e+00, -7.2268e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1879.8469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.5294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8355, device='cuda:0')



h[100].sum tensor(77.4968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.4151, device='cuda:0')



h[200].sum tensor(20.3932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0347, 0.0273, 0.0000,  ..., 0.0601, 0.0000, 0.0196],
        [0.0253, 0.0200, 0.0000,  ..., 0.0448, 0.0000, 0.0135],
        [0.0203, 0.0161, 0.0000,  ..., 0.0366, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46611.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.8749e-02, 8.6041e-02, 1.4667e-01,  ..., 1.2968e-02, 5.9573e-02,
         0.0000e+00],
        [5.8913e-02, 8.5727e-02, 1.4533e-01,  ..., 1.1889e-02, 6.0617e-02,
         0.0000e+00],
        [5.3922e-02, 8.2352e-02, 1.3087e-01,  ..., 9.2706e-03, 5.6973e-02,
         0.0000e+00],
        ...,
        [2.7699e-05, 5.1618e-02, 0.0000e+00,  ..., 0.0000e+00, 9.4520e-03,
         0.0000e+00],
        [2.7713e-05, 5.1625e-02, 0.0000e+00,  ..., 0.0000e+00, 9.4535e-03,
         0.0000e+00],
        [2.7729e-05, 5.1622e-02, 0.0000e+00,  ..., 0.0000e+00, 9.4531e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390372.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1545.4597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(200.1067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4263.2319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(745.3361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.7094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0538],
        [ 0.0890],
        [ 0.0915],
        ...,
        [-1.5091],
        [-1.5056],
        [-1.5047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248867.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0094],
        [1.0089],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367947.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0095],
        [1.0090],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367957.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0195e-05,  6.3067e-05, -6.1094e-04,  ...,  8.7897e-04,
          0.0000e+00, -7.1746e-04],
        [-2.0195e-05,  6.3067e-05, -6.1094e-04,  ...,  8.7897e-04,
          0.0000e+00, -7.1746e-04],
        [ 1.4331e-02,  1.1244e-02, -7.0660e-04,  ...,  2.4239e-02,
         -3.0850e-03,  8.5739e-03],
        ...,
        [-2.0195e-05,  6.3067e-05, -6.1094e-04,  ...,  8.7897e-04,
          0.0000e+00, -7.1746e-04],
        [-2.0195e-05,  6.3067e-05, -6.1094e-04,  ...,  8.7897e-04,
          0.0000e+00, -7.1746e-04],
        [-2.0195e-05,  6.3067e-05, -6.1094e-04,  ...,  8.7897e-04,
          0.0000e+00, -7.1746e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2398.9788, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.3431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6839, device='cuda:0')



h[100].sum tensor(81.0204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.9104, device='cuda:0')



h[200].sum tensor(32.2637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0145, 0.0116, 0.0000,  ..., 0.0272, 0.0000, 0.0087],
        [0.0479, 0.0376, 0.0000,  ..., 0.0816, 0.0000, 0.0296],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61006.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5411e-02, 5.9610e-02, 3.4381e-02,  ..., 2.1868e-03, 2.2354e-02,
         0.0000e+00],
        [5.6315e-02, 8.5624e-02, 1.4908e-01,  ..., 1.9317e-02, 5.5358e-02,
         0.0000e+00],
        [1.2494e-01, 1.2887e-01, 3.4561e-01,  ..., 5.1215e-02, 1.1085e-01,
         0.0000e+00],
        ...,
        [1.5581e-04, 5.1410e-02, 0.0000e+00,  ..., 0.0000e+00, 9.8127e-03,
         0.0000e+00],
        [1.5582e-04, 5.1417e-02, 0.0000e+00,  ..., 0.0000e+00, 9.8143e-03,
         0.0000e+00],
        [1.5583e-04, 5.1414e-02, 0.0000e+00,  ..., 0.0000e+00, 9.8139e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(453477.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2485.4614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.3660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4045.6218, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(948.3068, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-599.8920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1770],
        [-0.0856],
        [-0.0092],
        ...,
        [-1.5139],
        [-1.5106],
        [-1.5098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236321.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0095],
        [1.0090],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367957.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0095],
        [1.0091],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367968.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.8655e-05,  6.9442e-05, -6.1094e-04,  ...,  8.9619e-04,
          0.0000e+00, -7.1391e-04],
        [-1.8655e-05,  6.9442e-05, -6.1094e-04,  ...,  8.9619e-04,
          0.0000e+00, -7.1391e-04],
        [-1.8655e-05,  6.9442e-05, -6.1094e-04,  ...,  8.9619e-04,
          0.0000e+00, -7.1391e-04],
        ...,
        [-1.8655e-05,  6.9442e-05, -6.1094e-04,  ...,  8.9619e-04,
          0.0000e+00, -7.1391e-04],
        [-1.8655e-05,  6.9442e-05, -6.1094e-04,  ...,  8.9619e-04,
          0.0000e+00, -7.1391e-04],
        [-1.8655e-05,  6.9442e-05, -6.1094e-04,  ...,  8.9619e-04,
          0.0000e+00, -7.1391e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2042.4025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1997, device='cuda:0')



h[100].sum tensor(78.8119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.4937, device='cuda:0')



h[200].sum tensor(24.8770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1376, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50760.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0004, 0.0499, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0007, 0.0503, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0013, 0.0507, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        ...,
        [0.0004, 0.0512, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0004, 0.0512, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0004, 0.0512, 0.0000,  ..., 0.0000, 0.0102, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409164.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2008.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.7932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3849.6204, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(813.8290, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-491.4058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4577],
        [-1.2926],
        [-1.0536],
        ...,
        [-1.5107],
        [-1.5062],
        [-1.5042]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206776.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0095],
        [1.0091],
        ...,
        [1.0010],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367968.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0096],
        [1.0092],
        ...,
        [1.0010],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367978.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6997e-02,  1.3321e-02, -7.2435e-04,  ...,  2.8607e-02,
         -3.6292e-03,  1.0304e-02],
        [-1.3955e-05,  7.0050e-05, -6.1094e-04,  ...,  9.0871e-04,
          0.0000e+00, -7.1640e-04],
        [-1.3955e-05,  7.0050e-05, -6.1094e-04,  ...,  9.0871e-04,
          0.0000e+00, -7.1640e-04],
        ...,
        [-1.3955e-05,  7.0050e-05, -6.1094e-04,  ...,  9.0871e-04,
          0.0000e+00, -7.1640e-04],
        [-1.3955e-05,  7.0050e-05, -6.1094e-04,  ...,  9.0871e-04,
          0.0000e+00, -7.1640e-04],
        [-1.3955e-05,  7.0050e-05, -6.1094e-04,  ...,  9.0871e-04,
          0.0000e+00, -7.1640e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1765.3622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(13.5099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.1413, device='cuda:0')



h[100].sum tensor(77.0643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.3501, device='cuda:0')



h[200].sum tensor(19.1634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7965, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0480, 0.0377, 0.0000,  ..., 0.0820, 0.0000, 0.0290],
        [0.0312, 0.0246, 0.0000,  ..., 0.0544, 0.0000, 0.0188],
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43789.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0978, 0.1101, 0.2650,  ..., 0.0370, 0.0902, 0.0000],
        [0.0639, 0.0894, 0.1688,  ..., 0.0210, 0.0622, 0.0000],
        [0.0209, 0.0626, 0.0489,  ..., 0.0044, 0.0269, 0.0000],
        ...,
        [0.0008, 0.0510, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0008, 0.0510, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0008, 0.0510, 0.0000,  ..., 0.0000, 0.0105, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381514.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1585.6299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(178.0044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4106.1279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(717.4588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.5060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0019],
        [-0.1138],
        [-0.3683],
        ...,
        [-1.5192],
        [-1.5161],
        [-1.5153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241938.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0096],
        [1.0092],
        ...,
        [1.0010],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367978.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0097],
        [1.0093],
        ...,
        [1.0010],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367988.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1469e-06,  6.7795e-05, -6.1094e-04,  ...,  9.1122e-04,
          0.0000e+00, -7.2415e-04],
        [ 5.7432e-03,  4.5467e-03, -6.4928e-04,  ...,  1.0275e-02,
         -1.2222e-03,  3.0018e-03],
        [-7.1469e-06,  6.7795e-05, -6.1094e-04,  ...,  9.1122e-04,
          0.0000e+00, -7.2415e-04],
        ...,
        [-7.1469e-06,  6.7795e-05, -6.1094e-04,  ...,  9.1122e-04,
          0.0000e+00, -7.2415e-04],
        [-7.1469e-06,  6.7795e-05, -6.1094e-04,  ...,  9.1122e-04,
          0.0000e+00, -7.2415e-04],
        [-7.1469e-06,  6.7795e-05, -6.1094e-04,  ...,  9.1122e-04,
          0.0000e+00, -7.2415e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3167.2949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(41.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2674, device='cuda:0')



h[100].sum tensor(85.8363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(63.5822, device='cuda:0')



h[200].sum tensor(50.1172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.3721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0377, 0.0296, 0.0000,  ..., 0.0651, 0.0000, 0.0215],
        [0.0047, 0.0040, 0.0000,  ..., 0.0114, 0.0000, 0.0023],
        [0.0364, 0.0287, 0.0000,  ..., 0.0630, 0.0000, 0.0214],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78322.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0932, 0.1055, 0.2484,  ..., 0.0337, 0.0877, 0.0000],
        [0.0830, 0.0998, 0.2204,  ..., 0.0294, 0.0790, 0.0000],
        [0.1172, 0.1217, 0.3198,  ..., 0.0468, 0.1065, 0.0000],
        ...,
        [0.0011, 0.0504, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0011, 0.0504, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0011, 0.0504, 0.0000,  ..., 0.0000, 0.0108, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518797.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3658.8745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(483.0319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3650.3979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1201.4852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-786.7645, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0731],
        [-0.0880],
        [-0.1035],
        ...,
        [-1.5220],
        [-1.5194],
        [-1.5194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203125.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0097],
        [1.0093],
        ...,
        [1.0010],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367988.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0098],
        [1.0095],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367998.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1946e-02,  1.7153e-02, -7.5725e-04,  ...,  3.6654e-02,
         -4.6462e-03,  1.3487e-02],
        [ 2.3772e-02,  1.8576e-02, -7.6942e-04,  ...,  3.9628e-02,
         -5.0329e-03,  1.4670e-02],
        [ 2.2158e-02,  1.7318e-02, -7.5866e-04,  ...,  3.6998e-02,
         -4.6910e-03,  1.3624e-02],
        ...,
        [ 5.2493e-07,  6.1534e-05, -6.1094e-04,  ...,  9.1544e-04,
          0.0000e+00, -7.3258e-04],
        [ 5.2493e-07,  6.1534e-05, -6.1094e-04,  ...,  9.1544e-04,
          0.0000e+00, -7.3258e-04],
        [ 5.2493e-07,  6.1534e-05, -6.1094e-04,  ...,  9.1544e-04,
          0.0000e+00, -7.3258e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2281.6287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3415, device='cuda:0')



h[100].sum tensor(80.1875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.8969, device='cuda:0')



h[200].sum tensor(30.9627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3765, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.3577e-02, 6.5338e-02, 0.0000e+00,  ..., 1.3978e-01, 0.0000e+00,
         5.1206e-02],
        [9.6663e-02, 7.5531e-02, 0.0000e+00,  ..., 1.6111e-01, 0.0000e+00,
         5.9672e-02],
        [1.0053e-01, 7.8546e-02, 0.0000e+00,  ..., 1.6742e-01, 0.0000e+00,
         6.2179e-02],
        ...,
        [2.1527e-06, 2.5234e-04, 0.0000e+00,  ..., 3.7541e-03, 0.0000e+00,
         0.0000e+00],
        [2.1529e-06, 2.5237e-04, 0.0000e+00,  ..., 3.7545e-03, 0.0000e+00,
         0.0000e+00],
        [2.1528e-06, 2.5236e-04, 0.0000e+00,  ..., 3.7543e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57853.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2062, 0.1778, 0.5791,  ..., 0.0944, 0.1773, 0.0000],
        [0.2114, 0.1813, 0.5944,  ..., 0.0972, 0.1815, 0.0000],
        [0.2009, 0.1746, 0.5634,  ..., 0.0916, 0.1732, 0.0000],
        ...,
        [0.0015, 0.0503, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0015, 0.0503, 0.0000,  ..., 0.0000, 0.0108, 0.0000],
        [0.0015, 0.0503, 0.0000,  ..., 0.0000, 0.0108, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440963.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2685.2983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.5018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3739.7783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(924.0535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-565.7914, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0458],
        [-0.0401],
        [-0.0285],
        ...,
        [-1.5366],
        [-1.5336],
        [-1.5329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193504.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0098],
        [1.0095],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367998.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0098],
        [1.0096],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368008.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5091e-02,  1.9592e-02, -7.7828e-04,  ...,  4.1803e-02,
         -5.2935e-03,  1.5525e-02],
        [ 1.7524e-02,  1.3698e-02, -7.2783e-04,  ...,  2.9479e-02,
         -3.6976e-03,  1.0622e-02],
        [ 5.8552e-03,  4.6109e-03, -6.5003e-04,  ...,  1.0474e-02,
         -1.2367e-03,  3.0610e-03],
        ...,
        [-8.6611e-06,  4.4349e-05, -6.1094e-04,  ...,  9.2450e-04,
          0.0000e+00, -7.3839e-04],
        [-8.6611e-06,  4.4349e-05, -6.1094e-04,  ...,  9.2450e-04,
          0.0000e+00, -7.3839e-04],
        [-8.6611e-06,  4.4349e-05, -6.1094e-04,  ...,  9.2450e-04,
          0.0000e+00, -7.3839e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2138.3076, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1055, device='cuda:0')



h[100].sum tensor(79.5881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2016, device='cuda:0')



h[200].sum tensor(27.9466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0915, 0.0714, 0.0000,  ..., 0.1527, 0.0000, 0.0563],
        [0.0513, 0.0401, 0.0000,  ..., 0.0873, 0.0000, 0.0303],
        [0.0440, 0.0344, 0.0000,  ..., 0.0754, 0.0000, 0.0255],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53330.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1682, 0.1533, 0.4649,  ..., 0.0743, 0.1473, 0.0000],
        [0.1289, 0.1280, 0.3476,  ..., 0.0525, 0.1163, 0.0000],
        [0.1041, 0.1121, 0.2739,  ..., 0.0388, 0.0968, 0.0000],
        ...,
        [0.0018, 0.0506, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0018, 0.0506, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0018, 0.0506, 0.0000,  ..., 0.0000, 0.0105, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423107.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2468.0405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.0545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3762.6360, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(866.7410, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-516.3278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0664],
        [ 0.0860],
        [ 0.0977],
        ...,
        [-1.5621],
        [-1.5589],
        [-1.5581]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210938.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0098],
        [1.0096],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368008.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(521.7338, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0099],
        [1.0097],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368017.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7273e-05,  2.0519e-05, -6.1094e-04,  ...,  9.2715e-04,
          0.0000e+00, -7.4111e-04],
        [-1.7273e-05,  2.0519e-05, -6.1094e-04,  ...,  9.2715e-04,
          0.0000e+00, -7.4111e-04],
        [ 6.7583e-03,  5.2967e-03, -6.5611e-04,  ...,  1.1963e-02,
         -1.4234e-03,  3.6490e-03],
        ...,
        [-1.7273e-05,  2.0519e-05, -6.1094e-04,  ...,  9.2715e-04,
          0.0000e+00, -7.4111e-04],
        [-1.7273e-05,  2.0519e-05, -6.1094e-04,  ...,  9.2715e-04,
          0.0000e+00, -7.4111e-04],
        [-1.7273e-05,  2.0519e-05, -6.1094e-04,  ...,  9.2715e-04,
          0.0000e+00, -7.4111e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1997.0529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.6293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8774, device='cuda:0')



h[100].sum tensor(79.1852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.5302, device='cuda:0')



h[200].sum tensor(24.7165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1017, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.2546e-05, 0.0000e+00,  ..., 3.7298e-03, 0.0000e+00,
         0.0000e+00],
        [6.8326e-03, 5.4171e-03, 0.0000e+00,  ..., 1.4903e-02, 0.0000e+00,
         3.6891e-03],
        [1.3833e-02, 1.0882e-02, 0.0000e+00,  ..., 2.6333e-02, 0.0000e+00,
         7.4877e-03],
        ...,
        [0.0000e+00, 8.4169e-05, 0.0000e+00,  ..., 3.8031e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4178e-05, 0.0000e+00,  ..., 3.8035e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4175e-05, 0.0000e+00,  ..., 3.8034e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52009.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0119, 0.0552, 0.0141,  ..., 0.0000, 0.0189, 0.0000],
        [0.0214, 0.0612, 0.0399,  ..., 0.0028, 0.0271, 0.0000],
        [0.0355, 0.0695, 0.0780,  ..., 0.0072, 0.0397, 0.0000],
        ...,
        [0.0018, 0.0509, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0018, 0.0509, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0018, 0.0509, 0.0000,  ..., 0.0000, 0.0099, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425558.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2453.9722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.2356, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3916.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(851.8292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-499.7817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3712],
        [-0.1661],
        [-0.0232],
        ...,
        [-1.5931],
        [-1.5897],
        [-1.5888]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237120.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0099],
        [1.0097],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368017.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0100],
        [1.0098],
        ...,
        [1.0011],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368027.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8908e-02,  1.4742e-02, -7.3715e-04,  ...,  3.1758e-02,
         -3.9618e-03,  1.1523e-02],
        [ 1.7530e-02,  1.3670e-02, -7.2797e-04,  ...,  2.9515e-02,
         -3.6735e-03,  1.0630e-02],
        [ 4.4248e-03,  3.4651e-03, -6.4060e-04,  ...,  8.1693e-03,
         -9.3098e-04,  2.1388e-03],
        ...,
        [-2.3923e-05,  1.0579e-06, -6.1094e-04,  ...,  9.2334e-04,
          0.0000e+00, -7.4378e-04],
        [-2.3923e-05,  1.0579e-06, -6.1094e-04,  ...,  9.2334e-04,
          0.0000e+00, -7.4378e-04],
        [-2.3923e-05,  1.0579e-06, -6.1094e-04,  ...,  9.2334e-04,
          0.0000e+00, -7.4378e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2206.5366, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0477, device='cuda:0')



h[100].sum tensor(80.9502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.0185, device='cuda:0')



h[200].sum tensor(29.1225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.5845e-02, 6.6923e-02, 0.0000e+00,  ..., 1.4369e-01, 0.0000e+00,
         5.2693e-02],
        [6.3249e-02, 4.9328e-02, 0.0000e+00,  ..., 1.0691e-01, 0.0000e+00,
         3.8039e-02],
        [8.0246e-02, 6.2563e-02, 0.0000e+00,  ..., 1.3459e-01, 0.0000e+00,
         4.9051e-02],
        ...,
        [0.0000e+00, 4.3399e-06, 0.0000e+00,  ..., 3.7880e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.3403e-06, 0.0000e+00,  ..., 3.7884e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.3402e-06, 0.0000e+00,  ..., 3.7883e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54783.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1789, 0.1609, 0.4947,  ..., 0.0826, 0.1545, 0.0000],
        [0.1672, 0.1537, 0.4598,  ..., 0.0760, 0.1454, 0.0000],
        [0.1678, 0.1543, 0.4611,  ..., 0.0762, 0.1458, 0.0000],
        ...,
        [0.0018, 0.0511, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0018, 0.0511, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0018, 0.0511, 0.0000,  ..., 0.0000, 0.0094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431841.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2514.1646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.5058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3950.4146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(892.7227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-528.0521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0084],
        [-0.0102],
        [-0.0090],
        ...,
        [-1.6147],
        [-1.6109],
        [-1.6099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251781.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0100],
        [1.0098],
        ...,
        [1.0011],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368027.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0100],
        [1.0099],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368037.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6976e-05, -7.9709e-06, -6.1094e-04,  ...,  9.1192e-04,
          0.0000e+00, -7.5025e-04],
        [ 7.1156e-03,  5.5535e-03, -6.5856e-04,  ...,  1.2545e-02,
         -1.4889e-03,  3.8777e-03],
        [-2.6976e-05, -7.9709e-06, -6.1094e-04,  ...,  9.1192e-04,
          0.0000e+00, -7.5025e-04],
        ...,
        [-2.6976e-05, -7.9709e-06, -6.1094e-04,  ...,  9.1192e-04,
          0.0000e+00, -7.5025e-04],
        [-2.6976e-05, -7.9709e-06, -6.1094e-04,  ...,  9.1192e-04,
          0.0000e+00, -7.5025e-04],
        [-2.6976e-05, -7.9709e-06, -6.1094e-04,  ...,  9.1192e-04,
          0.0000e+00, -7.5025e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2085.1577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0026, device='cuda:0')



h[100].sum tensor(80.3904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.8940, device='cuda:0')



h[200].sum tensor(26.5286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0056, 0.0000,  ..., 0.0154, 0.0000, 0.0039],
        [0.0059, 0.0046, 0.0000,  ..., 0.0133, 0.0000, 0.0031],
        [0.0301, 0.0235, 0.0000,  ..., 0.0528, 0.0000, 0.0165],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54350.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0184, 0.0595, 0.0325,  ..., 0.0026, 0.0241, 0.0000],
        [0.0272, 0.0646, 0.0550,  ..., 0.0050, 0.0321, 0.0000],
        [0.0449, 0.0750, 0.1029,  ..., 0.0116, 0.0476, 0.0000],
        ...,
        [0.0017, 0.0512, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0017, 0.0512, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0017, 0.0512, 0.0000,  ..., 0.0000, 0.0093, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(439166.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2539.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(268.5739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4100.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(884.1303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-522.4257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3392],
        [-0.1305],
        [-0.0123],
        ...,
        [-1.6412],
        [-1.6376],
        [-1.6368]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264178.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0100],
        [1.0099],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368037.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0101],
        [1.0100],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368046.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4728e-05, -9.0517e-06, -6.1094e-04,  ...,  9.0704e-04,
          0.0000e+00, -7.6044e-04],
        [-2.4728e-05, -9.0517e-06, -6.1094e-04,  ...,  9.0704e-04,
          0.0000e+00, -7.6044e-04],
        [-2.4728e-05, -9.0517e-06, -6.1094e-04,  ...,  9.0704e-04,
          0.0000e+00, -7.6044e-04],
        ...,
        [-2.4728e-05, -9.0517e-06, -6.1094e-04,  ...,  9.0704e-04,
          0.0000e+00, -7.6044e-04],
        [-2.4728e-05, -9.0517e-06, -6.1094e-04,  ...,  9.0704e-04,
          0.0000e+00, -7.6044e-04],
        [-2.4728e-05, -9.0517e-06, -6.1094e-04,  ...,  9.0704e-04,
          0.0000e+00, -7.6044e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2185.7612, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.9356, device='cuda:0')



h[100].sum tensor(80.9224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.6836, device='cuda:0')



h[200].sum tensor(29.2958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53033.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0500, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0019, 0.0500, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0027, 0.0504, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        ...,
        [0.0016, 0.0509, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0016, 0.0509, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0016, 0.0509, 0.0000,  ..., 0.0000, 0.0095, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422893.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2293.1284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.7916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4157.8433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(863.5511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-512.2055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1524],
        [-1.2982],
        [-1.3311],
        ...,
        [-1.6459],
        [-1.6432],
        [-1.6434]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266660.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0101],
        [1.0100],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368046.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0102],
        [1.0101],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368056.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3431e-05, -9.1344e-06, -6.1094e-04,  ...,  9.1337e-04,
          0.0000e+00, -7.6792e-04],
        [-2.3431e-05, -9.1344e-06, -6.1094e-04,  ...,  9.1337e-04,
          0.0000e+00, -7.6792e-04],
        [ 5.5878e-03,  4.3604e-03, -6.4834e-04,  ...,  1.0053e-02,
         -1.1605e-03,  2.8665e-03],
        ...,
        [-2.3431e-05, -9.1344e-06, -6.1094e-04,  ...,  9.1337e-04,
          0.0000e+00, -7.6792e-04],
        [-2.3431e-05, -9.1344e-06, -6.1094e-04,  ...,  9.1337e-04,
          0.0000e+00, -7.6792e-04],
        [-2.3431e-05, -9.1344e-06, -6.1094e-04,  ...,  9.1337e-04,
          0.0000e+00, -7.6792e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1958.0010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.0013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6001, device='cuda:0')



h[100].sum tensor(79.5413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.7012, device='cuda:0')



h[200].sum tensor(24.9392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0057, 0.0044, 0.0000,  ..., 0.0129, 0.0000, 0.0029],
        [0.0189, 0.0147, 0.0000,  ..., 0.0346, 0.0000, 0.0107],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50607.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0404, 0.0728, 0.0956,  ..., 0.0118, 0.0422, 0.0000],
        [0.0480, 0.0772, 0.1150,  ..., 0.0140, 0.0491, 0.0000],
        [0.0608, 0.0848, 0.1495,  ..., 0.0192, 0.0603, 0.0000],
        ...,
        [0.0015, 0.0507, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0062, 0.0534, 0.0063,  ..., 0.0000, 0.0138, 0.0000],
        [0.0169, 0.0596, 0.0321,  ..., 0.0022, 0.0231, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421123.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2261.5457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(233.3890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4105.5225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(828.4002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-491.0189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0077],
        [ 0.0169],
        [ 0.0394],
        ...,
        [-1.3518],
        [-1.0024],
        [-0.5900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241677.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0102],
        [1.0101],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368056.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0103],
        [1.0103],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368066.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9167e-03,  4.6170e-03, -6.5052e-04,  ...,  1.0599e-02,
         -1.2234e-03,  3.0744e-03],
        [ 5.5734e-03,  4.3496e-03, -6.4824e-04,  ...,  1.0039e-02,
         -1.1526e-03,  2.8520e-03],
        [-2.1913e-05, -7.6717e-06, -6.1094e-04,  ...,  9.2507e-04,
          0.0000e+00, -7.7182e-04],
        ...,
        [-2.1913e-05, -7.6717e-06, -6.1094e-04,  ...,  9.2507e-04,
          0.0000e+00, -7.7182e-04],
        [-2.1913e-05, -7.6717e-06, -6.1094e-04,  ...,  9.2507e-04,
          0.0000e+00, -7.7182e-04],
        [-2.1913e-05, -7.6717e-06, -6.1094e-04,  ...,  9.2507e-04,
          0.0000e+00, -7.7182e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2038.2548, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.4479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2577, device='cuda:0')



h[100].sum tensor(80.2542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.6669, device='cuda:0')



h[200].sum tensor(27.1959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0243, 0.0000,  ..., 0.0547, 0.0000, 0.0172],
        [0.0155, 0.0121, 0.0000,  ..., 0.0290, 0.0000, 0.0085],
        [0.0056, 0.0044, 0.0000,  ..., 0.0129, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51835.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0495, 0.0768, 0.1168,  ..., 0.0122, 0.0521, 0.0000],
        [0.0361, 0.0693, 0.0804,  ..., 0.0070, 0.0404, 0.0000],
        [0.0195, 0.0598, 0.0370,  ..., 0.0022, 0.0261, 0.0000],
        ...,
        [0.0013, 0.0507, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0013, 0.0507, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0013, 0.0507, 0.0000,  ..., 0.0000, 0.0097, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(426098.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2235.8630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.2121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4113.2651, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(839.8178, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.3439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0320],
        [-0.0682],
        [-0.2813],
        ...,
        [-1.6595],
        [-1.6566],
        [-1.6566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266495.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0103],
        [1.0103],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368066.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0103],
        [1.0104],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368076.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        ...,
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1977.8754, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.1393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6286, device='cuda:0')



h[100].sum tensor(80.3173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.7863, device='cuda:0')



h[200].sum tensor(26.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50258.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0074, 0.0523, 0.0061,  ..., 0.0000, 0.0160, 0.0000],
        [0.0066, 0.0522, 0.0046,  ..., 0.0000, 0.0152, 0.0000],
        [0.0073, 0.0529, 0.0043,  ..., 0.0000, 0.0157, 0.0000],
        ...,
        [0.0011, 0.0509, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0011, 0.0509, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0011, 0.0509, 0.0000,  ..., 0.0000, 0.0096, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(418833.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2094.9233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(225.4995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4057.3855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(816.6099, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-492.2716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4047],
        [-0.4433],
        [-0.4195],
        ...,
        [-1.6780],
        [-1.6744],
        [-1.6738]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257306.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0103],
        [1.0104],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368076.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0103],
        [1.0104],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368076.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        ...,
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04],
        [-2.1399e-05, -6.6653e-06, -6.1094e-04,  ...,  9.3775e-04,
          0.0000e+00, -7.7169e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2595.9229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.0560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0574, device='cuda:0')



h[100].sum tensor(84.1583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.0062, device='cuda:0')



h[200].sum tensor(39.5509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66083.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0496, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0010, 0.0498, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0010, 0.0500, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        ...,
        [0.0011, 0.0509, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0011, 0.0509, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0011, 0.0509, 0.0000,  ..., 0.0000, 0.0096, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487342.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3028.8743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(365.2915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4020.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1033.4814, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-662.7485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5924],
        [-1.7387],
        [-1.8378],
        ...,
        [-1.6758],
        [-1.6735],
        [-1.6739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259672.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0103],
        [1.0104],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368076.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0104],
        [1.0105],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368086.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6460e-03,  5.1861e-03, -6.5536e-04,  ...,  1.1801e-02,
         -1.3620e-03,  3.5443e-03],
        [-1.8199e-05, -4.0162e-06, -6.1094e-04,  ...,  9.4387e-04,
          0.0000e+00, -7.7128e-04],
        [-1.8199e-05, -4.0162e-06, -6.1094e-04,  ...,  9.4387e-04,
          0.0000e+00, -7.7128e-04],
        ...,
        [-1.8199e-05, -4.0162e-06, -6.1094e-04,  ...,  9.4387e-04,
          0.0000e+00, -7.7128e-04],
        [-1.8199e-05, -4.0162e-06, -6.1094e-04,  ...,  9.4387e-04,
          0.0000e+00, -7.7128e-04],
        [-1.8199e-05, -4.0162e-06, -6.1094e-04,  ...,  9.4387e-04,
          0.0000e+00, -7.7128e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1940.9403, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.2915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1652, device='cuda:0')



h[100].sum tensor(80.4921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.4009, device='cuda:0')



h[200].sum tensor(25.6788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0140, 0.0109, 0.0000,  ..., 0.0266, 0.0000, 0.0075],
        [0.0067, 0.0052, 0.0000,  ..., 0.0148, 0.0000, 0.0036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48447.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0372, 0.0706, 0.0854,  ..., 0.0078, 0.0410, 0.0000],
        [0.0200, 0.0610, 0.0401,  ..., 0.0029, 0.0260, 0.0000],
        [0.0064, 0.0532, 0.0086,  ..., 0.0000, 0.0142, 0.0000],
        ...,
        [0.0010, 0.0511, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0010, 0.0511, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0010, 0.0511, 0.0000,  ..., 0.0000, 0.0097, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410575.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1920.2458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(207.1044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4147.0483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(788.2706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.3697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2486],
        [-0.6405],
        [-1.0772],
        ...,
        [-1.6884],
        [-1.6848],
        [-1.6839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263043.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0104],
        [1.0105],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368086.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0105],
        [1.0106],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368095.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0574e-05, -1.7854e-06, -6.1094e-04,  ...,  9.6705e-04,
          0.0000e+00, -7.6447e-04],
        [ 1.2154e-02,  9.4800e-03, -6.9208e-04,  ...,  2.0805e-02,
         -2.4783e-03,  7.1186e-03],
        [ 1.0535e-02,  8.2192e-03, -6.8129e-04,  ...,  1.8167e-02,
         -2.1488e-03,  6.0703e-03],
        ...,
        [-2.0574e-05, -1.7854e-06, -6.1094e-04,  ...,  9.6705e-04,
          0.0000e+00, -7.6447e-04],
        [-2.0574e-05, -1.7854e-06, -6.1094e-04,  ...,  9.6705e-04,
          0.0000e+00, -7.6447e-04],
        [-2.0574e-05, -1.7854e-06, -6.1094e-04,  ...,  9.6705e-04,
          0.0000e+00, -7.6447e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2076.3350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.5473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3688, device='cuda:0')



h[100].sum tensor(82.0028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.9992, device='cuda:0')



h[200].sum tensor(28.9186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0095, 0.0000,  ..., 0.0239, 0.0000, 0.0072],
        [0.0207, 0.0161, 0.0000,  ..., 0.0377, 0.0000, 0.0119],
        [0.0649, 0.0506, 0.0000,  ..., 0.1098, 0.0000, 0.0390],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50264.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.9620e-02, 6.7604e-02, 6.8164e-02,  ..., 7.0851e-03, 3.3550e-02,
         0.0000e+00],
        [5.4035e-02, 8.2782e-02, 1.3625e-01,  ..., 1.6946e-02, 5.3697e-02,
         0.0000e+00],
        [9.9993e-02, 1.1123e-01, 2.6638e-01,  ..., 4.0003e-02, 9.1028e-02,
         0.0000e+00],
        ...,
        [1.4892e-02, 5.9966e-02, 2.6806e-02,  ..., 4.6254e-04, 2.1685e-02,
         0.0000e+00],
        [1.1408e-02, 5.7891e-02, 2.0131e-02,  ..., 2.4785e-04, 1.8725e-02,
         0.0000e+00],
        [4.3142e-03, 5.3648e-02, 3.9441e-03,  ..., 0.0000e+00, 1.2703e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413784.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1929.5405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(220.2991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3982.9512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(810.1080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-496.5193, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1609],
        [-0.0485],
        [ 0.0317],
        ...,
        [-0.7513],
        [-0.9401],
        [-1.2238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264815.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0105],
        [1.0106],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368095.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(449.7896, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0106],
        [1.0107],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368104.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9604e-05,  2.1398e-06, -6.1094e-04,  ...,  9.8228e-04,
          0.0000e+00, -7.5962e-04],
        [-1.9604e-05,  2.1398e-06, -6.1094e-04,  ...,  9.8228e-04,
          0.0000e+00, -7.5962e-04],
        [-1.9604e-05,  2.1398e-06, -6.1094e-04,  ...,  9.8228e-04,
          0.0000e+00, -7.5962e-04],
        ...,
        [-1.9604e-05,  2.1398e-06, -6.1094e-04,  ...,  9.8228e-04,
          0.0000e+00, -7.5962e-04],
        [-1.9604e-05,  2.1398e-06, -6.1094e-04,  ...,  9.8228e-04,
          0.0000e+00, -7.5962e-04],
        [-1.9604e-05,  2.1398e-06, -6.1094e-04,  ...,  9.8228e-04,
          0.0000e+00, -7.5962e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1747.3916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.9289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-6.9707, device='cuda:0')



h[100].sum tensor(80.5157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.8399, device='cuda:0')



h[200].sum tensor(22.1529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.6135e-06, 0.0000e+00,  ..., 3.9540e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.6524e-06, 0.0000e+00,  ..., 3.9719e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.6568e-06, 0.0000e+00,  ..., 3.9739e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 8.7881e-06, 0.0000e+00,  ..., 4.0342e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.7889e-06, 0.0000e+00,  ..., 4.0346e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.7887e-06, 0.0000e+00,  ..., 4.0345e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44434.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0518, 0.0011,  ..., 0.0000, 0.0116, 0.0000],
        [0.0031, 0.0524, 0.0021,  ..., 0.0000, 0.0117, 0.0000],
        [0.0060, 0.0544, 0.0095,  ..., 0.0000, 0.0141, 0.0000],
        ...,
        [0.0006, 0.0520, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0006, 0.0520, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0006, 0.0520, 0.0000,  ..., 0.0000, 0.0098, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(397217.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1645.7329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(166.2294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3958.9604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(727.7211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.1699, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6189],
        [-0.6633],
        [-0.5928],
        ...,
        [-1.7073],
        [-1.7036],
        [-1.7027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261643.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0106],
        [1.0107],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368104.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0107],
        [1.0108],
        ...,
        [1.0012],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368113.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6416e-05,  3.4808e-06, -6.1094e-04,  ...,  9.8291e-04,
          0.0000e+00, -7.6186e-04],
        [ 5.6827e-03,  4.4426e-03, -6.4892e-04,  ...,  1.0271e-02,
         -1.1510e-03,  2.9276e-03],
        [ 5.8223e-03,  4.5513e-03, -6.4985e-04,  ...,  1.0498e-02,
         -1.1792e-03,  3.0180e-03],
        ...,
        [-1.6416e-05,  3.4808e-06, -6.1094e-04,  ...,  9.8291e-04,
          0.0000e+00, -7.6186e-04],
        [-1.6416e-05,  3.4808e-06, -6.1094e-04,  ...,  9.8291e-04,
          0.0000e+00, -7.6186e-04],
        [-1.6416e-05,  3.4808e-06, -6.1094e-04,  ...,  9.8291e-04,
          0.0000e+00, -7.6186e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2161.1177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9496, device='cuda:0')



h[100].sum tensor(83.4822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.7356, device='cuda:0')



h[200].sum tensor(31.1541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.7207e-03, 4.4828e-03, 0.0000e+00,  ..., 1.3307e-02, 0.0000e+00,
         2.9472e-03],
        [1.9130e-02, 1.4941e-02, 0.0000e+00,  ..., 3.5206e-02, 0.0000e+00,
         1.0864e-02],
        [4.5648e-02, 3.5622e-02, 0.0000e+00,  ..., 7.8480e-02, 0.0000e+00,
         2.6512e-02],
        ...,
        [0.0000e+00, 1.4298e-05, 0.0000e+00,  ..., 4.0374e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.4299e-05, 0.0000e+00,  ..., 4.0378e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.4299e-05, 0.0000e+00,  ..., 4.0377e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54049.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0245, 0.0643, 0.0535,  ..., 0.0036, 0.0311, 0.0000],
        [0.0487, 0.0789, 0.1183,  ..., 0.0116, 0.0515, 0.0000],
        [0.0745, 0.0943, 0.1887,  ..., 0.0230, 0.0731, 0.0000],
        ...,
        [0.0005, 0.0523, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0005, 0.0524, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0005, 0.0524, 0.0000,  ..., 0.0000, 0.0099, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(430950.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2136.2693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(250.4299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3809.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(858.0311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-539.8591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1115],
        [ 0.0062],
        [ 0.0823],
        ...,
        [-1.7180],
        [-1.7143],
        [-1.7134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255370.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0107],
        [1.0108],
        ...,
        [1.0012],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368113.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0108],
        [1.0109],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368123.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.3220e-06,  5.6128e-06, -6.1094e-04,  ...,  9.7213e-04,
          0.0000e+00, -7.6966e-04],
        [-8.3220e-06,  5.6128e-06, -6.1094e-04,  ...,  9.7213e-04,
          0.0000e+00, -7.6966e-04],
        [-8.3220e-06,  5.6128e-06, -6.1094e-04,  ...,  9.7213e-04,
          0.0000e+00, -7.6966e-04],
        ...,
        [-8.3220e-06,  5.6128e-06, -6.1094e-04,  ...,  9.7213e-04,
          0.0000e+00, -7.6966e-04],
        [-8.3220e-06,  5.6128e-06, -6.1094e-04,  ...,  9.7213e-04,
          0.0000e+00, -7.6966e-04],
        [-8.3220e-06,  5.6128e-06, -6.1094e-04,  ...,  9.7213e-04,
          0.0000e+00, -7.6966e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2215.1282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4798, device='cuda:0')



h[100].sum tensor(84.0739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.3208, device='cuda:0')



h[200].sum tensor(32.3547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.2598e-05, 0.0000e+00,  ..., 3.9139e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2700e-05, 0.0000e+00,  ..., 3.9316e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2712e-05, 0.0000e+00,  ..., 3.9337e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.3058e-05, 0.0000e+00,  ..., 3.9937e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3061e-05, 0.0000e+00,  ..., 3.9940e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3060e-05, 0.0000e+00,  ..., 3.9939e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55875.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0516, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        [0.0004, 0.0514, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0004, 0.0515, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        ...,
        [0.0005, 0.0525, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0005, 0.0525, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        [0.0005, 0.0525, 0.0000,  ..., 0.0000, 0.0099, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446149.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2330.0967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(266.3540, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3885.4822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(881.5121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-559.7681, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2724],
        [-1.5659],
        [-1.7538],
        ...,
        [-1.7276],
        [-1.7239],
        [-1.7230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261647.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0108],
        [1.0109],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368123.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0109],
        [1.0110],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368132.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04],
        [ 1.1279e-02,  8.7912e-03, -6.8611e-04,  ...,  1.9348e-02,
         -2.2602e-03,  6.5259e-03],
        [-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04],
        ...,
        [-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04],
        [-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04],
        [-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2102.9414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2077, device='cuda:0')



h[100].sum tensor(83.7210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.5176, device='cuda:0')



h[200].sum tensor(29.8846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1356e-02, 8.8625e-03, 0.0000e+00,  ..., 2.2392e-02, 0.0000e+00,
         6.5705e-03],
        [1.8263e-02, 1.4243e-02, 0.0000e+00,  ..., 3.3664e-02, 0.0000e+00,
         1.1036e-02],
        [8.6824e-02, 6.7653e-02, 0.0000e+00,  ..., 1.4540e-01, 0.0000e+00,
         5.3056e-02],
        ...,
        [0.0000e+00, 1.5218e-05, 0.0000e+00,  ..., 3.9624e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5220e-05, 0.0000e+00,  ..., 3.9628e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5219e-05, 0.0000e+00,  ..., 3.9626e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51508.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.7951e-02, 9.1811e-02, 1.7473e-01,  ..., 2.3054e-02, 6.5321e-02,
         0.0000e+00],
        [1.0320e-01, 1.1364e-01, 2.7389e-01,  ..., 4.1067e-02, 9.3620e-02,
         0.0000e+00],
        [1.6618e-01, 1.5198e-01, 4.5021e-01,  ..., 7.2708e-02, 1.4461e-01,
         0.0000e+00],
        ...,
        [3.7214e-04, 5.2857e-02, 0.0000e+00,  ..., 0.0000e+00, 9.7501e-03,
         0.0000e+00],
        [3.7227e-04, 5.2862e-02, 0.0000e+00,  ..., 0.0000e+00, 9.7515e-03,
         0.0000e+00],
        [3.7229e-04, 5.2861e-02, 0.0000e+00,  ..., 0.0000e+00, 9.7514e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420373.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1975.7872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(225.6256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3966.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(822.6675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-514.5891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0798],
        [ 0.0789],
        [ 0.0809],
        ...,
        [-1.7411],
        [-1.7373],
        [-1.7364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253164.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0109],
        [1.0110],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368132.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0109],
        [1.0110],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368132.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3869e-02,  4.1967e-02, -9.6988e-04,  ...,  8.8749e-02,
         -1.0793e-02,  3.4086e-02],
        [ 2.1368e-02,  1.6650e-02, -7.5333e-04,  ...,  3.5788e-02,
         -4.2816e-03,  1.3055e-02],
        [ 1.1211e-02,  8.7378e-03, -6.8565e-04,  ...,  1.9236e-02,
         -2.2465e-03,  6.4815e-03],
        ...,
        [-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04],
        [-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04],
        [-2.0507e-06,  3.7039e-06, -6.1094e-04,  ...,  9.6439e-04,
          0.0000e+00, -7.7425e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2195.2034, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.5998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0149, device='cuda:0')



h[100].sum tensor(84.2874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9310, device='cuda:0')



h[200].sum tensor(31.8587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.1336e-02, 7.1168e-02, 0.0000e+00,  ..., 1.5273e-01, 0.0000e+00,
         5.5992e-02],
        [1.1684e-01, 9.1032e-02, 0.0000e+00,  ..., 1.9430e-01, 0.0000e+00,
         7.2479e-02],
        [6.7886e-02, 5.2901e-02, 0.0000e+00,  ..., 1.1454e-01, 0.0000e+00,
         4.0801e-02],
        ...,
        [0.0000e+00, 1.5218e-05, 0.0000e+00,  ..., 3.9624e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5220e-05, 0.0000e+00,  ..., 3.9628e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5219e-05, 0.0000e+00,  ..., 3.9626e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55272.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1947e-01, 1.8410e-01, 6.0146e-01,  ..., 1.0072e-01, 1.8723e-01,
         0.0000e+00],
        [2.2420e-01, 1.8690e-01, 6.1438e-01,  ..., 1.0277e-01, 1.9150e-01,
         0.0000e+00],
        [1.6947e-01, 1.5314e-01, 4.5659e-01,  ..., 7.2434e-02, 1.4852e-01,
         0.0000e+00],
        ...,
        [3.7214e-04, 5.2857e-02, 0.0000e+00,  ..., 0.0000e+00, 9.7501e-03,
         0.0000e+00],
        [3.7227e-04, 5.2862e-02, 0.0000e+00,  ..., 0.0000e+00, 9.7515e-03,
         0.0000e+00],
        [3.7229e-04, 5.2861e-02, 0.0000e+00,  ..., 0.0000e+00, 9.7514e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438443.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2275.9089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.2191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3829.8269, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(875.5037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-555.5347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0731],
        [ 0.0822],
        [ 0.0961],
        ...,
        [-1.7411],
        [-1.7373],
        [-1.7364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240609.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0109],
        [1.0110],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368132.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0110],
        [1.0111],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368141.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0318e-02,  2.3615e-02, -8.1289e-04,  ...,  5.0346e-02,
         -6.0486e-03,  1.8831e-02],
        [ 2.1780e-02,  1.6964e-02, -7.5600e-04,  ...,  3.6433e-02,
         -4.3448e-03,  1.3307e-02],
        [ 6.2563e-06,  3.8398e-06, -6.1094e-04,  ...,  9.5457e-04,
          0.0000e+00, -7.8009e-04],
        ...,
        [ 6.2563e-06,  3.8398e-06, -6.1094e-04,  ...,  9.5457e-04,
          0.0000e+00, -7.8009e-04],
        [ 6.2563e-06,  3.8398e-06, -6.1094e-04,  ...,  9.5457e-04,
          0.0000e+00, -7.8009e-04],
        [ 1.1299e-02,  8.8004e-03, -6.8618e-04,  ...,  1.9356e-02,
         -2.2535e-03,  6.5261e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2740.7727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.1158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4098, device='cuda:0')



h[100].sum tensor(87.9174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(49.0599, device='cuda:0')



h[200].sum tensor(43.4442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.1546e-01, 8.9935e-02, 0.0000e+00,  ..., 1.9194e-01, 0.0000e+00,
         7.1544e-02],
        [5.6098e-02, 4.3694e-02, 0.0000e+00,  ..., 9.5230e-02, 0.0000e+00,
         3.3913e-02],
        [3.0509e-02, 2.3761e-02, 0.0000e+00,  ..., 5.3535e-02, 0.0000e+00,
         1.8144e-02],
        ...,
        [2.5709e-05, 1.5779e-05, 0.0000e+00,  ..., 3.9226e-03, 0.0000e+00,
         0.0000e+00],
        [1.1626e-02, 9.0520e-03, 0.0000e+00,  ..., 2.2825e-02, 0.0000e+00,
         6.7039e-03],
        [2.0989e-02, 1.6346e-02, 0.0000e+00,  ..., 3.8082e-02, 0.0000e+00,
         1.1960e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69661.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2230, 0.1862, 0.6121,  ..., 0.1041, 0.1895, 0.0000],
        [0.1377, 0.1345, 0.3709,  ..., 0.0595, 0.1210, 0.0000],
        [0.0727, 0.0954, 0.1891,  ..., 0.0266, 0.0687, 0.0000],
        ...,
        [0.0112, 0.0595, 0.0213,  ..., 0.0000, 0.0189, 0.0000],
        [0.0295, 0.0704, 0.0684,  ..., 0.0067, 0.0340, 0.0000],
        [0.0535, 0.0847, 0.1337,  ..., 0.0158, 0.0538, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504851.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3250.2844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(383.6627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3728.6389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1074.3334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-711.6271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0161],
        [-0.0489],
        [-0.2852],
        ...,
        [-0.7005],
        [-0.4152],
        [-0.1513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227658.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0110],
        [1.0111],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368141.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0111],
        [1.0112],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368151.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0956e-06, -8.2890e-07, -6.1094e-04,  ...,  9.4365e-04,
          0.0000e+00, -7.8125e-04],
        [ 9.0956e-06, -8.2890e-07, -6.1094e-04,  ...,  9.4365e-04,
          0.0000e+00, -7.8125e-04],
        [ 9.0956e-06, -8.2890e-07, -6.1094e-04,  ...,  9.4365e-04,
          0.0000e+00, -7.8125e-04],
        ...,
        [ 9.0956e-06, -8.2890e-07, -6.1094e-04,  ...,  9.4365e-04,
          0.0000e+00, -7.8125e-04],
        [ 9.0956e-06, -8.2890e-07, -6.1094e-04,  ...,  9.4365e-04,
          0.0000e+00, -7.8125e-04],
        [ 9.0956e-06, -8.2890e-07, -6.1094e-04,  ...,  9.4365e-04,
          0.0000e+00, -7.8125e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2512.3867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.6658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0407, device='cuda:0')



h[100].sum tensor(87.0144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.9771, device='cuda:0')



h[200].sum tensor(38.1802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5661, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.6629e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8002e-03, 0.0000e+00,
         0.0000e+00],
        [3.6795e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8174e-03, 0.0000e+00,
         0.0000e+00],
        [3.6816e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8196e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.7382e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8783e-03, 0.0000e+00,
         0.0000e+00],
        [3.7385e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8786e-03, 0.0000e+00,
         0.0000e+00],
        [3.7383e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8785e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59349.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0578, 0.0187,  ..., 0.0000, 0.0191, 0.0000],
        [0.0112, 0.0581, 0.0188,  ..., 0.0000, 0.0192, 0.0000],
        [0.0110, 0.0582, 0.0176,  ..., 0.0000, 0.0192, 0.0000],
        ...,
        [0.0003, 0.0535, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0003, 0.0535, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0003, 0.0535, 0.0000,  ..., 0.0000, 0.0097, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446292.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2290.4243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.2852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4099.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(929.0397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.7551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3386],
        [-0.2625],
        [-0.1898],
        ...,
        [-1.7661],
        [-1.7623],
        [-1.7613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257505., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0111],
        [1.0112],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368151.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0112],
        [1.0113],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368160.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4685e-05, -4.9933e-06, -6.1094e-04,  ...,  9.2956e-04,
          0.0000e+00, -7.8396e-04],
        [ 1.5000e-02,  1.1667e-02, -7.1078e-04,  ...,  2.5347e-02,
         -2.9667e-03,  8.9119e-03],
        [ 1.4685e-05, -4.9933e-06, -6.1094e-04,  ...,  9.2956e-04,
          0.0000e+00, -7.8396e-04],
        ...,
        [ 1.4685e-05, -4.9933e-06, -6.1094e-04,  ...,  9.2956e-04,
          0.0000e+00, -7.8396e-04],
        [ 1.4685e-05, -4.9933e-06, -6.1094e-04,  ...,  9.2956e-04,
          0.0000e+00, -7.8396e-04],
        [ 1.4685e-05, -4.9933e-06, -6.1094e-04,  ...,  9.2956e-04,
          0.0000e+00, -7.8396e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2675.9873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.7905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0896, device='cuda:0')



h[100].sum tensor(88.4705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.1127, device='cuda:0')



h[200].sum tensor(41.1913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6830, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.4875e-02, 4.2677e-02, 0.0000e+00,  ..., 9.3063e-02, 0.0000e+00,
         3.2310e-02],
        [1.2424e-02, 9.6262e-03, 0.0000e+00,  ..., 2.3909e-02, 0.0000e+00,
         7.2082e-03],
        [1.5212e-02, 1.1798e-02, 0.0000e+00,  ..., 2.8454e-02, 0.0000e+00,
         9.0116e-03],
        ...,
        [6.0361e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8209e-03, 0.0000e+00,
         0.0000e+00],
        [6.0366e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8212e-03, 0.0000e+00,
         0.0000e+00],
        [6.0363e-05, 0.0000e+00, 0.0000e+00,  ..., 3.8210e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67288.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0576, 0.0866, 0.1461,  ..., 0.0190, 0.0566, 0.0000],
        [0.0338, 0.0727, 0.0813,  ..., 0.0079, 0.0371, 0.0000],
        [0.0265, 0.0685, 0.0615,  ..., 0.0057, 0.0310, 0.0000],
        ...,
        [0.0003, 0.0538, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0003, 0.0538, 0.0000,  ..., 0.0000, 0.0096, 0.0000],
        [0.0003, 0.0538, 0.0000,  ..., 0.0000, 0.0096, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492028.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2993.0063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.4238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4076.9033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1042.7318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.6021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5093],
        [-0.6547],
        [-0.8765],
        ...,
        [-1.7783],
        [-1.7742],
        [-1.7730]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223011.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0112],
        [1.0113],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368160.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0113],
        [1.0114],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368170.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.4681e-03,  5.0138e-03, -6.5392e-04,  ...,  1.1425e-02,
         -1.2720e-03,  3.3876e-03],
        [ 1.2757e-02,  9.9118e-03, -6.9582e-04,  ...,  2.1672e-02,
         -2.5122e-03,  7.4570e-03],
        [ 1.7522e-05, -1.0335e-05, -6.1094e-04,  ...,  9.1424e-04,
          0.0000e+00, -7.8671e-04],
        ...,
        [ 1.7522e-05, -1.0335e-05, -6.1094e-04,  ...,  9.1424e-04,
          0.0000e+00, -7.8671e-04],
        [ 1.7522e-05, -1.0335e-05, -6.1094e-04,  ...,  9.1424e-04,
          0.0000e+00, -7.8671e-04],
        [ 1.7522e-05, -1.0335e-05, -6.1094e-04,  ...,  9.1424e-04,
          0.0000e+00, -7.8671e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1937.0195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.7918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2918, device='cuda:0')



h[100].sum tensor(84.3749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.7898, device='cuda:0')



h[200].sum tensor(24.8006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.2817e-02, 2.5474e-02, 0.0000e+00,  ..., 5.7041e-02, 0.0000e+00,
         1.8814e-02],
        [1.7119e-02, 1.3258e-02, 0.0000e+00,  ..., 3.1479e-02, 0.0000e+00,
         9.4410e-03],
        [4.6909e-02, 3.6438e-02, 0.0000e+00,  ..., 8.0021e-02, 0.0000e+00,
         2.7124e-02],
        ...,
        [7.2035e-05, 0.0000e+00, 0.0000e+00,  ..., 3.7584e-03, 0.0000e+00,
         0.0000e+00],
        [7.2040e-05, 0.0000e+00, 0.0000e+00,  ..., 3.7587e-03, 0.0000e+00,
         0.0000e+00],
        [7.2037e-05, 0.0000e+00, 0.0000e+00,  ..., 3.7586e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47548.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0897, 0.1051, 0.2328,  ..., 0.0339, 0.0838, 0.0000],
        [0.0603, 0.0880, 0.1518,  ..., 0.0197, 0.0596, 0.0000],
        [0.0580, 0.0871, 0.1457,  ..., 0.0188, 0.0575, 0.0000],
        ...,
        [0.0003, 0.0542, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0003, 0.0542, 0.0000,  ..., 0.0000, 0.0095, 0.0000],
        [0.0003, 0.0542, 0.0000,  ..., 0.0000, 0.0095, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410621.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1728.5894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(190.0323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4501.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(769.4971, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-464.3279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0914],
        [ 0.0685],
        [ 0.0020],
        ...,
        [-1.8002],
        [-1.7963],
        [-1.7953]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280793.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0113],
        [1.0114],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368170.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0113],
        [1.0115],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368180.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3466e-05, -1.4411e-05, -6.1094e-04,  ...,  8.9929e-04,
          0.0000e+00, -7.8945e-04],
        [ 2.3466e-05, -1.4411e-05, -6.1094e-04,  ...,  8.9929e-04,
          0.0000e+00, -7.8945e-04],
        [ 2.3466e-05, -1.4411e-05, -6.1094e-04,  ...,  8.9929e-04,
          0.0000e+00, -7.8945e-04],
        ...,
        [ 2.3466e-05, -1.4411e-05, -6.1094e-04,  ...,  8.9929e-04,
          0.0000e+00, -7.8945e-04],
        [ 2.3466e-05, -1.4411e-05, -6.1094e-04,  ...,  8.9929e-04,
          0.0000e+00, -7.8945e-04],
        [ 2.3466e-05, -1.4411e-05, -6.1094e-04,  ...,  8.9929e-04,
          0.0000e+00, -7.8945e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2351.5906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.7105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0517, device='cuda:0')



h[100].sum tensor(87.3352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.0306, device='cuda:0')



h[200].sum tensor(32.9850, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.4521e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6223e-03, 0.0000e+00,
         0.0000e+00],
        [9.4947e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6387e-03, 0.0000e+00,
         0.0000e+00],
        [9.5012e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6412e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [9.6481e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6975e-03, 0.0000e+00,
         0.0000e+00],
        [9.6489e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6978e-03, 0.0000e+00,
         0.0000e+00],
        [9.6484e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6976e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59405.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0531, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0003, 0.0533, 0.0000,  ..., 0.0000, 0.0091, 0.0000],
        [0.0003, 0.0535, 0.0000,  ..., 0.0000, 0.0092, 0.0000],
        ...,
        [0.0010, 0.0549, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0003, 0.0545, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0003, 0.0545, 0.0000,  ..., 0.0000, 0.0094, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(459682.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2634.7253, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.5408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4178.8721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(943.7447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.6688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0331],
        [-2.0176],
        [-1.9718],
        ...,
        [-1.7282],
        [-1.7850],
        [-1.8060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214574.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0113],
        [1.0115],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368180.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(467.3013, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0114],
        [1.0116],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368189.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2229e-02,  9.4892e-03, -6.9229e-04,  ...,  2.0794e-02,
         -2.3888e-03,  7.1221e-03],
        [ 1.2478e-02,  9.6826e-03, -6.9395e-04,  ...,  2.1198e-02,
         -2.4374e-03,  7.2828e-03],
        [ 1.6556e-02,  1.2858e-02, -7.2112e-04,  ...,  2.7844e-02,
         -3.2352e-03,  9.9227e-03],
        ...,
        [ 2.0152e-05, -1.8864e-05, -6.1094e-04,  ...,  8.9699e-04,
          0.0000e+00, -7.8181e-04],
        [ 2.0152e-05, -1.8864e-05, -6.1094e-04,  ...,  8.9699e-04,
          0.0000e+00, -7.8181e-04],
        [ 2.0152e-05, -1.8864e-05, -6.1094e-04,  ...,  8.9699e-04,
          0.0000e+00, -7.8181e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2230.3035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0188, device='cuda:0')



h[100].sum tensor(87.3161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9424, device='cuda:0')



h[200].sum tensor(29.6972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.5698e-02, 4.3236e-02, 0.0000e+00,  ..., 9.4249e-02, 0.0000e+00,
         3.2856e-02],
        [5.3115e-02, 4.1224e-02, 0.0000e+00,  ..., 9.0056e-02, 0.0000e+00,
         3.1169e-02],
        [5.6808e-02, 4.4119e-02, 0.0000e+00,  ..., 9.6075e-02, 0.0000e+00,
         3.4349e-02],
        ...,
        [8.2869e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6885e-03, 0.0000e+00,
         0.0000e+00],
        [8.2875e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6888e-03, 0.0000e+00,
         0.0000e+00],
        [8.2872e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6887e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53542.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.4754e-02, 1.1042e-01, 2.5020e-01,  ..., 3.9456e-02, 8.6708e-02,
         0.0000e+00],
        [1.1213e-01, 1.2108e-01, 2.9879e-01,  ..., 4.8279e-02, 1.0093e-01,
         0.0000e+00],
        [1.1995e-01, 1.2613e-01, 3.2237e-01,  ..., 5.3354e-02, 1.0690e-01,
         0.0000e+00],
        ...,
        [2.8461e-04, 5.5171e-02, 0.0000e+00,  ..., 0.0000e+00, 9.2036e-03,
         0.0000e+00],
        [2.8463e-04, 5.5176e-02, 0.0000e+00,  ..., 0.0000e+00, 9.2048e-03,
         0.0000e+00],
        [2.8462e-04, 5.5174e-02, 0.0000e+00,  ..., 0.0000e+00, 9.2044e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433147.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2034.1096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.5577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4604.7012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(856.5361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-521.9950, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0613],
        [ 0.0757],
        [ 0.0756],
        ...,
        [-1.8372],
        [-1.8331],
        [-1.8321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292721.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0114],
        [1.0116],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368189.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0115],
        [1.0117],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368199.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0095e-02,  1.5609e-02, -7.4469e-04,  ...,  3.3610e-02,
         -3.9118e-03,  1.2219e-02],
        [ 2.2704e-05, -2.1859e-05, -6.1094e-04,  ...,  8.9573e-04,
          0.0000e+00, -7.7784e-04],
        [ 2.2704e-05, -2.1859e-05, -6.1094e-04,  ...,  8.9573e-04,
          0.0000e+00, -7.7784e-04],
        ...,
        [ 8.5451e-03,  6.6145e-03, -6.6773e-04,  ...,  1.4786e-02,
         -1.6609e-03,  4.7405e-03],
        [ 1.8866e-02,  1.4652e-02, -7.3650e-04,  ...,  3.1607e-02,
         -3.6724e-03,  1.1424e-02],
        [ 1.3388e-02,  1.0386e-02, -7.0000e-04,  ...,  2.2679e-02,
         -2.6048e-03,  7.8765e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2366.8408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.1249, device='cuda:0')



h[100].sum tensor(88.6319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.2493, device='cuda:0')



h[200].sum tensor(31.9897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0748, 0.0581, 0.0000,  ..., 0.1254, 0.0000, 0.0460],
        [0.0486, 0.0377, 0.0000,  ..., 0.0826, 0.0000, 0.0290],
        [0.0220, 0.0170, 0.0000,  ..., 0.0393, 0.0000, 0.0126],
        ...,
        [0.0356, 0.0276, 0.0000,  ..., 0.0615, 0.0000, 0.0214],
        [0.0625, 0.0485, 0.0000,  ..., 0.1054, 0.0000, 0.0372],
        [0.0735, 0.0571, 0.0000,  ..., 0.1233, 0.0000, 0.0443]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56281.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1960, 0.1734, 0.5420,  ..., 0.0970, 0.1665, 0.0000],
        [0.1503, 0.1454, 0.4102,  ..., 0.0712, 0.1306, 0.0000],
        [0.1219, 0.1281, 0.3292,  ..., 0.0557, 0.1081, 0.0000],
        ...,
        [0.0762, 0.1008, 0.1989,  ..., 0.0310, 0.0720, 0.0000],
        [0.1070, 0.1194, 0.2844,  ..., 0.0462, 0.0973, 0.0000],
        [0.1097, 0.1213, 0.2930,  ..., 0.0482, 0.0991, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445261.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2188.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.7086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4708.9009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(898.3964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-550.0545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0515],
        [-0.0402],
        [-0.0313],
        ...,
        [-0.1619],
        [ 0.0245],
        [ 0.0082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302002.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0115],
        [1.0117],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368199.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0115],
        [1.0117],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368199.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6826e-02,  2.0850e-02, -7.8954e-04,  ...,  4.4580e-02,
         -5.2236e-03,  1.6577e-02],
        [ 2.8974e-02,  2.2523e-02, -8.0385e-04,  ...,  4.8081e-02,
         -5.6422e-03,  1.7968e-02],
        [ 2.6621e-02,  2.0690e-02, -7.8817e-04,  ...,  4.4245e-02,
         -5.1836e-03,  1.6445e-02],
        ...,
        [ 2.2704e-05, -2.1859e-05, -6.1094e-04,  ...,  8.9573e-04,
          0.0000e+00, -7.7784e-04],
        [ 2.2704e-05, -2.1859e-05, -6.1094e-04,  ...,  8.9573e-04,
          0.0000e+00, -7.7784e-04],
        [ 2.2704e-05, -2.1859e-05, -6.1094e-04,  ...,  8.9573e-04,
          0.0000e+00, -7.7784e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2151.3157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.5342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1234, device='cuda:0')



h[100].sum tensor(87.3238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.2657, device='cuda:0')



h[200].sum tensor(27.4356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.5463e-02, 7.4178e-02, 0.0000e+00,  ..., 1.5905e-01, 0.0000e+00,
         5.8620e-02],
        [9.8398e-02, 7.6463e-02, 0.0000e+00,  ..., 1.6384e-01, 0.0000e+00,
         6.0506e-02],
        [9.0790e-02, 7.0538e-02, 0.0000e+00,  ..., 1.5145e-01, 0.0000e+00,
         5.5578e-02],
        ...,
        [9.3373e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6838e-03, 0.0000e+00,
         0.0000e+00],
        [9.3380e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6841e-03, 0.0000e+00,
         0.0000e+00],
        [9.3376e-05, 0.0000e+00, 0.0000e+00,  ..., 3.6839e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51937.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7402e-01, 1.5992e-01, 4.7952e-01,  ..., 8.5108e-02, 1.4912e-01,
         0.0000e+00],
        [1.6798e-01, 1.5669e-01, 4.6272e-01,  ..., 8.2161e-02, 1.4401e-01,
         0.0000e+00],
        [1.4812e-01, 1.4470e-01, 4.0603e-01,  ..., 7.1340e-02, 1.2819e-01,
         0.0000e+00],
        ...,
        [3.2103e-04, 5.5591e-02, 0.0000e+00,  ..., 0.0000e+00, 9.1491e-03,
         0.0000e+00],
        [3.2105e-04, 5.5596e-02, 0.0000e+00,  ..., 0.0000e+00, 9.1503e-03,
         0.0000e+00],
        [3.2103e-04, 5.5594e-02, 0.0000e+00,  ..., 0.0000e+00, 9.1500e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(428506.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2018.7810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(223.3867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4632.7041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(840.6974, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.4521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0397],
        [ 0.0171],
        [-0.0311],
        ...,
        [-1.8538],
        [-1.8497],
        [-1.8486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273010.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0115],
        [1.0117],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368199.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0115],
        [1.0118],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368208.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9582e-03,  5.3690e-03, -6.5708e-04,  ...,  1.2187e-02,
         -1.3442e-03,  3.7053e-03],
        [ 1.0852e-02,  8.4011e-03, -6.8303e-04,  ...,  1.8534e-02,
         -2.1001e-03,  6.2270e-03],
        [ 2.2979e-02,  1.7844e-02, -7.6383e-04,  ...,  3.8300e-02,
         -4.4540e-03,  1.4080e-02],
        ...,
        [ 3.3509e-05, -2.2918e-05, -6.1094e-04,  ...,  8.9986e-04,
          0.0000e+00, -7.7888e-04],
        [ 3.3509e-05, -2.2918e-05, -6.1094e-04,  ...,  8.9986e-04,
          0.0000e+00, -7.7888e-04],
        [ 3.3509e-05, -2.2918e-05, -6.1094e-04,  ...,  8.9986e-04,
          0.0000e+00, -7.7888e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2276.4854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.0857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2392, device='cuda:0')



h[100].sum tensor(88.2422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6013, device='cuda:0')



h[200].sum tensor(29.7885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0252, 0.0195, 0.0000,  ..., 0.0445, 0.0000, 0.0139],
        [0.0578, 0.0448, 0.0000,  ..., 0.0977, 0.0000, 0.0342],
        [0.0732, 0.0568, 0.0000,  ..., 0.1228, 0.0000, 0.0442],
        ...,
        [0.0001, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0001, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55204.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0714, 0.0949, 0.1815,  ..., 0.0267, 0.0698, 0.0000],
        [0.1171, 0.1236, 0.3109,  ..., 0.0511, 0.1061, 0.0000],
        [0.1486, 0.1438, 0.4011,  ..., 0.0685, 0.1308, 0.0000],
        ...,
        [0.0006, 0.0558, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0006, 0.0558, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0006, 0.0558, 0.0000,  ..., 0.0000, 0.0093, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442788.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2273.1218, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(251.4495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4675.8271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(889.1429, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-539.1178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0623],
        [ 0.0863],
        [ 0.0821],
        ...,
        [-1.8657],
        [-1.8614],
        [-1.8600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275820.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0115],
        [1.0118],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368208.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0116],
        [1.0120],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368218.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0483e-02,  8.0997e-03, -6.8045e-04,  ...,  1.7914e-02,
         -2.0167e-03,  5.9720e-03],
        [ 1.9490e-02,  1.5113e-02, -7.4046e-04,  ...,  3.2597e-02,
         -3.7582e-03,  1.1805e-02],
        [ 4.8950e-03,  3.7489e-03, -6.4321e-04,  ...,  8.8048e-03,
         -9.3637e-04,  2.3532e-03],
        ...,
        [ 5.1879e-05, -2.2011e-05, -6.1094e-04,  ...,  9.0983e-04,
          0.0000e+00, -7.8317e-04],
        [ 5.1879e-05, -2.2011e-05, -6.1094e-04,  ...,  9.0983e-04,
          0.0000e+00, -7.8317e-04],
        [ 5.1879e-05, -2.2011e-05, -6.1094e-04,  ...,  9.0983e-04,
          0.0000e+00, -7.8317e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2180.3218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1162, device='cuda:0')



h[100].sum tensor(87.4266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.2440, device='cuda:0')



h[200].sum tensor(27.7694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0801, 0.0621, 0.0000,  ..., 0.1339, 0.0000, 0.0486],
        [0.0439, 0.0339, 0.0000,  ..., 0.0748, 0.0000, 0.0251],
        [0.0412, 0.0318, 0.0000,  ..., 0.0705, 0.0000, 0.0234],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52692.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1247, 0.1282, 0.3322,  ..., 0.0555, 0.1120, 0.0000],
        [0.1048, 0.1153, 0.2736,  ..., 0.0434, 0.0973, 0.0000],
        [0.0883, 0.1045, 0.2260,  ..., 0.0340, 0.0849, 0.0000],
        ...,
        [0.0009, 0.0558, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0009, 0.0558, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0009, 0.0558, 0.0000,  ..., 0.0000, 0.0097, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(435445.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2199.1733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.2970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4819.2134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(855.7108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-510.5754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1291],
        [ 0.1339],
        [ 0.1345],
        ...,
        [-1.8715],
        [-1.8674],
        [-1.8664]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280093.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0116],
        [1.0120],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368218.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0117],
        [1.0121],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368228.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5747e-02,  1.9968e-02, -7.8199e-04,  ...,  4.2777e-02,
         -4.9431e-03,  1.5838e-02],
        [ 2.6675e-02,  2.0691e-02, -7.8817e-04,  ...,  4.4291e-02,
         -5.1219e-03,  1.6439e-02],
        [ 1.2038e-02,  9.2946e-03, -6.9064e-04,  ...,  2.0427e-02,
         -2.3032e-03,  6.9592e-03],
        ...,
        [ 7.7470e-05, -1.7523e-05, -6.1094e-04,  ...,  9.2698e-04,
          0.0000e+00, -7.8692e-04],
        [ 7.7470e-05, -1.7523e-05, -6.1094e-04,  ...,  9.2698e-04,
          0.0000e+00, -7.8692e-04],
        [ 7.7470e-05, -1.7523e-05, -6.1094e-04,  ...,  9.2698e-04,
          0.0000e+00, -7.8692e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2287.8721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.1090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0519, device='cuda:0')



h[100].sum tensor(87.6244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.0415, device='cuda:0')



h[200].sum tensor(30.3754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0796, 0.0616, 0.0000,  ..., 0.1329, 0.0000, 0.0482],
        [0.0801, 0.0621, 0.0000,  ..., 0.1339, 0.0000, 0.0485],
        [0.0765, 0.0593, 0.0000,  ..., 0.1280, 0.0000, 0.0462],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55206.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1290, 0.1319, 0.3481,  ..., 0.0598, 0.1144, 0.0000],
        [0.1303, 0.1328, 0.3515,  ..., 0.0603, 0.1156, 0.0000],
        [0.1138, 0.1226, 0.3044,  ..., 0.0511, 0.1028, 0.0000],
        ...,
        [0.0013, 0.0556, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0013, 0.0556, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0013, 0.0556, 0.0000,  ..., 0.0000, 0.0105, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(443283.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2486.9407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.1397, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4662.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(896.3803, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-542.3969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0808],
        [ 0.1002],
        [ 0.1091],
        ...,
        [-1.8660],
        [-1.8558],
        [-1.8279]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230202.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0117],
        [1.0121],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368228.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0117],
        [1.0122],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368237.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9606e-03,  4.5594e-03, -6.5011e-04,  ...,  1.0544e-02,
         -1.1273e-03,  3.0277e-03],
        [ 5.5784e-03,  4.2618e-03, -6.4756e-04,  ...,  9.9210e-03,
         -1.0540e-03,  2.7801e-03],
        [ 1.1456e-02,  8.8377e-03, -6.8672e-04,  ...,  1.9506e-02,
         -2.1813e-03,  6.5873e-03],
        ...,
        [ 8.3150e-05, -1.6533e-05, -6.1094e-04,  ...,  9.5967e-04,
          0.0000e+00, -7.7946e-04],
        [ 8.3150e-05, -1.6533e-05, -6.1094e-04,  ...,  9.5967e-04,
          0.0000e+00, -7.7946e-04],
        [ 8.3150e-05, -1.6533e-05, -6.1094e-04,  ...,  9.5967e-04,
          0.0000e+00, -7.7946e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1983.8829, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.2631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9502, device='cuda:0')



h[100].sum tensor(85.9366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.7684, device='cuda:0')



h[200].sum tensor(24.0168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8867, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0147, 0.0000,  ..., 0.0347, 0.0000, 0.0107],
        [0.0402, 0.0309, 0.0000,  ..., 0.0688, 0.0000, 0.0226],
        [0.0194, 0.0148, 0.0000,  ..., 0.0349, 0.0000, 0.0100],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47711.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0800, 0.1100,  ..., 0.0135, 0.0495, 0.0000],
        [0.0643, 0.0912, 0.1596,  ..., 0.0220, 0.0649, 0.0000],
        [0.0504, 0.0828, 0.1211,  ..., 0.0152, 0.0534, 0.0000],
        ...,
        [0.0014, 0.0559, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0014, 0.0559, 0.0000,  ..., 0.0000, 0.0107, 0.0000],
        [0.0014, 0.0559, 0.0000,  ..., 0.0000, 0.0107, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(416506.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2003.8260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(185.2949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4817.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(790.8429, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-456.5156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0194],
        [ 0.0658],
        [-0.0132],
        ...,
        [-1.8722],
        [-1.8683],
        [-1.8672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283378.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0117],
        [1.0122],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368237.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0118],
        [1.0123],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368247.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.1208e-05, -1.5784e-05, -6.1094e-04,  ...,  9.9880e-04,
          0.0000e+00, -7.7242e-04],
        [ 8.1208e-05, -1.5784e-05, -6.1094e-04,  ...,  9.9880e-04,
          0.0000e+00, -7.7242e-04],
        [ 8.1208e-05, -1.5784e-05, -6.1094e-04,  ...,  9.9880e-04,
          0.0000e+00, -7.7242e-04],
        ...,
        [ 8.1208e-05, -1.5784e-05, -6.1094e-04,  ...,  9.9880e-04,
          0.0000e+00, -7.7242e-04],
        [ 8.1208e-05, -1.5784e-05, -6.1094e-04,  ...,  9.9880e-04,
          0.0000e+00, -7.7242e-04],
        [ 8.1208e-05, -1.5784e-05, -6.1094e-04,  ...,  9.9880e-04,
          0.0000e+00, -7.7242e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2473.4023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4455, device='cuda:0')



h[100].sum tensor(89.1142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.2080, device='cuda:0')



h[200].sum tensor(34.3583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58844.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0562, 0.0018,  ..., 0.0000, 0.0130, 0.0000],
        [0.0049, 0.0569, 0.0038,  ..., 0.0000, 0.0138, 0.0000],
        [0.0055, 0.0574, 0.0055,  ..., 0.0000, 0.0145, 0.0000],
        ...,
        [0.0072, 0.0599, 0.0103,  ..., 0.0000, 0.0157, 0.0000],
        [0.0028, 0.0572, 0.0008,  ..., 0.0000, 0.0120, 0.0000],
        [0.0015, 0.0564, 0.0000,  ..., 0.0000, 0.0109, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457790.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2711.9600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.1563, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4332.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(951.5503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-580.5229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1911],
        [-1.0621],
        [-0.9444],
        ...,
        [-1.4890],
        [-1.7028],
        [-1.8174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239986.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0118],
        [1.0123],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368247.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0119],
        [1.0125],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368256.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1909e-02,  9.1956e-03, -6.8977e-04,  ...,  2.0326e-02,
         -2.2510e-03,  6.8954e-03],
        [ 7.8663e-05, -1.4159e-05, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -7.6951e-04],
        [ 7.8663e-05, -1.4159e-05, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -7.6951e-04],
        ...,
        [ 7.8663e-05, -1.4159e-05, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -7.6951e-04],
        [ 7.8663e-05, -1.4159e-05, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -7.6951e-04],
        [ 7.8663e-05, -1.4159e-05, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -7.6951e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2117.7432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.1735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0705, device='cuda:0')



h[100].sum tensor(87.1779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1177, device='cuda:0')



h[200].sum tensor(26.8482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0144, 0.0000,  ..., 0.0343, 0.0000, 0.0104],
        [0.0123, 0.0093, 0.0000,  ..., 0.0237, 0.0000, 0.0070],
        [0.0003, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51395.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0703, 0.0978, 0.1825,  ..., 0.0286, 0.0674, 0.0000],
        [0.0505, 0.0859, 0.1276,  ..., 0.0200, 0.0509, 0.0000],
        [0.0339, 0.0758, 0.0811,  ..., 0.0117, 0.0371, 0.0000],
        ...,
        [0.0015, 0.0568, 0.0000,  ..., 0.0000, 0.0109, 0.0000],
        [0.0015, 0.0568, 0.0000,  ..., 0.0000, 0.0109, 0.0000],
        [0.0015, 0.0568, 0.0000,  ..., 0.0000, 0.0109, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433393.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2255.6221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.4435, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4467.8262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(845.9648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-495.8724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0153],
        [-0.0231],
        [-0.0244],
        ...,
        [-1.8939],
        [-1.8901],
        [-1.8892]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285548.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0119],
        [1.0125],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368256.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0120],
        [1.0126],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368266.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9256e-03,  5.3205e-03, -6.5658e-04,  ...,  1.2216e-02,
         -1.2979e-03,  3.6659e-03],
        [ 7.7061e-05, -1.1143e-05, -6.1094e-04,  ...,  1.0417e-03,
          0.0000e+00, -7.7175e-04],
        [ 7.7061e-05, -1.1143e-05, -6.1094e-04,  ...,  1.0417e-03,
          0.0000e+00, -7.7175e-04],
        ...,
        [ 7.7061e-05, -1.1143e-05, -6.1094e-04,  ...,  1.0417e-03,
          0.0000e+00, -7.7175e-04],
        [ 7.7061e-05, -1.1143e-05, -6.1094e-04,  ...,  1.0417e-03,
          0.0000e+00, -7.7175e-04],
        [ 7.7061e-05, -1.1143e-05, -6.1094e-04,  ...,  1.0417e-03,
          0.0000e+00, -7.7175e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2342.8394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1043, device='cuda:0')



h[100].sum tensor(88.6109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.1982, device='cuda:0')



h[200].sum tensor(31.4545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0086, 0.0000,  ..., 0.0222, 0.0000, 0.0056],
        [0.0072, 0.0054, 0.0000,  ..., 0.0155, 0.0000, 0.0037],
        [0.0003, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58520.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0258, 0.0694, 0.0542,  ..., 0.0041, 0.0326, 0.0000],
        [0.0173, 0.0650, 0.0319,  ..., 0.0018, 0.0248, 0.0000],
        [0.0093, 0.0604, 0.0102,  ..., 0.0000, 0.0180, 0.0000],
        ...,
        [0.0014, 0.0571, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0014, 0.0571, 0.0000,  ..., 0.0000, 0.0110, 0.0000],
        [0.0014, 0.0571, 0.0000,  ..., 0.0000, 0.0110, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472072.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2858.7844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.4838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4184.9214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(949.0590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-576.5136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9488],
        [-0.9952],
        [-0.9418],
        ...,
        [-1.9087],
        [-1.9049],
        [-1.9039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250220.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0120],
        [1.0126],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368266.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(441.9459, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0121],
        [1.0127],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368275.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1332e-02,  8.7511e-03, -6.8590e-04,  ...,  1.9391e-02,
         -2.1233e-03,  6.5085e-03],
        [ 1.9616e-02,  1.5200e-02, -7.4110e-04,  ...,  3.2908e-02,
         -3.6869e-03,  1.1876e-02],
        [ 6.3921e-03,  4.9055e-03, -6.5298e-04,  ...,  1.1332e-02,
         -1.1909e-03,  3.3078e-03],
        ...,
        [ 8.3173e-05, -5.9263e-06, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8017e-04],
        [ 8.3173e-05, -5.9263e-06, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8017e-04],
        [ 8.3173e-05, -5.9263e-06, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8017e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2345.6318, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.4335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0069, device='cuda:0')



h[100].sum tensor(88.4463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9070, device='cuda:0')



h[200].sum tensor(31.4077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0898, 0.0696, 0.0000,  ..., 0.1502, 0.0000, 0.0548],
        [0.0625, 0.0484, 0.0000,  ..., 0.1057, 0.0000, 0.0371],
        [0.0340, 0.0262, 0.0000,  ..., 0.0592, 0.0000, 0.0203],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57473.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1513, 0.1487, 0.4134,  ..., 0.0712, 0.1339, 0.0000],
        [0.1268, 0.1332, 0.3425,  ..., 0.0572, 0.1147, 0.0000],
        [0.0848, 0.1072, 0.2236,  ..., 0.0351, 0.0805, 0.0000],
        ...,
        [0.0014, 0.0571, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0014, 0.0571, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0014, 0.0571, 0.0000,  ..., 0.0000, 0.0111, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460697.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2753.2737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.2834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4135.9351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(938.4034, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-568.5380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0695],
        [ 0.0600],
        [ 0.0191],
        ...,
        [-1.9249],
        [-1.9211],
        [-1.9201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226712.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0121],
        [1.0127],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368275.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0122],
        [1.0129],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368285.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3556e-05, -2.4427e-06, -6.1094e-04,  ...,  1.0463e-03,
          0.0000e+00, -7.8183e-04],
        [ 1.0737e-02,  8.2992e-03, -6.8200e-04,  ...,  1.8447e-02,
         -2.0048e-03,  6.1279e-03],
        [ 2.0846e-02,  1.6169e-02, -7.4936e-04,  ...,  3.4941e-02,
         -3.9051e-03,  1.2678e-02],
        ...,
        [ 7.3556e-05, -2.4427e-06, -6.1094e-04,  ...,  1.0463e-03,
          0.0000e+00, -7.8183e-04],
        [ 7.3556e-05, -2.4427e-06, -6.1094e-04,  ...,  1.0463e-03,
          0.0000e+00, -7.8183e-04],
        [ 7.3556e-05, -2.4427e-06, -6.1094e-04,  ...,  1.0463e-03,
          0.0000e+00, -7.8183e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2511.0166, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.2043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6566, device='cuda:0')



h[100].sum tensor(89.6407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.8389, device='cuda:0')



h[200].sum tensor(34.5504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0152, 0.0000,  ..., 0.0361, 0.0000, 0.0111],
        [0.0385, 0.0298, 0.0000,  ..., 0.0666, 0.0000, 0.0224],
        [0.0576, 0.0446, 0.0000,  ..., 0.0978, 0.0000, 0.0340],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60723.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0487, 0.0853, 0.1210,  ..., 0.0157, 0.0505, 0.0000],
        [0.0884, 0.1104, 0.2336,  ..., 0.0366, 0.0830, 0.0000],
        [0.1250, 0.1335, 0.3379,  ..., 0.0562, 0.1129, 0.0000],
        ...,
        [0.0060, 0.0603, 0.0071,  ..., 0.0000, 0.0151, 0.0000],
        [0.0012, 0.0575, 0.0000,  ..., 0.0000, 0.0109, 0.0000],
        [0.0012, 0.0575, 0.0000,  ..., 0.0000, 0.0109, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472313.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2837.2209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.8340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4136.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(980.9408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-601.7197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0461],
        [ 0.0591],
        [ 0.0617],
        ...,
        [-1.4350],
        [-1.7164],
        [-1.8681]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241553.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0122],
        [1.0129],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368285.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0123],
        [1.0130],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368294.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1400e-03,  4.7348e-03, -6.5147e-04,  ...,  1.0974e-02,
         -1.1388e-03,  3.1582e-03],
        [ 1.5709e-02,  1.2184e-02, -7.1524e-04,  ...,  2.6587e-02,
         -2.9305e-03,  9.3585e-03],
        [ 1.9183e-02,  1.4889e-02, -7.3839e-04,  ...,  3.2257e-02,
         -3.5811e-03,  1.1610e-02],
        ...,
        [ 5.7786e-05, -9.0161e-08, -6.1094e-04,  ...,  1.0496e-03,
          0.0000e+00, -7.8290e-04],
        [ 5.7786e-05, -9.0161e-08, -6.1094e-04,  ...,  1.0496e-03,
          0.0000e+00, -7.8290e-04],
        [ 5.7786e-05, -9.0161e-08, -6.1094e-04,  ...,  1.0496e-03,
          0.0000e+00, -7.8290e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2602.5786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.4819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.5418, device='cuda:0')



h[100].sum tensor(90.4756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.4855, device='cuda:0')



h[200].sum tensor(35.8652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0323, 0.0249, 0.0000,  ..., 0.0565, 0.0000, 0.0184],
        [0.0596, 0.0462, 0.0000,  ..., 0.1011, 0.0000, 0.0353],
        [0.0869, 0.0675, 0.0000,  ..., 0.1457, 0.0000, 0.0530],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62147.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0955, 0.1151, 0.2547,  ..., 0.0404, 0.0890, 0.0000],
        [0.1201, 0.1307, 0.3249,  ..., 0.0537, 0.1090, 0.0000],
        [0.1379, 0.1425, 0.3765,  ..., 0.0638, 0.1231, 0.0000],
        ...,
        [0.0010, 0.0580, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0010, 0.0580, 0.0000,  ..., 0.0000, 0.0104, 0.0000],
        [0.0010, 0.0580, 0.0000,  ..., 0.0000, 0.0104, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476173.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2770.9385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(304.0400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4207.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(997.8690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-614.1483, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0477],
        [-0.0367],
        [-0.0246],
        ...,
        [-1.9813],
        [-1.9771],
        [-1.9761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282060.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0123],
        [1.0130],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368294.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0124],
        [1.0131],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368304.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8063e-05,  4.9748e-06, -6.1094e-04,  ...,  1.0432e-03,
          0.0000e+00, -7.8433e-04],
        [ 4.8063e-05,  4.9748e-06, -6.1094e-04,  ...,  1.0432e-03,
          0.0000e+00, -7.8433e-04],
        [ 4.8063e-05,  4.9748e-06, -6.1094e-04,  ...,  1.0432e-03,
          0.0000e+00, -7.8433e-04],
        ...,
        [ 4.8063e-05,  4.9748e-06, -6.1094e-04,  ...,  1.0432e-03,
          0.0000e+00, -7.8433e-04],
        [ 4.8063e-05,  4.9748e-06, -6.1094e-04,  ...,  1.0432e-03,
          0.0000e+00, -7.8433e-04],
        [ 4.8063e-05,  4.9748e-06, -6.1094e-04,  ...,  1.0432e-03,
          0.0000e+00, -7.8433e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2096.1514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8573, device='cuda:0')



h[100].sum tensor(87.7247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.4803, device='cuda:0')



h[200].sum tensor(24.8778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9879, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.9377e-04, 2.0056e-05, 0.0000e+00,  ..., 4.2055e-03, 0.0000e+00,
         0.0000e+00],
        [1.9468e-04, 2.0150e-05, 0.0000e+00,  ..., 4.2252e-03, 0.0000e+00,
         0.0000e+00],
        [1.9486e-04, 2.0169e-05, 0.0000e+00,  ..., 4.2292e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.9795e-04, 2.0489e-05, 0.0000e+00,  ..., 4.2963e-03, 0.0000e+00,
         0.0000e+00],
        [1.9795e-04, 2.0489e-05, 0.0000e+00,  ..., 4.2962e-03, 0.0000e+00,
         0.0000e+00],
        [1.9796e-04, 2.0489e-05, 0.0000e+00,  ..., 4.2964e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50057.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0568, 0.0000,  ..., 0.0000, 0.0097, 0.0000],
        [0.0007, 0.0570, 0.0000,  ..., 0.0000, 0.0098, 0.0000],
        [0.0007, 0.0572, 0.0000,  ..., 0.0000, 0.0099, 0.0000],
        ...,
        [0.0007, 0.0584, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0007, 0.0584, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0007, 0.0584, 0.0000,  ..., 0.0000, 0.0101, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431893.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2049.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(196.7785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4457.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(830.6439, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-485.8017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2214],
        [-2.2321],
        [-2.2281],
        ...,
        [-1.9965],
        [-1.9950],
        [-1.9943]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286035.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0124],
        [1.0131],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368304.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0125],
        [1.0132],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368314.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9284e-05,  2.7752e-05, -6.1094e-04,  ...,  1.0236e-03,
          0.0000e+00, -7.9350e-04],
        [ 5.9284e-05,  2.7752e-05, -6.1094e-04,  ...,  1.0236e-03,
          0.0000e+00, -7.9350e-04],
        [ 5.9284e-05,  2.7752e-05, -6.1094e-04,  ...,  1.0236e-03,
          0.0000e+00, -7.9350e-04],
        ...,
        [ 5.9284e-05,  2.7752e-05, -6.1094e-04,  ...,  1.0236e-03,
          0.0000e+00, -7.9350e-04],
        [ 5.9284e-05,  2.7752e-05, -6.1094e-04,  ...,  1.0236e-03,
          0.0000e+00, -7.9350e-04],
        [ 5.9284e-05,  2.7752e-05, -6.1094e-04,  ...,  1.0236e-03,
          0.0000e+00, -7.9350e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2081.1816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.6733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4932, device='cuda:0')



h[100].sum tensor(87.4032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.3917, device='cuda:0')



h[200].sum tensor(24.6848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0001, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0001, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0001, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0001, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0001, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0001, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49247.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0567, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0006, 0.0570, 0.0000,  ..., 0.0000, 0.0101, 0.0000],
        [0.0007, 0.0572, 0.0000,  ..., 0.0000, 0.0102, 0.0000],
        ...,
        [0.0007, 0.0583, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0007, 0.0583, 0.0000,  ..., 0.0000, 0.0105, 0.0000],
        [0.0007, 0.0584, 0.0000,  ..., 0.0000, 0.0105, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427802.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1939.1926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.1126, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4625.7959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(816.0275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-477.4414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9740],
        [-2.0877],
        [-2.1725],
        ...,
        [-2.0132],
        [-2.0090],
        [-2.0080]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301558.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0125],
        [1.0132],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368314.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0127],
        [1.0134],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368324.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.1085e-05,  5.5958e-05, -6.1094e-04,  ...,  9.9922e-04,
          0.0000e+00, -8.0612e-04],
        [ 8.1085e-05,  5.5958e-05, -6.1094e-04,  ...,  9.9922e-04,
          0.0000e+00, -8.0612e-04],
        [ 6.5358e-03,  5.0816e-03, -6.5395e-04,  ...,  1.1528e-02,
         -1.1940e-03,  3.3763e-03],
        ...,
        [ 8.1085e-05,  5.5958e-05, -6.1094e-04,  ...,  9.9922e-04,
          0.0000e+00, -8.0612e-04],
        [ 8.1085e-05,  5.5958e-05, -6.1094e-04,  ...,  9.9922e-04,
          0.0000e+00, -8.0612e-04],
        [ 8.1085e-05,  5.5958e-05, -6.1094e-04,  ...,  9.9922e-04,
          0.0000e+00, -8.0612e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2263.8809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0852, device='cuda:0')



h[100].sum tensor(88.0805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1513, device='cuda:0')



h[200].sum tensor(28.7673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0100, 0.0077, 0.0000,  ..., 0.0198, 0.0000, 0.0046],
        [0.0069, 0.0053, 0.0000,  ..., 0.0147, 0.0000, 0.0034],
        [0.0057, 0.0044, 0.0000,  ..., 0.0128, 0.0000, 0.0026],
        ...,
        [0.0003, 0.0002, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0002, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0002, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53283.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0277, 0.0714, 0.0610,  ..., 0.0025, 0.0360, 0.0000],
        [0.0200, 0.0675, 0.0405,  ..., 0.0002, 0.0287, 0.0000],
        [0.0170, 0.0660, 0.0325,  ..., 0.0003, 0.0259, 0.0000],
        ...,
        [0.0009, 0.0581, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0009, 0.0581, 0.0000,  ..., 0.0000, 0.0111, 0.0000],
        [0.0009, 0.0581, 0.0000,  ..., 0.0000, 0.0111, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442660.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2205.2446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(231.7536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4687.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(871.1778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-525.2162, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1734],
        [-0.4238],
        [-0.7208],
        ...,
        [-2.0125],
        [-2.0084],
        [-2.0074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281691.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0127],
        [1.0134],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368324.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0128],
        [1.0135],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368334.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.0414e-03,  3.9285e-03, -6.4383e-04,  ...,  9.0313e-03,
         -9.0922e-04,  2.3844e-03],
        [ 5.0414e-03,  3.9285e-03, -6.4383e-04,  ...,  9.0313e-03,
         -9.0922e-04,  2.3844e-03],
        [ 1.0589e-04,  8.5418e-05, -6.1094e-04,  ...,  9.8138e-04,
          0.0000e+00, -8.1352e-04],
        ...,
        [ 1.0589e-04,  8.5418e-05, -6.1094e-04,  ...,  9.8138e-04,
          0.0000e+00, -8.1352e-04],
        [ 1.0589e-04,  8.5418e-05, -6.1094e-04,  ...,  9.8138e-04,
          0.0000e+00, -8.1352e-04],
        [ 1.0589e-04,  8.5418e-05, -6.1094e-04,  ...,  9.8138e-04,
          0.0000e+00, -8.1352e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2616.6948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.4327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3903, device='cuda:0')



h[100].sum tensor(89.8843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0325, device='cuda:0')



h[200].sum tensor(36.4319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4935, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0144, 0.0000,  ..., 0.0334, 0.0000, 0.0092],
        [0.0140, 0.0109, 0.0000,  ..., 0.0262, 0.0000, 0.0063],
        [0.0095, 0.0074, 0.0000,  ..., 0.0188, 0.0000, 0.0042],
        ...,
        [0.0004, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63872.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0409, 0.0787, 0.0974,  ..., 0.0070, 0.0481, 0.0000],
        [0.0362, 0.0758, 0.0839,  ..., 0.0051, 0.0445, 0.0000],
        [0.0263, 0.0705, 0.0575,  ..., 0.0028, 0.0355, 0.0000],
        ...,
        [0.0011, 0.0579, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0011, 0.0579, 0.0000,  ..., 0.0000, 0.0118, 0.0000],
        [0.0011, 0.0579, 0.0000,  ..., 0.0000, 0.0118, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495661.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3003.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.5028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4639.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1013.5560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-641.1404, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0476],
        [-0.1082],
        [-0.3482],
        ...,
        [-2.0045],
        [-2.0006],
        [-1.9998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260428.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0128],
        [1.0135],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368334.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0129],
        [1.0136],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368343.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2115.6831, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.3228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2505, device='cuda:0')



h[100].sum tensor(86.9115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.6662, device='cuda:0')



h[200].sum tensor(26.4578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0004, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50929.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0565, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0013, 0.0566, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0011, 0.0567, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        ...,
        [0.0012, 0.0579, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0012, 0.0579, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0012, 0.0579, 0.0000,  ..., 0.0000, 0.0123, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434876.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2205.3235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.8422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4746.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(835.0552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.6494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3945],
        [-1.6661],
        [-1.8713],
        ...,
        [-1.9903],
        [-1.9864],
        [-1.9852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243012.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0129],
        [1.0136],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368343.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0129],
        [1.0136],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368352.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2273.8101, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6403, device='cuda:0')



h[100].sum tensor(88.3054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.8212, device='cuda:0')



h[200].sum tensor(30.0097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0752, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53586.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0569, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0027, 0.0578, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0065, 0.0600, 0.0081,  ..., 0.0000, 0.0179, 0.0000],
        ...,
        [0.0010, 0.0584, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0010, 0.0584, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0010, 0.0584, 0.0000,  ..., 0.0000, 0.0125, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(443095.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2307.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(233.6611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4541.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(872.0507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-537.3976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4649],
        [-1.1507],
        [-0.7819],
        ...,
        [-2.0063],
        [-2.0026],
        [-2.0018]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226629.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0129],
        [1.0136],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368352.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0130],
        [1.0137],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368362., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.7021e-05,  1.3086e-04, -6.1094e-04,  ...,  1.0136e-03,
          0.0000e+00, -7.9434e-04],
        [ 9.7021e-05,  1.3086e-04, -6.1094e-04,  ...,  1.0136e-03,
          0.0000e+00, -7.9434e-04],
        [ 9.7021e-05,  1.3086e-04, -6.1094e-04,  ...,  1.0136e-03,
          0.0000e+00, -7.9434e-04],
        ...,
        [ 9.7021e-05,  1.3086e-04, -6.1094e-04,  ...,  1.0136e-03,
          0.0000e+00, -7.9434e-04],
        [ 9.7021e-05,  1.3086e-04, -6.1094e-04,  ...,  1.0136e-03,
          0.0000e+00, -7.9434e-04],
        [ 9.7021e-05,  1.3086e-04, -6.1094e-04,  ...,  1.0136e-03,
          0.0000e+00, -7.9434e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3318.9287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(41.0776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.9856, device='cuda:0')



h[100].sum tensor(95.2710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(56.7607, device='cuda:0')



h[200].sum tensor(51.6471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.1176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81483.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0047, 0.0602, 0.0072,  ..., 0.0000, 0.0156, 0.0000],
        [0.0013, 0.0583, 0.0000,  ..., 0.0000, 0.0127, 0.0000],
        [0.0034, 0.0597, 0.0031,  ..., 0.0000, 0.0146, 0.0000],
        ...,
        [0.0005, 0.0592, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0005, 0.0592, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0005, 0.0592, 0.0000,  ..., 0.0000, 0.0124, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570373.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3969.5708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(472.8437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4124.2275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1255.7069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-840.8568, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8237],
        [-1.1759],
        [-1.3467],
        ...,
        [-2.0244],
        [-2.0206],
        [-2.0198]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223614.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0130],
        [1.0137],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368362., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(523.4196, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0131],
        [1.0138],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368370.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.2592e-03,  6.5101e-03, -6.6557e-04,  ...,  1.4415e-02,
         -1.4858e-03,  4.5264e-03],
        [ 6.0174e-05,  1.2305e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.8484e-04],
        [ 6.0174e-05,  1.2305e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.8484e-04],
        ...,
        [ 6.0174e-05,  1.2305e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.8484e-04],
        [ 6.0174e-05,  1.2305e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.8484e-04],
        [ 6.0174e-05,  1.2305e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.8484e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2642.9504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9194, device='cuda:0')



h[100].sum tensor(92.1743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.6247, device='cuda:0')



h[200].sum tensor(37.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0211, 0.0168, 0.0000,  ..., 0.0383, 0.0000, 0.0120],
        [0.0085, 0.0070, 0.0000,  ..., 0.0177, 0.0000, 0.0046],
        [0.0002, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63341.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.8562e-02, 9.5446e-02, 1.5506e-01,  ..., 1.7626e-02, 6.0901e-02,
         0.0000e+00],
        [2.6072e-02, 7.5336e-02, 6.3239e-02,  ..., 4.9081e-03, 3.3879e-02,
         0.0000e+00],
        [7.2871e-03, 6.3673e-02, 1.5302e-02,  ..., 0.0000e+00, 1.7930e-02,
         0.0000e+00],
        ...,
        [1.4154e-04, 6.0349e-02, 0.0000e+00,  ..., 0.0000e+00, 1.2094e-02,
         0.0000e+00],
        [1.4161e-04, 6.0350e-02, 0.0000e+00,  ..., 0.0000e+00, 1.2095e-02,
         0.0000e+00],
        [1.4168e-04, 6.0355e-02, 0.0000e+00,  ..., 0.0000e+00, 1.2096e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(490424.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2617.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.4099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4188.8545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(999.7084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-641.8726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2099],
        [-0.5162],
        [-0.8656],
        ...,
        [-2.0563],
        [-2.0522],
        [-2.0513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289251., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0131],
        [1.0138],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368370.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0132],
        [1.0138],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368378.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3221e-05,  1.1770e-04, -6.1094e-04,  ...,  1.0594e-03,
          0.0000e+00, -7.8002e-04],
        [ 3.3221e-05,  1.1770e-04, -6.1094e-04,  ...,  1.0594e-03,
          0.0000e+00, -7.8002e-04],
        [ 7.3475e-03,  5.8161e-03, -6.5967e-04,  ...,  1.2993e-02,
         -1.3200e-03,  3.9570e-03],
        ...,
        [ 3.3221e-05,  1.1770e-04, -6.1094e-04,  ...,  1.0594e-03,
          0.0000e+00, -7.8002e-04],
        [ 3.3221e-05,  1.1770e-04, -6.1094e-04,  ...,  1.0594e-03,
          0.0000e+00, -7.8002e-04],
        [ 3.3221e-05,  1.1770e-04, -6.1094e-04,  ...,  1.0594e-03,
          0.0000e+00, -7.8002e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2928.2798, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.9262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8189, device='cuda:0')



h[100].sum tensor(94.5886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.2932, device='cuda:0')



h[200].sum tensor(43.3200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0076, 0.0063, 0.0000,  ..., 0.0164, 0.0000, 0.0040],
        [0.0062, 0.0052, 0.0000,  ..., 0.0142, 0.0000, 0.0031],
        ...,
        [0.0001, 0.0005, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0001, 0.0005, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68586.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0618, 0.0054,  ..., 0.0000, 0.0149, 0.0000],
        [0.0123, 0.0675, 0.0245,  ..., 0.0000, 0.0231, 0.0000],
        [0.0188, 0.0712, 0.0413,  ..., 0.0000, 0.0297, 0.0000],
        ...,
        [0.0000, 0.0612, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0000, 0.0612, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0000, 0.0612, 0.0000,  ..., 0.0000, 0.0119, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509230.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2895.9482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(352.0737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3753.1575, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1075.4965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-701.8680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5570],
        [-1.1167],
        [-0.6568],
        ...,
        [-2.0747],
        [-2.0704],
        [-2.0695]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259513.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0132],
        [1.0138],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368378.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0118],
        [1.0134],
        [1.0139],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368387.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0360e-02,  8.1715e-03, -6.7978e-04,  ...,  1.7934e-02,
         -1.8569e-03,  5.9083e-03],
        [ 1.8315e-02,  1.4370e-02, -7.3277e-04,  ...,  3.0915e-02,
         -3.2866e-03,  1.1059e-02],
        [ 3.3045e-02,  2.5847e-02, -8.3090e-04,  ...,  5.4948e-02,
         -5.9337e-03,  2.0595e-02],
        ...,
        [ 2.6614e-05,  1.2077e-04, -6.1094e-04,  ...,  1.0753e-03,
          0.0000e+00, -7.8132e-04],
        [ 2.6614e-05,  1.2077e-04, -6.1094e-04,  ...,  1.0753e-03,
          0.0000e+00, -7.8132e-04],
        [ 2.6614e-05,  1.2077e-04, -6.1094e-04,  ...,  1.0753e-03,
          0.0000e+00, -7.8132e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2393.0618, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7959, device='cuda:0')



h[100].sum tensor(91.7902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.2761, device='cuda:0')



h[200].sum tensor(32.6540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[3.4068e-02, 2.6947e-02, 0.0000e+00,  ..., 5.9748e-02, 0.0000e+00,
         2.0411e-02],
        [8.0181e-02, 6.2877e-02, 0.0000e+00,  ..., 1.3500e-01, 0.0000e+00,
         4.8672e-02],
        [9.8626e-02, 7.7249e-02, 0.0000e+00,  ..., 1.6510e-01, 0.0000e+00,
         6.0611e-02],
        ...,
        [1.0973e-04, 4.9795e-04, 0.0000e+00,  ..., 4.4335e-03, 0.0000e+00,
         0.0000e+00],
        [1.0973e-04, 4.9794e-04, 0.0000e+00,  ..., 4.4334e-03, 0.0000e+00,
         0.0000e+00],
        [1.0974e-04, 4.9798e-04, 0.0000e+00,  ..., 4.4337e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56020.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0855, 0.1134, 0.2294,  ..., 0.0294, 0.0839, 0.0000],
        [0.1463, 0.1523, 0.4019,  ..., 0.0620, 0.1325, 0.0000],
        [0.1911, 0.1810, 0.5302,  ..., 0.0870, 0.1678, 0.0000],
        ...,
        [0.0073, 0.0664, 0.0153,  ..., 0.0000, 0.0184, 0.0000],
        [0.0000, 0.0617, 0.0000,  ..., 0.0000, 0.0119, 0.0000],
        [0.0000, 0.0617, 0.0000,  ..., 0.0000, 0.0119, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(454791.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2070.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(239.9381, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3938.1040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(899.6982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-566.6454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0578],
        [ 0.0841],
        [ 0.0864],
        ...,
        [-1.1329],
        [-1.6088],
        [-1.9078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291795.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0118],
        [1.0134],
        [1.0139],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368387.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0135],
        [1.0140],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368395.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.1782e-03,  7.2518e-03, -6.7183e-04,  ...,  1.6001e-02,
         -1.6358e-03,  5.1249e-03],
        [ 3.6901e-05,  1.2913e-04, -6.1094e-04,  ...,  1.0873e-03,
          0.0000e+00, -7.9039e-04],
        [ 9.1782e-03,  7.2518e-03, -6.7183e-04,  ...,  1.6001e-02,
         -1.6358e-03,  5.1249e-03],
        ...,
        [ 3.6901e-05,  1.2913e-04, -6.1094e-04,  ...,  1.0873e-03,
          0.0000e+00, -7.9039e-04],
        [ 3.6901e-05,  1.2913e-04, -6.1094e-04,  ...,  1.0873e-03,
          0.0000e+00, -7.9039e-04],
        [ 3.6901e-05,  1.2913e-04, -6.1094e-04,  ...,  1.0873e-03,
          0.0000e+00, -7.9039e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2561.1577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.3979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4591, device='cuda:0')



h[100].sum tensor(92.7211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.2484, device='cuda:0')



h[200].sum tensor(36.7126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0064, 0.0000,  ..., 0.0167, 0.0000, 0.0041],
        [0.0338, 0.0268, 0.0000,  ..., 0.0593, 0.0000, 0.0186],
        [0.0077, 0.0064, 0.0000,  ..., 0.0168, 0.0000, 0.0041],
        ...,
        [0.0002, 0.0005, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0002, 0.0005, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0002, 0.0005, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61291.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0205, 0.0726, 0.0441,  ..., 0.0006, 0.0295, 0.0000],
        [0.0390, 0.0840, 0.0935,  ..., 0.0036, 0.0461, 0.0000],
        [0.0328, 0.0799, 0.0762,  ..., 0.0015, 0.0415, 0.0000],
        ...,
        [0.0002, 0.0618, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0002, 0.0618, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0002, 0.0618, 0.0000,  ..., 0.0000, 0.0121, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482038.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2647.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.9838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3758.4138, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(974.0801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-626.8275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6302],
        [-0.2388],
        [-0.0195],
        ...,
        [-2.0991],
        [-2.0949],
        [-2.0939]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252857.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0135],
        [1.0140],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368395.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0136],
        [1.0140],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368404.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7443e-05,  1.4628e-04, -6.1094e-04,  ...,  1.0884e-03,
          0.0000e+00, -8.0569e-04],
        [ 5.6949e-03,  4.5312e-03, -6.4842e-04,  ...,  1.0268e-02,
         -1.0026e-03,  2.8334e-03],
        [ 5.4119e-03,  4.3107e-03, -6.4653e-04,  ...,  9.8065e-03,
         -9.5222e-04,  2.6504e-03],
        ...,
        [ 6.7443e-05,  1.4628e-04, -6.1094e-04,  ...,  1.0884e-03,
          0.0000e+00, -8.0569e-04],
        [ 6.7443e-05,  1.4628e-04, -6.1094e-04,  ...,  1.0884e-03,
          0.0000e+00, -8.0569e-04],
        [ 6.7443e-05,  1.4628e-04, -6.1094e-04,  ...,  1.0884e-03,
          0.0000e+00, -8.0569e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2248.5544, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4015, device='cuda:0')



h[100].sum tensor(90.3993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1074, device='cuda:0')



h[200].sum tensor(31.2542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0050, 0.0000,  ..., 0.0137, 0.0000, 0.0029],
        [0.0189, 0.0151, 0.0000,  ..., 0.0348, 0.0000, 0.0104],
        [0.0391, 0.0308, 0.0000,  ..., 0.0677, 0.0000, 0.0218],
        ...,
        [0.0003, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53186.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0239, 0.0728, 0.0503,  ..., 0.0015, 0.0332, 0.0000],
        [0.0449, 0.0858, 0.1049,  ..., 0.0068, 0.0512, 0.0000],
        [0.0629, 0.0969, 0.1526,  ..., 0.0133, 0.0666, 0.0000],
        ...,
        [0.0009, 0.0613, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0009, 0.0613, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        [0.0009, 0.0613, 0.0000,  ..., 0.0000, 0.0125, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(442737.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2317.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(211.6612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3867.6191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(862.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-539.2177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3228],
        [-0.0606],
        [ 0.0508],
        ...,
        [-2.0926],
        [-2.0885],
        [-2.0876]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258336.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0136],
        [1.0140],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368404.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0137],
        [1.0141],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368413., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6317e-05,  1.6013e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.1789e-04],
        [ 9.6317e-05,  1.6013e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.1789e-04],
        [ 9.6317e-05,  1.6013e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.1789e-04],
        ...,
        [ 9.6317e-05,  1.6013e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.1789e-04],
        [ 9.6317e-05,  1.6013e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.1789e-04],
        [ 9.6317e-05,  1.6013e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.1789e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2609.9580, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8151, device='cuda:0')



h[100].sum tensor(92.2970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.3129, device='cuda:0')



h[200].sum tensor(39.2684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59631.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0592, 0.0000,  ..., 0.0000, 0.0123, 0.0000],
        [0.0015, 0.0595, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0016, 0.0597, 0.0000,  ..., 0.0000, 0.0125, 0.0000],
        ...,
        [0.0016, 0.0610, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0016, 0.0610, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0016, 0.0610, 0.0000,  ..., 0.0000, 0.0128, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461799.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2816.4153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(268.2846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3866.7102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(952.2503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.9682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7657],
        [-1.9919],
        [-2.1404],
        ...,
        [-2.0893],
        [-2.0854],
        [-2.0846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235786.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0137],
        [1.0141],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368413., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0138],
        [1.0141],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368421.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2913.6492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.3317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4451, device='cuda:0')



h[100].sum tensor(94.3195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.1756, device='cuda:0')



h[200].sum tensor(45.6047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69996.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0125, 0.0650, 0.0188,  ..., 0.0000, 0.0217, 0.0000],
        [0.0105, 0.0641, 0.0111,  ..., 0.0000, 0.0200, 0.0000],
        [0.0309, 0.0766, 0.0692,  ..., 0.0075, 0.0365, 0.0000],
        ...,
        [0.0021, 0.0607, 0.0000,  ..., 0.0000, 0.0130, 0.0000],
        [0.0021, 0.0607, 0.0000,  ..., 0.0000, 0.0130, 0.0000],
        [0.0021, 0.0607, 0.0000,  ..., 0.0000, 0.0130, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(520421.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3844.4917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.7527, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3714.8403, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1099.1981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-722.2543, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1225],
        [-0.0844],
        [-0.0031],
        ...,
        [-2.0942],
        [-2.0903],
        [-2.0896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210074.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0138],
        [1.0141],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368421.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0139],
        [1.0142],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368429.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0137,  0.0107, -0.0007,  ...,  0.0232, -0.0024,  0.0080],
        [ 0.0200,  0.0157, -0.0007,  ...,  0.0336, -0.0035,  0.0121],
        [ 0.0137,  0.0107, -0.0007,  ...,  0.0232, -0.0024,  0.0080],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2854.7051, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.7635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8667, device='cuda:0')



h[100].sum tensor(95.2006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(44.4464, device='cuda:0')



h[200].sum tensor(44.0024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0739, 0.0579, 0.0000,  ..., 0.1242, 0.0000, 0.0442],
        [0.0793, 0.0621, 0.0000,  ..., 0.1332, 0.0000, 0.0477],
        [0.1355, 0.1059, 0.0000,  ..., 0.2248, 0.0000, 0.0840],
        ...,
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66638.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1641, 0.1573, 0.4254,  ..., 0.0695, 0.1448, 0.0000],
        [0.1927, 0.1751, 0.5046,  ..., 0.0852, 0.1674, 0.0000],
        [0.2491, 0.2096, 0.6621,  ..., 0.1166, 0.2123, 0.0000],
        ...,
        [0.0020, 0.0611, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0020, 0.0611, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0044, 0.0624, 0.0015,  ..., 0.0000, 0.0153, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494607.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3392.1072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(322.2909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3784.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1053.8075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-685.3417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1356],
        [ 0.1392],
        [ 0.1406],
        ...,
        [-2.0477],
        [-1.9272],
        [-1.7292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238150.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0139],
        [1.0142],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368429.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0140],
        [1.0142],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368437.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.7429e-05,  1.6046e-04, -6.1094e-04,  ...,  1.1070e-03,
          0.0000e+00, -7.8632e-04],
        [ 9.7429e-05,  1.6046e-04, -6.1094e-04,  ...,  1.1070e-03,
          0.0000e+00, -7.8632e-04],
        [ 9.7429e-05,  1.6046e-04, -6.1094e-04,  ...,  1.1070e-03,
          0.0000e+00, -7.8632e-04],
        ...,
        [ 9.7429e-05,  1.6046e-04, -6.1094e-04,  ...,  1.1070e-03,
          0.0000e+00, -7.8632e-04],
        [ 9.7429e-05,  1.6046e-04, -6.1094e-04,  ...,  1.1070e-03,
          0.0000e+00, -7.8632e-04],
        [ 9.7429e-05,  1.6046e-04, -6.1094e-04,  ...,  1.1070e-03,
          0.0000e+00, -7.8632e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2607.1125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2741, device='cuda:0')



h[100].sum tensor(95.2492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.6955, device='cuda:0')



h[200].sum tensor(38.3807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61194.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0056, 0.0619, 0.0024,  ..., 0.0000, 0.0163, 0.0000],
        [0.0023, 0.0604, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0017, 0.0602, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        ...,
        [0.0017, 0.0615, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0017, 0.0615, 0.0000,  ..., 0.0000, 0.0132, 0.0000],
        [0.0018, 0.0615, 0.0000,  ..., 0.0000, 0.0132, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476094.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3057.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.6932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3704.5056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(981.8742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-628.4951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0587],
        [-1.4548],
        [-1.7581],
        ...,
        [-2.1174],
        [-2.1134],
        [-2.1124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237224.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0140],
        [1.0142],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368437.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0141],
        [1.0143],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368445.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9787e-02,  2.3281e-02, -8.0870e-04,  ...,  4.9625e-02,
         -5.1831e-03,  1.8455e-02],
        [ 3.2736e-02,  2.5578e-02, -8.2835e-04,  ...,  5.4445e-02,
         -5.6980e-03,  2.0365e-02],
        [ 2.0880e-02,  1.6344e-02, -7.4937e-04,  ...,  3.5068e-02,
         -3.6281e-03,  1.2687e-02],
        ...,
        [ 9.8459e-05,  1.5833e-04, -6.1094e-04,  ...,  1.1039e-03,
          0.0000e+00, -7.7114e-04],
        [ 9.8459e-05,  1.5833e-04, -6.1094e-04,  ...,  1.1039e-03,
          0.0000e+00, -7.7114e-04],
        [ 9.8459e-05,  1.5833e-04, -6.1094e-04,  ...,  1.1039e-03,
          0.0000e+00, -7.7114e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2190.5220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4239, device='cuda:0')



h[100].sum tensor(93.9808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.1845, device='cuda:0')



h[200].sum tensor(29.2383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9396, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1393, 0.1088, 0.0000,  ..., 0.2314, 0.0000, 0.0868],
        [0.1169, 0.0914, 0.0000,  ..., 0.1949, 0.0000, 0.0723],
        [0.0891, 0.0697, 0.0000,  ..., 0.1494, 0.0000, 0.0543],
        ...,
        [0.0004, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50537.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2474, 0.2098, 0.6684,  ..., 0.1235, 0.2112, 0.0000],
        [0.2262, 0.1970, 0.6077,  ..., 0.1108, 0.1946, 0.0000],
        [0.1886, 0.1741, 0.5012,  ..., 0.0887, 0.1650, 0.0000],
        ...,
        [0.0016, 0.0617, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0016, 0.0617, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0016, 0.0617, 0.0000,  ..., 0.0000, 0.0134, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(434236., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2321.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(172.8419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4028.3647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(833.5041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-510.8088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0082],
        [ 0.0093],
        [ 0.0161],
        ...,
        [-2.1422],
        [-2.1383],
        [-2.1375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268955.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0141],
        [1.0143],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368445.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(508.7515, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0142],
        [1.0144],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368453.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0036, -0.0006,  ...,  0.0084, -0.0008,  0.0021],
        [ 0.0046,  0.0036, -0.0006,  ...,  0.0084, -0.0008,  0.0021],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2356.8911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.8113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9182, device='cuda:0')



h[100].sum tensor(95.6673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.6520, device='cuda:0')



h[200].sum tensor(31.8260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1062, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0070, 0.0000,  ..., 0.0177, 0.0000, 0.0037],
        [0.0086, 0.0070, 0.0000,  ..., 0.0178, 0.0000, 0.0038],
        [0.0086, 0.0070, 0.0000,  ..., 0.0178, 0.0000, 0.0038],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53469.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0204, 0.0681, 0.0329,  ..., 0.0000, 0.0325, 0.0000],
        [0.0222, 0.0691, 0.0372,  ..., 0.0000, 0.0345, 0.0000],
        [0.0206, 0.0686, 0.0333,  ..., 0.0000, 0.0330, 0.0000],
        ...,
        [0.0015, 0.0618, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0015, 0.0618, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0015, 0.0618, 0.0000,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(444879.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2520.0168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(192.9716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4052.3755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(879.5474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-544.1870, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5470],
        [-0.4388],
        [-0.4680],
        ...,
        [-2.1601],
        [-2.1562],
        [-2.1554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234413.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0142],
        [1.0144],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368453.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0143],
        [1.0144],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368461.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0106,  0.0083, -0.0007,  ...,  0.0183, -0.0018,  0.0061],
        [ 0.0202,  0.0158, -0.0007,  ...,  0.0339, -0.0035,  0.0123],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2156.4902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.4627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0711, device='cuda:0')



h[100].sum tensor(95.0542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.1299, device='cuda:0')



h[200].sum tensor(26.8386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9002, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0111, 0.0088, 0.0000,  ..., 0.0216, 0.0000, 0.0061],
        [0.0376, 0.0295, 0.0000,  ..., 0.0651, 0.0000, 0.0226],
        [0.0921, 0.0720, 0.0000,  ..., 0.1543, 0.0000, 0.0563],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49670.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0398, 0.0833, 0.0920,  ..., 0.0132, 0.0444, 0.0000],
        [0.0912, 0.1149, 0.2352,  ..., 0.0394, 0.0861, 0.0000],
        [0.1638, 0.1593, 0.4383,  ..., 0.0808, 0.1446, 0.0000],
        ...,
        [0.0015, 0.0619, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0015, 0.0619, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0015, 0.0619, 0.0000,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(435421.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2264.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(162.3207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4480.5894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(824.9915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-496.8855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6093],
        [-0.2630],
        [-0.0795],
        ...,
        [-2.1832],
        [-2.1791],
        [-2.1782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276839.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0143],
        [1.0144],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368461.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0145],
        [1.0145],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368470.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0073,  0.0057, -0.0007,  ...,  0.0127, -0.0012,  0.0039],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0118,  0.0092, -0.0007,  ...,  0.0201, -0.0020,  0.0068],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2513.5388, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.3243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3710, device='cuda:0')



h[100].sum tensor(97.3133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.9955, device='cuda:0')



h[200].sum tensor(33.0991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0377, 0.0295, 0.0000,  ..., 0.0649, 0.0000, 0.0218],
        [0.0352, 0.0276, 0.0000,  ..., 0.0609, 0.0000, 0.0194],
        [0.0247, 0.0194, 0.0000,  ..., 0.0437, 0.0000, 0.0141],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0160, 0.0126, 0.0000,  ..., 0.0296, 0.0000, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56732.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1013, 0.1192, 0.2582,  ..., 0.0432, 0.0961, 0.0000],
        [0.0801, 0.1059, 0.1976,  ..., 0.0302, 0.0796, 0.0000],
        [0.0731, 0.1022, 0.1796,  ..., 0.0272, 0.0735, 0.0000],
        ...,
        [0.0035, 0.0630, 0.0019,  ..., 0.0000, 0.0151, 0.0000],
        [0.0120, 0.0681, 0.0217,  ..., 0.0009, 0.0221, 0.0000],
        [0.0372, 0.0834, 0.0842,  ..., 0.0113, 0.0427, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463301.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2683.3411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(223.4214, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4595.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(926.2237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-571.9901, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0141],
        [ 0.0258],
        [ 0.0283],
        ...,
        [-1.8918],
        [-1.5047],
        [-1.0075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271758.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0145],
        [1.0145],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368470.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0146],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368478.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0127,  0.0099, -0.0007,  ...,  0.0216, -0.0022,  0.0074],
        [ 0.0111,  0.0087, -0.0007,  ...,  0.0189, -0.0019,  0.0063],
        [ 0.0109,  0.0085, -0.0007,  ...,  0.0186, -0.0018,  0.0062],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2493.1113, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0178, device='cuda:0')



h[100].sum tensor(96.6838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9395, device='cuda:0')



h[200].sum tensor(32.0685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0465, 0.0362, 0.0000,  ..., 0.0791, 0.0000, 0.0266],
        [0.0613, 0.0478, 0.0000,  ..., 0.1034, 0.0000, 0.0362],
        [0.0586, 0.0457, 0.0000,  ..., 0.0989, 0.0000, 0.0344],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57565.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1086, 0.1226, 0.2773,  ..., 0.0473, 0.1025, 0.0000],
        [0.1360, 0.1399, 0.3541,  ..., 0.0632, 0.1244, 0.0000],
        [0.1396, 0.1425, 0.3645,  ..., 0.0653, 0.1273, 0.0000],
        ...,
        [0.0016, 0.0615, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0016, 0.0615, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0016, 0.0615, 0.0000,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469836.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2799.8408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(234.3593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4863.6802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(938.7598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-579.3435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0088],
        [ 0.0314],
        [ 0.0339],
        ...,
        [-2.0921],
        [-1.9991],
        [-1.9386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258117.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0146],
        [1.0146],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368478.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0147],
        [1.0147],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368486.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0044, -0.0006,  ...,  0.0099, -0.0009,  0.0028],
        [ 0.0057,  0.0045, -0.0006,  ...,  0.0101, -0.0010,  0.0028],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2292.8042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.2936, device='cuda:0')



h[100].sum tensor(95.1784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.7847, device='cuda:0')



h[200].sum tensor(27.4844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0390, 0.0304, 0.0000,  ..., 0.0668, 0.0000, 0.0218],
        [0.0193, 0.0151, 0.0000,  ..., 0.0345, 0.0000, 0.0105],
        [0.0063, 0.0049, 0.0000,  ..., 0.0132, 0.0000, 0.0029],
        ...,
        [0.0006, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52106.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0628, 0.0937, 0.1495,  ..., 0.0218, 0.0664, 0.0000],
        [0.0461, 0.0843, 0.1046,  ..., 0.0131, 0.0525, 0.0000],
        [0.0273, 0.0736, 0.0540,  ..., 0.0046, 0.0367, 0.0000],
        ...,
        [0.0015, 0.0614, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0015, 0.0614, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0015, 0.0614, 0.0000,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446368.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2373.7241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.9865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5184.9976, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(859.6029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-515.2447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0441],
        [ 0.0281],
        [-0.0146],
        ...,
        [-2.2331],
        [-2.2285],
        [-2.2267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287904.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0147],
        [1.0147],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368486.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0148],
        [1.0149],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368495.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2657.0635, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.1177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5468, device='cuda:0')



h[100].sum tensor(96.9729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5108, device='cuda:0')



h[200].sum tensor(34.2765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3994, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0064, 0.0000,  ..., 0.0162, 0.0000, 0.0033],
        [0.0044, 0.0035, 0.0000,  ..., 0.0100, 0.0000, 0.0017],
        [0.0007, 0.0005, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0005, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0005, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0005, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61062.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0160, 0.0654, 0.0226,  ..., 0.0000, 0.0283, 0.0000],
        [0.0117, 0.0640, 0.0126,  ..., 0.0000, 0.0240, 0.0000],
        [0.0067, 0.0623, 0.0043,  ..., 0.0000, 0.0189, 0.0000],
        ...,
        [0.0014, 0.0615, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0014, 0.0615, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0014, 0.0615, 0.0000,  ..., 0.0000, 0.0140, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487201.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2956.3596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.0989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5105.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(985.6730, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.5103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4065],
        [-1.5940],
        [-1.8066],
        ...,
        [-2.2500],
        [-2.2457],
        [-2.2448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266078.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0148],
        [1.0149],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368495.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0150],
        [1.0150],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368503.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2313.9707, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4003, device='cuda:0')



h[100].sum tensor(95.2430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1036, device='cuda:0')



h[200].sum tensor(26.6575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0004, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0006, 0.0004, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0006, 0.0004, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0005, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54961.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0603, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0009, 0.0606, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0009, 0.0608, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        ...,
        [0.0010, 0.0621, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0010, 0.0621, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0010, 0.0621, 0.0000,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466620.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2557.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(218.1926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5169.7642, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(902.7644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-546.2769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3296],
        [-2.4015],
        [-2.4537],
        ...,
        [-2.2626],
        [-2.2608],
        [-2.2613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275852.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0150],
        [1.0150],
        ...,
        [1.0011],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368503.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0151],
        [1.0151],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368510.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2243.3184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.5306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.7754, device='cuda:0')



h[100].sum tensor(94.9124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.2354, device='cuda:0')



h[200].sum tensor(24.9697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0049, 0.0000,  ..., 0.0133, 0.0000, 0.0029],
        [0.0005, 0.0004, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51820.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0257, 0.0752, 0.0570,  ..., 0.0085, 0.0353, 0.0000],
        [0.0098, 0.0662, 0.0138,  ..., 0.0007, 0.0219, 0.0000],
        [0.0070, 0.0648, 0.0077,  ..., 0.0000, 0.0196, 0.0000],
        ...,
        [0.0005, 0.0626, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0005, 0.0626, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0005, 0.0626, 0.0000,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(452104.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2149.6047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.3896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5195.4058, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(853.5591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-509.2947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4367],
        [-0.7497],
        [-0.9265],
        ...,
        [-2.2950],
        [-2.2906],
        [-2.2896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313576.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0151],
        [1.0151],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368510.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0152],
        [1.0152],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368518.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1445e-04,  9.8090e-05, -6.1094e-04,  ...,  9.7049e-04,
          0.0000e+00, -7.6727e-04],
        [ 1.1445e-04,  9.8090e-05, -6.1094e-04,  ...,  9.7049e-04,
          0.0000e+00, -7.6727e-04],
        [ 1.1445e-04,  9.8090e-05, -6.1094e-04,  ...,  9.7049e-04,
          0.0000e+00, -7.6727e-04],
        ...,
        [ 1.1445e-04,  9.8090e-05, -6.1094e-04,  ...,  9.7049e-04,
          0.0000e+00, -7.6727e-04],
        [ 1.1445e-04,  9.8090e-05, -6.1094e-04,  ...,  9.7049e-04,
          0.0000e+00, -7.6727e-04],
        [ 1.1445e-04,  9.8090e-05, -6.1094e-04,  ...,  9.7049e-04,
          0.0000e+00, -7.6727e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2511.2920, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.6623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2493, device='cuda:0')



h[100].sum tensor(96.2478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6317, device='cuda:0')



h[200].sum tensor(30.1269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0004, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56084.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0612, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0003, 0.0615, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0003, 0.0617, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        ...,
        [0.0003, 0.0630, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0003, 0.0630, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0003, 0.0630, 0.0000,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(462468.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2274.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.3037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4912.9409, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(914.7483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-559.2913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4481],
        [-2.4846],
        [-2.5027],
        ...,
        [-2.3080],
        [-2.3029],
        [-2.3015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278571.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0152],
        [1.0152],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368518.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0154],
        [1.0153],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368527.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0177,  0.0138, -0.0007,  ...,  0.0299, -0.0030,  0.0107],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2418.2251, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1727, device='cuda:0')



h[100].sum tensor(94.9851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.4129, device='cuda:0')



h[200].sum tensor(28.5813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0004, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0330, 0.0257, 0.0000,  ..., 0.0573, 0.0000, 0.0196],
        [0.0655, 0.0511, 0.0000,  ..., 0.1107, 0.0000, 0.0399],
        ...,
        [0.0004, 0.0004, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0004, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55134.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.0543e-02, 8.0040e-02, 7.6909e-02,  ..., 1.2703e-02, 3.9024e-02,
         0.0000e+00],
        [7.6492e-02, 1.0905e-01, 2.0426e-01,  ..., 3.8278e-02, 7.6964e-02,
         0.0000e+00],
        [1.2400e-01, 1.3881e-01, 3.3957e-01,  ..., 6.7113e-02, 1.1592e-01,
         0.0000e+00],
        ...,
        [1.9205e-04, 6.3092e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4264e-02,
         0.0000e+00],
        [1.9212e-04, 6.3091e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4264e-02,
         0.0000e+00],
        [1.9219e-04, 6.3095e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4265e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(462584.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2182.9094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.5762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4825.2729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(897.8256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-549.8019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2038],
        [-0.0959],
        [-0.0450],
        ...,
        [-2.3141],
        [-2.3098],
        [-2.3088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288967.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0154],
        [1.0153],
        ...,
        [1.0012],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368527.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(489.8296, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0155],
        [1.0154],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368535.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0040, -0.0006,  ...,  0.0092, -0.0008,  0.0025],
        [ 0.0062,  0.0049, -0.0007,  ...,  0.0110, -0.0010,  0.0032],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2778.1646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.6422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3830, device='cuda:0')



h[100].sum tensor(96.0718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0108, device='cuda:0')



h[200].sum tensor(36.1880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4927, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0408, 0.0319, 0.0000,  ..., 0.0702, 0.0000, 0.0231],
        [0.0191, 0.0150, 0.0000,  ..., 0.0347, 0.0000, 0.0106],
        [0.0127, 0.0100, 0.0000,  ..., 0.0241, 0.0000, 0.0064],
        ...,
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66004.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.3233e-02, 9.8240e-02, 1.6143e-01,  ..., 2.6423e-02, 6.9386e-02,
         0.0000e+00],
        [4.7168e-02, 8.8602e-02, 1.1702e-01,  ..., 1.7427e-02, 5.6055e-02,
         0.0000e+00],
        [3.5525e-02, 8.1724e-02, 8.5022e-02,  ..., 1.1075e-02, 4.6222e-02,
         0.0000e+00],
        ...,
        [1.3213e-04, 6.3031e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4742e-02,
         0.0000e+00],
        [1.3219e-04, 6.3030e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4743e-02,
         0.0000e+00],
        [1.3226e-04, 6.3033e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4743e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522351.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2980.0042, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.5943, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4521.4985, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1045.0961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-669.0161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0411],
        [ 0.0210],
        [-0.0145],
        ...,
        [-2.3098],
        [-2.3056],
        [-2.3047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281389.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0155],
        [1.0154],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368535.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0157],
        [1.0155],
        ...,
        [1.0014],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368544.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2885.2239, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.4167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0277, device='cuda:0')



h[100].sum tensor(95.5130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.9380, device='cuda:0')



h[200].sum tensor(38.8151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66814.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0611, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0007, 0.0616, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0022, 0.0625, 0.0014,  ..., 0.0000, 0.0168, 0.0000],
        ...,
        [0.0001, 0.0629, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0017, 0.0640, 0.0023,  ..., 0.0000, 0.0167, 0.0000],
        [0.0075, 0.0676, 0.0161,  ..., 0.0004, 0.0215, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513830.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2839.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(336.6727, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4479.9556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1054.3643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-680.4271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8436],
        [-1.7250],
        [-1.5438],
        ...,
        [-2.2131],
        [-2.0383],
        [-1.7102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276971.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0157],
        [1.0155],
        ...,
        [1.0014],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368544.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0159],
        [1.0157],
        ...,
        [1.0014],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368552.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0065,  0.0051, -0.0007,  ...,  0.0115, -0.0011,  0.0034],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2395.2686, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7656, device='cuda:0')



h[100].sum tensor(91.6234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.1958, device='cuda:0')



h[200].sum tensor(29.4432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0069, 0.0056, 0.0000,  ..., 0.0149, 0.0000, 0.0034],
        [0.0058, 0.0047, 0.0000,  ..., 0.0129, 0.0000, 0.0026],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56441.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0633, 0.0045,  ..., 0.0000, 0.0186, 0.0000],
        [0.0108, 0.0674, 0.0203,  ..., 0.0008, 0.0251, 0.0000],
        [0.0139, 0.0692, 0.0279,  ..., 0.0010, 0.0281, 0.0000],
        ...,
        [0.0001, 0.0628, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0001, 0.0628, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0001, 0.0628, 0.0000,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472081.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2329.7661, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.3826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4323.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(911.1714, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-571.6237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2849],
        [-1.4804],
        [-1.5831],
        ...,
        [-2.3085],
        [-2.3046],
        [-2.3037]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242477.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0159],
        [1.0157],
        ...,
        [1.0014],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368552.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0160],
        [1.0158],
        ...,
        [1.0015],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368560.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4486e-05,  1.3116e-04, -6.1094e-04,  ...,  1.0466e-03,
          0.0000e+00, -7.9920e-04],
        [ 9.4486e-05,  1.3116e-04, -6.1094e-04,  ...,  1.0466e-03,
          0.0000e+00, -7.9920e-04],
        [ 9.4486e-05,  1.3116e-04, -6.1094e-04,  ...,  1.0466e-03,
          0.0000e+00, -7.9920e-04],
        ...,
        [ 9.4486e-05,  1.3116e-04, -6.1094e-04,  ...,  1.0466e-03,
          0.0000e+00, -7.9920e-04],
        [ 9.4486e-05,  1.3116e-04, -6.1094e-04,  ...,  1.0466e-03,
          0.0000e+00, -7.9920e-04],
        [ 9.4486e-05,  1.3116e-04, -6.1094e-04,  ...,  1.0466e-03,
          0.0000e+00, -7.9920e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2923.1865, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.6885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.5937, device='cuda:0')



h[100].sum tensor(93.9103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.6302, device='cuda:0')



h[200].sum tensor(39.9172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66835.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0612, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0000, 0.0615, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0000, 0.0617, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        ...,
        [0.0148, 0.0725, 0.0329,  ..., 0.0023, 0.0284, 0.0000],
        [0.0140, 0.0721, 0.0308,  ..., 0.0019, 0.0277, 0.0000],
        [0.0106, 0.0698, 0.0233,  ..., 0.0009, 0.0246, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515980.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2881.7104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.9810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4134.5962, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1052.0924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.1204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2914],
        [-2.3559],
        [-2.3806],
        ...,
        [-0.7272],
        [-0.7419],
        [-0.8018]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236345.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0160],
        [1.0158],
        ...,
        [1.0015],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368560.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0162],
        [1.0159],
        ...,
        [1.0015],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368567.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2343e-05,  1.1716e-04, -6.1094e-04,  ...,  1.0516e-03,
          0.0000e+00, -8.0460e-04],
        [ 7.2343e-05,  1.1716e-04, -6.1094e-04,  ...,  1.0516e-03,
          0.0000e+00, -8.0460e-04],
        [ 7.2343e-05,  1.1716e-04, -6.1094e-04,  ...,  1.0516e-03,
          0.0000e+00, -8.0460e-04],
        ...,
        [ 7.2343e-05,  1.1716e-04, -6.1094e-04,  ...,  1.0516e-03,
          0.0000e+00, -8.0460e-04],
        [ 7.2343e-05,  1.1716e-04, -6.1094e-04,  ...,  1.0516e-03,
          0.0000e+00, -8.0460e-04],
        [ 8.1124e-03,  6.3715e-03, -6.6453e-04,  ...,  1.4226e-02,
         -1.3188e-03,  4.4111e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2497.2024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8842, device='cuda:0')



h[100].sum tensor(91.0173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.5402, device='cuda:0')



h[200].sum tensor(31.2110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0005, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0086, 0.0069, 0.0000,  ..., 0.0180, 0.0000, 0.0046],
        [0.0071, 0.0058, 0.0000,  ..., 0.0155, 0.0000, 0.0036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57841.5117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0050, 0.0648, 0.0095,  ..., 0.0000, 0.0193, 0.0000],
        [0.0021, 0.0634, 0.0025,  ..., 0.0000, 0.0169, 0.0000],
        [0.0048, 0.0651, 0.0088,  ..., 0.0000, 0.0196, 0.0000],
        ...,
        [0.0040, 0.0661, 0.0076,  ..., 0.0000, 0.0188, 0.0000],
        [0.0135, 0.0721, 0.0297,  ..., 0.0019, 0.0276, 0.0000],
        [0.0176, 0.0745, 0.0405,  ..., 0.0025, 0.0315, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(475477.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2163.3655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(266.7642, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4242.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(924.5245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-585.4441, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6417],
        [-0.8034],
        [-0.8458],
        ...,
        [-2.0118],
        [-1.7474],
        [-1.5352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271584., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0162],
        [1.0159],
        ...,
        [1.0015],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368567.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0163],
        [1.0160],
        ...,
        [1.0015],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368575.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9523e-05,  1.0273e-04, -6.1094e-04,  ...,  1.0521e-03,
          0.0000e+00, -8.0991e-04],
        [ 4.9523e-05,  1.0273e-04, -6.1094e-04,  ...,  1.0521e-03,
          0.0000e+00, -8.0991e-04],
        [ 4.9523e-05,  1.0273e-04, -6.1094e-04,  ...,  1.0521e-03,
          0.0000e+00, -8.0991e-04],
        ...,
        [ 4.9523e-05,  1.0273e-04, -6.1094e-04,  ...,  1.0521e-03,
          0.0000e+00, -8.0991e-04],
        [ 4.9523e-05,  1.0273e-04, -6.1094e-04,  ...,  1.0521e-03,
          0.0000e+00, -8.0991e-04],
        [ 4.9523e-05,  1.0273e-04, -6.1094e-04,  ...,  1.0521e-03,
          0.0000e+00, -8.0991e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2358.5171, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.8021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7558, device='cuda:0')



h[100].sum tensor(89.8264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.1667, device='cuda:0')



h[200].sum tensor(28.0617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0004, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0004, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0004, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0004, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0004, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0004, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54871.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0071, 0.0665, 0.0147,  ..., 0.0000, 0.0215, 0.0000],
        [0.0030, 0.0641, 0.0048,  ..., 0.0000, 0.0174, 0.0000],
        [0.0032, 0.0644, 0.0054,  ..., 0.0000, 0.0177, 0.0000],
        ...,
        [0.0000, 0.0638, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0000, 0.0638, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0000, 0.0638, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465985.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1960.6588, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.8271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4140.5889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(883.6310, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-554.7673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5182],
        [-0.7341],
        [-0.8562],
        ...,
        [-2.3912],
        [-2.3867],
        [-2.3857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265790.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0163],
        [1.0160],
        ...,
        [1.0015],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368575.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0165],
        [1.0162],
        ...,
        [1.0016],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368582.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2313e-05,  8.5901e-05, -6.1094e-04,  ...,  1.0553e-03,
          0.0000e+00, -8.1465e-04],
        [ 2.2313e-05,  8.5901e-05, -6.1094e-04,  ...,  1.0553e-03,
          0.0000e+00, -8.1465e-04],
        [ 2.2313e-05,  8.5901e-05, -6.1094e-04,  ...,  1.0553e-03,
          0.0000e+00, -8.1465e-04],
        ...,
        [ 2.2313e-05,  8.5901e-05, -6.1094e-04,  ...,  1.0553e-03,
          0.0000e+00, -8.1465e-04],
        [ 2.2313e-05,  8.5901e-05, -6.1094e-04,  ...,  1.0553e-03,
          0.0000e+00, -8.1465e-04],
        [ 2.2313e-05,  8.5901e-05, -6.1094e-04,  ...,  1.0553e-03,
          0.0000e+00, -8.1465e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2153.2554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.6352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0877, device='cuda:0')



h[100].sum tensor(88.4974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.1794, device='cuda:0')



h[200].sum tensor(23.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9021, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.0216e-05, 3.4732e-04, 0.0000e+00,  ..., 4.2669e-03, 0.0000e+00,
         0.0000e+00],
        [9.0696e-05, 3.4916e-04, 0.0000e+00,  ..., 4.2896e-03, 0.0000e+00,
         0.0000e+00],
        [9.0793e-05, 3.4954e-04, 0.0000e+00,  ..., 4.2942e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [9.2266e-05, 3.5521e-04, 0.0000e+00,  ..., 4.3638e-03, 0.0000e+00,
         0.0000e+00],
        [9.2265e-05, 3.5521e-04, 0.0000e+00,  ..., 4.3638e-03, 0.0000e+00,
         0.0000e+00],
        [9.2269e-05, 3.5522e-04, 0.0000e+00,  ..., 4.3640e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50713.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0623, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0000, 0.0627, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0000, 0.0629, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        ...,
        [0.0000, 0.0642, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0000, 0.0642, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0000, 0.0642, 0.0000,  ..., 0.0000, 0.0140, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(454606.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1614.9996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(209.7151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4222.6079, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(823.3449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-508.9943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3719],
        [-2.2290],
        [-1.9955],
        ...,
        [-2.4279],
        [-2.4233],
        [-2.4221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302110.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0165],
        [1.0162],
        ...,
        [1.0016],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368582.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0166],
        [1.0163],
        ...,
        [1.0016],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368590.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.2517e-02,  2.5365e-02, -8.2754e-04,  ...,  5.4309e-02,
         -5.2630e-03,  2.0249e-02],
        [ 2.5595e-02,  1.9981e-02, -7.8142e-04,  ...,  4.2970e-02,
         -4.1423e-03,  1.5763e-02],
        [ 2.1778e-02,  1.7012e-02, -7.5598e-04,  ...,  3.6717e-02,
         -3.5243e-03,  1.3289e-02],
        ...,
        [ 1.0233e-05,  7.8683e-05, -6.1094e-04,  ...,  1.0592e-03,
          0.0000e+00, -8.1824e-04],
        [ 1.0233e-05,  7.8683e-05, -6.1094e-04,  ...,  1.0592e-03,
          0.0000e+00, -8.1824e-04],
        [ 1.0233e-05,  7.8683e-05, -6.1094e-04,  ...,  1.0592e-03,
          0.0000e+00, -8.1824e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2623.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5312, device='cuda:0')



h[100].sum tensor(91.0368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.4641, device='cuda:0')



h[200].sum tensor(32.7681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.6867e-02, 7.5639e-02, 0.0000e+00,  ..., 1.6290e-01, 0.0000e+00,
         5.9443e-02],
        [1.2186e-01, 9.5080e-02, 0.0000e+00,  ..., 2.0386e-01, 0.0000e+00,
         7.5622e-02],
        [1.2859e-01, 1.0032e-01, 0.0000e+00,  ..., 2.1488e-01, 0.0000e+00,
         7.9979e-02],
        ...,
        [4.2321e-05, 3.2540e-04, 0.0000e+00,  ..., 4.3806e-03, 0.0000e+00,
         0.0000e+00],
        [4.2321e-05, 3.2540e-04, 0.0000e+00,  ..., 4.3805e-03, 0.0000e+00,
         0.0000e+00],
        [4.2323e-05, 3.2541e-04, 0.0000e+00,  ..., 4.3808e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58474.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2147, 0.2042, 0.6033,  ..., 0.1213, 0.1918, 0.0000],
        [0.2571, 0.2325, 0.7262,  ..., 0.1482, 0.2259, 0.0000],
        [0.2743, 0.2439, 0.7762,  ..., 0.1593, 0.2397, 0.0000],
        ...,
        [0.0000, 0.0646, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0000, 0.0646, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0000, 0.0646, 0.0000,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477601.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1853.1990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(282.4037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4222.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(927.4807, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.2476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0256],
        [-0.0715],
        [-0.1151],
        ...,
        [-2.4544],
        [-2.4496],
        [-2.4484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343045.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0166],
        [1.0163],
        ...,
        [1.0016],
        [1.0006],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368590.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0168],
        [1.0164],
        ...,
        [1.0016],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368598.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6295e-02,  2.0529e-02, -7.8609e-04,  ...,  4.4129e-02,
         -4.2377e-03,  1.6215e-02],
        [ 1.7685e-02,  1.3832e-02, -7.2873e-04,  ...,  3.0026e-02,
         -2.8499e-03,  1.0637e-02],
        [ 3.5796e-02,  2.7921e-02, -8.4939e-04,  ...,  5.9692e-02,
         -5.7694e-03,  2.2371e-02],
        ...,
        [ 6.6221e-06,  7.9310e-05, -6.1094e-04,  ...,  1.0685e-03,
          0.0000e+00, -8.1697e-04],
        [ 6.6221e-06,  7.9310e-05, -6.1094e-04,  ...,  1.0685e-03,
          0.0000e+00, -8.1697e-04],
        [ 6.6221e-06,  7.9310e-05, -6.1094e-04,  ...,  1.0685e-03,
          0.0000e+00, -8.1697e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2839.1943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4679, device='cuda:0')



h[100].sum tensor(92.3666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.2543, device='cuda:0')



h[200].sum tensor(37.2300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.6412e-02, 5.9743e-02, 0.0000e+00,  ..., 1.2944e-01, 0.0000e+00,
         4.6187e-02],
        [1.1376e-01, 8.8794e-02, 0.0000e+00,  ..., 1.9063e-01, 0.0000e+00,
         7.0364e-02],
        [6.2257e-02, 4.8733e-02, 0.0000e+00,  ..., 1.0628e-01, 0.0000e+00,
         3.6994e-02],
        ...,
        [2.7389e-05, 3.2803e-04, 0.0000e+00,  ..., 4.4194e-03, 0.0000e+00,
         0.0000e+00],
        [2.7389e-05, 3.2803e-04, 0.0000e+00,  ..., 4.4194e-03, 0.0000e+00,
         0.0000e+00],
        [2.7390e-05, 3.2804e-04, 0.0000e+00,  ..., 4.4196e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65150.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1497, 0.1621, 0.4127,  ..., 0.0791, 0.1395, 0.0000],
        [0.1810, 0.1831, 0.5023,  ..., 0.0983, 0.1651, 0.0000],
        [0.1434, 0.1585, 0.3945,  ..., 0.0750, 0.1348, 0.0000],
        ...,
        [0.0000, 0.0650, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0000, 0.0650, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0000, 0.0650, 0.0000,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508368.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2436.0835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.1841, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3884.3044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1025.0577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.3533, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0943],
        [ 0.0987],
        [ 0.0714],
        ...,
        [-2.4589],
        [-2.4536],
        [-2.4517]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281440., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0168],
        [1.0164],
        ...,
        [1.0016],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368598.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0148],
        [1.0169],
        [1.0165],
        ...,
        [1.0016],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368606.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8009e-02,  1.4081e-02, -7.3072e-04,  ...,  3.0522e-02,
         -2.8857e-03,  1.0823e-02],
        [ 2.8231e-02,  2.2033e-02, -7.9881e-04,  ...,  4.7263e-02,
         -4.5262e-03,  1.7443e-02],
        [ 3.2082e-02,  2.5029e-02, -8.2447e-04,  ...,  5.3570e-02,
         -5.1443e-03,  1.9937e-02],
        ...,
        [ 2.8994e-05,  9.3406e-05, -6.1094e-04,  ...,  1.0741e-03,
          0.0000e+00, -8.2219e-04],
        [ 2.8994e-05,  9.3406e-05, -6.1094e-04,  ...,  1.0741e-03,
          0.0000e+00, -8.2219e-04],
        [ 2.8994e-05,  9.3406e-05, -6.1094e-04,  ...,  1.0741e-03,
          0.0000e+00, -8.2219e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2408.9473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4725, device='cuda:0')



h[100].sum tensor(89.6326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.3092, device='cuda:0')



h[200].sum tensor(29.4256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.6821e-02, 6.7828e-02, 0.0000e+00,  ..., 1.4635e-01, 0.0000e+00,
         5.2829e-02],
        [9.0270e-02, 7.0513e-02, 0.0000e+00,  ..., 1.5202e-01, 0.0000e+00,
         5.5045e-02],
        [1.0202e-01, 7.9656e-02, 0.0000e+00,  ..., 1.7127e-01, 0.0000e+00,
         6.2653e-02],
        ...,
        [1.1994e-04, 3.8638e-04, 0.0000e+00,  ..., 4.4430e-03, 0.0000e+00,
         0.0000e+00],
        [1.1994e-04, 3.8638e-04, 0.0000e+00,  ..., 4.4430e-03, 0.0000e+00,
         0.0000e+00],
        [1.1994e-04, 3.8640e-04, 0.0000e+00,  ..., 4.4432e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54452.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1927, 0.1904, 0.5312,  ..., 0.1040, 0.1731, 0.0000],
        [0.1887, 0.1881, 0.5194,  ..., 0.1013, 0.1700, 0.0000],
        [0.1791, 0.1820, 0.4934,  ..., 0.0962, 0.1620, 0.0000],
        ...,
        [0.0000, 0.0651, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0000, 0.0651, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0000, 0.0651, 0.0000,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463471.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1901.8171, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.6630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4141.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(873.8832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-552.3494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0667],
        [ 0.0650],
        [ 0.0450],
        ...,
        [-2.4641],
        [-2.4596],
        [-2.4592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310259.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0148],
        [1.0169],
        [1.0165],
        ...,
        [1.0016],
        [1.0007],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368606.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(516.1053, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0170],
        [1.0166],
        ...,
        [1.0017],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368615.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1406e-05,  1.1765e-04, -6.1094e-04,  ...,  1.0836e-03,
          0.0000e+00, -8.2524e-04],
        [ 6.1406e-05,  1.1765e-04, -6.1094e-04,  ...,  1.0836e-03,
          0.0000e+00, -8.2524e-04],
        [ 6.1406e-05,  1.1765e-04, -6.1094e-04,  ...,  1.0836e-03,
          0.0000e+00, -8.2524e-04],
        ...,
        [ 6.1406e-05,  1.1765e-04, -6.1094e-04,  ...,  1.0836e-03,
          0.0000e+00, -8.2524e-04],
        [ 6.1406e-05,  1.1765e-04, -6.1094e-04,  ...,  1.0836e-03,
          0.0000e+00, -8.2524e-04],
        [ 6.1406e-05,  1.1765e-04, -6.1094e-04,  ...,  1.0836e-03,
          0.0000e+00, -8.2524e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2355.5854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.0519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6839, device='cuda:0')



h[100].sum tensor(89.0528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.9517, device='cuda:0')



h[200].sum tensor(29.3553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0801, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0005, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0005, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0005, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0005, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54252.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0636, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0084, 0.0690, 0.0160,  ..., 0.0000, 0.0209, 0.0000],
        [0.0196, 0.0764, 0.0449,  ..., 0.0034, 0.0304, 0.0000],
        ...,
        [0.0001, 0.0652, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        [0.0001, 0.0652, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        [0.0001, 0.0652, 0.0000,  ..., 0.0000, 0.0136, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(462354.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2117.0986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.0156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4139.2007, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(870.4473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-551.4844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3675],
        [-0.9037],
        [-0.4446],
        ...,
        [-2.4510],
        [-2.4462],
        [-2.4408]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303284.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0170],
        [1.0166],
        ...,
        [1.0017],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368615.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0172],
        [1.0167],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368623.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6715e-05,  1.3817e-04, -6.1094e-04,  ...,  1.0930e-03,
          0.0000e+00, -8.2242e-04],
        [ 8.6715e-05,  1.3817e-04, -6.1094e-04,  ...,  1.0930e-03,
          0.0000e+00, -8.2242e-04],
        [ 8.6715e-05,  1.3817e-04, -6.1094e-04,  ...,  1.0930e-03,
          0.0000e+00, -8.2242e-04],
        ...,
        [ 8.6715e-05,  1.3817e-04, -6.1094e-04,  ...,  1.0930e-03,
          0.0000e+00, -8.2242e-04],
        [ 8.6715e-05,  1.3817e-04, -6.1094e-04,  ...,  1.0930e-03,
          0.0000e+00, -8.2242e-04],
        [ 8.6715e-05,  1.3817e-04, -6.1094e-04,  ...,  1.0930e-03,
          0.0000e+00, -8.2242e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2224.7859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.2484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2898, device='cuda:0')



h[100].sum tensor(88.5285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.7837, device='cuda:0')



h[200].sum tensor(27.5770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0051, 0.0000,  ..., 0.0139, 0.0000, 0.0029],
        [0.0004, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0065, 0.0054, 0.0000,  ..., 0.0146, 0.0000, 0.0032],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51420.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1185e-02, 6.9892e-02, 1.7135e-02,  ..., 0.0000e+00, 2.3396e-02,
         0.0000e+00],
        [6.8623e-03, 6.7701e-02, 6.0427e-03,  ..., 0.0000e+00, 1.9360e-02,
         0.0000e+00],
        [1.1963e-02, 7.0987e-02, 1.9173e-02,  ..., 1.9230e-05, 2.4117e-02,
         0.0000e+00],
        ...,
        [5.3364e-04, 6.5435e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3696e-02,
         0.0000e+00],
        [5.3375e-04, 6.5437e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3697e-02,
         0.0000e+00],
        [5.3380e-04, 6.5439e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3698e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(453915.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2184.8228, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(206.4718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4149.0732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(831.1251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-523.2939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7716],
        [-1.9381],
        [-1.9591],
        ...,
        [-2.4513],
        [-2.4470],
        [-2.4459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292342.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0172],
        [1.0167],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368623.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0174],
        [1.0168],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368632.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2681.4292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4532, device='cuda:0')



h[100].sum tensor(91.3478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.2309, device='cuda:0')



h[200].sum tensor(36.9186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60243., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0641, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0019, 0.0647, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0046, 0.0663, 0.0018,  ..., 0.0000, 0.0173, 0.0000],
        ...,
        [0.0010, 0.0658, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0010, 0.0658, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0010, 0.0658, 0.0000,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482542.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2807.7104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(275.1154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3948.7095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(956.4727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-622.3882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8746],
        [-1.6360],
        [-1.3376],
        ...,
        [-2.4524],
        [-2.4481],
        [-2.4471]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253534.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0174],
        [1.0168],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368632.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0174],
        [1.0168],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368632.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0051, -0.0007,  ...,  0.0116, -0.0010,  0.0033],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0065,  0.0051, -0.0007,  ...,  0.0116, -0.0010,  0.0033],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3113.1060, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.0409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3061, device='cuda:0')



h[100].sum tensor(93.7452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.7497, device='cuda:0')



h[200].sum tensor(45.3509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0047, 0.0000,  ..., 0.0131, 0.0000, 0.0026],
        [0.0240, 0.0190, 0.0000,  ..., 0.0431, 0.0000, 0.0119],
        [0.0057, 0.0047, 0.0000,  ..., 0.0132, 0.0000, 0.0026],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71198.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0219, 0.0767, 0.0430,  ..., 0.0001, 0.0324, 0.0000],
        [0.0295, 0.0813, 0.0619,  ..., 0.0014, 0.0401, 0.0000],
        [0.0222, 0.0768, 0.0427,  ..., 0.0003, 0.0340, 0.0000],
        ...,
        [0.0010, 0.0658, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0010, 0.0658, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0010, 0.0658, 0.0000,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532019.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3552.3767, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(369.6101, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3733.8564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1108.8961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-740.5414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2903],
        [-0.3061],
        [-0.2532],
        ...,
        [-2.4568],
        [-2.4525],
        [-2.4515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241192.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0174],
        [1.0168],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368632.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0175],
        [1.0170],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368640.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6026e-05,  1.4615e-04, -6.1094e-04,  ...,  1.1090e-03,
          0.0000e+00, -8.1102e-04],
        [ 6.8203e-03,  5.3782e-03, -6.5572e-04,  ...,  1.2118e-02,
         -1.0604e-03,  3.5396e-03],
        [ 9.6026e-05,  1.4615e-04, -6.1094e-04,  ...,  1.1090e-03,
          0.0000e+00, -8.1102e-04],
        ...,
        [ 9.6026e-05,  1.4615e-04, -6.1094e-04,  ...,  1.1090e-03,
          0.0000e+00, -8.1102e-04],
        [ 9.6026e-05,  1.4615e-04, -6.1094e-04,  ...,  1.1090e-03,
          0.0000e+00, -8.1102e-04],
        [ 9.6026e-05,  1.4615e-04, -6.1094e-04,  ...,  1.1090e-03,
          0.0000e+00, -8.1102e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2393.6382, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.2329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7087, device='cuda:0')



h[100].sum tensor(90.9193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0257, device='cuda:0')



h[200].sum tensor(31.4701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0206, 0.0163, 0.0000,  ..., 0.0376, 0.0000, 0.0106],
        [0.0183, 0.0145, 0.0000,  ..., 0.0338, 0.0000, 0.0099],
        [0.0289, 0.0228, 0.0000,  ..., 0.0513, 0.0000, 0.0152],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54768.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0897, 0.0937,  ..., 0.0071, 0.0507, 0.0000],
        [0.0441, 0.0917, 0.0996,  ..., 0.0077, 0.0524, 0.0000],
        [0.0449, 0.0922, 0.1014,  ..., 0.0078, 0.0535, 0.0000],
        ...,
        [0.0010, 0.0665, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0010, 0.0665, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0010, 0.0665, 0.0000,  ..., 0.0000, 0.0131, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464676.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2532.5449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.0485, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4004.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(880.9101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-566.5754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2111],
        [-0.0817],
        [-0.1479],
        ...,
        [-2.4740],
        [-2.4697],
        [-2.4686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268597.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0175],
        [1.0170],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368640.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0155],
        [1.0177],
        [1.0171],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368647.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0816e-02,  1.6271e-02, -7.4901e-04,  ...,  3.5067e-02,
         -3.2558e-03,  1.2615e-02],
        [ 1.3755e-02,  1.0776e-02, -7.0199e-04,  ...,  2.3506e-02,
         -2.1470e-03,  8.0466e-03],
        [ 1.6074e-02,  1.2580e-02, -7.1743e-04,  ...,  2.7303e-02,
         -2.5111e-03,  9.5469e-03],
        ...,
        [ 8.0205e-05,  1.3571e-04, -6.1094e-04,  ...,  1.1197e-03,
          0.0000e+00, -7.9966e-04],
        [ 8.0205e-05,  1.3571e-04, -6.1094e-04,  ...,  1.1197e-03,
          0.0000e+00, -7.9966e-04],
        [ 8.0205e-05,  1.3571e-04, -6.1094e-04,  ...,  1.1197e-03,
          0.0000e+00, -7.9966e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2440.8804, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1722, device='cuda:0')



h[100].sum tensor(92.6835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.4116, device='cuda:0')



h[200].sum tensor(32.2406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0587, 0.0460, 0.0000,  ..., 0.1001, 0.0000, 0.0345],
        [0.0677, 0.0529, 0.0000,  ..., 0.1148, 0.0000, 0.0403],
        [0.0614, 0.0481, 0.0000,  ..., 0.1045, 0.0000, 0.0362],
        ...,
        [0.0003, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57663.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1281, 0.1493, 0.3304,  ..., 0.0553, 0.1191, 0.0000],
        [0.1283, 0.1497, 0.3312,  ..., 0.0556, 0.1194, 0.0000],
        [0.1180, 0.1430, 0.3026,  ..., 0.0496, 0.1111, 0.0000],
        ...,
        [0.0008, 0.0674, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0008, 0.0674, 0.0000,  ..., 0.0000, 0.0126, 0.0000],
        [0.0008, 0.0674, 0.0000,  ..., 0.0000, 0.0126, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485026.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2753.1284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.3239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3955.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(922.4006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-601.4313, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1465],
        [ 0.1484],
        [ 0.1500],
        ...,
        [-2.5059],
        [-2.5015],
        [-2.5004]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282656.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0155],
        [1.0177],
        [1.0171],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368647.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0178],
        [1.0171],
        ...,
        [1.0017],
        [1.0008],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368655.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9050e-05,  1.2796e-04, -6.1094e-04,  ...,  1.1245e-03,
          0.0000e+00, -7.8882e-04],
        [ 6.9050e-05,  1.2796e-04, -6.1094e-04,  ...,  1.1245e-03,
          0.0000e+00, -7.8882e-04],
        [ 6.9050e-05,  1.2796e-04, -6.1094e-04,  ...,  1.1245e-03,
          0.0000e+00, -7.8882e-04],
        ...,
        [ 6.9050e-05,  1.2796e-04, -6.1094e-04,  ...,  1.1245e-03,
          0.0000e+00, -7.8882e-04],
        [ 6.9050e-05,  1.2796e-04, -6.1094e-04,  ...,  1.1245e-03,
          0.0000e+00, -7.8882e-04],
        [ 6.9050e-05,  1.2796e-04, -6.1094e-04,  ...,  1.1245e-03,
          0.0000e+00, -7.8882e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2314.8796, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9627, device='cuda:0')



h[100].sum tensor(93.2493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7954, device='cuda:0')



h[200].sum tensor(29.5936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0041, 0.0000,  ..., 0.0122, 0.0000, 0.0022],
        [0.0003, 0.0005, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0005, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52878.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0248, 0.0814, 0.0500,  ..., 0.0014, 0.0340, 0.0000],
        [0.0133, 0.0745, 0.0233,  ..., 0.0000, 0.0235, 0.0000],
        [0.0051, 0.0695, 0.0051,  ..., 0.0000, 0.0159, 0.0000],
        ...,
        [0.0006, 0.0681, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0006, 0.0681, 0.0000,  ..., 0.0000, 0.0122, 0.0000],
        [0.0006, 0.0681, 0.0000,  ..., 0.0000, 0.0122, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461368.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2321.2690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(190.5035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4139.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(858.0817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-552.4220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2809],
        [-0.6875],
        [-1.1311],
        ...,
        [-2.5330],
        [-2.5283],
        [-2.5271]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309677.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0178],
        [1.0171],
        ...,
        [1.0017],
        [1.0008],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368655.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0157],
        [1.0179],
        [1.0172],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368664.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6064e-05,  1.2804e-04, -6.1094e-04,  ...,  1.1252e-03,
          0.0000e+00, -7.7860e-04],
        [ 6.6064e-05,  1.2804e-04, -6.1094e-04,  ...,  1.1252e-03,
          0.0000e+00, -7.7860e-04],
        [ 6.6064e-05,  1.2804e-04, -6.1094e-04,  ...,  1.1252e-03,
          0.0000e+00, -7.7860e-04],
        ...,
        [ 6.6064e-05,  1.2804e-04, -6.1094e-04,  ...,  1.1252e-03,
          0.0000e+00, -7.7860e-04],
        [ 6.6064e-05,  1.2804e-04, -6.1094e-04,  ...,  1.1252e-03,
          0.0000e+00, -7.7860e-04],
        [ 6.6064e-05,  1.2804e-04, -6.1094e-04,  ...,  1.1252e-03,
          0.0000e+00, -7.7860e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3357.2048, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.7431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.5109, device='cuda:0')



h[100].sum tensor(100.0404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.3413, device='cuda:0')



h[200].sum tensor(49.7856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0297, 0.0234, 0.0000,  ..., 0.0527, 0.0000, 0.0175],
        [0.0003, 0.0005, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0005, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74259.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0863, 0.1235, 0.2206,  ..., 0.0358, 0.0823, 0.0000],
        [0.0270, 0.0845, 0.0633,  ..., 0.0078, 0.0335, 0.0000],
        [0.0076, 0.0718, 0.0103,  ..., 0.0000, 0.0176, 0.0000],
        ...,
        [0.0006, 0.0686, 0.0000,  ..., 0.0000, 0.0120, 0.0000],
        [0.0006, 0.0686, 0.0000,  ..., 0.0000, 0.0120, 0.0000],
        [0.0006, 0.0686, 0.0000,  ..., 0.0000, 0.0120, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541102.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3486.9998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(371.1183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3906.3296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1155.1854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-783.6271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0724],
        [-0.2585],
        [-0.4552],
        ...,
        [-2.5497],
        [-2.5450],
        [-2.5438]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292319.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0157],
        [1.0179],
        [1.0172],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368664.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0180],
        [1.0173],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368672.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6220e-03,  5.2281e-03, -6.5452e-04,  ...,  1.1828e-02,
         -1.0144e-03,  3.4589e-03],
        [ 1.5179e-02,  1.1887e-02, -7.1148e-04,  ...,  2.5834e-02,
         -2.3404e-03,  8.9936e-03],
        [ 7.6382e-05,  1.3430e-04, -6.1094e-04,  ...,  1.1144e-03,
          0.0000e+00, -7.7482e-04],
        ...,
        [ 7.6382e-05,  1.3430e-04, -6.1094e-04,  ...,  1.1144e-03,
          0.0000e+00, -7.7482e-04],
        [ 7.6382e-05,  1.3430e-04, -6.1094e-04,  ...,  1.1144e-03,
          0.0000e+00, -7.7482e-04],
        [ 7.6382e-05,  1.3430e-04, -6.1094e-04,  ...,  1.1144e-03,
          0.0000e+00, -7.7482e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2556.1731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1124, device='cuda:0')



h[100].sum tensor(96.2486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2223, device='cuda:0')



h[200].sum tensor(34.1798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0724, 0.0566, 0.0000,  ..., 0.1225, 0.0000, 0.0435],
        [0.0377, 0.0297, 0.0000,  ..., 0.0658, 0.0000, 0.0218],
        [0.0394, 0.0309, 0.0000,  ..., 0.0685, 0.0000, 0.0229],
        ...,
        [0.0003, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56503., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1378, 0.1579, 0.3562,  ..., 0.0604, 0.1261, 0.0000],
        [0.1006, 0.1334, 0.2538,  ..., 0.0390, 0.0962, 0.0000],
        [0.0791, 0.1190, 0.1957,  ..., 0.0274, 0.0786, 0.0000],
        ...,
        [0.0008, 0.0690, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0008, 0.0690, 0.0000,  ..., 0.0000, 0.0121, 0.0000],
        [0.0008, 0.0690, 0.0000,  ..., 0.0000, 0.0121, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470129.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2530.0764, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(214.3578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4097.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(912.6921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-592.3110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1439],
        [ 0.1083],
        [-0.0530],
        ...,
        [-2.5558],
        [-2.5511],
        [-2.5499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293855.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0180],
        [1.0173],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368672.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0181],
        [1.0174],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368680.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9056e-05,  1.4356e-04, -6.1094e-04,  ...,  1.0898e-03,
          0.0000e+00, -7.7972e-04],
        [ 9.9056e-05,  1.4356e-04, -6.1094e-04,  ...,  1.0898e-03,
          0.0000e+00, -7.7972e-04],
        [ 9.9056e-05,  1.4356e-04, -6.1094e-04,  ...,  1.0898e-03,
          0.0000e+00, -7.7972e-04],
        ...,
        [ 9.9056e-05,  1.4356e-04, -6.1094e-04,  ...,  1.0898e-03,
          0.0000e+00, -7.7972e-04],
        [ 9.9056e-05,  1.4356e-04, -6.1094e-04,  ...,  1.0898e-03,
          0.0000e+00, -7.7972e-04],
        [ 9.9056e-05,  1.4356e-04, -6.1094e-04,  ...,  1.0898e-03,
          0.0000e+00, -7.7972e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2282.1519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4905, device='cuda:0')



h[100].sum tensor(94.8875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.3836, device='cuda:0')



h[200].sum tensor(28.7583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0178, 0.0141, 0.0000,  ..., 0.0329, 0.0000, 0.0096],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52682.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0691, 0.0037,  ..., 0.0000, 0.0146, 0.0000],
        [0.0216, 0.0808, 0.0475,  ..., 0.0045, 0.0292, 0.0000],
        [0.0578, 0.1047, 0.1391,  ..., 0.0192, 0.0601, 0.0000],
        ...,
        [0.0012, 0.0692, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0012, 0.0692, 0.0000,  ..., 0.0000, 0.0124, 0.0000],
        [0.0012, 0.0692, 0.0000,  ..., 0.0000, 0.0124, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461991.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2517.9585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.0087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4255.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(860.8334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-548.1329, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6133],
        [-0.3611],
        [-0.1115],
        ...,
        [-2.5559],
        [-2.5514],
        [-2.5503]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293558., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0181],
        [1.0174],
        ...,
        [1.0017],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368680.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(447.5724, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0182],
        [1.0175],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368689.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2465.0100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9212, device='cuda:0')



h[100].sum tensor(95.9480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.6611, device='cuda:0')



h[200].sum tensor(32.4187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0177, 0.0140, 0.0000,  ..., 0.0325, 0.0000, 0.0095],
        ...,
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56319.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0092, 0.0720, 0.0142,  ..., 0.0000, 0.0189, 0.0000],
        [0.0286, 0.0850, 0.0630,  ..., 0.0062, 0.0354, 0.0000],
        [0.0596, 0.1052, 0.1417,  ..., 0.0184, 0.0619, 0.0000],
        ...,
        [0.0017, 0.0692, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0017, 0.0692, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0017, 0.0692, 0.0000,  ..., 0.0000, 0.0129, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474161., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2789.3413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(220.4498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4337.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(909.4213, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-582.7358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5215],
        [-0.2020],
        [-0.0035],
        ...,
        [-2.4849],
        [-2.5034],
        [-2.5166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295885.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0182],
        [1.0175],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368689.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0182],
        [1.0176],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368697.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2564.3586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6560, device='cuda:0')



h[100].sum tensor(96.5092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.8578, device='cuda:0')



h[200].sum tensor(34.4308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57803.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0219, 0.0799, 0.0419,  ..., 0.0011, 0.0297, 0.0000],
        [0.0249, 0.0822, 0.0489,  ..., 0.0020, 0.0323, 0.0000],
        [0.0248, 0.0824, 0.0486,  ..., 0.0020, 0.0323, 0.0000],
        ...,
        [0.0021, 0.0691, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0021, 0.0691, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0021, 0.0691, 0.0000,  ..., 0.0000, 0.0134, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476813.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2950.3708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.1075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4389.4521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(930.0558, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.4512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2148],
        [-0.0801],
        [ 0.0089],
        ...,
        [-2.5365],
        [-2.5323],
        [-2.5313]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280003.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0182],
        [1.0176],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368697.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0183],
        [1.0177],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368705.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0052, -0.0007,  ...,  0.0115, -0.0010,  0.0034],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0066,  0.0052, -0.0007,  ...,  0.0115, -0.0010,  0.0034],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2541.1143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3713, device='cuda:0')



h[100].sum tensor(96.6667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.0065, device='cuda:0')



h[200].sum tensor(33.8614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1568, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0085, 0.0000,  ..., 0.0205, 0.0000, 0.0057],
        [0.0338, 0.0265, 0.0000,  ..., 0.0584, 0.0000, 0.0182],
        [0.0107, 0.0086, 0.0000,  ..., 0.0206, 0.0000, 0.0057],
        ...,
        [0.0007, 0.0008, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57364.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0324, 0.0859, 0.0669,  ..., 0.0040, 0.0400, 0.0000],
        [0.0491, 0.0967, 0.1101,  ..., 0.0102, 0.0547, 0.0000],
        [0.0338, 0.0873, 0.0705,  ..., 0.0041, 0.0416, 0.0000],
        ...,
        [0.0024, 0.0692, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0024, 0.0692, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0024, 0.0692, 0.0000,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474981.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3046.1003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.7671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4378.6729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(927.3742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-591.7106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4317],
        [-0.1930],
        [-0.2385],
        ...,
        [-2.5274],
        [-2.5229],
        [-2.5175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241412.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0183],
        [1.0177],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368705.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0162],
        [1.0184],
        [1.0178],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368714.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0144,  0.0113, -0.0007,  ...,  0.0244, -0.0022,  0.0085],
        [ 0.0067,  0.0053, -0.0007,  ...,  0.0117, -0.0010,  0.0035],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2354.3901, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.7042, device='cuda:0')



h[100].sum tensor(96.5648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.0227, device='cuda:0')



h[200].sum tensor(29.6704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0191, 0.0151, 0.0000,  ..., 0.0343, 0.0000, 0.0104],
        [0.0206, 0.0163, 0.0000,  ..., 0.0368, 0.0000, 0.0113],
        [0.0336, 0.0263, 0.0000,  ..., 0.0579, 0.0000, 0.0181],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52724.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0604, 0.1049, 0.1447,  ..., 0.0189, 0.0630, 0.0000],
        [0.0567, 0.1024, 0.1332,  ..., 0.0158, 0.0608, 0.0000],
        [0.0641, 0.1070, 0.1510,  ..., 0.0185, 0.0676, 0.0000],
        ...,
        [0.0021, 0.0697, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0021, 0.0697, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0021, 0.0697, 0.0000,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457790.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2700.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(190.9699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4571.0024, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(864.7519, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-539.0729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1686],
        [ 0.1786],
        [ 0.1866],
        ...,
        [-2.5561],
        [-2.5520],
        [-2.5510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267158.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0162],
        [1.0184],
        [1.0178],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368714.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0185],
        [1.0179],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368722., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094,  0.0074, -0.0007,  ...,  0.0161, -0.0014,  0.0052],
        [ 0.0127,  0.0099, -0.0007,  ...,  0.0214, -0.0019,  0.0073],
        [ 0.0124,  0.0097, -0.0007,  ...,  0.0211, -0.0019,  0.0072],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3168.7307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.2653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9264, device='cuda:0')



h[100].sum tensor(101.9098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.6145, device='cuda:0')



h[200].sum tensor(44.5556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0209, 0.0165, 0.0000,  ..., 0.0372, 0.0000, 0.0116],
        [0.0330, 0.0258, 0.0000,  ..., 0.0569, 0.0000, 0.0185],
        [0.0713, 0.0557, 0.0000,  ..., 0.1196, 0.0000, 0.0426],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71310.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0677, 0.1103, 0.1642,  ..., 0.0227, 0.0692, 0.0000],
        [0.0944, 0.1285, 0.2384,  ..., 0.0386, 0.0905, 0.0000],
        [0.1355, 0.1560, 0.3544,  ..., 0.0643, 0.1231, 0.0000],
        ...,
        [0.0017, 0.0702, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0017, 0.0702, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0017, 0.0702, 0.0000,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535319.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3663.2300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.0749, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4584.2759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1121.3075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-736.0195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1163],
        [ 0.1376],
        [ 0.1284],
        ...,
        [-2.5867],
        [-2.5822],
        [-2.5812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288249.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0185],
        [1.0179],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368722., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0186],
        [1.0180],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368730., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0111,  0.0087, -0.0007,  ...,  0.0188, -0.0016,  0.0063],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2376.8662, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0298, device='cuda:0')



h[100].sum tensor(98.3821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.9960, device='cuda:0')



h[200].sum tensor(28.6922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0117, 0.0093, 0.0000,  ..., 0.0222, 0.0000, 0.0064],
        [0.0097, 0.0077, 0.0000,  ..., 0.0189, 0.0000, 0.0051],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53133.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0734, 0.0120,  ..., 0.0000, 0.0191, 0.0000],
        [0.0248, 0.0842, 0.0523,  ..., 0.0052, 0.0322, 0.0000],
        [0.0440, 0.0969, 0.1046,  ..., 0.0139, 0.0482, 0.0000],
        ...,
        [0.0015, 0.0705, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0015, 0.0705, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0015, 0.0705, 0.0000,  ..., 0.0000, 0.0134, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463671.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2555.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.1062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4843.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(872.2687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-538.7564, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3076],
        [-1.1305],
        [-0.7437],
        ...,
        [-2.6071],
        [-2.6026],
        [-2.5941]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309301.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0186],
        [1.0180],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368730., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0188],
        [1.0181],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368738., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0166,  0.0130, -0.0007,  ...,  0.0279, -0.0025,  0.0099],
        [ 0.0382,  0.0298, -0.0009,  ...,  0.0633, -0.0057,  0.0239],
        [ 0.0157,  0.0123, -0.0007,  ...,  0.0264, -0.0023,  0.0093],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2631.1201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.5532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1161, device='cuda:0')



h[100].sum tensor(100.6137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2335, device='cuda:0')



h[200].sum tensor(33.0563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0818, 0.0638, 0.0000,  ..., 0.1367, 0.0000, 0.0495],
        [0.0648, 0.0506, 0.0000,  ..., 0.1090, 0.0000, 0.0385],
        [0.0660, 0.0516, 0.0000,  ..., 0.1110, 0.0000, 0.0400],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58498.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1374, 0.1572, 0.3642,  ..., 0.0680, 0.1243, 0.0000],
        [0.1353, 0.1566, 0.3578,  ..., 0.0664, 0.1224, 0.0000],
        [0.1208, 0.1475, 0.3192,  ..., 0.0589, 0.1104, 0.0000],
        ...,
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0133, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482401.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2829.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(237.5334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4730.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(950.1823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.9062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0716],
        [ 0.0324],
        [-0.1178],
        ...,
        [-2.6326],
        [-2.6280],
        [-2.6269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290814.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0188],
        [1.0181],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368738., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0188],
        [1.0181],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368738., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0121,  0.0095, -0.0007,  ...,  0.0206, -0.0018,  0.0070],
        [ 0.0071,  0.0055, -0.0007,  ...,  0.0123, -0.0010,  0.0037],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2452.2915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6393, device='cuda:0')



h[100].sum tensor(99.6385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.8181, device='cuda:0')



h[200].sum tensor(29.6280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0411, 0.0322, 0.0000,  ..., 0.0702, 0.0000, 0.0231],
        [0.0227, 0.0179, 0.0000,  ..., 0.0402, 0.0000, 0.0128],
        [0.0076, 0.0061, 0.0000,  ..., 0.0155, 0.0000, 0.0038],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54892.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0678, 0.1112, 0.1665,  ..., 0.0237, 0.0696, 0.0000],
        [0.0486, 0.0994, 0.1154,  ..., 0.0142, 0.0532, 0.0000],
        [0.0254, 0.0845, 0.0538,  ..., 0.0049, 0.0338, 0.0000],
        ...,
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0133, 0.0000],
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0133, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470259.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2621.3416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.0651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4869.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(898.5712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-557.8988, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0414],
        [-0.1236],
        [-0.4113],
        ...,
        [-2.6326],
        [-2.6280],
        [-2.6269]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299016.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0188],
        [1.0181],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368738., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0188],
        [1.0182],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368746.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2370.7417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.0836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8105, device='cuda:0')



h[100].sum tensor(99.6923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.3404, device='cuda:0')



h[200].sum tensor(27.8635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52633.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0686, 0.0000,  ..., 0.0000, 0.0128, 0.0000],
        [0.0011, 0.0690, 0.0000,  ..., 0.0000, 0.0129, 0.0000],
        [0.0011, 0.0692, 0.0000,  ..., 0.0000, 0.0130, 0.0000],
        ...,
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0012, 0.0707, 0.0000,  ..., 0.0000, 0.0134, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(461109.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2476.5591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(188.0999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4936.1655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(868.3128, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-533.4815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5019],
        [-1.9343],
        [-2.2499],
        ...,
        [-2.6468],
        [-2.6421],
        [-2.6411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300872.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0188],
        [1.0182],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368746.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0189],
        [1.0183],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368754.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2585.1531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8002, device='cuda:0')



h[100].sum tensor(100.9587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.2888, device='cuda:0')



h[200].sum tensor(32.0817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58479.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0682, 0.0000,  ..., 0.0000, 0.0131, 0.0000],
        [0.0015, 0.0687, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0024, 0.0693, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        ...,
        [0.0030, 0.0712, 0.0006,  ..., 0.0000, 0.0153, 0.0000],
        [0.0063, 0.0730, 0.0051,  ..., 0.0000, 0.0184, 0.0000],
        [0.0080, 0.0739, 0.0081,  ..., 0.0000, 0.0200, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487172.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2932.0522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.6968, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4808.0254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(951.5871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-599.2584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4713],
        [-2.2663],
        [-1.9623],
        ...,
        [-2.3686],
        [-2.1755],
        [-2.0537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266397., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0189],
        [1.0183],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368754.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(439.6649, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0190],
        [1.0184],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368763.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0087,  0.0068, -0.0007,  ...,  0.0150, -0.0013,  0.0048],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2202.9807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.6814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.2854, device='cuda:0')



h[100].sum tensor(98.8390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.7810, device='cuda:0')



h[200].sum tensor(25.0171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8126, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0094, 0.0075, 0.0000,  ..., 0.0182, 0.0000, 0.0049],
        [0.0298, 0.0234, 0.0000,  ..., 0.0516, 0.0000, 0.0173],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50509.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0167, 0.0769, 0.0303,  ..., 0.0006, 0.0268, 0.0000],
        [0.0343, 0.0887, 0.0792,  ..., 0.0102, 0.0412, 0.0000],
        [0.0771, 0.1166, 0.1985,  ..., 0.0331, 0.0758, 0.0000],
        ...,
        [0.0016, 0.0698, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0016, 0.0698, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0016, 0.0698, 0.0000,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460234.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2520.7056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(173.3068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5015.4136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(839.0510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-509.6697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3414],
        [-0.2191],
        [-0.0715],
        ...,
        [-2.6622],
        [-2.6576],
        [-2.6566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303737.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0190],
        [1.0184],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368763.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0191],
        [1.0184],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368771.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0262,  0.0204, -0.0008,  ...,  0.0435, -0.0038,  0.0161],
        [ 0.0239,  0.0187, -0.0008,  ...,  0.0399, -0.0035,  0.0146],
        [ 0.0225,  0.0175, -0.0008,  ...,  0.0374, -0.0033,  0.0137],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2690.1414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.0812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5723, device='cuda:0')



h[100].sum tensor(101.6181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.5973, device='cuda:0')



h[200].sum tensor(34.6549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2907, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0556, 0.0435, 0.0000,  ..., 0.0938, 0.0000, 0.0332],
        [0.0788, 0.0615, 0.0000,  ..., 0.1318, 0.0000, 0.0474],
        [0.0853, 0.0666, 0.0000,  ..., 0.1425, 0.0000, 0.0516],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60066.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1491, 0.1616, 0.3980,  ..., 0.0754, 0.1341, 0.0000],
        [0.1675, 0.1739, 0.4489,  ..., 0.0860, 0.1491, 0.0000],
        [0.1732, 0.1779, 0.4640,  ..., 0.0890, 0.1538, 0.0000],
        ...,
        [0.0017, 0.0696, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0017, 0.0696, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0017, 0.0696, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495796.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3114.1658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(254.8268, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4766.1504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(973.5626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-614.7456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0835],
        [ 0.0857],
        [ 0.0888],
        ...,
        [-2.6751],
        [-2.6705],
        [-2.6695]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280588.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0191],
        [1.0184],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368771.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0192],
        [1.0185],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368779.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0132,  0.0103, -0.0007,  ...,  0.0223, -0.0019,  0.0077],
        [ 0.0058,  0.0046, -0.0006,  ...,  0.0103, -0.0008,  0.0029],
        [ 0.0057,  0.0045, -0.0006,  ...,  0.0100, -0.0008,  0.0028],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2418.7319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.2516, device='cuda:0')



h[100].sum tensor(100.3224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.6593, device='cuda:0')



h[200].sum tensor(29.6637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0266, 0.0208, 0.0000,  ..., 0.0463, 0.0000, 0.0136],
        [0.0365, 0.0286, 0.0000,  ..., 0.0626, 0.0000, 0.0200],
        [0.0302, 0.0237, 0.0000,  ..., 0.0522, 0.0000, 0.0159],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54722.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0746, 0.1115, 0.1830,  ..., 0.0259, 0.0768, 0.0000],
        [0.0790, 0.1150, 0.1952,  ..., 0.0286, 0.0802, 0.0000],
        [0.0704, 0.1095, 0.1719,  ..., 0.0239, 0.0732, 0.0000],
        ...,
        [0.0017, 0.0694, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0017, 0.0694, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0017, 0.0694, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474720.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2784.6199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(209.5560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4897.1782, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(899.0987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-556.6563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1571],
        [ 0.1545],
        [ 0.1501],
        ...,
        [-2.6950],
        [-2.6903],
        [-2.6892]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292999.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0192],
        [1.0185],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368779.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0193],
        [1.0186],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368787.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0186,  0.0145, -0.0007,  ...,  0.0312, -0.0027,  0.0112],
        [ 0.0085,  0.0066, -0.0007,  ...,  0.0146, -0.0012,  0.0046],
        [ 0.0093,  0.0073, -0.0007,  ...,  0.0160, -0.0013,  0.0052],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2474.2417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.3130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7822, device='cuda:0')



h[100].sum tensor(101.0226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.2454, device='cuda:0')



h[200].sum tensor(30.8345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0465, 0.0364, 0.0000,  ..., 0.0791, 0.0000, 0.0266],
        [0.0510, 0.0398, 0.0000,  ..., 0.0863, 0.0000, 0.0294],
        [0.0304, 0.0238, 0.0000,  ..., 0.0526, 0.0000, 0.0169],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55759.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0785, 0.1160, 0.1988,  ..., 0.0319, 0.0775, 0.0000],
        [0.0793, 0.1171, 0.2013,  ..., 0.0326, 0.0779, 0.0000],
        [0.0658, 0.1087, 0.1640,  ..., 0.0249, 0.0669, 0.0000],
        ...,
        [0.0016, 0.0695, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0016, 0.0695, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0016, 0.0695, 0.0000,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478701.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2827.5181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.2917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4842.2778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(915.9301, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-569.6403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1068],
        [ 0.0418],
        [-0.0994],
        ...,
        [-2.7196],
        [-2.7147],
        [-2.7135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283058.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0193],
        [1.0186],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368787.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0194],
        [1.0187],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368795.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099,  0.0077, -0.0007,  ...,  0.0169, -0.0014,  0.0055],
        [ 0.0064,  0.0050, -0.0007,  ...,  0.0113, -0.0009,  0.0033],
        [ 0.0050,  0.0040, -0.0006,  ...,  0.0090, -0.0007,  0.0024],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2863.5029, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.7628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3969, device='cuda:0')



h[100].sum tensor(103.3910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0522, device='cuda:0')



h[200].sum tensor(38.2032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0342, 0.0268, 0.0000,  ..., 0.0589, 0.0000, 0.0194],
        [0.0294, 0.0230, 0.0000,  ..., 0.0511, 0.0000, 0.0163],
        [0.0513, 0.0401, 0.0000,  ..., 0.0870, 0.0000, 0.0297],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63761.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1166, 0.1405, 0.3064,  ..., 0.0551, 0.1084, 0.0000],
        [0.0954, 0.1270, 0.2453,  ..., 0.0413, 0.0919, 0.0000],
        [0.1043, 0.1328, 0.2693,  ..., 0.0461, 0.0992, 0.0000],
        ...,
        [0.0014, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509594.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3159.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.9803, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4824.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1024.6636, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-652.1548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0437],
        [ 0.0656],
        [ 0.0778],
        ...,
        [-2.7469],
        [-2.7419],
        [-2.7407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317756.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0194],
        [1.0187],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368795.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0194],
        [1.0187],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368795.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2255.2139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9421, device='cuda:0')



h[100].sum tensor(100.1047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.7442, device='cuda:0')



h[200].sum tensor(26.6557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0353, 0.0277, 0.0000,  ..., 0.0608, 0.0000, 0.0209],
        [0.0006, 0.0006, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51607.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0710, 0.1116, 0.1812,  ..., 0.0300, 0.0705, 0.0000],
        [0.0264, 0.0835, 0.0607,  ..., 0.0076, 0.0342, 0.0000],
        [0.0114, 0.0742, 0.0197,  ..., 0.0008, 0.0220, 0.0000],
        ...,
        [0.0014, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464696.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2512.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(182.1181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4960.1831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(856.6732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-521.4563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1056],
        [-0.4518],
        [-0.9484],
        ...,
        [-2.7429],
        [-2.7364],
        [-2.7330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326110.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0194],
        [1.0187],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368795.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0173],
        [1.0195],
        [1.0188],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368803.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2830.1689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.1391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9574, device='cuda:0')



h[100].sum tensor(103.1435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.7384, device='cuda:0')



h[200].sum tensor(37.7831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0182, 0.0143, 0.0000,  ..., 0.0328, 0.0000, 0.0098],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64670.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0722, 0.0118,  ..., 0.0000, 0.0195, 0.0000],
        [0.0160, 0.0771, 0.0339,  ..., 0.0028, 0.0258, 0.0000],
        [0.0441, 0.0947, 0.1054,  ..., 0.0139, 0.0498, 0.0000],
        ...,
        [0.0013, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0013, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0013, 0.0695, 0.0000,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517793.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3383.6997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.7003, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4555.0869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1041.8060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-666.9267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3666],
        [-1.1259],
        [-0.6469],
        ...,
        [-2.7638],
        [-2.7587],
        [-2.7575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259872.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0173],
        [1.0195],
        [1.0188],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368803.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0196],
        [1.0189],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368811.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0045, -0.0006,  ...,  0.0101, -0.0008,  0.0028],
        [ 0.0059,  0.0047, -0.0006,  ...,  0.0105, -0.0008,  0.0030],
        [ 0.0115,  0.0090, -0.0007,  ...,  0.0196, -0.0016,  0.0066],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2991.0015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.5978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2984, device='cuda:0')



h[100].sum tensor(104.0647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.7474, device='cuda:0')



h[200].sum tensor(41.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0197, 0.0155, 0.0000,  ..., 0.0353, 0.0000, 0.0108],
        [0.0396, 0.0310, 0.0000,  ..., 0.0679, 0.0000, 0.0221],
        [0.0321, 0.0251, 0.0000,  ..., 0.0556, 0.0000, 0.0172],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68468.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0680, 0.1091, 0.1713,  ..., 0.0255, 0.0699, 0.0000],
        [0.0729, 0.1126, 0.1829,  ..., 0.0267, 0.0745, 0.0000],
        [0.0761, 0.1146, 0.1916,  ..., 0.0285, 0.0773, 0.0000],
        ...,
        [0.0011, 0.0696, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        [0.0011, 0.0696, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        [0.0011, 0.0697, 0.0000,  ..., 0.0000, 0.0136, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538942.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3542.7534, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.7623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4620.6040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1091.9001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-706.3109, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1322],
        [ 0.1438],
        [ 0.1456],
        ...,
        [-2.7773],
        [-2.7709],
        [-2.7686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300042.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0196],
        [1.0189],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368811.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0197],
        [1.0190],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368819.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0110,  0.0087, -0.0007,  ...,  0.0189, -0.0016,  0.0063],
        [ 0.0237,  0.0185, -0.0008,  ...,  0.0396, -0.0034,  0.0145],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2597.6877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8287, device='cuda:0')



h[100].sum tensor(101.5116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.3741, device='cuda:0')



h[200].sum tensor(34.1890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0284, 0.0224, 0.0000,  ..., 0.0497, 0.0000, 0.0165],
        [0.0494, 0.0387, 0.0000,  ..., 0.0840, 0.0000, 0.0293],
        [0.0392, 0.0308, 0.0000,  ..., 0.0674, 0.0000, 0.0227],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58337.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0802, 0.1180, 0.2076,  ..., 0.0335, 0.0788, 0.0000],
        [0.1026, 0.1327, 0.2702,  ..., 0.0466, 0.0970, 0.0000],
        [0.0960, 0.1284, 0.2509,  ..., 0.0422, 0.0920, 0.0000],
        ...,
        [0.0010, 0.0696, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0010, 0.0696, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0010, 0.0696, 0.0000,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489417.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2860.2092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.6463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4636.6865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(952.4935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-600.4456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1008],
        [ 0.1014],
        [ 0.0897],
        ...,
        [-2.7821],
        [-2.7771],
        [-2.7759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285331.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0197],
        [1.0190],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368819.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0176],
        [1.0198],
        [1.0191],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368828.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0108,  0.0085, -0.0007,  ...,  0.0185, -0.0015,  0.0061],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2969.3564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.0071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9565, device='cuda:0')



h[100].sum tensor(102.9196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.7251, device='cuda:0')



h[200].sum tensor(41.6425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0400, 0.0314, 0.0000,  ..., 0.0686, 0.0000, 0.0224],
        [0.0095, 0.0076, 0.0000,  ..., 0.0187, 0.0000, 0.0050],
        [0.0115, 0.0092, 0.0000,  ..., 0.0220, 0.0000, 0.0063],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65357.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0530, 0.1003, 0.1313,  ..., 0.0159, 0.0575, 0.0000],
        [0.0330, 0.0880, 0.0769,  ..., 0.0055, 0.0408, 0.0000],
        [0.0426, 0.0943, 0.1035,  ..., 0.0110, 0.0487, 0.0000],
        ...,
        [0.0011, 0.0696, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0011, 0.0696, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0011, 0.0696, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(514847.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3214.0698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.4752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4577.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1049.1581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-678.1798, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0621],
        [ 0.0609],
        [ 0.0702],
        ...,
        [-2.7828],
        [-2.7779],
        [-2.7768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286976.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0176],
        [1.0198],
        [1.0191],
        ...,
        [1.0016],
        [1.0008],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368828.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(453.6130, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0198],
        [1.0191],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368836.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2702.7090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5172, device='cuda:0')



h[100].sum tensor(100.8845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.4326, device='cuda:0')



h[200].sum tensor(36.8945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60977.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0675, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0010, 0.0679, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0010, 0.0681, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        ...,
        [0.0011, 0.0696, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0018, 0.0700, 0.0000,  ..., 0.0000, 0.0150, 0.0000],
        [0.0042, 0.0713, 0.0035,  ..., 0.0000, 0.0173, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500075.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3019.5725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(264.8411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4630.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(988.7800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-632.6177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4588],
        [-2.6398],
        [-2.7761],
        ...,
        [-2.7466],
        [-2.6671],
        [-2.5222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274680.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0198],
        [1.0191],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368836.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0199],
        [1.0192],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368844.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0194,  0.0152, -0.0007,  ...,  0.0326, -0.0027,  0.0117],
        [ 0.0117,  0.0092, -0.0007,  ...,  0.0199, -0.0016,  0.0067],
        [ 0.0058,  0.0046, -0.0006,  ...,  0.0103, -0.0008,  0.0029],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2366.8496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5427, device='cuda:0')



h[100].sum tensor(98.7455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5397, device='cuda:0')



h[200].sum tensor(30.6217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0569, 0.0446, 0.0000,  ..., 0.0963, 0.0000, 0.0333],
        [0.0556, 0.0435, 0.0000,  ..., 0.0941, 0.0000, 0.0324],
        [0.0491, 0.0385, 0.0000,  ..., 0.0836, 0.0000, 0.0282],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53198.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1454, 0.1593, 0.3902,  ..., 0.0682, 0.1330, 0.0000],
        [0.1331, 0.1516, 0.3547,  ..., 0.0601, 0.1236, 0.0000],
        [0.1227, 0.1452, 0.3246,  ..., 0.0533, 0.1154, 0.0000],
        ...,
        [0.0010, 0.0696, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0696, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0696, 0.0000,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469344.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2503.7419, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(198.8850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4883.8447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(880.4002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-549.4321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0939],
        [ 0.1083],
        [ 0.1167],
        ...,
        [-2.7942],
        [-2.7896],
        [-2.7885]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286385.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0199],
        [1.0192],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368844.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0178],
        [1.0199],
        [1.0193],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368851.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0157,  0.0123, -0.0007,  ...,  0.0265, -0.0022,  0.0093],
        [ 0.0165,  0.0129, -0.0007,  ...,  0.0278, -0.0023,  0.0098],
        [ 0.0224,  0.0175, -0.0008,  ...,  0.0374, -0.0031,  0.0136],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2922.2007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.8192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2601, device='cuda:0')



h[100].sum tensor(101.5667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.6432, device='cuda:0')



h[200].sum tensor(41.0038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0875, 0.0684, 0.0000,  ..., 0.1464, 0.0000, 0.0531],
        [0.0656, 0.0514, 0.0000,  ..., 0.1107, 0.0000, 0.0397],
        [0.0358, 0.0281, 0.0000,  ..., 0.0619, 0.0000, 0.0212],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0078, 0.0063, 0.0000,  ..., 0.0161, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65285.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9663e-01, 1.9261e-01, 5.4200e-01,  ..., 1.0288e-01, 1.7328e-01,
         0.0000e+00],
        [1.5071e-01, 1.6364e-01, 4.1106e-01,  ..., 7.4690e-02, 1.3637e-01,
         0.0000e+00],
        [1.0083e-01, 1.3196e-01, 2.6976e-01,  ..., 4.4570e-02, 9.6248e-02,
         0.0000e+00],
        ...,
        [1.6250e-03, 7.0290e-02, 1.1520e-04,  ..., 0.0000e+00, 1.5275e-02,
         0.0000e+00],
        [6.5702e-03, 7.3377e-02, 1.1427e-02,  ..., 0.0000e+00, 1.9523e-02,
         0.0000e+00],
        [2.3205e-02, 8.3788e-02, 5.2121e-02,  ..., 3.6121e-03, 3.3697e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516890.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3202.5654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.0961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4585.4155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1052.0479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.2558, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0754],
        [ 0.0947],
        [ 0.1103],
        ...,
        [-2.4686],
        [-1.9788],
        [-1.2864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247038.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0178],
        [1.0199],
        [1.0193],
        ...,
        [1.0016],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368851.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0199],
        [1.0194],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368859.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0165,  0.0129, -0.0007,  ...,  0.0279, -0.0023,  0.0098],
        [ 0.0097,  0.0076, -0.0007,  ...,  0.0166, -0.0013,  0.0054],
        [ 0.0058,  0.0046, -0.0006,  ...,  0.0103, -0.0008,  0.0029],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2496.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7153, device='cuda:0')



h[100].sum tensor(99.6953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0456, device='cuda:0')



h[200].sum tensor(32.7113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0492, 0.0386, 0.0000,  ..., 0.0839, 0.0000, 0.0283],
        [0.0404, 0.0317, 0.0000,  ..., 0.0695, 0.0000, 0.0226],
        [0.0236, 0.0186, 0.0000,  ..., 0.0420, 0.0000, 0.0117],
        ...,
        [0.0005, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56155.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0795, 0.1181, 0.2083,  ..., 0.0287, 0.0807, 0.0000],
        [0.0722, 0.1131, 0.1869,  ..., 0.0236, 0.0757, 0.0000],
        [0.0598, 0.1046, 0.1509,  ..., 0.0152, 0.0667, 0.0000],
        ...,
        [0.0004, 0.0701, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0004, 0.0701, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0004, 0.0701, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482490.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2473.2144, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.1641, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4822.8423, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(923.1901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-582.0197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1567],
        [ 0.1709],
        [ 0.1580],
        ...,
        [-2.8465],
        [-2.8417],
        [-2.8407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304938.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0199],
        [1.0194],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368859.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0199],
        [1.0194],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368859.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0156,  0.0122, -0.0007,  ...,  0.0264, -0.0022,  0.0093],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2605.0195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7240, device='cuda:0')



h[100].sum tensor(100.2722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.0612, device='cuda:0')



h[200].sum tensor(34.7422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1961, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0225, 0.0178, 0.0000,  ..., 0.0403, 0.0000, 0.0127],
        [0.0435, 0.0341, 0.0000,  ..., 0.0746, 0.0000, 0.0254],
        ...,
        [0.0005, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57820.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0161, 0.0780, 0.0379,  ..., 0.0021, 0.0269, 0.0000],
        [0.0533, 0.1021, 0.1383,  ..., 0.0182, 0.0579, 0.0000],
        [0.0963, 0.1298, 0.2583,  ..., 0.0414, 0.0930, 0.0000],
        ...,
        [0.0004, 0.0701, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0004, 0.0701, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0004, 0.0701, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489295., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2610.2529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.0378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4719.0830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(948.2509, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-602.2181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4870],
        [-0.9152],
        [-0.4098],
        ...,
        [-2.8464],
        [-2.8412],
        [-2.8398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289007.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0199],
        [1.0194],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368859.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0200],
        [1.0195],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368866.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0132,  0.0103, -0.0007,  ...,  0.0225, -0.0018,  0.0077],
        [ 0.0118,  0.0093, -0.0007,  ...,  0.0202, -0.0016,  0.0068],
        [ 0.0120,  0.0094, -0.0007,  ...,  0.0204, -0.0017,  0.0069],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2816.4048, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5628, device='cuda:0')



h[100].sum tensor(101.5063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5585, device='cuda:0')



h[200].sum tensor(38.3270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0526, 0.0412, 0.0000,  ..., 0.0896, 0.0000, 0.0306],
        [0.0356, 0.0280, 0.0000,  ..., 0.0618, 0.0000, 0.0196],
        [0.0369, 0.0290, 0.0000,  ..., 0.0638, 0.0000, 0.0204],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64629.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.0526e-01, 1.3544e-01, 2.8398e-01,  ..., 4.6166e-02, 1.0051e-01,
         0.0000e+00],
        [7.8320e-02, 1.1831e-01, 2.0634e-01,  ..., 2.8682e-02, 7.9482e-02,
         0.0000e+00],
        [6.7066e-02, 1.1133e-01, 1.7459e-01,  ..., 2.2000e-02, 7.0445e-02,
         0.0000e+00],
        ...,
        [2.1458e-04, 7.0326e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3896e-02,
         0.0000e+00],
        [2.1495e-04, 7.0340e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3900e-02,
         0.0000e+00],
        [2.1503e-04, 7.0341e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3900e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523651.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2973.9966, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(297.3262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4757.7690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1041.2983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-673.8342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1305],
        [-0.2122],
        [-0.3213],
        ...,
        [-2.8751],
        [-2.8699],
        [-2.8686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302063.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0200],
        [1.0195],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368866.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0200],
        [1.0196],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368874.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070,  0.0055, -0.0007,  ...,  0.0122, -0.0010,  0.0036],
        [ 0.0046,  0.0037, -0.0006,  ...,  0.0084, -0.0006,  0.0021],
        [ 0.0115,  0.0090, -0.0007,  ...,  0.0196, -0.0016,  0.0066],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2350.6919, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.4406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5475, device='cuda:0')



h[100].sum tensor(98.9386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5540, device='cuda:0')



h[200].sum tensor(29.3885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9534, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0195, 0.0155, 0.0000,  ..., 0.0354, 0.0000, 0.0108],
        [0.0492, 0.0385, 0.0000,  ..., 0.0839, 0.0000, 0.0283],
        [0.0291, 0.0229, 0.0000,  ..., 0.0511, 0.0000, 0.0161],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53292.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.2992e-02, 1.0208e-01, 1.3600e-01,  ..., 1.5183e-02, 5.8485e-02,
         0.0000e+00],
        [8.0574e-02, 1.2030e-01, 2.1149e-01,  ..., 2.8998e-02, 8.1321e-02,
         0.0000e+00],
        [7.0062e-02, 1.1361e-01, 1.8225e-01,  ..., 2.3068e-02, 7.2913e-02,
         0.0000e+00],
        ...,
        [1.6983e-04, 7.0517e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3739e-02,
         0.0000e+00],
        [1.7019e-04, 7.0531e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3743e-02,
         0.0000e+00],
        [1.7027e-04, 7.0532e-02, 0.0000e+00,  ..., 0.0000e+00, 1.3743e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477121.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2227.2231, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(202.2714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5014.7954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(883.3037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-548.7979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2076],
        [ 0.0373],
        [ 0.0604],
        ...,
        [-2.8930],
        [-2.8878],
        [-2.8865]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-349129.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0200],
        [1.0196],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368874.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0200],
        [1.0197],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368882.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0137,  0.0107, -0.0007,  ...,  0.0232, -0.0019,  0.0080],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3010.5286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.3632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0003, device='cuda:0')



h[100].sum tensor(102.5019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.8561, device='cuda:0')



h[200].sum tensor(41.8345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0117, 0.0093, 0.0000,  ..., 0.0226, 0.0000, 0.0065],
        [0.0143, 0.0113, 0.0000,  ..., 0.0268, 0.0000, 0.0081],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67430.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0494, 0.1005, 0.1275,  ..., 0.0153, 0.0543, 0.0000],
        [0.0328, 0.0903, 0.0817,  ..., 0.0068, 0.0409, 0.0000],
        [0.0209, 0.0829, 0.0486,  ..., 0.0013, 0.0312, 0.0000],
        ...,
        [0.0002, 0.0708, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0002, 0.0709, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0002, 0.0709, 0.0000,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534876.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3135.8955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4725.6147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1081.2531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-703.1561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0441],
        [-0.0169],
        [-0.0671],
        ...,
        [-2.9021],
        [-2.8969],
        [-2.8957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315634.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0200],
        [1.0197],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368882.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0200],
        [1.0198],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368890.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2648.3186, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7673, device='cuda:0')



h[100].sum tensor(100.5100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.1907, device='cuda:0')



h[200].sum tensor(35.1566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0061, 0.0050, 0.0000,  ..., 0.0134, 0.0000, 0.0028],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59719.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0693, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0051, 0.0724, 0.0090,  ..., 0.0000, 0.0177, 0.0000],
        [0.0159, 0.0792, 0.0342,  ..., 0.0009, 0.0276, 0.0000],
        ...,
        [0.0004, 0.0710, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0004, 0.0711, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0004, 0.0711, 0.0000,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498508.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2733.2886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.3101, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4742.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(977.6347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-620.9204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5945],
        [-2.1451],
        [-1.5284],
        ...,
        [-2.9021],
        [-2.8971],
        [-2.8959]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300295.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0200],
        [1.0198],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368890.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0201],
        [1.0199],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368898.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0115,  0.0091, -0.0007,  ...,  0.0197, -0.0016,  0.0066],
        [ 0.0068,  0.0054, -0.0007,  ...,  0.0120, -0.0009,  0.0036],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2863.6257, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.0824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6419, device='cuda:0')



h[100].sum tensor(101.6445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.7952, device='cuda:0')



h[200].sum tensor(39.2915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0278, 0.0219, 0.0000,  ..., 0.0488, 0.0000, 0.0145],
        [0.0169, 0.0134, 0.0000,  ..., 0.0310, 0.0000, 0.0082],
        [0.0434, 0.0340, 0.0000,  ..., 0.0743, 0.0000, 0.0246],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64776.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0966, 0.1130,  ..., 0.0085, 0.0540, 0.0000],
        [0.0512, 0.1001, 0.1268,  ..., 0.0115, 0.0583, 0.0000],
        [0.0861, 0.1230, 0.2235,  ..., 0.0311, 0.0863, 0.0000],
        ...,
        [0.0006, 0.0713, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0006, 0.0713, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0006, 0.0713, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(520407.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3174.2859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.8871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4645.9399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1049.9398, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-676.3489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1491],
        [ 0.1536],
        [ 0.1629],
        ...,
        [-2.8978],
        [-2.8929],
        [-2.8918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278544.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0201],
        [1.0199],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368898.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(460.6624, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0202],
        [1.0200],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368906.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2405.1128, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4607, device='cuda:0')



h[100].sum tensor(99.1738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.2947, device='cuda:0')



h[200].sum tensor(30.7511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53517.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0699, 0.0002,  ..., 0.0000, 0.0152, 0.0000],
        [0.0009, 0.0698, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0009, 0.0700, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        ...,
        [0.0008, 0.0715, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0008, 0.0715, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0008, 0.0715, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473278.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2528.6001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.2225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4955.0566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(893.3901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-552.5531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7362],
        [-1.7037],
        [-1.5565],
        ...,
        [-2.8915],
        [-2.8867],
        [-2.8855]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303913.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0202],
        [1.0200],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368906.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0202],
        [1.0201],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368914.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3637.1802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(42.2521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.1501, device='cuda:0')



h[100].sum tensor(105.7622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(57.2523, device='cuda:0')



h[200].sum tensor(53.5805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.1359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78115.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0696, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0009, 0.0700, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0009, 0.0702, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        ...,
        [0.0010, 0.0717, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0717, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0717, 0.0000,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(569904., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3951.6589, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(422.6049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4791.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1231.2235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-814.7604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7198],
        [-2.7774],
        [-2.7571],
        ...,
        [-2.8940],
        [-2.8893],
        [-2.8882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291138.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0202],
        [1.0201],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368914.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0202],
        [1.0201],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368914.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0060,  0.0047, -0.0006,  ...,  0.0105, -0.0008,  0.0030],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2279.2402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.7257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.3675, device='cuda:0')



h[100].sum tensor(98.5971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.0264, device='cuda:0')



h[200].sum tensor(28.3142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0066, 0.0053, 0.0000,  ..., 0.0140, 0.0000, 0.0031],
        [0.0190, 0.0150, 0.0000,  ..., 0.0343, 0.0000, 0.0104],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51047.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0102, 0.0753, 0.0210,  ..., 0.0000, 0.0217, 0.0000],
        [0.0291, 0.0872, 0.0677,  ..., 0.0057, 0.0378, 0.0000],
        [0.0545, 0.1031, 0.1355,  ..., 0.0154, 0.0590, 0.0000],
        ...,
        [0.0010, 0.0717, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0717, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0717, 0.0000,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464567.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2439.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(186.3242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5110.9663, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(858.6122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-523.6232, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5945],
        [-0.2701],
        [-0.0368],
        ...,
        [-2.8940],
        [-2.8893],
        [-2.8882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309242.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0202],
        [1.0201],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368914.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0203],
        [1.0202],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368922.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0251,  0.0196, -0.0008,  ...,  0.0418, -0.0034,  0.0154],
        [ 0.0213,  0.0167, -0.0008,  ...,  0.0357, -0.0029,  0.0129],
        [ 0.0204,  0.0160, -0.0007,  ...,  0.0342, -0.0027,  0.0124],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2588.4497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8219, device='cuda:0')



h[100].sum tensor(100.3956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3642, device='cuda:0')



h[200].sum tensor(33.9592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0674, 0.0527, 0.0000,  ..., 0.1134, 0.0000, 0.0401],
        [0.0935, 0.0730, 0.0000,  ..., 0.1561, 0.0000, 0.0570],
        [0.0654, 0.0512, 0.0000,  ..., 0.1102, 0.0000, 0.0388],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57200.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1230, 0.1455, 0.3263,  ..., 0.0558, 0.1141, 0.0000],
        [0.1440, 0.1593, 0.3859,  ..., 0.0690, 0.1308, 0.0000],
        [0.1229, 0.1464, 0.3262,  ..., 0.0561, 0.1137, 0.0000],
        ...,
        [0.0012, 0.0719, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0012, 0.0719, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0012, 0.0719, 0.0000,  ..., 0.0000, 0.0146, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485641.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2848.6592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.6588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4954.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(946.1713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.4503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1451],
        [ 0.1303],
        [ 0.1104],
        ...,
        [-2.8972],
        [-2.8925],
        [-2.8914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291115.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0203],
        [1.0202],
        ...,
        [1.0015],
        [1.0007],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368922.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0183],
        [1.0204],
        [1.0203],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368929.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2991.1541, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.5537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0872, device='cuda:0')



h[100].sum tensor(102.7435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.1264, device='cuda:0')



h[200].sum tensor(41.1650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0148, 0.0117, 0.0000,  ..., 0.0273, 0.0000, 0.0076],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66906.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0743, 0.0141,  ..., 0.0000, 0.0207, 0.0000],
        [0.0208, 0.0821, 0.0463,  ..., 0.0024, 0.0308, 0.0000],
        [0.0437, 0.0963, 0.1051,  ..., 0.0113, 0.0500, 0.0000],
        ...,
        [0.0012, 0.0723, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0012, 0.0723, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0012, 0.0723, 0.0000,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527009.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3577.9292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.0682, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4648.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1085.5156, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-698.4321, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9445],
        [-0.4816],
        [-0.1468],
        ...,
        [-2.9069],
        [-2.9019],
        [-2.9002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236796.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0183],
        [1.0204],
        [1.0203],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368929.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0183],
        [1.0205],
        [1.0204],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368937.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0185,  0.0144, -0.0007,  ...,  0.0310, -0.0025,  0.0111],
        [ 0.0106,  0.0083, -0.0007,  ...,  0.0181, -0.0014,  0.0060],
        [ 0.0151,  0.0118, -0.0007,  ...,  0.0255, -0.0020,  0.0089],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0092,  0.0072, -0.0007,  ...,  0.0159, -0.0012,  0.0051],
        [ 0.0092,  0.0072, -0.0007,  ...,  0.0159, -0.0012,  0.0051]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2629.0779, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0431, device='cuda:0')



h[100].sum tensor(101.0734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.0255, device='cuda:0')



h[200].sum tensor(33.9439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0265, 0.0208, 0.0000,  ..., 0.0464, 0.0000, 0.0144],
        [0.0565, 0.0441, 0.0000,  ..., 0.0955, 0.0000, 0.0331],
        [0.0326, 0.0255, 0.0000,  ..., 0.0564, 0.0000, 0.0184],
        ...,
        [0.0178, 0.0140, 0.0000,  ..., 0.0322, 0.0000, 0.0095],
        [0.0178, 0.0140, 0.0000,  ..., 0.0322, 0.0000, 0.0095],
        [0.0178, 0.0140, 0.0000,  ..., 0.0323, 0.0000, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57038.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0698, 0.1129, 0.1774,  ..., 0.0253, 0.0703, 0.0000],
        [0.0848, 0.1228, 0.2193,  ..., 0.0344, 0.0824, 0.0000],
        [0.0697, 0.1134, 0.1774,  ..., 0.0255, 0.0704, 0.0000],
        ...,
        [0.0315, 0.0912, 0.0729,  ..., 0.0059, 0.0395, 0.0000],
        [0.0379, 0.0951, 0.0899,  ..., 0.0079, 0.0449, 0.0000],
        [0.0379, 0.0951, 0.0899,  ..., 0.0079, 0.0449, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(484391.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2789.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.6392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5161.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(945.1507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-585.3127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0513],
        [ 0.0650],
        [ 0.0534],
        ...,
        [-1.5647],
        [-1.2480],
        [-1.2472]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305637.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0183],
        [1.0205],
        [1.0204],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368937.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0184],
        [1.0205],
        [1.0205],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368944.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2444.8989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5039, device='cuda:0')



h[100].sum tensor(100.3101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.4237, device='cuda:0')



h[200].sum tensor(30.0925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53642.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0707, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0012, 0.0712, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0044, 0.0731, 0.0044,  ..., 0.0000, 0.0171, 0.0000],
        ...,
        [0.0010, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0010, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0010, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474525., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2598.8013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(210.2485, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5296.8721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(898.2602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-546.4275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6158],
        [-2.2998],
        [-1.8326],
        ...,
        [-2.9450],
        [-2.9401],
        [-2.9389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324316.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0184],
        [1.0205],
        [1.0205],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368944.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0185],
        [1.0206],
        [1.0206],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368952.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0048,  0.0037, -0.0006,  ...,  0.0085, -0.0006,  0.0022],
        [ 0.0078,  0.0061, -0.0007,  ...,  0.0134, -0.0010,  0.0042],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2629.4563, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0877, device='cuda:0')



h[100].sum tensor(101.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1587, device='cuda:0')



h[200].sum tensor(33.2200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0437, 0.0341, 0.0000,  ..., 0.0745, 0.0000, 0.0247],
        [0.0158, 0.0124, 0.0000,  ..., 0.0289, 0.0000, 0.0082],
        [0.0086, 0.0068, 0.0000,  ..., 0.0171, 0.0000, 0.0043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59557.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0780, 0.0272,  ..., 0.0011, 0.0235, 0.0000],
        [0.0033, 0.0725, 0.0029,  ..., 0.0000, 0.0156, 0.0000],
        [0.0010, 0.0714, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        ...,
        [0.0764, 0.1196, 0.1942,  ..., 0.0293, 0.0756, 0.0000],
        [0.0496, 0.1028, 0.1221,  ..., 0.0158, 0.0539, 0.0000],
        [0.0258, 0.0882, 0.0583,  ..., 0.0060, 0.0343, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504316.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3080.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.7867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5165.7915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(980.6603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-609.8439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0154],
        [-1.6613],
        [-2.2182],
        ...,
        [-0.1282],
        [-0.5595],
        [-1.2690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301880.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0185],
        [1.0206],
        [1.0206],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368952.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0207],
        [1.0207],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368959.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2340.3677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.6431, device='cuda:0')



h[100].sum tensor(99.8364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.8504, device='cuda:0')



h[200].sum tensor(27.6560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52097.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0707, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0010, 0.0711, 0.0000,  ..., 0.0000, 0.0135, 0.0000],
        [0.0010, 0.0713, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        ...,
        [0.0087, 0.0774, 0.0145,  ..., 0.0000, 0.0206, 0.0000],
        [0.0036, 0.0744, 0.0030,  ..., 0.0000, 0.0162, 0.0000],
        [0.0011, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470134.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2578.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(197.1171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5376.9033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(875.9072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-528.1495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5248],
        [-2.2852],
        [-1.9078],
        ...,
        [-2.1043],
        [-2.4691],
        [-2.7454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311485.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0207],
        [1.0207],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368959.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0187],
        [1.0208],
        [1.0208],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368966.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0136,  0.0106, -0.0007,  ...,  0.0230, -0.0018,  0.0080]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3540.2935, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(40.4827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.7550, device='cuda:0')



h[100].sum tensor(106.3278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(53.0814, device='cuda:0')



h[200].sum tensor(49.6771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9803, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0147, 0.0115, 0.0000,  ..., 0.0270, 0.0000, 0.0083],
        [0.0261, 0.0204, 0.0000,  ..., 0.0457, 0.0000, 0.0149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78191.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0708, 0.0000,  ..., 0.0000, 0.0134, 0.0000],
        [0.0061, 0.0742, 0.0095,  ..., 0.0000, 0.0178, 0.0000],
        [0.0230, 0.0848, 0.0537,  ..., 0.0061, 0.0316, 0.0000],
        ...,
        [0.0107, 0.0790, 0.0220,  ..., 0.0006, 0.0217, 0.0000],
        [0.0354, 0.0946, 0.0862,  ..., 0.0115, 0.0416, 0.0000],
        [0.0655, 0.1135, 0.1688,  ..., 0.0268, 0.0657, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579044., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4230.3062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(420.3983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4848.4990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1238.7023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-810.9248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1472],
        [-1.5944],
        [-0.8849],
        ...,
        [-2.1545],
        [-1.4830],
        [-0.8033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273884.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0187],
        [1.0208],
        [1.0208],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368966.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(452.4475, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0188],
        [1.0209],
        [1.0209],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368974.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2304.8313, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.2483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.3307, device='cuda:0')



h[100].sum tensor(99.9902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.9164, device='cuda:0')



h[200].sum tensor(26.7815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0059, 0.0000,  ..., 0.0153, 0.0000, 0.0029],
        [0.0042, 0.0033, 0.0000,  ..., 0.0097, 0.0000, 0.0015],
        [0.0312, 0.0244, 0.0000,  ..., 0.0540, 0.0000, 0.0182],
        ...,
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50826.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0204, 0.0804, 0.0408,  ..., 0.0009, 0.0315, 0.0000],
        [0.0301, 0.0876, 0.0682,  ..., 0.0062, 0.0387, 0.0000],
        [0.0671, 0.1118, 0.1708,  ..., 0.0260, 0.0678, 0.0000],
        ...,
        [0.0011, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0011, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0011, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466989.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2445.8989, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(188.8179, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5591.7080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(853.3239, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-509.6337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1223],
        [-0.5468],
        [-0.1279],
        ...,
        [-3.0056],
        [-3.0004],
        [-2.9992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-348712.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0188],
        [1.0209],
        [1.0209],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368974.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0189],
        [1.0210],
        [1.0210],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368981.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3377.7224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.7318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3795, device='cuda:0')



h[100].sum tensor(105.8304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.9693, device='cuda:0')



h[200].sum tensor(46.5016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0150, 0.0000,  ..., 0.0344, 0.0000, 0.0105],
        [0.0200, 0.0156, 0.0000,  ..., 0.0356, 0.0000, 0.0109],
        [0.0073, 0.0058, 0.0000,  ..., 0.0149, 0.0000, 0.0035],
        ...,
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72475.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.0948, 0.0993,  ..., 0.0100, 0.0478, 0.0000],
        [0.0421, 0.0956, 0.1003,  ..., 0.0102, 0.0480, 0.0000],
        [0.0271, 0.0867, 0.0605,  ..., 0.0044, 0.0357, 0.0000],
        ...,
        [0.0010, 0.0729, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0010, 0.0729, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0010, 0.0729, 0.0000,  ..., 0.0000, 0.0140, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554567.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3892.4624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(367.5325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4909.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1159.2407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-751.3016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7543],
        [-0.8411],
        [-1.2229],
        ...,
        [-3.0216],
        [-3.0164],
        [-3.0149]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253267.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0189],
        [1.0210],
        [1.0210],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368981.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0211],
        [1.0212],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368988.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0047, -0.0007,  ...,  0.0106, -0.0008,  0.0030],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3306.7144, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.3112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5269, device='cuda:0')



h[100].sum tensor(105.6318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.4201, device='cuda:0')



h[200].sum tensor(45.5876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0203, 0.0159, 0.0000,  ..., 0.0362, 0.0000, 0.0111],
        [0.0067, 0.0053, 0.0000,  ..., 0.0139, 0.0000, 0.0031],
        [0.0007, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74956.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0490, 0.0997, 0.1198,  ..., 0.0140, 0.0537, 0.0000],
        [0.0318, 0.0897, 0.0739,  ..., 0.0057, 0.0395, 0.0000],
        [0.0251, 0.0862, 0.0568,  ..., 0.0023, 0.0336, 0.0000],
        ...,
        [0.0009, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0009, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0009, 0.0729, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576306.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4031.5884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(394.3867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5084.4409, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1187.9515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-773.8268, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0234],
        [-0.1729],
        [-0.3189],
        ...,
        [-3.0183],
        [-3.0129],
        [-3.0114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303926.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0211],
        [1.0212],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368988.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0212],
        [1.0213],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368996.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0110,  0.0086, -0.0007,  ...,  0.0188, -0.0014,  0.0062],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2678.7690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2501, device='cuda:0')



h[100].sum tensor(102.4364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.6445, device='cuda:0')



h[200].sum tensor(34.3941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0511, 0.0399, 0.0000,  ..., 0.0867, 0.0000, 0.0303],
        [0.0193, 0.0151, 0.0000,  ..., 0.0346, 0.0000, 0.0105],
        [0.0007, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60030.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1269, 0.1498, 0.3386,  ..., 0.0616, 0.1151, 0.0000],
        [0.0895, 0.1266, 0.2343,  ..., 0.0391, 0.0853, 0.0000],
        [0.0631, 0.1102, 0.1621,  ..., 0.0242, 0.0641, 0.0000],
        ...,
        [0.0009, 0.0728, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0009, 0.0728, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0009, 0.0728, 0.0000,  ..., 0.0000, 0.0142, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504293.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3119.2654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(257.7303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4995.7393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(987.3694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.2880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1133],
        [ 0.1158],
        [ 0.1174],
        ...,
        [-3.0356],
        [-3.0304],
        [-3.0291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267065.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0212],
        [1.0213],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368996.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0213],
        [1.0214],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369003.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3341.1721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.2583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.7868, device='cuda:0')



h[100].sum tensor(106.3623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.1972, device='cuda:0')



h[200].sum tensor(46.5599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76449.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0710, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0009, 0.0713, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0007, 0.0714, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        ...,
        [0.0007, 0.0730, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0007, 0.0730, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0007, 0.0730, 0.0000,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591541.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4174.2905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(404.4358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4914.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1209.9781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-792.9641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7504],
        [-2.1423],
        [-2.4924],
        ...,
        [-3.0560],
        [-3.0507],
        [-3.0493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310963.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0213],
        [1.0214],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369003.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0214],
        [1.0215],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369010.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0039, -0.0006,  ...,  0.0088, -0.0006,  0.0023],
        [ 0.0134,  0.0105, -0.0007,  ...,  0.0227, -0.0017,  0.0078],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2693.2627, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.2579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4641, device='cuda:0')



h[100].sum tensor(103.3055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2841, device='cuda:0')



h[200].sum tensor(34.8058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1671, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0723, 0.0565, 0.0000,  ..., 0.1216, 0.0000, 0.0433],
        [0.0204, 0.0161, 0.0000,  ..., 0.0368, 0.0000, 0.0113],
        [0.0141, 0.0111, 0.0000,  ..., 0.0264, 0.0000, 0.0080],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58492.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1050, 0.1363, 0.2784,  ..., 0.0477, 0.0984, 0.0000],
        [0.0612, 0.1089, 0.1574,  ..., 0.0222, 0.0635, 0.0000],
        [0.0348, 0.0928, 0.0858,  ..., 0.0100, 0.0422, 0.0000],
        ...,
        [0.0006, 0.0731, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0006, 0.0731, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0006, 0.0731, 0.0000,  ..., 0.0000, 0.0140, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(497119.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2826.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.5498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4966.5850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(966.0260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-604.2734, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0769],
        [-0.1885],
        [-0.6668],
        ...,
        [-3.0569],
        [-3.0631],
        [-3.0639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299382.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0214],
        [1.0215],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369010.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0215],
        [1.0216],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369018.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0075,  0.0059, -0.0007,  ...,  0.0131, -0.0009,  0.0040],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3042.3721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.4703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.4472, device='cuda:0')



h[100].sum tensor(105.4492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.2027, device='cuda:0')



h[200].sum tensor(41.3534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0276, 0.0217, 0.0000,  ..., 0.0486, 0.0000, 0.0144],
        [0.0067, 0.0054, 0.0000,  ..., 0.0143, 0.0000, 0.0032],
        [0.0257, 0.0202, 0.0000,  ..., 0.0455, 0.0000, 0.0147],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63670.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0310, 0.0896, 0.0738,  ..., 0.0039, 0.0396, 0.0000],
        [0.0341, 0.0923, 0.0843,  ..., 0.0080, 0.0416, 0.0000],
        [0.0644, 0.1118, 0.1691,  ..., 0.0254, 0.0658, 0.0000],
        ...,
        [0.0005, 0.0733, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0005, 0.0733, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0005, 0.0733, 0.0000,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511016.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2950.7605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.3655, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4895.2378, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1037.3411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-660.0225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3967],
        [-0.9088],
        [-0.3968],
        ...,
        [-3.0871],
        [-3.0816],
        [-3.0802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307384.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0215],
        [1.0216],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369018.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0216],
        [1.0217],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369026.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0052, -0.0007,  ...,  0.0117, -0.0008,  0.0034],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2612.7139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5879, device='cuda:0')



h[100].sum tensor(102.9889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.6647, device='cuda:0')



h[200].sum tensor(34.0514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0209, 0.0165, 0.0000,  ..., 0.0376, 0.0000, 0.0116],
        [0.0072, 0.0058, 0.0000,  ..., 0.0153, 0.0000, 0.0035],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58005.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0769, 0.1189, 0.2025,  ..., 0.0312, 0.0760, 0.0000],
        [0.0431, 0.0982, 0.1092,  ..., 0.0144, 0.0488, 0.0000],
        [0.0245, 0.0868, 0.0609,  ..., 0.0051, 0.0334, 0.0000],
        ...,
        [0.0005, 0.0732, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0005, 0.0732, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0005, 0.0732, 0.0000,  ..., 0.0000, 0.0142, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496809.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2752.2583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(237.9663, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4938.2041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(959.7939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-601.1545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.8986e-04],
        [-3.8005e-02],
        [-6.9449e-02],
        ...,
        [-3.0746e+00],
        [-3.0690e+00],
        [-3.0674e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301691.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0216],
        [1.0217],
        ...,
        [1.0015],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369026.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0217],
        [1.0218],
        ...,
        [1.0014],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369033.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2863.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.3327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5400, device='cuda:0')



h[100].sum tensor(104.1385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.5006, device='cuda:0')



h[200].sum tensor(39.0155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0121, 0.0097, 0.0000,  ..., 0.0233, 0.0000, 0.0059],
        [0.0040, 0.0034, 0.0000,  ..., 0.0101, 0.0000, 0.0015],
        [0.0069, 0.0056, 0.0000,  ..., 0.0148, 0.0000, 0.0033],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63759.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0313, 0.0885, 0.0736,  ..., 0.0038, 0.0414, 0.0000],
        [0.0267, 0.0863, 0.0614,  ..., 0.0016, 0.0375, 0.0000],
        [0.0294, 0.0884, 0.0688,  ..., 0.0024, 0.0396, 0.0000],
        ...,
        [0.0005, 0.0731, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0005, 0.0731, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0005, 0.0731, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522427.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3151.1606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(286.1745, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4749.4028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1040.1312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-665.2613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3331],
        [-0.1741],
        [-0.0841],
        ...,
        [-3.0894],
        [-3.0841],
        [-3.0827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284506.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0217],
        [1.0218],
        ...,
        [1.0014],
        [1.0006],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369033.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0218],
        [1.0219],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369041.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0051,  0.0041, -0.0006,  ...,  0.0093, -0.0006,  0.0024],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2996.4165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.2682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5439, device='cuda:0')



h[100].sum tensor(104.8078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5019, device='cuda:0')



h[200].sum tensor(41.7170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3991, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0143, 0.0114, 0.0000,  ..., 0.0271, 0.0000, 0.0073],
        [0.0189, 0.0150, 0.0000,  ..., 0.0345, 0.0000, 0.0094],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64829.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0342, 0.0915, 0.0839,  ..., 0.0053, 0.0428, 0.0000],
        [0.0399, 0.0948, 0.0973,  ..., 0.0063, 0.0484, 0.0000],
        [0.0506, 0.1012, 0.1252,  ..., 0.0115, 0.0577, 0.0000],
        ...,
        [0.0004, 0.0732, 0.0000,  ..., 0.0000, 0.0143, 0.0000],
        [0.0004, 0.0732, 0.0000,  ..., 0.0000, 0.0143, 0.0000],
        [0.0004, 0.0732, 0.0000,  ..., 0.0000, 0.0143, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(522340.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3110.9614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.6409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4710.0576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1053.5406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-676.5816, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1139],
        [ 0.1196],
        [ 0.1332],
        ...,
        [-3.1009],
        [-3.0955],
        [-3.0941]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299766.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0218],
        [1.0219],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369041.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(501.4519, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0196],
        [1.0219],
        [1.0220],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369048.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0256,  0.0200, -0.0008,  ...,  0.0428, -0.0032,  0.0157],
        [ 0.0173,  0.0136, -0.0007,  ...,  0.0292, -0.0021,  0.0103],
        [ 0.0092,  0.0073, -0.0007,  ...,  0.0160, -0.0011,  0.0051],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2832.4717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.5817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1452, device='cuda:0')



h[100].sum tensor(103.6843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.3204, device='cuda:0')



h[200].sum tensor(38.9819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2431, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0791, 0.0619, 0.0000,  ..., 0.1330, 0.0000, 0.0476],
        [0.0781, 0.0612, 0.0000,  ..., 0.1315, 0.0000, 0.0470],
        [0.0388, 0.0305, 0.0000,  ..., 0.0671, 0.0000, 0.0231],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61891.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1516, 0.1667, 0.4127,  ..., 0.0753, 0.1357, 0.0000],
        [0.1428, 0.1616, 0.3878,  ..., 0.0697, 0.1287, 0.0000],
        [0.1022, 0.1361, 0.2742,  ..., 0.0455, 0.0964, 0.0000],
        ...,
        [0.0005, 0.0732, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0005, 0.0732, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0005, 0.0732, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511589.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2960.8374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(270.1815, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4809.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1013.0764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-647.1025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1224],
        [ 0.1135],
        [ 0.0823],
        ...,
        [-3.1043],
        [-3.0990],
        [-3.0976]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292329.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0196],
        [1.0219],
        [1.0220],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369048.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0220],
        [1.0221],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369056.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2984.1123, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.0577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5035, device='cuda:0')



h[100].sum tensor(104.2108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.3813, device='cuda:0')



h[200].sum tensor(41.7596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64179.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0004, 0.0710, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0004, 0.0714, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0005, 0.0716, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        ...,
        [0.0005, 0.0732, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0005, 0.0731, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0005, 0.0731, 0.0000,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(519229.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3073.1414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.8058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4844.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1042.1227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-669.8631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1347],
        [-3.0083],
        [-2.7903],
        ...,
        [-3.1098],
        [-3.1046],
        [-3.1032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287895.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0220],
        [1.0221],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369056.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0199],
        [1.0220],
        [1.0222],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369063.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2635.6255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3703, device='cuda:0')



h[100].sum tensor(102.2966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0140, device='cuda:0')



h[200].sum tensor(35.2600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58382.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0757, 0.0161,  ..., 0.0000, 0.0206, 0.0000],
        [0.0011, 0.0718, 0.0004,  ..., 0.0000, 0.0147, 0.0000],
        [0.0004, 0.0716, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        ...,
        [0.0004, 0.0732, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0004, 0.0732, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0004, 0.0731, 0.0000,  ..., 0.0000, 0.0146, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498207.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2732.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(244.3637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5031.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(960.9945, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-605.7522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5976],
        [-2.0903],
        [-2.3722],
        ...,
        [-3.1185],
        [-3.1129],
        [-3.1113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299371.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0199],
        [1.0220],
        [1.0222],
        ...,
        [1.0014],
        [1.0005],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369063.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0200],
        [1.0221],
        [1.0223],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369071.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0070, -0.0007,  ...,  0.0155, -0.0011,  0.0049],
        [ 0.0104,  0.0082, -0.0007,  ...,  0.0179, -0.0013,  0.0058],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3081.2603, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.4763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0573, device='cuda:0')



h[100].sum tensor(104.6028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.0368, device='cuda:0')



h[200].sum tensor(43.0531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0902, 0.0706, 0.0000,  ..., 0.1510, 0.0000, 0.0548],
        [0.0320, 0.0252, 0.0000,  ..., 0.0557, 0.0000, 0.0187],
        [0.0111, 0.0089, 0.0000,  ..., 0.0216, 0.0000, 0.0060],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66306.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1088e-01, 2.0460e-01, 5.8912e-01,  ..., 1.1579e-01, 1.8251e-01,
         0.0000e+00],
        [1.2285e-01, 1.4922e-01, 3.3810e-01,  ..., 6.0824e-02, 1.1254e-01,
         0.0000e+00],
        [6.2535e-02, 1.1125e-01, 1.6695e-01,  ..., 2.5379e-02, 6.4584e-02,
         0.0000e+00],
        ...,
        [3.7071e-04, 7.3219e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4655e-02,
         0.0000e+00],
        [3.7077e-04, 7.3213e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4654e-02,
         0.0000e+00],
        [3.7041e-04, 7.3203e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4651e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527432., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3152.0627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.0547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4937.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1071.2383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-691.7307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1074],
        [-0.0849],
        [-0.0868],
        ...,
        [-3.1176],
        [-3.1125],
        [-3.1120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281596., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0200],
        [1.0221],
        [1.0223],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369071.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0201],
        [1.0222],
        [1.0224],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369078.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2797.3257, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.7475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7069, device='cuda:0')



h[100].sum tensor(103.3396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.0100, device='cuda:0')



h[200].sum tensor(37.5004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0097, 0.0078, 0.0000,  ..., 0.0192, 0.0000, 0.0042],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62486.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0336, 0.0920, 0.0857,  ..., 0.0089, 0.0419, 0.0000],
        [0.0473, 0.1011, 0.1236,  ..., 0.0153, 0.0531, 0.0000],
        [0.0641, 0.1113, 0.1684,  ..., 0.0224, 0.0673, 0.0000],
        ...,
        [0.0003, 0.0734, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0003, 0.0733, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0003, 0.0733, 0.0000,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516044.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2963.4556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(278.6976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5038.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1018.5718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-649.3512, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-9.6018e-02],
        [-3.5919e-02],
        [-2.3290e-03],
        ...,
        [-3.1531e+00],
        [-3.1474e+00],
        [-3.1457e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275010.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0201],
        [1.0222],
        [1.0224],
        ...,
        [1.0014],
        [1.0005],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369078.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0223],
        [1.0225],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369086.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0177,  0.0139, -0.0007,  ...,  0.0298, -0.0021,  0.0106],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2631.8452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.8702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4088, device='cuda:0')



h[100].sum tensor(102.7513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1291, device='cuda:0')



h[200].sum tensor(34.0190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0494, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0186, 0.0147, 0.0000,  ..., 0.0337, 0.0000, 0.0108],
        [0.0153, 0.0121, 0.0000,  ..., 0.0284, 0.0000, 0.0087],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56339.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0095, 0.0774, 0.0234,  ..., 0.0000, 0.0217, 0.0000],
        [0.0326, 0.0927, 0.0853,  ..., 0.0095, 0.0405, 0.0000],
        [0.0461, 0.1014, 0.1230,  ..., 0.0155, 0.0514, 0.0000],
        ...,
        [0.0002, 0.0735, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0002, 0.0735, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0002, 0.0735, 0.0000,  ..., 0.0000, 0.0146, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486774.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2434.8604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.0318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5396.3701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(931.2139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-580.0942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4499],
        [-1.8878],
        [-1.3255],
        ...,
        [-3.1721],
        [-3.1667],
        [-3.1655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302477.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0223],
        [1.0225],
        ...,
        [1.0013],
        [1.0004],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369086.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0224],
        [1.0226],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369093.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0148,  0.0116, -0.0007,  ...,  0.0250, -0.0018,  0.0087],
        [ 0.0116,  0.0091, -0.0007,  ...,  0.0198, -0.0014,  0.0066],
        [ 0.0319,  0.0249, -0.0008,  ...,  0.0530, -0.0039,  0.0198],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2947.8835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.8227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7957, device='cuda:0')



h[100].sum tensor(104.4930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2650, device='cuda:0')



h[200].sum tensor(39.5341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3157, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0339, 0.0266, 0.0000,  ..., 0.0587, 0.0000, 0.0199],
        [0.0910, 0.0711, 0.0000,  ..., 0.1522, 0.0000, 0.0553],
        [0.0854, 0.0668, 0.0000,  ..., 0.1431, 0.0000, 0.0517],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63144.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.0123e-02, 1.2884e-01, 2.4763e-01,  ..., 4.2261e-02, 8.6388e-02,
         0.0000e+00],
        [1.5521e-01, 1.7051e-01, 4.3141e-01,  ..., 8.1526e-02, 1.3836e-01,
         0.0000e+00],
        [1.8010e-01, 1.8645e-01, 5.0066e-01,  ..., 9.5762e-02, 1.5844e-01,
         0.0000e+00],
        ...,
        [1.6148e-04, 7.3566e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4669e-02,
         0.0000e+00],
        [1.6152e-04, 7.3559e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4667e-02,
         0.0000e+00],
        [1.6126e-04, 7.3550e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4664e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516499.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2817.6482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.2517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5354.1260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1022.4381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-649.3133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2420],
        [ 0.0291],
        [ 0.0926],
        ...,
        [-3.1852],
        [-3.1796],
        [-3.1781]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316893., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0224],
        [1.0226],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369093.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0225],
        [1.0228],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369101.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0088,  0.0069, -0.0007,  ...,  0.0152, -0.0010,  0.0048],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0047,  0.0037, -0.0006,  ...,  0.0085, -0.0005,  0.0021],
        ...,
        [ 0.0152,  0.0119, -0.0007,  ...,  0.0257, -0.0018,  0.0089],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3379.7590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.7524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1575, device='cuda:0')



h[100].sum tensor(106.6757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.3159, device='cuda:0')



h[200].sum tensor(47.4303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0325, 0.0256, 0.0000,  ..., 0.0566, 0.0000, 0.0183],
        [0.0250, 0.0197, 0.0000,  ..., 0.0443, 0.0000, 0.0125],
        [0.0135, 0.0108, 0.0000,  ..., 0.0255, 0.0000, 0.0059],
        ...,
        [0.0134, 0.0106, 0.0000,  ..., 0.0252, 0.0000, 0.0074],
        [0.0162, 0.0129, 0.0000,  ..., 0.0299, 0.0000, 0.0093],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70603.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0919, 0.1286, 0.2457,  ..., 0.0379, 0.0899, 0.0000],
        [0.0702, 0.1143, 0.1828,  ..., 0.0231, 0.0738, 0.0000],
        [0.0506, 0.1017, 0.1278,  ..., 0.0114, 0.0586, 0.0000],
        ...,
        [0.0353, 0.0961, 0.0914,  ..., 0.0087, 0.0435, 0.0000],
        [0.0274, 0.0911, 0.0697,  ..., 0.0063, 0.0371, 0.0000],
        [0.0086, 0.0790, 0.0204,  ..., 0.0000, 0.0218, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544086.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3261.5376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.5504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5277.8872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1126.0562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-730.3527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1639],
        [ 0.1763],
        [ 0.1777],
        ...,
        [-1.5191],
        [-1.9455],
        [-2.4662]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298757.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0225],
        [1.0228],
        ...,
        [1.0013],
        [1.0004],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369101.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0226],
        [1.0229],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369108.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2990.7676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.4329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.1332, device='cuda:0')



h[100].sum tensor(105.0188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.2741, device='cuda:0')



h[200].sum tensor(40.5141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65906.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0716, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0002, 0.0720, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0002, 0.0722, 0.0000,  ..., 0.0000, 0.0143, 0.0000],
        ...,
        [0.0002, 0.0738, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        [0.0002, 0.0738, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        [0.0002, 0.0738, 0.0000,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534334.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3152.8577, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(309.4066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5174.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1061.1718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-678.3802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3061],
        [-3.3094],
        [-3.2841],
        ...,
        [-3.1975],
        [-3.1921],
        [-3.1907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298825.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0226],
        [1.0229],
        ...,
        [1.0013],
        [1.0003],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369108.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0227],
        [1.0230],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369114.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3386.9810, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.6370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2146, device='cuda:0')



h[100].sum tensor(107.6111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.4866, device='cuda:0')



h[200].sum tensor(47.9033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6970, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72210.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0718, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0004, 0.0723, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0008, 0.0726, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        ...,
        [0.0003, 0.0740, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0003, 0.0740, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0003, 0.0740, 0.0000,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554011.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3460.4397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(361.5562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5047.2881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1147.0220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-746.2695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0994],
        [-2.9187],
        [-2.6468],
        ...,
        [-3.2121],
        [-3.2065],
        [-3.2050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288937.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0227],
        [1.0230],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369114.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(501.7591, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0229],
        [1.0231],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369121.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0079,  0.0062, -0.0007,  ...,  0.0138, -0.0009,  0.0042],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3189.6304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.2721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.7195, device='cuda:0')



h[100].sum tensor(107.2805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.0168, device='cuda:0')



h[200].sum tensor(44.4040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0252, 0.0198, 0.0000,  ..., 0.0448, 0.0000, 0.0143],
        [0.0087, 0.0069, 0.0000,  ..., 0.0177, 0.0000, 0.0044],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69373.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0745, 0.0071,  ..., 0.0000, 0.0169, 0.0000],
        [0.0146, 0.0815, 0.0340,  ..., 0.0006, 0.0261, 0.0000],
        [0.0278, 0.0899, 0.0678,  ..., 0.0054, 0.0376, 0.0000],
        ...,
        [0.0754, 0.1217, 0.1992,  ..., 0.0310, 0.0759, 0.0000],
        [0.0384, 0.0985, 0.0977,  ..., 0.0125, 0.0459, 0.0000],
        [0.0119, 0.0817, 0.0285,  ..., 0.0009, 0.0240, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552258.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3452.6108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.7635, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4865.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1108.5854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-714.7389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1552],
        [-0.6443],
        [-0.2411],
        ...,
        [-0.2927],
        [-0.9877],
        [-1.8757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290436.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0229],
        [1.0231],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369121.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0211],
        [1.0230],
        [1.0232],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369128.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2648.9668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3902, device='cuda:0')



h[100].sum tensor(104.8469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0735, device='cuda:0')



h[200].sum tensor(34.8408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0054, 0.0000,  ..., 0.0145, 0.0000, 0.0032],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56187.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0186, 0.0835, 0.0421,  ..., 0.0019, 0.0302, 0.0000],
        [0.0055, 0.0758, 0.0106,  ..., 0.0000, 0.0185, 0.0000],
        [0.0013, 0.0737, 0.0002,  ..., 0.0000, 0.0152, 0.0000],
        ...,
        [0.0003, 0.0743, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0003, 0.0743, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0003, 0.0743, 0.0000,  ..., 0.0000, 0.0142, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488620.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2479.5151, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.1640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5192.5015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(922.4127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-568.6768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1009],
        [-1.6375],
        [-1.9204],
        ...,
        [-3.2475],
        [-3.2417],
        [-3.2402]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330205.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0211],
        [1.0230],
        [1.0232],
        ...,
        [1.0012],
        [1.0003],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369128.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0231],
        [1.0233],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369134.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0065, -0.0007,  ...,  0.0145, -0.0010,  0.0045],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0235,  0.0184, -0.0008,  ...,  0.0393, -0.0028,  0.0143],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2999.0859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.1254, device='cuda:0')



h[100].sum tensor(106.7092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.2509, device='cuda:0')



h[200].sum tensor(41.2959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0059, 0.0000,  ..., 0.0155, 0.0000, 0.0036],
        [0.0466, 0.0365, 0.0000,  ..., 0.0797, 0.0000, 0.0264],
        [0.0235, 0.0185, 0.0000,  ..., 0.0419, 0.0000, 0.0132],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65031.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0638, 0.1120, 0.1647,  ..., 0.0230, 0.0666, 0.0000],
        [0.0872, 0.1269, 0.2276,  ..., 0.0354, 0.0861, 0.0000],
        [0.0854, 0.1260, 0.2231,  ..., 0.0349, 0.0843, 0.0000],
        ...,
        [0.0003, 0.0742, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0003, 0.0742, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0003, 0.0742, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525258.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3124.0989, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.8122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4948.8921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1047.2787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.0432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1751],
        [ 0.1823],
        [ 0.1866],
        ...,
        [-3.2550],
        [-3.2492],
        [-3.2477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281756.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0231],
        [1.0233],
        ...,
        [1.0012],
        [1.0002],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369134.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0232],
        [1.0234],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369142.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0112,  0.0088, -0.0007,  ...,  0.0191, -0.0013,  0.0063],
        [ 0.0071,  0.0056, -0.0007,  ...,  0.0125, -0.0008,  0.0037],
        [ 0.0056,  0.0044, -0.0006,  ...,  0.0100, -0.0006,  0.0027],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2622.9312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.2063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8702, device='cuda:0')



h[100].sum tensor(104.7877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.5188, device='cuda:0')



h[200].sum tensor(34.9612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0355, 0.0278, 0.0000,  ..., 0.0615, 0.0000, 0.0193],
        [0.0323, 0.0254, 0.0000,  ..., 0.0564, 0.0000, 0.0172],
        [0.0123, 0.0097, 0.0000,  ..., 0.0235, 0.0000, 0.0059],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56071.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0611, 0.1096, 0.1530,  ..., 0.0174, 0.0663, 0.0000],
        [0.0508, 0.1034, 0.1255,  ..., 0.0122, 0.0578, 0.0000],
        [0.0292, 0.0902, 0.0684,  ..., 0.0043, 0.0400, 0.0000],
        ...,
        [0.0004, 0.0743, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0004, 0.0743, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0004, 0.0743, 0.0000,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489419.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2572.1807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.4550, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5182.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(920.0361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-567.6983, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0443],
        [-0.3303],
        [-0.9739],
        ...,
        [-3.2470],
        [-3.2421],
        [-3.2417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315263.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0232],
        [1.0234],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369142.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0232],
        [1.0234],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369142.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2970.2705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.7470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.6512, device='cuda:0')



h[100].sum tensor(106.5424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.8330, device='cuda:0')



h[200].sum tensor(41.1786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2995, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0249, 0.0196, 0.0000,  ..., 0.0441, 0.0000, 0.0140],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63932.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0052, 0.0753, 0.0098,  ..., 0.0000, 0.0183, 0.0000],
        [0.0195, 0.0847, 0.0464,  ..., 0.0040, 0.0300, 0.0000],
        [0.0632, 0.1124, 0.1629,  ..., 0.0236, 0.0657, 0.0000],
        ...,
        [0.0004, 0.0743, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0004, 0.0743, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0004, 0.0743, 0.0000,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(519320.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3038.5542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(283.2322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5022.1670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1029.7156, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-653.8239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5931],
        [-0.8577],
        [-0.2696],
        ...,
        [-3.2519],
        [-3.2463],
        [-3.2448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295439.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0232],
        [1.0234],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369142.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0215],
        [1.0234],
        [1.0235],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369149.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0047, -0.0006,  ...,  0.0106, -0.0007,  0.0029],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2666.2090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0904, device='cuda:0')



h[100].sum tensor(105.0450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1771, device='cuda:0')



h[200].sum tensor(36.0849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0157, 0.0000,  ..., 0.0359, 0.0000, 0.0100],
        [0.0157, 0.0124, 0.0000,  ..., 0.0290, 0.0000, 0.0081],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58714.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0438, 0.0981, 0.1058,  ..., 0.0078, 0.0528, 0.0000],
        [0.0342, 0.0929, 0.0810,  ..., 0.0050, 0.0445, 0.0000],
        [0.0144, 0.0810, 0.0307,  ..., 0.0000, 0.0274, 0.0000],
        ...,
        [0.0005, 0.0745, 0.0000,  ..., 0.0000, 0.0151, 0.0000],
        [0.0005, 0.0744, 0.0000,  ..., 0.0000, 0.0151, 0.0000],
        [0.0005, 0.0744, 0.0000,  ..., 0.0000, 0.0151, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505272.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2808.9146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(239.6383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5088.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(955.5078, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.6435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1757],
        [-0.4192],
        [-0.8457],
        ...,
        [-3.2495],
        [-3.2441],
        [-3.2426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317572.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0215],
        [1.0234],
        [1.0235],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369149.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0235],
        [1.0236],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369156.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3048.0105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.6182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0188, device='cuda:0')



h[100].sum tensor(107.1713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.9322, device='cuda:0')



h[200].sum tensor(43.2345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0063, 0.0000,  ..., 0.0162, 0.0000, 0.0030],
        [0.0121, 0.0097, 0.0000,  ..., 0.0233, 0.0000, 0.0058],
        [0.0165, 0.0131, 0.0000,  ..., 0.0304, 0.0000, 0.0086],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64901.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0293, 0.0890, 0.0677,  ..., 0.0010, 0.0414, 0.0000],
        [0.0322, 0.0916, 0.0757,  ..., 0.0027, 0.0433, 0.0000],
        [0.0355, 0.0943, 0.0852,  ..., 0.0044, 0.0456, 0.0000],
        ...,
        [0.0005, 0.0747, 0.0000,  ..., 0.0000, 0.0155, 0.0000],
        [0.0005, 0.0747, 0.0000,  ..., 0.0000, 0.0155, 0.0000],
        [0.0005, 0.0747, 0.0000,  ..., 0.0000, 0.0155, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524331.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3090.9192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.7993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5073.0562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1043.7711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-661.8423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0455],
        [-0.1489],
        [-0.3110],
        ...,
        [-3.2474],
        [-3.2422],
        [-3.2407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294051.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0235],
        [1.0236],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369156.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0235],
        [1.0237],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369163.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0069,  0.0055, -0.0007,  ...,  0.0122, -0.0008,  0.0036],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0210,  0.0164, -0.0007,  ...,  0.0352, -0.0024,  0.0127],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2808.7075, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8283, device='cuda:0')



h[100].sum tensor(106.2286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3834, device='cuda:0')



h[200].sum tensor(39.2225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0051, 0.0000,  ..., 0.0136, 0.0000, 0.0028],
        [0.0289, 0.0228, 0.0000,  ..., 0.0508, 0.0000, 0.0167],
        [0.0181, 0.0144, 0.0000,  ..., 0.0331, 0.0000, 0.0105],
        ...,
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58847.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0356, 0.0934, 0.0862,  ..., 0.0038, 0.0462, 0.0000],
        [0.0533, 0.1053, 0.1360,  ..., 0.0150, 0.0596, 0.0000],
        [0.0522, 0.1052, 0.1345,  ..., 0.0157, 0.0582, 0.0000],
        ...,
        [0.0004, 0.0750, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0004, 0.0750, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0004, 0.0750, 0.0000,  ..., 0.0000, 0.0160, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495351.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2667.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(232.2299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5126.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(963.3748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.5706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0349],
        [ 0.0513],
        [-0.0856],
        ...,
        [-3.2440],
        [-3.2389],
        [-3.2374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288306.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0235],
        [1.0237],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369163.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0235],
        [1.0237],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369163.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0052, -0.0007,  ...,  0.0117, -0.0008,  0.0034],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2809.6299, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8620, device='cuda:0')



h[100].sum tensor(106.2333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.4842, device='cuda:0')



h[200].sum tensor(39.2390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1000, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0117, 0.0000,  ..., 0.0273, 0.0000, 0.0075],
        [0.0073, 0.0059, 0.0000,  ..., 0.0153, 0.0000, 0.0035],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59929.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0513, 0.1035, 0.1293,  ..., 0.0148, 0.0583, 0.0000],
        [0.0233, 0.0869, 0.0542,  ..., 0.0031, 0.0356, 0.0000],
        [0.0074, 0.0773, 0.0133,  ..., 0.0000, 0.0222, 0.0000],
        ...,
        [0.0004, 0.0750, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0004, 0.0750, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0004, 0.0750, 0.0000,  ..., 0.0000, 0.0160, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501836.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2785.1697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.6014, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5068.2139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(979.2897, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.4353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2348],
        [-0.7158],
        [-1.3040],
        ...,
        [-3.2440],
        [-3.2389],
        [-3.2374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277966.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0235],
        [1.0237],
        ...,
        [1.0011],
        [1.0002],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369163.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0218],
        [1.0236],
        [1.0237],
        ...,
        [1.0011],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369170.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0049, -0.0007,  ...,  0.0109, -0.0007,  0.0031],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2711.8301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9468, device='cuda:0')



h[100].sum tensor(106.0798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7479, device='cuda:0')



h[200].sum tensor(37.4772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0153, 0.0123, 0.0000,  ..., 0.0286, 0.0000, 0.0080],
        [0.0067, 0.0055, 0.0000,  ..., 0.0145, 0.0000, 0.0032],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58213.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0400, 0.0969, 0.0983,  ..., 0.0073, 0.0496, 0.0000],
        [0.0240, 0.0877, 0.0565,  ..., 0.0015, 0.0363, 0.0000],
        [0.0143, 0.0821, 0.0315,  ..., 0.0000, 0.0283, 0.0000],
        ...,
        [0.0003, 0.0753, 0.0000,  ..., 0.0000, 0.0162, 0.0000],
        [0.0003, 0.0753, 0.0000,  ..., 0.0000, 0.0162, 0.0000],
        [0.0003, 0.0753, 0.0000,  ..., 0.0000, 0.0162, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496911.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2683.6680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.1647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5041.4121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(959.3371, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-592.8157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1173],
        [-0.2964],
        [-0.4157],
        ...,
        [-3.2517],
        [-3.2471],
        [-3.2462]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269854.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0218],
        [1.0236],
        [1.0237],
        ...,
        [1.0011],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369170.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(400.1007, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0219],
        [1.0238],
        [1.0238],
        ...,
        [1.0011],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369177.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0242,  0.0190, -0.0008,  ...,  0.0406, -0.0028,  0.0149],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0151,  0.0119, -0.0007,  ...,  0.0256, -0.0017,  0.0089],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3240.5889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.5104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2587, device='cuda:0')



h[100].sum tensor(109.4022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.6390, device='cuda:0')



h[200].sum tensor(46.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0676, 0.0531, 0.0000,  ..., 0.1143, 0.0000, 0.0411],
        [0.0805, 0.0631, 0.0000,  ..., 0.1354, 0.0000, 0.0487],
        [0.0191, 0.0152, 0.0000,  ..., 0.0349, 0.0000, 0.0105],
        ...,
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67412.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8763e-01, 1.8858e-01, 5.1206e-01,  ..., 9.8025e-02, 1.6585e-01,
         0.0000e+00],
        [1.5065e-01, 1.6617e-01, 4.0758e-01,  ..., 7.3967e-02, 1.3701e-01,
         0.0000e+00],
        [8.5795e-02, 1.2627e-01, 2.2621e-01,  ..., 3.3099e-02, 8.6249e-02,
         0.0000e+00],
        ...,
        [3.7201e-05, 7.5754e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6214e-02,
         0.0000e+00],
        [3.7451e-05, 7.5755e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6215e-02,
         0.0000e+00],
        [3.7142e-05, 7.5740e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6210e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531244.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3009.2170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.1714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5043.4150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1085.7455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-688.3754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0191],
        [ 0.0585],
        [ 0.0927],
        ...,
        [-3.2775],
        [-3.2723],
        [-3.2708]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294772.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0219],
        [1.0238],
        [1.0238],
        ...,
        [1.0011],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369177.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0220],
        [1.0239],
        [1.0239],
        ...,
        [1.0011],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369184.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0140,  0.0110, -0.0007,  ...,  0.0237, -0.0016,  0.0082],
        [ 0.0139,  0.0109, -0.0007,  ...,  0.0237, -0.0016,  0.0082],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2958.6860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9224, device='cuda:0')



h[100].sum tensor(108.3117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.6542, device='cuda:0')



h[200].sum tensor(41.2937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0117, 0.0000,  ..., 0.0274, 0.0000, 0.0083],
        [0.0263, 0.0208, 0.0000,  ..., 0.0465, 0.0000, 0.0151],
        [0.0798, 0.0626, 0.0000,  ..., 0.1342, 0.0000, 0.0482],
        ...,
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60808.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0342, 0.0950, 0.0891,  ..., 0.0095, 0.0440, 0.0000],
        [0.0649, 0.1142, 0.1731,  ..., 0.0241, 0.0685, 0.0000],
        [0.1173, 0.1463, 0.3182,  ..., 0.0555, 0.1102, 0.0000],
        ...,
        [0.0000, 0.0759, 0.0000,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0759, 0.0000,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0759, 0.0000,  ..., 0.0000, 0.0165, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505779.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2620.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.6796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5144.4600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(996.7617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.9358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9078],
        [-0.4817],
        [-0.1074],
        ...,
        [-3.2885],
        [-3.2833],
        [-3.2817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278211.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0220],
        [1.0239],
        [1.0239],
        ...,
        [1.0011],
        [1.0001],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369184.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0221],
        [1.0239],
        [1.0239],
        ...,
        [1.0011],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369191.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0090,  0.0071, -0.0007,  ...,  0.0156, -0.0010,  0.0050],
        [ 0.0295,  0.0231, -0.0008,  ...,  0.0492, -0.0033,  0.0183],
        [ 0.0139,  0.0109, -0.0007,  ...,  0.0235, -0.0016,  0.0081],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2669.5967, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5519, device='cuda:0')



h[100].sum tensor(107.0918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5673, device='cuda:0')



h[200].sum tensor(35.7498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0897, 0.0703, 0.0000,  ..., 0.1503, 0.0000, 0.0547],
        [0.0673, 0.0528, 0.0000,  ..., 0.1136, 0.0000, 0.0401],
        [0.0686, 0.0538, 0.0000,  ..., 0.1158, 0.0000, 0.0410],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56854.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1291, 0.1522, 0.3497,  ..., 0.0618, 0.1202, 0.0000],
        [0.1351, 0.1564, 0.3648,  ..., 0.0643, 0.1252, 0.0000],
        [0.1339, 0.1558, 0.3612,  ..., 0.0635, 0.1243, 0.0000],
        ...,
        [0.0000, 0.0758, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0758, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0758, 0.0000,  ..., 0.0000, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495153.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2358.3254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.6180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5403.7827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(939.1878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-570.8596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0593],
        [ 0.0694],
        [ 0.0585],
        ...,
        [-3.3060],
        [-3.3008],
        [-3.2993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304903.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0221],
        [1.0239],
        [1.0239],
        ...,
        [1.0011],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369191.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0222],
        [1.0240],
        [1.0239],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369198.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3425.7246, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.1840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6432, device='cuda:0')



h[100].sum tensor(110.9307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.7782, device='cuda:0')



h[200].sum tensor(48.8888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0057, 0.0000,  ..., 0.0147, 0.0000, 0.0034],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0253, 0.0200, 0.0000,  ..., 0.0448, 0.0000, 0.0145],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71488.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0351, 0.0942, 0.0886,  ..., 0.0041, 0.0465, 0.0000],
        [0.0441, 0.1005, 0.1153,  ..., 0.0117, 0.0532, 0.0000],
        [0.0981, 0.1337, 0.2654,  ..., 0.0442, 0.0958, 0.0000],
        ...,
        [0.0000, 0.0757, 0.0000,  ..., 0.0000, 0.0169, 0.0000],
        [0.0000, 0.0757, 0.0000,  ..., 0.0000, 0.0169, 0.0000],
        [0.0000, 0.0757, 0.0000,  ..., 0.0000, 0.0169, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557220.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3254.4644, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(336.1094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5231.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1140.8234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-727.0206, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0678],
        [ 0.0329],
        [-0.0052],
        ...,
        [-3.3174],
        [-3.3122],
        [-3.3106]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293964.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0222],
        [1.0240],
        [1.0239],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369198.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0240],
        [1.0240],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369205.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106,  0.0083, -0.0007,  ...,  0.0182, -0.0012,  0.0060],
        [ 0.0052,  0.0041, -0.0006,  ...,  0.0094, -0.0006,  0.0025],
        [ 0.0167,  0.0131, -0.0007,  ...,  0.0281, -0.0019,  0.0100],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3056.1543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.4609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.6679, device='cuda:0')



h[100].sum tensor(109.1111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.8830, device='cuda:0')



h[200].sum tensor(42.0430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3014, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0353, 0.0278, 0.0000,  ..., 0.0611, 0.0000, 0.0194],
        [0.0456, 0.0359, 0.0000,  ..., 0.0781, 0.0000, 0.0261],
        [0.0333, 0.0262, 0.0000,  ..., 0.0577, 0.0000, 0.0181],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62824.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0861, 0.1233, 0.2238,  ..., 0.0312, 0.0888, 0.0000],
        [0.0914, 0.1272, 0.2386,  ..., 0.0346, 0.0927, 0.0000],
        [0.0873, 0.1249, 0.2271,  ..., 0.0318, 0.0897, 0.0000],
        ...,
        [0.0000, 0.0755, 0.0000,  ..., 0.0000, 0.0171, 0.0000],
        [0.0000, 0.0755, 0.0000,  ..., 0.0000, 0.0171, 0.0000],
        [0.0000, 0.0755, 0.0000,  ..., 0.0000, 0.0171, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512375.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2601.8386, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.2010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5498.3877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1019.7620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-632.0421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1514],
        [ 0.1279],
        [ 0.0920],
        ...,
        [-3.3304],
        [-3.3259],
        [-3.3245]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299961.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0240],
        [1.0240],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369205.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0224],
        [1.0241],
        [1.0240],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369211.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0149,  0.0117, -0.0007,  ...,  0.0252, -0.0017,  0.0088],
        [ 0.0212,  0.0166, -0.0008,  ...,  0.0354, -0.0024,  0.0128],
        [ 0.0186,  0.0146, -0.0007,  ...,  0.0312, -0.0021,  0.0112],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3174.3130, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.4321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6230, device='cuda:0')



h[100].sum tensor(109.6527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.7385, device='cuda:0')



h[200].sum tensor(43.8601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4079, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0600, 0.0471, 0.0000,  ..., 0.1015, 0.0000, 0.0354],
        [0.0726, 0.0569, 0.0000,  ..., 0.1221, 0.0000, 0.0435],
        [0.0758, 0.0594, 0.0000,  ..., 0.1274, 0.0000, 0.0456],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63870.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1114, 0.1392, 0.2967,  ..., 0.0497, 0.1075, 0.0000],
        [0.1343, 0.1535, 0.3600,  ..., 0.0638, 0.1257, 0.0000],
        [0.1410, 0.1580, 0.3791,  ..., 0.0683, 0.1308, 0.0000],
        ...,
        [0.0018, 0.0763, 0.0023,  ..., 0.0000, 0.0189, 0.0000],
        [0.0027, 0.0768, 0.0047,  ..., 0.0000, 0.0197, 0.0000],
        [0.0018, 0.0763, 0.0023,  ..., 0.0000, 0.0189, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515467.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2636.3950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.9366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5559.7920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1032.3014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-641.2292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0922],
        [ 0.1146],
        [ 0.1119],
        ...,
        [-2.9544],
        [-2.8783],
        [-2.8765]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306414.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0224],
        [1.0241],
        [1.0240],
        ...,
        [1.0010],
        [1.0001],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369211.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0225],
        [1.0241],
        [1.0240],
        ...,
        [1.0010],
        [1.0000],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369218.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0304,  0.0237, -0.0008,  ...,  0.0505, -0.0034,  0.0188],
        [ 0.0118,  0.0093, -0.0007,  ...,  0.0202, -0.0013,  0.0068],
        [ 0.0163,  0.0128, -0.0007,  ...,  0.0275, -0.0018,  0.0097],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2950.0698, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7279, device='cuda:0')



h[100].sum tensor(108.4316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.0727, device='cuda:0')



h[200].sum tensor(39.5387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0597, 0.0467, 0.0000,  ..., 0.1008, 0.0000, 0.0351],
        [0.0735, 0.0575, 0.0000,  ..., 0.1235, 0.0000, 0.0440],
        [0.0668, 0.0523, 0.0000,  ..., 0.1125, 0.0000, 0.0397],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61184.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1329, 0.1527, 0.3606,  ..., 0.0665, 0.1236, 0.0000],
        [0.1316, 0.1522, 0.3559,  ..., 0.0649, 0.1229, 0.0000],
        [0.1202, 0.1452, 0.3216,  ..., 0.0561, 0.1143, 0.0000],
        ...,
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0172, 0.0000],
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0172, 0.0000],
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0172, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509777.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2551.5537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.3395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5705.5474, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(993.1348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.8358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1295],
        [ 0.1429],
        [ 0.1520],
        ...,
        [-3.3604],
        [-3.3551],
        [-3.3537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301118.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0225],
        [1.0241],
        [1.0240],
        ...,
        [1.0010],
        [1.0000],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369218.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0225],
        [1.0242],
        [1.0241],
        ...,
        [1.0010],
        [1.0000],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369224.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0167,  0.0130, -0.0007,  ...,  0.0280, -0.0018,  0.0099],
        [ 0.0138,  0.0108, -0.0007,  ...,  0.0233, -0.0015,  0.0080],
        [ 0.0164,  0.0129, -0.0007,  ...,  0.0277, -0.0018,  0.0098],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2762.5981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3390, device='cuda:0')



h[100].sum tensor(107.8367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.9205, device='cuda:0')



h[200].sum tensor(35.7101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0505, 0.0396, 0.0000,  ..., 0.0858, 0.0000, 0.0292],
        [0.0659, 0.0516, 0.0000,  ..., 0.1110, 0.0000, 0.0391],
        [0.0739, 0.0578, 0.0000,  ..., 0.1241, 0.0000, 0.0443],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57345.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1469, 0.1611, 0.3960,  ..., 0.0730, 0.1351, 0.0000],
        [0.1750, 0.1785, 0.4750,  ..., 0.0912, 0.1573, 0.0000],
        [0.1799, 0.1817, 0.4904,  ..., 0.0952, 0.1613, 0.0000],
        ...,
        [0.0000, 0.0751, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0000, 0.0751, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0170, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496082.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2295.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.9859, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5816.6938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(939.4427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-568.7806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0138],
        [-0.0254],
        [-0.0265],
        ...,
        [-3.3933],
        [-3.3880],
        [-3.3864]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301814.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0225],
        [1.0242],
        [1.0241],
        ...,
        [1.0010],
        [1.0000],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369224.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0242],
        [1.0241],
        ...,
        [1.0010],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369229.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0062,  0.0048, -0.0007,  ...,  0.0108, -0.0007,  0.0031],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3673.9973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.8953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9540, device='cuda:0')



h[100].sum tensor(113.2902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(50.6868, device='cuda:0')



h[200].sum tensor(51.2023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0141, 0.0111, 0.0000,  ..., 0.0262, 0.0000, 0.0072],
        [0.0067, 0.0054, 0.0000,  ..., 0.0141, 0.0000, 0.0032],
        [0.0005, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80011.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0347, 0.0931, 0.0873,  ..., 0.0071, 0.0471, 0.0000],
        [0.0178, 0.0841, 0.0432,  ..., 0.0015, 0.0331, 0.0000],
        [0.0064, 0.0776, 0.0138,  ..., 0.0000, 0.0229, 0.0000],
        ...,
        [0.0000, 0.0752, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0016, 0.0766, 0.0039,  ..., 0.0000, 0.0184, 0.0000],
        [0.0087, 0.0810, 0.0222,  ..., 0.0000, 0.0243, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604996.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3717.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.6636, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5524.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1248.8186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-810.4318, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4273],
        [-1.0243],
        [-1.6577],
        ...,
        [-3.3151],
        [-3.0727],
        [-2.6367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306963.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0242],
        [1.0241],
        ...,
        [1.0010],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369229.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0227],
        [1.0242],
        [1.0242],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369235.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2758.8340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4724, device='cuda:0')



h[100].sum tensor(109.6565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.3194, device='cuda:0')



h[200].sum tensor(34.7509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57554.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0733, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0737, 0.0000,  ..., 0.0000, 0.0155, 0.0000],
        [0.0000, 0.0739, 0.0000,  ..., 0.0000, 0.0156, 0.0000],
        ...,
        [0.0000, 0.0755, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0755, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0755, 0.0000,  ..., 0.0000, 0.0161, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501346.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2104.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.1773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5829.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(936.9887, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-566.9077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5877],
        [-3.6144],
        [-3.6140],
        ...,
        [-3.4678],
        [-3.4623],
        [-3.4608]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350117.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0227],
        [1.0242],
        [1.0242],
        ...,
        [1.0009],
        [1.0000],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369235.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(479.0418, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0228],
        [1.0243],
        [1.0243],
        ...,
        [1.0009],
        [0.9999],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369241.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0048,  0.0038, -0.0006,  ...,  0.0087, -0.0005,  0.0023],
        [ 0.0048,  0.0038, -0.0006,  ...,  0.0087, -0.0005,  0.0023],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2747.4646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.6870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3792, device='cuda:0')



h[100].sum tensor(109.7884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0406, device='cuda:0')



h[200].sum tensor(34.6851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0092, 0.0073, 0.0000,  ..., 0.0183, 0.0000, 0.0041],
        [0.0092, 0.0073, 0.0000,  ..., 0.0185, 0.0000, 0.0041],
        [0.0092, 0.0073, 0.0000,  ..., 0.0185, 0.0000, 0.0041],
        ...,
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58030.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0203, 0.0857, 0.0501,  ..., 0.0002, 0.0354, 0.0000],
        [0.0169, 0.0838, 0.0406,  ..., 0.0000, 0.0332, 0.0000],
        [0.0159, 0.0834, 0.0378,  ..., 0.0000, 0.0324, 0.0000],
        ...,
        [0.0000, 0.0758, 0.0000,  ..., 0.0000, 0.0159, 0.0000],
        [0.0000, 0.0758, 0.0000,  ..., 0.0000, 0.0159, 0.0000],
        [0.0000, 0.0758, 0.0000,  ..., 0.0000, 0.0159, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507879.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2111.2690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.8960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5839.9224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(941.1390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-571.3651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3393],
        [-1.8503],
        [-2.3127],
        ...,
        [-3.4856],
        [-3.4796],
        [-3.4777]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-376986.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0228],
        [1.0243],
        [1.0243],
        ...,
        [1.0009],
        [0.9999],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369241.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0244],
        [1.0243],
        ...,
        [1.0009],
        [0.9999],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369248.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3116.4597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.9267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0565, device='cuda:0')



h[100].sum tensor(110.3942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.0449, device='cuda:0')



h[200].sum tensor(42.3046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66127.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0199, 0.0861, 0.0520,  ..., 0.0047, 0.0337, 0.0000],
        [0.0185, 0.0853, 0.0482,  ..., 0.0040, 0.0321, 0.0000],
        [0.0149, 0.0835, 0.0391,  ..., 0.0027, 0.0291, 0.0000],
        ...,
        [0.0000, 0.0754, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0754, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0754, 0.0000,  ..., 0.0000, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539607.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2754.7046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.9995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5587.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1051.8029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-662.4928, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3407],
        [-0.3989],
        [-0.5151],
        ...,
        [-3.0889],
        [-3.0577],
        [-2.9347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329702.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0244],
        [1.0243],
        ...,
        [1.0009],
        [0.9999],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369248.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0244],
        [1.0244],
        ...,
        [1.0008],
        [0.9998],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369255.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0047,  0.0037, -0.0006,  ...,  0.0084, -0.0005,  0.0021],
        [ 0.0068,  0.0054, -0.0007,  ...,  0.0120, -0.0007,  0.0035],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3421.9233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.2531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1761, device='cuda:0')



h[100].sum tensor(110.4752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.3819, device='cuda:0')



h[200].sum tensor(48.8042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5812, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0383, 0.0301, 0.0000,  ..., 0.0658, 0.0000, 0.0212],
        [0.0146, 0.0116, 0.0000,  ..., 0.0270, 0.0000, 0.0074],
        [0.0075, 0.0060, 0.0000,  ..., 0.0154, 0.0000, 0.0036],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72160.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0667, 0.1127, 0.1731,  ..., 0.0214, 0.0726, 0.0000],
        [0.0427, 0.0988, 0.1094,  ..., 0.0105, 0.0532, 0.0000],
        [0.0206, 0.0861, 0.0511,  ..., 0.0027, 0.0350, 0.0000],
        ...,
        [0.0017, 0.0761, 0.0024,  ..., 0.0000, 0.0191, 0.0000],
        [0.0027, 0.0766, 0.0049,  ..., 0.0000, 0.0200, 0.0000],
        [0.0017, 0.0761, 0.0024,  ..., 0.0000, 0.0191, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559446.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3253.5991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.4735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5454.0557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1136.3722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-733.8586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0261],
        [-0.3173],
        [-0.8988],
        ...,
        [-3.0212],
        [-2.9571],
        [-3.0147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284168., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0244],
        [1.0244],
        ...,
        [1.0008],
        [0.9998],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369255.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0244],
        [1.0244],
        ...,
        [1.0008],
        [0.9998],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369255.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0084,  0.0066, -0.0007,  ...,  0.0146, -0.0009,  0.0046],
        [ 0.0147,  0.0115, -0.0007,  ...,  0.0249, -0.0016,  0.0087],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2953.5127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6239, device='cuda:0')



h[100].sum tensor(108.1513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.7618, device='cuda:0')



h[200].sum tensor(40.5883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0159, 0.0126, 0.0000,  ..., 0.0292, 0.0000, 0.0083],
        [0.0277, 0.0218, 0.0000,  ..., 0.0485, 0.0000, 0.0151],
        [0.0474, 0.0372, 0.0000,  ..., 0.0808, 0.0000, 0.0271],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60690.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0416, 0.0978, 0.1077,  ..., 0.0120, 0.0519, 0.0000],
        [0.0745, 0.1176, 0.1965,  ..., 0.0274, 0.0789, 0.0000],
        [0.1086, 0.1381, 0.2894,  ..., 0.0470, 0.1066, 0.0000],
        ...,
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0174, 0.0000],
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0174, 0.0000],
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0174, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507464.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2412.6060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(244.4986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5835.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(973.7342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-606.6995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7058],
        [-0.1773],
        [ 0.0968],
        ...,
        [-3.4221],
        [-3.4170],
        [-3.4155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326672.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0244],
        [1.0244],
        ...,
        [1.0008],
        [0.9998],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369255.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0245],
        [1.0245],
        ...,
        [1.0008],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369261.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0120,  0.0094, -0.0007,  ...,  0.0204, -0.0013,  0.0069],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0094,  0.0074, -0.0007,  ...,  0.0162, -0.0010,  0.0052],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2625.3618, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.7516, device='cuda:0')



h[100].sum tensor(105.4294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.1747, device='cuda:0')



h[200].sum tensor(35.8024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0197, 0.0156, 0.0000,  ..., 0.0353, 0.0000, 0.0099],
        [0.0422, 0.0331, 0.0000,  ..., 0.0721, 0.0000, 0.0236],
        [0.0590, 0.0463, 0.0000,  ..., 0.0997, 0.0000, 0.0353],
        ...,
        [0.0007, 0.0008, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55354.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.1777e-02, 1.2026e-01, 2.1274e-01,  ..., 2.8344e-02, 8.5849e-02,
         0.0000e+00],
        [1.2628e-01, 1.4784e-01, 3.3727e-01,  ..., 5.6469e-02, 1.2089e-01,
         0.0000e+00],
        [1.7642e-01, 1.7874e-01, 4.8051e-01,  ..., 9.0222e-02, 1.6005e-01,
         0.0000e+00],
        ...,
        [7.9185e-05, 7.4767e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7917e-02,
         0.0000e+00],
        [7.9699e-05, 7.4776e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7921e-02,
         0.0000e+00],
        [7.9389e-05, 7.4760e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7916e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(488575.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2292.3511, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(199.1338, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5785.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(899.2325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-552.3680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1371],
        [ 0.1139],
        [ 0.0751],
        ...,
        [-3.3927],
        [-3.3880],
        [-3.3867]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296766.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0245],
        [1.0245],
        ...,
        [1.0008],
        [0.9998],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369261.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0246],
        [1.0246],
        ...,
        [1.0007],
        [0.9997],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369268.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0087,  0.0069, -0.0007,  ...,  0.0150, -0.0009,  0.0047],
        [ 0.0059,  0.0047, -0.0006,  ...,  0.0105, -0.0006,  0.0029],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2796.9683, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0422, device='cuda:0')



h[100].sum tensor(105.8767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0330, device='cuda:0')



h[200].sum tensor(39.7906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0085, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0076, 0.0000,  ..., 0.0185, 0.0000, 0.0048],
        [0.0138, 0.0110, 0.0000,  ..., 0.0257, 0.0000, 0.0068],
        [0.0519, 0.0408, 0.0000,  ..., 0.0882, 0.0000, 0.0299],
        ...,
        [0.0008, 0.0009, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0009, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0009, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56473.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8317e-02, 8.9467e-02, 7.1332e-02,  ..., 4.7199e-03, 4.1436e-02,
         0.0000e+00],
        [5.1789e-02, 1.0368e-01, 1.3357e-01,  ..., 1.4364e-02, 6.0915e-02,
         0.0000e+00],
        [9.2392e-02, 1.2822e-01, 2.4354e-01,  ..., 3.5237e-02, 9.3613e-02,
         0.0000e+00],
        ...,
        [1.8552e-04, 7.4941e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8113e-02,
         0.0000e+00],
        [1.8607e-04, 7.4951e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8117e-02,
         0.0000e+00],
        [1.8570e-04, 7.4935e-02, 0.0000e+00,  ..., 0.0000e+00, 1.8112e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487328.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2334.0166, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.6875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5769.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(915.4620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-569.7487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1442],
        [ 0.0039],
        [ 0.0997],
        ...,
        [-3.3767],
        [-3.3718],
        [-3.3660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275148.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0246],
        [1.0246],
        ...,
        [1.0007],
        [0.9997],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369268.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0234],
        [1.0247],
        [1.0246],
        ...,
        [1.0007],
        [0.9997],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369273.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2782.3169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.7303, device='cuda:0')



h[100].sum tensor(106.0547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.1006, device='cuda:0')



h[200].sum tensor(40.1553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0008, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59913.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.3160e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7164e-02,
         0.0000e+00],
        [6.6190e-04, 7.4224e-02, 1.3670e-03,  ..., 0.0000e+00, 1.8159e-02,
         0.0000e+00],
        [8.3518e-03, 7.9000e-02, 2.1397e-02,  ..., 0.0000e+00, 2.4524e-02,
         0.0000e+00],
        ...,
        [1.7610e-05, 7.5390e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7934e-02,
         0.0000e+00],
        [1.8105e-05, 7.5399e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7938e-02,
         0.0000e+00],
        [1.7847e-05, 7.5384e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7932e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510879.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2663.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(228.8854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5387.5806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(966.7568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-613.5037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1085],
        [-1.7700],
        [-1.2252],
        ...,
        [-3.3869],
        [-3.3831],
        [-3.3823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256369.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0234],
        [1.0247],
        [1.0246],
        ...,
        [1.0007],
        [0.9997],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369273.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0236],
        [1.0247],
        [1.0247],
        ...,
        [1.0006],
        [0.9997],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369279.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2985.2451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.1560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5835, device='cuda:0')



h[100].sum tensor(107.7940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6411, device='cuda:0')



h[200].sum tensor(43.8893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1805, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0008, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0008, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0008, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62256.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0738, 0.0000,  ..., 0.0000, 0.0167, 0.0000],
        [0.0000, 0.0742, 0.0000,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0744, 0.0000,  ..., 0.0000, 0.0169, 0.0000],
        ...,
        [0.0000, 0.0760, 0.0000,  ..., 0.0000, 0.0175, 0.0000],
        [0.0000, 0.0760, 0.0000,  ..., 0.0000, 0.0175, 0.0000],
        [0.0000, 0.0760, 0.0000,  ..., 0.0000, 0.0175, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516302.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2566.2192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(245.9971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5296.9595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(998.8205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-641.4834, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3203],
        [-3.2663],
        [-3.1553],
        ...,
        [-3.4208],
        [-3.4158],
        [-3.4143]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280222.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0236],
        [1.0247],
        [1.0247],
        ...,
        [1.0006],
        [0.9997],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369279.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0248],
        [1.0247],
        ...,
        [1.0006],
        [0.9996],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369284.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2814.2988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.4797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0685, device='cuda:0')



h[100].sum tensor(107.7485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1119, device='cuda:0')



h[200].sum tensor(40.9331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0115, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0007, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0007, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0005, 0.0007, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58849.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0746, 0.0000,  ..., 0.0000, 0.0163, 0.0000],
        [0.0000, 0.0750, 0.0000,  ..., 0.0000, 0.0164, 0.0000],
        [0.0000, 0.0752, 0.0000,  ..., 0.0000, 0.0165, 0.0000],
        ...,
        [0.0000, 0.0768, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0000, 0.0768, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0000, 0.0768, 0.0000,  ..., 0.0000, 0.0170, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503430.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2237.3076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(210.0942, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5155.1377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(954.1924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.6039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1606],
        [-3.3102],
        [-3.4323],
        ...,
        [-3.4524],
        [-3.4468],
        [-3.4448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284739.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0248],
        [1.0247],
        ...,
        [1.0006],
        [0.9996],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369284.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0249],
        [1.0248],
        ...,
        [1.0006],
        [0.9996],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369290.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.4397e-05,  1.4520e-04, -6.1094e-04,  ...,  1.1582e-03,
          0.0000e+00, -7.6635e-04],
        [ 8.4397e-05,  1.4520e-04, -6.1094e-04,  ...,  1.1582e-03,
          0.0000e+00, -7.6635e-04],
        [ 8.4397e-05,  1.4520e-04, -6.1094e-04,  ...,  1.1582e-03,
          0.0000e+00, -7.6635e-04],
        ...,
        [ 8.4397e-05,  1.4520e-04, -6.1094e-04,  ...,  1.1582e-03,
          0.0000e+00, -7.6635e-04],
        [ 8.4397e-05,  1.4520e-04, -6.1094e-04,  ...,  1.1582e-03,
          0.0000e+00, -7.6635e-04],
        [ 8.4397e-05,  1.4520e-04, -6.1094e-04,  ...,  1.1582e-03,
          0.0000e+00, -7.6635e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2887.7783, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.8653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7121, device='cuda:0')



h[100].sum tensor(108.6008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0358, device='cuda:0')



h[200].sum tensor(42.1232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0006, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60957.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0751, 0.0000,  ..., 0.0000, 0.0159, 0.0000],
        [0.0000, 0.0756, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0758, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        ...,
        [0.0000, 0.0774, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0774, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0774, 0.0000,  ..., 0.0000, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517879.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2249.8152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.4606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5216.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(981.3394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-634.9865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2567],
        [-3.4371],
        [-3.5392],
        ...,
        [-3.4912],
        [-3.4859],
        [-3.4844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316296.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0249],
        [1.0248],
        ...,
        [1.0006],
        [0.9996],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369290.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(409.3194, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0249],
        [1.0249],
        ...,
        [1.0006],
        [0.9996],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369295.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.6816e-05,  1.3273e-04, -6.1094e-04,  ...,  1.1530e-03,
          0.0000e+00, -7.7429e-04],
        [ 6.4137e-03,  5.0787e-03, -6.5313e-04,  ...,  1.1537e-02,
         -6.6385e-04,  3.3319e-03],
        [ 4.7965e-03,  3.8165e-03, -6.4236e-04,  ...,  8.8873e-03,
         -4.9443e-04,  2.2840e-03],
        ...,
        [ 7.6816e-05,  1.3273e-04, -6.1094e-04,  ...,  1.1530e-03,
          0.0000e+00, -7.7429e-04],
        [ 7.6816e-05,  1.3273e-04, -6.1094e-04,  ...,  1.1530e-03,
          0.0000e+00, -7.7429e-04],
        [ 7.6816e-05,  1.3273e-04, -6.1094e-04,  ...,  1.1530e-03,
          0.0000e+00, -7.7429e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2666.3550, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.3149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9256, device='cuda:0')



h[100].sum tensor(107.5969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6948, device='cuda:0')



h[200].sum tensor(38.0018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0056, 0.0000,  ..., 0.0153, 0.0000, 0.0034],
        [0.0144, 0.0116, 0.0000,  ..., 0.0278, 0.0000, 0.0076],
        [0.0488, 0.0384, 0.0000,  ..., 0.0842, 0.0000, 0.0282],
        ...,
        [0.0003, 0.0006, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0003, 0.0006, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57212.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0214, 0.0897, 0.0591,  ..., 0.0023, 0.0360, 0.0000],
        [0.0469, 0.1060, 0.1281,  ..., 0.0119, 0.0567, 0.0000],
        [0.0900, 0.1330, 0.2463,  ..., 0.0339, 0.0909, 0.0000],
        ...,
        [0.0000, 0.0776, 0.0000,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0776, 0.0000,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0775, 0.0000,  ..., 0.0000, 0.0162, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504166.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1976.4619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(196.7635, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5374.1104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(926.8857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-591.8298, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4063],
        [-0.1463],
        [ 0.0311],
        ...,
        [-3.5181],
        [-3.5125],
        [-3.5110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-363401., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0249],
        [1.0249],
        ...,
        [1.0006],
        [0.9996],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369295.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0241],
        [1.0250],
        [1.0249],
        ...,
        [1.0005],
        [0.9996],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369301., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2285e-03,  4.9288e-03, -6.5187e-04,  ...,  1.1215e-02,
         -6.4089e-04,  3.1985e-03],
        [ 8.0417e-05,  1.3015e-04, -6.1094e-04,  ...,  1.1413e-03,
          0.0000e+00, -7.8491e-04],
        [ 8.0417e-05,  1.3015e-04, -6.1094e-04,  ...,  1.1413e-03,
          0.0000e+00, -7.8491e-04],
        ...,
        [ 8.0417e-05,  1.3015e-04, -6.1094e-04,  ...,  1.1413e-03,
          0.0000e+00, -7.8491e-04],
        [ 8.0417e-05,  1.3015e-04, -6.1094e-04,  ...,  1.1413e-03,
          0.0000e+00, -7.8491e-04],
        [ 8.0417e-05,  1.3015e-04, -6.1094e-04,  ...,  1.1413e-03,
          0.0000e+00, -7.8491e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3256.4673, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.6421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8358, device='cuda:0')



h[100].sum tensor(110.3572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.3746, device='cuda:0')



h[200].sum tensor(48.1483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0191, 0.0152, 0.0000,  ..., 0.0355, 0.0000, 0.0106],
        [0.0342, 0.0270, 0.0000,  ..., 0.0602, 0.0000, 0.0196],
        [0.0284, 0.0225, 0.0000,  ..., 0.0507, 0.0000, 0.0166],
        ...,
        [0.0003, 0.0005, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0003, 0.0005, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66500.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0665, 0.1174, 0.1802,  ..., 0.0198, 0.0723, 0.0000],
        [0.0854, 0.1301, 0.2334,  ..., 0.0320, 0.0866, 0.0000],
        [0.0887, 0.1326, 0.2434,  ..., 0.0349, 0.0887, 0.0000],
        ...,
        [0.0000, 0.0776, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0776, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0776, 0.0000,  ..., 0.0000, 0.0160, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538998.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2593.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.4294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5254.5869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1056.8324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-694.1423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1123],
        [ 0.0972],
        [ 0.0820],
        ...,
        [-3.5349],
        [-3.5295],
        [-3.5280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321872.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0241],
        [1.0250],
        [1.0249],
        ...,
        [1.0005],
        [0.9996],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369301., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0242],
        [1.0251],
        [1.0249],
        ...,
        [1.0005],
        [0.9995],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369306.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0180,  0.0141, -0.0007,  ...,  0.0304, -0.0019,  0.0108],
        [ 0.0256,  0.0200, -0.0008,  ...,  0.0429, -0.0026,  0.0157],
        [ 0.0184,  0.0144, -0.0007,  ...,  0.0311, -0.0019,  0.0111],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3292.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.9704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9626, device='cuda:0')



h[100].sum tensor(109.9747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.7538, device='cuda:0')



h[200].sum tensor(48.6687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0969, 0.0759, 0.0000,  ..., 0.1626, 0.0000, 0.0592],
        [0.0849, 0.0665, 0.0000,  ..., 0.1430, 0.0000, 0.0514],
        [0.0706, 0.0553, 0.0000,  ..., 0.1195, 0.0000, 0.0421],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68733.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2044, 0.2027, 0.5639,  ..., 0.1074, 0.1793, 0.0000],
        [0.1920, 0.1954, 0.5285,  ..., 0.0989, 0.1698, 0.0000],
        [0.1722, 0.1833, 0.4718,  ..., 0.0855, 0.1545, 0.0000],
        ...,
        [0.0000, 0.0774, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0774, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0774, 0.0000,  ..., 0.0000, 0.0161, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544184.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2850.9038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.1147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5318.1978, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1086.8510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-715.2671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0387],
        [-0.0394],
        [-0.0358],
        ...,
        [-3.2568],
        [-3.1697],
        [-3.1679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304715.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0242],
        [1.0251],
        [1.0249],
        ...,
        [1.0005],
        [0.9995],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369306.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0242],
        [1.0252],
        [1.0250],
        ...,
        [1.0005],
        [0.9995],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369312.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3216.3276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.7103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3142, device='cuda:0')



h[100].sum tensor(108.8310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.8154, device='cuda:0')



h[200].sum tensor(47.1780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66366.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0264, 0.0914, 0.0678,  ..., 0.0037, 0.0383, 0.0000],
        [0.0282, 0.0931, 0.0729,  ..., 0.0045, 0.0398, 0.0000],
        [0.0315, 0.0953, 0.0818,  ..., 0.0053, 0.0424, 0.0000],
        ...,
        [0.0000, 0.0771, 0.0000,  ..., 0.0000, 0.0164, 0.0000],
        [0.0000, 0.0771, 0.0000,  ..., 0.0000, 0.0164, 0.0000],
        [0.0000, 0.0771, 0.0000,  ..., 0.0000, 0.0164, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530875.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2931.2976, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(276.0924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5438.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1054.8617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-688.9720, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0718],
        [-0.0594],
        [-0.0357],
        ...,
        [-3.5336],
        [-3.5284],
        [-3.5270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255553.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0242],
        [1.0252],
        [1.0250],
        ...,
        [1.0005],
        [0.9995],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369312.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0243],
        [1.0253],
        [1.0250],
        ...,
        [1.0005],
        [0.9995],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369317.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2605.0198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.6858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.6052, device='cuda:0')



h[100].sum tensor(105.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.7368, device='cuda:0')



h[200].sum tensor(36.0281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0079, 0.0064, 0.0000,  ..., 0.0164, 0.0000, 0.0039],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53713.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.5118e-04, 7.5368e-02, 7.8918e-04,  ..., 0.0000e+00, 1.6460e-02,
         0.0000e+00],
        [6.1206e-03, 7.9115e-02, 1.4376e-02,  ..., 0.0000e+00, 2.1199e-02,
         0.0000e+00],
        [2.1972e-02, 8.9016e-02, 5.4332e-02,  ..., 2.2960e-03, 3.4823e-02,
         0.0000e+00],
        ...,
        [5.5055e-05, 7.7066e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6375e-02,
         0.0000e+00],
        [5.5601e-05, 7.7075e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6378e-02,
         0.0000e+00],
        [5.5337e-05, 7.7060e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6374e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(483503.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2130.5513, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(176.8681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6153.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(870.2850, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-540.3428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6824],
        [-2.0431],
        [-1.2769],
        ...,
        [-3.5492],
        [-3.5440],
        [-3.5426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340729.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0243],
        [1.0253],
        [1.0250],
        ...,
        [1.0005],
        [0.9995],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369317.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0254],
        [1.0250],
        ...,
        [1.0004],
        [0.9995],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369323.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0174,  0.0136, -0.0007,  ...,  0.0292, -0.0018,  0.0103],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2919.4302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2113, device='cuda:0')



h[100].sum tensor(107.3562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.5283, device='cuda:0')



h[200].sum tensor(40.8372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0183, 0.0143, 0.0000,  ..., 0.0331, 0.0000, 0.0106],
        [0.0151, 0.0118, 0.0000,  ..., 0.0278, 0.0000, 0.0085],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60139.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0094, 0.0807, 0.0233,  ..., 0.0000, 0.0233, 0.0000],
        [0.0306, 0.0942, 0.0788,  ..., 0.0068, 0.0407, 0.0000],
        [0.0398, 0.0999, 0.1032,  ..., 0.0091, 0.0485, 0.0000],
        ...,
        [0.0002, 0.0771, 0.0000,  ..., 0.0000, 0.0163, 0.0000],
        [0.0002, 0.0771, 0.0000,  ..., 0.0000, 0.0163, 0.0000],
        [0.0002, 0.0771, 0.0000,  ..., 0.0000, 0.0163, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509707.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2643.0894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(229.4555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6053.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(962.2236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.9135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4690],
        [-1.6940],
        [-0.9660],
        ...,
        [-3.5625],
        [-3.5574],
        [-3.5568]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310244., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0254],
        [1.0250],
        ...,
        [1.0004],
        [0.9995],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369323.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0255],
        [1.0250],
        ...,
        [1.0004],
        [0.9994],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369329.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0060,  0.0047, -0.0007,  ...,  0.0107, -0.0006,  0.0030],
        [ 0.0054,  0.0043, -0.0006,  ...,  0.0097, -0.0005,  0.0026],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2773.1921, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0181, device='cuda:0')



h[100].sum tensor(107.2131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.9612, device='cuda:0')



h[200].sum tensor(37.8788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0059, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0052, 0.0000,  ..., 0.0139, 0.0000, 0.0030],
        [0.0110, 0.0086, 0.0000,  ..., 0.0211, 0.0000, 0.0050],
        [0.0270, 0.0212, 0.0000,  ..., 0.0474, 0.0000, 0.0137],
        ...,
        [0.0006, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58393.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0929, 0.0726,  ..., 0.0040, 0.0410, 0.0000],
        [0.0444, 0.1016, 0.1104,  ..., 0.0091, 0.0538, 0.0000],
        [0.0594, 0.1104, 0.1491,  ..., 0.0148, 0.0665, 0.0000],
        ...,
        [0.0002, 0.0776, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0002, 0.0776, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0002, 0.0776, 0.0000,  ..., 0.0000, 0.0161, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506631.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2577.0483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.8268, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6171.9741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(937.7557, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-585.8730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5725e-01],
        [-1.8850e-03],
        [ 1.3414e-01],
        ...,
        [-3.5913e+00],
        [-3.5858e+00],
        [-3.5843e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328211.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0255],
        [1.0250],
        ...,
        [1.0004],
        [0.9994],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369329.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0245],
        [1.0256],
        [1.0250],
        ...,
        [1.0004],
        [0.9994],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369334.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0241,  0.0188, -0.0008,  ...,  0.0402, -0.0024,  0.0147],
        [ 0.0325,  0.0254, -0.0008,  ...,  0.0540, -0.0033,  0.0201],
        [ 0.0251,  0.0196, -0.0008,  ...,  0.0418, -0.0025,  0.0153],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3138.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7880, device='cuda:0')



h[100].sum tensor(109.8517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2421, device='cuda:0')



h[200].sum tensor(43.7966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3148, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1113, 0.0869, 0.0000,  ..., 0.1854, 0.0000, 0.0683],
        [0.1257, 0.0981, 0.0000,  ..., 0.2089, 0.0000, 0.0776],
        [0.1234, 0.0964, 0.0000,  ..., 0.2053, 0.0000, 0.0762],
        ...,
        [0.0006, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65712.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2572e-01, 2.1493e-01, 6.2110e-01,  ..., 1.2745e-01, 1.9427e-01,
         0.0000e+00],
        [2.5149e-01, 2.3128e-01, 6.9506e-01,  ..., 1.4503e-01, 2.1454e-01,
         0.0000e+00],
        [2.4713e-01, 2.2879e-01, 6.8262e-01,  ..., 1.4197e-01, 2.1130e-01,
         0.0000e+00],
        ...,
        [1.9939e-04, 7.8165e-02, 0.0000e+00,  ..., 0.0000e+00, 1.5919e-02,
         0.0000e+00],
        [2.0005e-04, 7.8177e-02, 0.0000e+00,  ..., 0.0000e+00, 1.5923e-02,
         0.0000e+00],
        [1.9965e-04, 7.8159e-02, 0.0000e+00,  ..., 0.0000e+00, 1.5918e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538434.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3048.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.5820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6052.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1042.1066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-663.1964, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7264e-03],
        [-1.6070e-02],
        [-3.1804e-02],
        ...,
        [-3.6153e+00],
        [-3.6095e+00],
        [-3.6077e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317385.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0245],
        [1.0256],
        [1.0250],
        ...,
        [1.0004],
        [0.9994],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369334.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0257],
        [1.0250],
        ...,
        [1.0003],
        [0.9993],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369340.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2850.6365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5297, device='cuda:0')



h[100].sum tensor(109.1655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.4905, device='cuda:0')



h[200].sum tensor(38.5168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0070, 0.0000,  ..., 0.0178, 0.0000, 0.0038],
        [0.0047, 0.0037, 0.0000,  ..., 0.0110, 0.0000, 0.0019],
        [0.0067, 0.0052, 0.0000,  ..., 0.0141, 0.0000, 0.0031],
        ...,
        [0.0005, 0.0004, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0004, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59182.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0204, 0.0872, 0.0463,  ..., 0.0000, 0.0347, 0.0000],
        [0.0173, 0.0859, 0.0385,  ..., 0.0000, 0.0320, 0.0000],
        [0.0207, 0.0881, 0.0472,  ..., 0.0000, 0.0351, 0.0000],
        ...,
        [0.0001, 0.0786, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0001, 0.0786, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0001, 0.0786, 0.0000,  ..., 0.0000, 0.0160, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509620.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2579.4019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(211.7642, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6137.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(956.7221, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.6536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2576],
        [-1.0855],
        [-0.8817],
        ...,
        [-3.6317],
        [-3.6262],
        [-3.6249]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335790.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0257],
        [1.0250],
        ...,
        [1.0003],
        [0.9993],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369340.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0247],
        [1.0258],
        [1.0251],
        ...,
        [1.0002],
        [0.9993],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369346.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0082, -0.0007,  ...,  0.0180, -0.0010,  0.0059],
        [ 0.0074,  0.0058, -0.0007,  ...,  0.0129, -0.0007,  0.0039],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2992.9216, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3836, device='cuda:0')



h[100].sum tensor(110.1704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.0436, device='cuda:0')



h[200].sum tensor(41.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0422, 0.0330, 0.0000,  ..., 0.0724, 0.0000, 0.0237],
        [0.0173, 0.0135, 0.0000,  ..., 0.0315, 0.0000, 0.0092],
        [0.0080, 0.0063, 0.0000,  ..., 0.0163, 0.0000, 0.0040],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60797.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.3044e-02, 1.2054e-01, 1.9113e-01,  ..., 2.8632e-02, 7.5264e-02,
         0.0000e+00],
        [5.0038e-02, 1.0663e-01, 1.2810e-01,  ..., 1.4513e-02, 5.7421e-02,
         0.0000e+00],
        [3.1883e-02, 9.5663e-02, 7.8921e-02,  ..., 5.8369e-03, 4.3156e-02,
         0.0000e+00],
        ...,
        [1.3128e-04, 7.8585e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6475e-02,
         0.0000e+00],
        [1.3197e-04, 7.8600e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6480e-02,
         0.0000e+00],
        [1.3154e-04, 7.8578e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6474e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512826.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2638.0107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.1241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5999.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(981.7266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.3427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0729],
        [-0.0672],
        [-0.3359],
        ...,
        [-3.6369],
        [-3.6313],
        [-3.6297]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323172., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0247],
        [1.0258],
        [1.0251],
        ...,
        [1.0002],
        [0.9993],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369346.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(480.4956, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0247],
        [1.0258],
        [1.0251],
        ...,
        [1.0002],
        [0.9993],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369352.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2856.5852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0611, device='cuda:0')



h[100].sum tensor(109.5501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0897, device='cuda:0')



h[200].sum tensor(39.1789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0107, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0005, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58341.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0761, 0.0000,  ..., 0.0000, 0.0163, 0.0000],
        [0.0001, 0.0765, 0.0000,  ..., 0.0000, 0.0164, 0.0000],
        [0.0009, 0.0773, 0.0006,  ..., 0.0000, 0.0174, 0.0000],
        ...,
        [0.0002, 0.0784, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0002, 0.0784, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0002, 0.0784, 0.0000,  ..., 0.0000, 0.0170, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504551.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2491.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(203.0335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5990.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(949.1114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-586.9651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4879],
        [-3.2698],
        [-2.9098],
        ...,
        [-3.2838],
        [-3.5223],
        [-3.5913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333565.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0247],
        [1.0258],
        [1.0251],
        ...,
        [1.0002],
        [0.9993],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369352.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0259],
        [1.0252],
        ...,
        [1.0002],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369358.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3378.9434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.5103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1007, device='cuda:0')



h[100].sum tensor(111.8067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.1665, device='cuda:0')



h[200].sum tensor(48.8415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68541.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0166, 0.0845, 0.0382,  ..., 0.0005, 0.0320, 0.0000],
        [0.0088, 0.0804, 0.0171,  ..., 0.0000, 0.0256, 0.0000],
        [0.0060, 0.0791, 0.0108,  ..., 0.0000, 0.0233, 0.0000],
        ...,
        [0.0003, 0.0779, 0.0000,  ..., 0.0000, 0.0178, 0.0000],
        [0.0003, 0.0779, 0.0000,  ..., 0.0000, 0.0178, 0.0000],
        [0.0003, 0.0779, 0.0000,  ..., 0.0000, 0.0178, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543961., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3208.5461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(286.9647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5500.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1095.9302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-704.6479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6365],
        [-1.0984],
        [-1.4651],
        ...,
        [-3.6144],
        [-3.5986],
        [-3.5459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274229.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0259],
        [1.0252],
        ...,
        [1.0002],
        [0.9992],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369358.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0249],
        [1.0260],
        [1.0253],
        ...,
        [1.0001],
        [0.9992],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369363.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0121,  0.0095, -0.0007,  ...,  0.0206, -0.0012,  0.0069],
        [ 0.0073,  0.0058, -0.0007,  ...,  0.0128, -0.0007,  0.0038],
        [ 0.0161,  0.0126, -0.0007,  ...,  0.0272, -0.0016,  0.0095],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3543.6787, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.0823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1196, device='cuda:0')



h[100].sum tensor(112.5678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.2129, device='cuda:0')



h[200].sum tensor(52.2754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0301, 0.0237, 0.0000,  ..., 0.0525, 0.0000, 0.0158],
        [0.0404, 0.0318, 0.0000,  ..., 0.0693, 0.0000, 0.0233],
        [0.0214, 0.0169, 0.0000,  ..., 0.0382, 0.0000, 0.0118],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70570.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0694, 0.1153, 0.1769,  ..., 0.0242, 0.0746, 0.0000],
        [0.0771, 0.1211, 0.1996,  ..., 0.0303, 0.0798, 0.0000],
        [0.0648, 0.1141, 0.1672,  ..., 0.0236, 0.0698, 0.0000],
        ...,
        [0.0005, 0.0776, 0.0000,  ..., 0.0000, 0.0177, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0177, 0.0000],
        [0.0005, 0.0776, 0.0000,  ..., 0.0000, 0.0177, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547887.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3291.2900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.5392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5471.9551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1119.6418, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-726.2993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1649],
        [ 0.1999],
        [ 0.1980],
        ...,
        [-3.5851],
        [-3.5650],
        [-3.5569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294925.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0249],
        [1.0260],
        [1.0253],
        ...,
        [1.0001],
        [0.9992],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369363.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0249],
        [1.0260],
        [1.0253],
        ...,
        [1.0001],
        [0.9992],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369363.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0046, -0.0006,  ...,  0.0103, -0.0006,  0.0029],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2812.5254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5634, device='cuda:0')



h[100].sum tensor(109.0247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.6017, device='cuda:0')



h[200].sum tensor(39.7239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0201, 0.0159, 0.0000,  ..., 0.0361, 0.0000, 0.0110],
        [0.0064, 0.0052, 0.0000,  ..., 0.0137, 0.0000, 0.0029],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58164.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0610, 0.1114, 0.1573,  ..., 0.0222, 0.0664, 0.0000],
        [0.0327, 0.0947, 0.0810,  ..., 0.0083, 0.0440, 0.0000],
        [0.0131, 0.0832, 0.0298,  ..., 0.0000, 0.0281, 0.0000],
        ...,
        [0.0005, 0.0776, 0.0000,  ..., 0.0000, 0.0177, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0177, 0.0000],
        [0.0005, 0.0776, 0.0000,  ..., 0.0000, 0.0177, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(502481.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2666.6814, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(197.8265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5643.5708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(949.4858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-594.7711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0370],
        [-0.3113],
        [-0.7621],
        ...,
        [-3.6213],
        [-3.6160],
        [-3.6143]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271777.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0249],
        [1.0260],
        [1.0253],
        ...,
        [1.0001],
        [0.9992],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369363.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0250],
        [1.0261],
        [1.0254],
        ...,
        [1.0001],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369367.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0112,  0.0088, -0.0007,  ...,  0.0191, -0.0011,  0.0063],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2712.5425, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.2880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9053, device='cuda:0')



h[100].sum tensor(108.8274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6341, device='cuda:0')



h[200].sum tensor(38.2482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8817, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0166, 0.0131, 0.0000,  ..., 0.0303, 0.0000, 0.0087],
        [0.0155, 0.0124, 0.0000,  ..., 0.0287, 0.0000, 0.0072],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56024.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0198, 0.0868, 0.0468,  ..., 0.0004, 0.0330, 0.0000],
        [0.0330, 0.0947, 0.0806,  ..., 0.0062, 0.0442, 0.0000],
        [0.0433, 0.1005, 0.1068,  ..., 0.0108, 0.0533, 0.0000],
        ...,
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0171, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0171, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0171, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496089.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2483.9868, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(187.1483, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5770.3911, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(910.4241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-564.8767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3894],
        [-0.2910],
        [-0.1042],
        ...,
        [-3.6198],
        [-3.6309],
        [-3.6315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337603.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0250],
        [1.0261],
        [1.0254],
        ...,
        [1.0001],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369367.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0251],
        [1.0262],
        [1.0255],
        ...,
        [1.0000],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369372.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2952.0488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9522, device='cuda:0')



h[100].sum tensor(110.3825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.7536, device='cuda:0')



h[200].sum tensor(42.5597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59684.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0768, 0.0026,  ..., 0.0000, 0.0179, 0.0000],
        [0.0029, 0.0772, 0.0038,  ..., 0.0000, 0.0183, 0.0000],
        [0.0044, 0.0782, 0.0075,  ..., 0.0000, 0.0196, 0.0000],
        ...,
        [0.0019, 0.0787, 0.0024,  ..., 0.0000, 0.0178, 0.0000],
        [0.0005, 0.0778, 0.0000,  ..., 0.0000, 0.0166, 0.0000],
        [0.0005, 0.0778, 0.0000,  ..., 0.0000, 0.0165, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509058.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2672.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.1993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5673.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(956.6469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-602.4279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0697],
        [-2.0364],
        [-1.7697],
        ...,
        [-3.3892],
        [-3.5747],
        [-3.6455]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346952.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0251],
        [1.0262],
        [1.0255],
        ...,
        [1.0000],
        [0.9991],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369372.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0253],
        [1.0263],
        [1.0257],
        ...,
        [1.0000],
        [0.9991],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369376.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3080.6946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0205, device='cuda:0')



h[100].sum tensor(111.0005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9475, device='cuda:0')



h[200].sum tensor(45.0079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62939.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0761, 0.0010,  ..., 0.0000, 0.0165, 0.0000],
        [0.0009, 0.0761, 0.0000,  ..., 0.0000, 0.0157, 0.0000],
        [0.0005, 0.0761, 0.0000,  ..., 0.0000, 0.0153, 0.0000],
        ...,
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0158, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0158, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0158, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(519299.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2898.3054, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(250.5382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5512.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(997.5027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-638.0138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2746],
        [-2.7694],
        [-3.0973],
        ...,
        [-3.7024],
        [-3.6970],
        [-3.6954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335214.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0253],
        [1.0263],
        [1.0257],
        ...,
        [1.0000],
        [0.9991],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369376.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0254],
        [1.0265],
        [1.0258],
        ...,
        [1.0000],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369380.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2847.8208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3636, device='cuda:0')



h[100].sum tensor(109.7292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.9939, device='cuda:0')



h[200].sum tensor(41.2118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58314.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0754, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0005, 0.0758, 0.0000,  ..., 0.0000, 0.0146, 0.0000],
        [0.0005, 0.0760, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        ...,
        [0.0006, 0.0776, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0006, 0.0777, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0006, 0.0776, 0.0000,  ..., 0.0000, 0.0152, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503720.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2701.7925, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.8128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5503.0142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(928.8754, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-586.8062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7474],
        [-3.6182],
        [-3.3965],
        ...,
        [-3.7252],
        [-3.7197],
        [-3.7180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339691.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0254],
        [1.0265],
        [1.0258],
        ...,
        [1.0000],
        [0.9990],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369380.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0255],
        [1.0266],
        [1.0259],
        ...,
        [0.9999],
        [0.9990],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369385.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0117,  0.0092, -0.0007,  ...,  0.0200, -0.0011,  0.0067],
        [ 0.0125,  0.0098, -0.0007,  ...,  0.0212, -0.0012,  0.0071],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3075.2803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2418, device='cuda:0')



h[100].sum tensor(110.7730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6091, device='cuda:0')



h[200].sum tensor(45.1439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0099, 0.0000,  ..., 0.0235, 0.0000, 0.0068],
        [0.0229, 0.0181, 0.0000,  ..., 0.0408, 0.0000, 0.0127],
        [0.0672, 0.0527, 0.0000,  ..., 0.1133, 0.0000, 0.0396],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63048., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0298, 0.0935, 0.0722,  ..., 0.0086, 0.0378, 0.0000],
        [0.0568, 0.1103, 0.1441,  ..., 0.0212, 0.0594, 0.0000],
        [0.1031, 0.1385, 0.2688,  ..., 0.0489, 0.0961, 0.0000],
        ...,
        [0.0006, 0.0776, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0006, 0.0776, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0006, 0.0776, 0.0000,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526164.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3052.3474, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(255.9530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5378.6162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(989.7944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-636.0094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4187],
        [-0.8988],
        [-0.3508],
        ...,
        [-3.7465],
        [-3.7408],
        [-3.7391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331988.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0255],
        [1.0266],
        [1.0259],
        ...,
        [0.9999],
        [0.9990],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369385.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0267],
        [1.0260],
        ...,
        [0.9999],
        [0.9990],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369390.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3422.0200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.1212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9504, device='cuda:0')



h[100].sum tensor(112.4513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.7070, device='cuda:0')



h[200].sum tensor(51.1729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71457.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0755, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        [0.0011, 0.0764, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0050, 0.0786, 0.0081,  ..., 0.0000, 0.0183, 0.0000],
        ...,
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0005, 0.0777, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558462.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3561.3374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.7134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5208.9429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1106.9073, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-730.5363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3892],
        [-2.1897],
        [-1.7302],
        ...,
        [-3.7451],
        [-3.7462],
        [-3.7467]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302428.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0267],
        [1.0260],
        ...,
        [0.9999],
        [0.9990],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369390.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(443.7335, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0269],
        [1.0261],
        ...,
        [0.9999],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369395.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0001,  0.0001, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3003.6624, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6543, device='cuda:0')



h[100].sum tensor(111.0088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.8527, device='cuda:0')



h[200].sum tensor(43.9447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0006, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64182.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0759, 0.0000,  ..., 0.0000, 0.0136, 0.0000],
        [0.0003, 0.0763, 0.0000,  ..., 0.0000, 0.0137, 0.0000],
        [0.0003, 0.0765, 0.0000,  ..., 0.0000, 0.0138, 0.0000],
        ...,
        [0.0004, 0.0781, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0004, 0.0781, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0004, 0.0781, 0.0000,  ..., 0.0000, 0.0142, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539098., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3059.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.5272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5495.5771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1000.1838, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-643.9822, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0711],
        [-3.2487],
        [-3.3514],
        ...,
        [-3.7734],
        [-3.7675],
        [-3.7658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-378557.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0269],
        [1.0261],
        ...,
        [0.9999],
        [0.9989],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369395.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0270],
        [1.0261],
        ...,
        [0.9998],
        [0.9989],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369401.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04],
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04],
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04],
        ...,
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04],
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04],
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3059.4004, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.7262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8403, device='cuda:0')



h[100].sum tensor(111.5979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.4089, device='cuda:0')



h[200].sum tensor(45.0155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2091, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62513.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0764, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0034, 0.0784, 0.0045,  ..., 0.0000, 0.0174, 0.0000],
        [0.0112, 0.0830, 0.0240,  ..., 0.0000, 0.0240, 0.0000],
        ...,
        [0.0002, 0.0785, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0002, 0.0785, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0002, 0.0785, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518523.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2865.2727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.7624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5191.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(988.0988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-634.5838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7065],
        [-2.1950],
        [-1.5835],
        ...,
        [-3.7739],
        [-3.7681],
        [-3.7664]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313740.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0270],
        [1.0261],
        ...,
        [0.9998],
        [0.9989],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369401.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0270],
        [1.0261],
        ...,
        [0.9998],
        [0.9989],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369401.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2544e-02,  9.8795e-03, -6.9373e-04,  ...,  2.1390e-02,
         -1.1796e-03,  7.2072e-03],
        [ 2.2956e-02,  1.8016e-02, -7.6300e-04,  ...,  3.8428e-02,
         -2.1666e-03,  1.3942e-02],
        [ 2.7745e-02,  2.1759e-02, -7.9486e-04,  ...,  4.6265e-02,
         -2.6206e-03,  1.7039e-02],
        ...,
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04],
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04],
        [ 9.9145e-05,  1.5480e-04, -6.1094e-04,  ...,  1.0257e-03,
          0.0000e+00, -8.4153e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2889.3213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5960, device='cuda:0')



h[100].sum tensor(110.7836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.6889, device='cuda:0')



h[200].sum tensor(42.1188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0703, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0821, 0.0645, 0.0000,  ..., 0.1379, 0.0000, 0.0494],
        [0.1092, 0.0856, 0.0000,  ..., 0.1822, 0.0000, 0.0669],
        [0.1088, 0.0853, 0.0000,  ..., 0.1816, 0.0000, 0.0666],
        ...,
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0006, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60298.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9289e-01, 1.9194e-01, 5.1399e-01,  ..., 1.0355e-01, 1.6696e-01,
         0.0000e+00],
        [2.2236e-01, 2.0997e-01, 5.9599e-01,  ..., 1.2310e-01, 1.9008e-01,
         0.0000e+00],
        [2.3203e-01, 2.1587e-01, 6.2246e-01,  ..., 1.2948e-01, 1.9769e-01,
         0.0000e+00],
        ...,
        [2.4284e-04, 7.8487e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4360e-02,
         0.0000e+00],
        [2.4369e-04, 7.8506e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4366e-02,
         0.0000e+00],
        [2.4310e-04, 7.8474e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4358e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515021.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2773.5532, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.8163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5265.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(954.9743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-607.9284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1812],
        [ 0.1770],
        [ 0.1781],
        ...,
        [-3.7716],
        [-3.7654],
        [-3.7632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329208.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0270],
        [1.0261],
        ...,
        [0.9998],
        [0.9989],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369401.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0271],
        [1.0262],
        ...,
        [0.9998],
        [0.9989],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369407.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0109,  0.0086, -0.0007,  ...,  0.0187, -0.0010,  0.0062],
        [ 0.0089,  0.0070, -0.0007,  ...,  0.0154, -0.0008,  0.0048],
        [ 0.0200,  0.0157, -0.0007,  ...,  0.0336, -0.0019,  0.0121],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2485.9116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.8284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-6.3073, device='cuda:0')



h[100].sum tensor(108.6359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.8566, device='cuda:0')



h[200].sum tensor(35.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7035, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0147, 0.0000,  ..., 0.0336, 0.0000, 0.0099],
        [0.0574, 0.0453, 0.0000,  ..., 0.0976, 0.0000, 0.0335],
        [0.0596, 0.0470, 0.0000,  ..., 0.1011, 0.0000, 0.0349],
        ...,
        [0.0004, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0007, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51488.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.6846e-02, 1.0456e-01, 1.1925e-01,  ..., 1.4716e-02, 5.1978e-02,
         0.0000e+00],
        [8.6613e-02, 1.2867e-01, 2.2592e-01,  ..., 3.7293e-02, 8.3412e-02,
         0.0000e+00],
        [1.0167e-01, 1.3782e-01, 2.6630e-01,  ..., 4.6290e-02, 9.5379e-02,
         0.0000e+00],
        ...,
        [2.2116e-04, 7.8651e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4910e-02,
         0.0000e+00],
        [2.2199e-04, 7.8671e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4916e-02,
         0.0000e+00],
        [2.2142e-04, 7.8638e-02, 0.0000e+00,  ..., 0.0000e+00, 1.4909e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(477932.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2093.7368, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(152.0057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5606.9482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(831.1760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-510.4224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1662],
        [-0.4172],
        [ 0.0261],
        ...,
        [-3.7584],
        [-3.7529],
        [-3.7513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-376514.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0271],
        [1.0262],
        ...,
        [0.9998],
        [0.9989],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369407.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0262],
        [1.0272],
        [1.0262],
        ...,
        [0.9998],
        [0.9988],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369413.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0046, -0.0006,  ...,  0.0103, -0.0005,  0.0028],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0058,  0.0046, -0.0006,  ...,  0.0103, -0.0005,  0.0028],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2854.9077, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.3865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9536, device='cuda:0')



h[100].sum tensor(109.9887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7683, device='cuda:0')



h[200].sum tensor(42.2707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9987, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0045, 0.0000,  ..., 0.0120, 0.0000, 0.0022],
        [0.0215, 0.0173, 0.0000,  ..., 0.0387, 0.0000, 0.0102],
        [0.0052, 0.0045, 0.0000,  ..., 0.0121, 0.0000, 0.0022],
        ...,
        [0.0005, 0.0009, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0009, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0009, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57694.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0127, 0.0834, 0.0278,  ..., 0.0000, 0.0265, 0.0000],
        [0.0223, 0.0890, 0.0523,  ..., 0.0000, 0.0350, 0.0000],
        [0.0128, 0.0841, 0.0281,  ..., 0.0000, 0.0268, 0.0000],
        ...,
        [0.0002, 0.0787, 0.0000,  ..., 0.0000, 0.0155, 0.0000],
        [0.0002, 0.0787, 0.0000,  ..., 0.0000, 0.0155, 0.0000],
        [0.0002, 0.0787, 0.0000,  ..., 0.0000, 0.0155, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496730.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2433.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(201.4264, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5389.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(921.4238, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-582.3306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4670],
        [-2.5333],
        [-2.7782],
        ...,
        [-3.7406],
        [-3.7353],
        [-3.7338]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339574.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0262],
        [1.0272],
        [1.0262],
        ...,
        [0.9998],
        [0.9988],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369413.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0273],
        [1.0262],
        ...,
        [0.9998],
        [0.9988],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369419.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0079,  0.0063, -0.0007,  ...,  0.0137, -0.0007,  0.0042],
        [ 0.0290,  0.0228, -0.0008,  ...,  0.0483, -0.0027,  0.0179],
        ...,
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0002, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4565.1626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(47.3406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1944, device='cuda:0')



h[100].sum tensor(117.6992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(66.3537, device='cuda:0')



h[200].sum tensor(71.6873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.4755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0173, 0.0000,  ..., 0.0385, 0.0000, 0.0119],
        [0.0624, 0.0493, 0.0000,  ..., 0.1055, 0.0000, 0.0375],
        [0.0833, 0.0657, 0.0000,  ..., 0.1397, 0.0000, 0.0502],
        ...,
        [0.0005, 0.0010, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0010, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0010, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(92457.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.3771e-02, 1.3132e-01, 2.4829e-01,  ..., 4.2327e-02, 9.0207e-02,
         0.0000e+00],
        [1.7235e-01, 1.7791e-01, 4.6387e-01,  ..., 9.0918e-02, 1.5212e-01,
         0.0000e+00],
        [2.3551e-01, 2.1512e-01, 6.3755e-01,  ..., 1.3096e-01, 2.0202e-01,
         0.0000e+00],
        ...,
        [3.1446e-04, 7.8512e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6338e-02,
         0.0000e+00],
        [3.1534e-04, 7.8532e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6344e-02,
         0.0000e+00],
        [3.1467e-04, 7.8498e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6336e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(644705.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4630.5635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(500.8047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4834.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1403.1686, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-961.1869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0549],
        [-0.0123],
        [-0.0251],
        ...,
        [-3.7168],
        [-3.7118],
        [-3.7104]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291410.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0273],
        [1.0262],
        ...,
        [0.9998],
        [0.9988],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369419.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0264],
        [1.0273],
        [1.0262],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369424.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2669.8398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.1613, device='cuda:0')



h[100].sum tensor(108.0716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.4098, device='cuda:0')



h[200].sum tensor(39.7860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7987, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0011, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0011, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0011, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0011, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0011, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0011, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54422.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0004, 0.0760, 0.0000,  ..., 0.0000, 0.0163, 0.0000],
        [0.0004, 0.0764, 0.0000,  ..., 0.0000, 0.0164, 0.0000],
        [0.0017, 0.0776, 0.0027,  ..., 0.0000, 0.0178, 0.0000],
        ...,
        [0.0004, 0.0783, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0004, 0.0783, 0.0000,  ..., 0.0000, 0.0170, 0.0000],
        [0.0004, 0.0782, 0.0000,  ..., 0.0000, 0.0170, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485522.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2249.1265, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(176.4528, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5528.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(873.3309, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-549.1368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4252],
        [-3.1664],
        [-2.7222],
        ...,
        [-3.6982],
        [-3.6935],
        [-3.6922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339240.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0264],
        [1.0273],
        [1.0262],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369424.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0274],
        [1.0263],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369430.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3047.7666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0191, device='cuda:0')



h[100].sum tensor(109.3028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9537, device='cuda:0')



h[200].sum tensor(46.3747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60907.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0004, 0.0758, 0.0000,  ..., 0.0000, 0.0167, 0.0000],
        [0.0004, 0.0763, 0.0000,  ..., 0.0000, 0.0168, 0.0000],
        [0.0004, 0.0765, 0.0000,  ..., 0.0000, 0.0169, 0.0000],
        ...,
        [0.0004, 0.0781, 0.0000,  ..., 0.0000, 0.0175, 0.0000],
        [0.0004, 0.0781, 0.0000,  ..., 0.0000, 0.0175, 0.0000],
        [0.0004, 0.0781, 0.0000,  ..., 0.0000, 0.0175, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507245.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2705.8853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.3779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5226.1807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(968.4460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-627.2258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4126],
        [-3.4613],
        [-3.4372],
        ...,
        [-3.6876],
        [-3.6830],
        [-3.6818]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271574., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0274],
        [1.0263],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369430.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0275],
        [1.0263],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369435.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0303,  0.0239, -0.0008,  ...,  0.0504, -0.0028,  0.0187],
        [ 0.0201,  0.0159, -0.0007,  ...,  0.0336, -0.0018,  0.0121],
        [ 0.0105,  0.0084, -0.0007,  ...,  0.0180, -0.0010,  0.0059],
        ...,
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3716.9888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.9722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8849, device='cuda:0')



h[100].sum tensor(112.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(44.5010, device='cuda:0')



h[200].sum tensor(57.5832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0920, 0.0727, 0.0000,  ..., 0.1537, 0.0000, 0.0558],
        [0.0677, 0.0537, 0.0000,  ..., 0.1139, 0.0000, 0.0400],
        [0.0382, 0.0306, 0.0000,  ..., 0.0656, 0.0000, 0.0217],
        ...,
        [0.0007, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74822.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8255e-01, 1.8059e-01, 4.8901e-01,  ..., 9.2798e-02, 1.6196e-01,
         0.0000e+00],
        [1.5688e-01, 1.6601e-01, 4.1783e-01,  ..., 7.5964e-02, 1.4201e-01,
         0.0000e+00],
        [1.2290e-01, 1.4643e-01, 3.2383e-01,  ..., 5.4103e-02, 1.1540e-01,
         0.0000e+00],
        ...,
        [3.7446e-04, 7.7998e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7546e-02,
         0.0000e+00],
        [3.7536e-04, 7.8018e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7553e-02,
         0.0000e+00],
        [3.7463e-04, 7.7984e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7543e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568700.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3644.6196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(343.9476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4854.2568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1163.0492, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-780.8936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1666],
        [ 0.1790],
        [ 0.1924],
        ...,
        [-3.6952],
        [-3.6906],
        [-3.6893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239275., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0275],
        [1.0263],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369435.5625, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:25.050686
evaluation loss: 500.98223876953125
epoch: 0 mean loss: 492.1796569824219
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0276],
        [1.0263],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369440.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0132,  0.0105, -0.0007,  ...,  0.0223, -0.0012,  0.0076],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3481.3601, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.2274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2984, device='cuda:0')



h[100].sum tensor(111.1038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.7577, device='cuda:0')



h[200].sum tensor(53.1128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0508, 0.0405, 0.0000,  ..., 0.0863, 0.0000, 0.0300],
        [0.0422, 0.0337, 0.0000,  ..., 0.0721, 0.0000, 0.0243],
        [0.0104, 0.0089, 0.0000,  ..., 0.0201, 0.0000, 0.0055],
        ...,
        [0.0007, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69188.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3730e-01, 1.5509e-01, 3.6589e-01,  ..., 6.4220e-02, 1.2563e-01,
         0.0000e+00],
        [1.1807e-01, 1.4444e-01, 3.1401e-01,  ..., 5.2534e-02, 1.1061e-01,
         0.0000e+00],
        [9.4450e-02, 1.3109e-01, 2.5131e-01,  ..., 3.9172e-02, 9.1997e-02,
         0.0000e+00],
        ...,
        [1.8112e-04, 7.8115e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7280e-02,
         0.0000e+00],
        [1.8191e-04, 7.8135e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7287e-02,
         0.0000e+00],
        [1.8136e-04, 7.8101e-02, 0.0000e+00,  ..., 0.0000e+00, 1.7277e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(540335.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3080.5562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.0827, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5238.9932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1082.6918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-719.2020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0967],
        [ 0.0789],
        [ 0.0556],
        ...,
        [-3.7211],
        [-3.7164],
        [-3.7151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261172.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0276],
        [1.0263],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369440.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 0.0 event: 0 loss: tensor(53.8494, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0268],
        [1.0276],
        [1.0264],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369445.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3333.7205, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.5667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2335, device='cuda:0')



h[100].sum tensor(110.8508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.5741, device='cuda:0')



h[200].sum tensor(49.8736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3645, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67573.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.6163e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6042e-02,
         0.0000e+00],
        [0.0000e+00, 7.6580e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6130e-02,
         0.0000e+00],
        [5.3576e-05, 7.6835e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6393e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.8393e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6746e-02,
         0.0000e+00],
        [0.0000e+00, 7.8412e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6752e-02,
         0.0000e+00],
        [0.0000e+00, 7.8378e-02, 0.0000e+00,  ..., 0.0000e+00, 1.6743e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541851.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2932.1067, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.8556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5261.7021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1057.9224, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-698.6959, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5995],
        [-3.4278],
        [-3.1632],
        ...,
        [-3.7640],
        [-3.7589],
        [-3.7573]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287102.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0268],
        [1.0276],
        [1.0264],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369445.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0277],
        [1.0264],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369450.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2858.3066, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.9412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6937, device='cuda:0')



h[100].sum tensor(109.1023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.9911, device='cuda:0')



h[200].sum tensor(40.9758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9697, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0451, 0.0361, 0.0000,  ..., 0.0771, 0.0000, 0.0264],
        [0.0303, 0.0244, 0.0000,  ..., 0.0528, 0.0000, 0.0176],
        [0.0252, 0.0205, 0.0000,  ..., 0.0445, 0.0000, 0.0143],
        ...,
        [0.0005, 0.0011, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0011, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0011, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59847.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1138, 0.1430, 0.3046,  ..., 0.0488, 0.1068, 0.0000],
        [0.0914, 0.1304, 0.2441,  ..., 0.0355, 0.0894, 0.0000],
        [0.0691, 0.1176, 0.1838,  ..., 0.0221, 0.0721, 0.0000],
        ...,
        [0.0000, 0.0787, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0787, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0786, 0.0000,  ..., 0.0000, 0.0161, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518828., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2400.0488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.3138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5534.4521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(948.3273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.9948, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1901],
        [ 0.1370],
        [-0.0747],
        ...,
        [-3.8091],
        [-3.8037],
        [-3.8022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325348.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0277],
        [1.0264],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369450.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0277],
        [1.0264],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369450.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0040, -0.0006,  ...,  0.0089, -0.0004,  0.0023],
        [ 0.0118,  0.0094, -0.0007,  ...,  0.0201, -0.0011,  0.0067],
        [ 0.0087,  0.0070, -0.0007,  ...,  0.0150, -0.0008,  0.0047],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2969.1882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5574, device='cuda:0')



h[100].sum tensor(109.6278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5734, device='cuda:0')



h[200].sum tensor(42.8453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0431, 0.0345, 0.0000,  ..., 0.0738, 0.0000, 0.0243],
        [0.0282, 0.0228, 0.0000,  ..., 0.0493, 0.0000, 0.0145],
        [0.0237, 0.0193, 0.0000,  ..., 0.0421, 0.0000, 0.0125],
        ...,
        [0.0005, 0.0011, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0011, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0011, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60412.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0769, 0.1206, 0.2020,  ..., 0.0241, 0.0792, 0.0000],
        [0.0692, 0.1161, 0.1804,  ..., 0.0188, 0.0736, 0.0000],
        [0.0593, 0.1109, 0.1546,  ..., 0.0142, 0.0657, 0.0000],
        ...,
        [0.0000, 0.0787, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0787, 0.0000,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0786, 0.0000,  ..., 0.0000, 0.0161, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515338.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2362.2991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(219.0836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5576.0557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(956.9028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-620.2415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1764],
        [ 0.1851],
        [ 0.1225],
        ...,
        [-3.8089],
        [-3.8033],
        [-3.8016]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315724.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0277],
        [1.0264],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369450.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0270],
        [1.0277],
        [1.0265],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369455.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2867.5400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.7039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8991, device='cuda:0')



h[100].sum tensor(109.3864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.6053, device='cuda:0')



h[200].sum tensor(40.4491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58146.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0766, 0.0000,  ..., 0.0000, 0.0150, 0.0000],
        [0.0001, 0.0775, 0.0006,  ..., 0.0000, 0.0158, 0.0000],
        [0.0034, 0.0795, 0.0089,  ..., 0.0000, 0.0190, 0.0000],
        ...,
        [0.0000, 0.0788, 0.0000,  ..., 0.0000, 0.0157, 0.0000],
        [0.0000, 0.0788, 0.0000,  ..., 0.0000, 0.0157, 0.0000],
        [0.0000, 0.0788, 0.0000,  ..., 0.0000, 0.0157, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506124.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2070.9072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(202.5656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5777.7314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(921.3862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-591.6473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7563],
        [-2.5345],
        [-1.9793],
        ...,
        [-3.7260],
        [-3.8090],
        [-3.8273]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-348564.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0270],
        [1.0277],
        [1.0265],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369455.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0271],
        [1.0278],
        [1.0265],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369460.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.3002e-03,  6.6781e-03, -6.6557e-04,  ...,  1.4393e-02,
         -7.3613e-04,  4.4955e-03],
        [ 9.0588e-05,  2.5417e-04, -6.1094e-04,  ...,  9.5807e-04,
          0.0000e+00, -8.2067e-04],
        [ 9.0588e-05,  2.5417e-04, -6.1094e-04,  ...,  9.5807e-04,
          0.0000e+00, -8.2067e-04],
        ...,
        [ 9.0588e-05,  2.5417e-04, -6.1094e-04,  ...,  9.5807e-04,
          0.0000e+00, -8.2067e-04],
        [ 9.0588e-05,  2.5417e-04, -6.1094e-04,  ...,  9.5807e-04,
          0.0000e+00, -8.2067e-04],
        [ 9.0588e-05,  2.5417e-04, -6.1094e-04,  ...,  9.5807e-04,
          0.0000e+00, -8.2067e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3388.0298, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.7523, device='cuda:0')



h[100].sum tensor(111.8088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.1249, device='cuda:0')



h[200].sum tensor(48.7773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0318, 0.0256, 0.0000,  ..., 0.0553, 0.0000, 0.0186],
        [0.0088, 0.0076, 0.0000,  ..., 0.0177, 0.0000, 0.0046],
        [0.0004, 0.0010, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69739.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0680, 0.1169, 0.1851,  ..., 0.0238, 0.0709, 0.0000],
        [0.0293, 0.0952, 0.0804,  ..., 0.0070, 0.0404, 0.0000],
        [0.0076, 0.0825, 0.0211,  ..., 0.0000, 0.0220, 0.0000],
        ...,
        [0.0000, 0.0790, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0790, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0790, 0.0000,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559740.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2861.8892, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.4563, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5431.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1084.1141, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-717.7522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4372],
        [-1.2388],
        [-2.1807],
        ...,
        [-3.8661],
        [-3.8603],
        [-3.8588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315364.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0271],
        [1.0278],
        [1.0265],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369460.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0278],
        [1.0265],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369466.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.1489e-05,  2.5678e-04, -6.1094e-04,  ...,  9.5103e-04,
          0.0000e+00, -8.2388e-04],
        [ 9.1489e-05,  2.5678e-04, -6.1094e-04,  ...,  9.5103e-04,
          0.0000e+00, -8.2388e-04],
        [ 9.1489e-05,  2.5678e-04, -6.1094e-04,  ...,  9.5103e-04,
          0.0000e+00, -8.2388e-04],
        ...,
        [ 9.1489e-05,  2.5678e-04, -6.1094e-04,  ...,  9.5103e-04,
          0.0000e+00, -8.2388e-04],
        [ 9.1489e-05,  2.5678e-04, -6.1094e-04,  ...,  9.5103e-04,
          0.0000e+00, -8.2388e-04],
        [ 9.1489e-05,  2.5678e-04, -6.1094e-04,  ...,  9.5103e-04,
          0.0000e+00, -8.2388e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3205.9272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2544, device='cuda:0')



h[100].sum tensor(110.5664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6468, device='cuda:0')



h[200].sum tensor(45.5632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0011, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65898.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0105, 0.0834, 0.0290,  ..., 0.0000, 0.0247, 0.0000],
        [0.0013, 0.0784, 0.0043,  ..., 0.0000, 0.0170, 0.0000],
        [0.0000, 0.0773, 0.0000,  ..., 0.0000, 0.0149, 0.0000],
        ...,
        [0.0000, 0.0789, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0789, 0.0000,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0789, 0.0000,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542366.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2504.8374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.9163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5837.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1026.6549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-672.8417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5754],
        [-2.3762],
        [-2.9667],
        ...,
        [-3.8744],
        [-3.8686],
        [-3.8671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344907.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0278],
        [1.0265],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369466.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0279],
        [1.0265],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369471.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0152,  0.0121, -0.0007,  ...,  0.0257, -0.0013,  0.0089],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2894.8030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9349, device='cuda:0')



h[100].sum tensor(107.9536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7123, device='cuda:0')



h[200].sum tensor(40.6278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0282, 0.0229, 0.0000,  ..., 0.0493, 0.0000, 0.0163],
        [0.0160, 0.0133, 0.0000,  ..., 0.0293, 0.0000, 0.0092],
        [0.0004, 0.0011, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0011, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59506.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0960, 0.1328, 0.2663,  ..., 0.0431, 0.0926, 0.0000],
        [0.0473, 0.1051, 0.1318,  ..., 0.0171, 0.0543, 0.0000],
        [0.0144, 0.0862, 0.0405,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0000, 0.0785, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0785, 0.0000,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0784, 0.0000,  ..., 0.0000, 0.0160, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513495.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2283.4209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.4292, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5749.3276, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(943.4658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-606.5895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1654],
        [-0.4406],
        [-0.6887],
        ...,
        [-3.8535],
        [-3.8473],
        [-3.8452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299904.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0279],
        [1.0265],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369471.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0274],
        [1.0279],
        [1.0266],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369477.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2694.3452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.4387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.3733, device='cuda:0')



h[100].sum tensor(105.9102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.0437, device='cuda:0')



h[200].sum tensor(37.5823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0224, 0.0183, 0.0000,  ..., 0.0397, 0.0000, 0.0125],
        [0.0115, 0.0098, 0.0000,  ..., 0.0219, 0.0000, 0.0063],
        [0.0197, 0.0162, 0.0000,  ..., 0.0353, 0.0000, 0.0116],
        ...,
        [0.0005, 0.0012, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0012, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0012, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55657.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0437, 0.1017, 0.1186,  ..., 0.0082, 0.0517, 0.0000],
        [0.0394, 0.0998, 0.1077,  ..., 0.0062, 0.0483, 0.0000],
        [0.0483, 0.1052, 0.1331,  ..., 0.0122, 0.0553, 0.0000],
        ...,
        [0.0000, 0.0782, 0.0000,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0782, 0.0000,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0781, 0.0000,  ..., 0.0000, 0.0165, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(499985.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2117.2229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(191.5200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5957.7827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(889.3617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-561.0935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5790],
        [-0.2962],
        [-0.0786],
        ...,
        [-3.8433],
        [-3.8379],
        [-3.8365]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315799.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0274],
        [1.0279],
        [1.0266],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369477.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0281],
        [1.0266],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369483., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0061,  0.0050, -0.0007,  ...,  0.0108, -0.0005,  0.0031],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2621.0867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.4641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-6.6198, device='cuda:0')



h[100].sum tensor(104.5053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.7910, device='cuda:0')



h[200].sum tensor(36.7187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7384, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0060, 0.0000,  ..., 0.0140, 0.0000, 0.0031],
        [0.0093, 0.0081, 0.0000,  ..., 0.0183, 0.0000, 0.0048],
        [0.0305, 0.0247, 0.0000,  ..., 0.0529, 0.0000, 0.0160],
        ...,
        [0.0006, 0.0013, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0013, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0013, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54579.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0179, 0.0858, 0.0478,  ..., 0.0000, 0.0322, 0.0000],
        [0.0272, 0.0912, 0.0723,  ..., 0.0020, 0.0401, 0.0000],
        [0.0429, 0.0998, 0.1134,  ..., 0.0043, 0.0531, 0.0000],
        ...,
        [0.0000, 0.0781, 0.0001,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0781, 0.0001,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0781, 0.0001,  ..., 0.0000, 0.0168, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496073.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2185.7954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(184.0309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5921.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(878.4909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-549.8788, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1578],
        [-0.7406],
        [-0.4271],
        ...,
        [-3.8304],
        [-3.8253],
        [-3.8208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284711.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0281],
        [1.0266],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369483., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0281],
        [1.0267],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369488.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0200,  0.0159, -0.0007,  ...,  0.0335, -0.0017,  0.0121],
        [ 0.0203,  0.0161, -0.0007,  ...,  0.0339, -0.0018,  0.0122],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3013.6997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.0597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5388, device='cuda:0')



h[100].sum tensor(105.8670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5177, device='cuda:0')



h[200].sum tensor(43.3524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0639, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0272, 0.0222, 0.0000,  ..., 0.0476, 0.0000, 0.0156],
        [0.0379, 0.0306, 0.0000,  ..., 0.0652, 0.0000, 0.0217],
        [0.0790, 0.0627, 0.0000,  ..., 0.1323, 0.0000, 0.0474],
        ...,
        [0.0005, 0.0013, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0013, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0005, 0.0013, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61057.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.6521e-02, 1.1391e-01, 1.8308e-01,  ..., 2.2975e-02, 7.0332e-02,
         0.0000e+00],
        [1.0520e-01, 1.3684e-01, 2.9034e-01,  ..., 4.6122e-02, 1.0042e-01,
         0.0000e+00],
        [1.5985e-01, 1.6867e-01, 4.4268e-01,  ..., 8.0726e-02, 1.4306e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.8333e-02, 3.7041e-04,  ..., 0.0000e+00, 1.6614e-02,
         0.0000e+00],
        [0.0000e+00, 7.8350e-02, 3.7191e-04,  ..., 0.0000e+00, 1.6619e-02,
         0.0000e+00],
        [0.0000e+00, 7.8316e-02, 3.7086e-04,  ..., 0.0000e+00, 1.6611e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517567.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2510.0737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5755.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(969.8196, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.6852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1250],
        [ 0.1286],
        [ 0.1256],
        ...,
        [-3.8373],
        [-3.8322],
        [-3.8309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276960.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0281],
        [1.0267],
        ...,
        [0.9997],
        [0.9988],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369488.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 10.0 event: 50 loss: tensor(416.8080, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0276],
        [1.0282],
        [1.0268],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369493.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0071,  0.0058, -0.0007,  ...,  0.0125, -0.0006,  0.0037],
        [ 0.0086,  0.0070, -0.0007,  ...,  0.0149, -0.0007,  0.0047],
        [ 0.0141,  0.0113, -0.0007,  ...,  0.0239, -0.0012,  0.0083],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3535.4014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.2387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.4058, device='cuda:0')



h[100].sum tensor(108.1592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0787, device='cuda:0')



h[200].sum tensor(51.9098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4952, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0379, 0.0306, 0.0000,  ..., 0.0653, 0.0000, 0.0210],
        [0.0410, 0.0330, 0.0000,  ..., 0.0704, 0.0000, 0.0229],
        [0.0286, 0.0233, 0.0000,  ..., 0.0500, 0.0000, 0.0149],
        ...,
        [0.0005, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69307.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0573, 0.1096, 0.1569,  ..., 0.0145, 0.0627, 0.0000],
        [0.0742, 0.1194, 0.2031,  ..., 0.0242, 0.0764, 0.0000],
        [0.0912, 0.1290, 0.2499,  ..., 0.0342, 0.0902, 0.0000],
        ...,
        [0.0000, 0.0789, 0.0005,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0789, 0.0005,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0789, 0.0005,  ..., 0.0000, 0.0162, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542680.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2809.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.3240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5587.6279, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1083.7604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-704.2279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1221],
        [ 0.1359],
        [ 0.1347],
        ...,
        [-3.8559],
        [-3.8506],
        [-3.8493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276532.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0276],
        [1.0282],
        [1.0268],
        ...,
        [0.9997],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369493.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0277],
        [1.0283],
        [1.0268],
        ...,
        [0.9996],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369497.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9005e-05,  3.0082e-04, -6.1094e-04,  ...,  9.7905e-04,
          0.0000e+00, -8.0594e-04],
        [ 9.9005e-05,  3.0082e-04, -6.1094e-04,  ...,  9.7905e-04,
          0.0000e+00, -8.0594e-04],
        [ 9.9005e-05,  3.0082e-04, -6.1094e-04,  ...,  9.7905e-04,
          0.0000e+00, -8.0594e-04],
        ...,
        [ 9.9005e-05,  3.0082e-04, -6.1094e-04,  ...,  9.7905e-04,
          0.0000e+00, -8.0594e-04],
        [ 9.9005e-05,  3.0082e-04, -6.1094e-04,  ...,  9.7905e-04,
          0.0000e+00, -8.0594e-04],
        [ 9.9005e-05,  3.0082e-04, -6.1094e-04,  ...,  9.7905e-04,
          0.0000e+00, -8.0594e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4203.0459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(39.7541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6740, device='cuda:0')



h[100].sum tensor(111.1660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.8289, device='cuda:0')



h[200].sum tensor(62.9696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0012, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0104, 0.0091, 0.0000,  ..., 0.0205, 0.0000, 0.0057],
        ...,
        [0.0004, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80844.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0777, 0.0032,  ..., 0.0000, 0.0161, 0.0000],
        [0.0055, 0.0810, 0.0168,  ..., 0.0000, 0.0203, 0.0000],
        [0.0192, 0.0894, 0.0551,  ..., 0.0010, 0.0320, 0.0000],
        ...,
        [0.0000, 0.0792, 0.0005,  ..., 0.0000, 0.0158, 0.0000],
        [0.0000, 0.0792, 0.0005,  ..., 0.0000, 0.0158, 0.0000],
        [0.0000, 0.0792, 0.0005,  ..., 0.0000, 0.0158, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590139.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3440.0718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(414.9908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5391.9536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1242.9235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-827.6725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9418],
        [-2.3473],
        [-1.5975],
        ...,
        [-3.8792],
        [-3.8737],
        [-3.8723]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283691.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0277],
        [1.0283],
        [1.0268],
        ...,
        [0.9996],
        [0.9987],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369497.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0278],
        [1.0284],
        [1.0269],
        ...,
        [0.9996],
        [0.9987],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369502.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0070e-02,  1.5939e-02, -7.4388e-04,  ...,  3.3683e-02,
         -1.7191e-03,  1.2133e-02],
        [ 2.9137e-02,  2.3038e-02, -8.0421e-04,  ...,  4.8524e-02,
         -2.4993e-03,  1.8007e-02],
        [ 2.0855e-02,  1.6554e-02, -7.4910e-04,  ...,  3.4969e-02,
         -1.7867e-03,  1.2642e-02],
        ...,
        [ 9.3150e-05,  2.9640e-04, -6.1094e-04,  ...,  9.8450e-04,
          0.0000e+00, -8.0815e-04],
        [ 9.3150e-05,  2.9640e-04, -6.1094e-04,  ...,  9.8450e-04,
          0.0000e+00, -8.0815e-04],
        [ 9.3150e-05,  2.9640e-04, -6.1094e-04,  ...,  9.8450e-04,
          0.0000e+00, -8.0815e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3538.4043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.7515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6587, device='cuda:0')



h[100].sum tensor(107.8554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.8349, device='cuda:0')



h[200].sum tensor(51.9517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0919, 0.0729, 0.0000,  ..., 0.1538, 0.0000, 0.0560],
        [0.1080, 0.0855, 0.0000,  ..., 0.1802, 0.0000, 0.0664],
        [0.0896, 0.0711, 0.0000,  ..., 0.1501, 0.0000, 0.0545],
        ...,
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70486.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6880e-01, 1.7504e-01, 4.7383e-01,  ..., 8.8796e-02, 1.4852e-01,
         0.0000e+00],
        [2.0714e-01, 1.9769e-01, 5.8124e-01,  ..., 1.1333e-01, 1.7840e-01,
         0.0000e+00],
        [2.0041e-01, 1.9413e-01, 5.6116e-01,  ..., 1.0809e-01, 1.7318e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.9152e-02, 5.0059e-04,  ..., 0.0000e+00, 1.5469e-02,
         0.0000e+00],
        [0.0000e+00, 7.9167e-02, 5.0211e-04,  ..., 0.0000e+00, 1.5474e-02,
         0.0000e+00],
        [0.0000e+00, 7.9132e-02, 5.0091e-04,  ..., 0.0000e+00, 1.5465e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553329.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2893.1313, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.2737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5509.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1097.0277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-714.6089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0148],
        [ 0.0546],
        [ 0.0527],
        ...,
        [-3.8973],
        [-3.8921],
        [-3.8913]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297052.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0278],
        [1.0284],
        [1.0269],
        ...,
        [0.9996],
        [0.9987],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369502.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0278],
        [1.0285],
        [1.0270],
        ...,
        [0.9996],
        [0.9987],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369506.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.9939e-05,  2.9184e-04, -6.1094e-04,  ...,  9.8768e-04,
          0.0000e+00, -8.1329e-04],
        [ 8.9939e-05,  2.9184e-04, -6.1094e-04,  ...,  9.8768e-04,
          0.0000e+00, -8.1329e-04],
        [ 8.9939e-05,  2.9184e-04, -6.1094e-04,  ...,  9.8768e-04,
          0.0000e+00, -8.1329e-04],
        ...,
        [ 8.9939e-05,  2.9184e-04, -6.1094e-04,  ...,  9.8768e-04,
          0.0000e+00, -8.1329e-04],
        [ 8.9939e-05,  2.9184e-04, -6.1094e-04,  ...,  9.8768e-04,
          0.0000e+00, -8.1329e-04],
        [ 8.9939e-05,  2.9184e-04, -6.1094e-04,  ...,  9.8768e-04,
          0.0000e+00, -8.1329e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3468.3848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.6934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9536, device='cuda:0')



h[100].sum tensor(107.3450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.7269, device='cuda:0')



h[200].sum tensor(50.8894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0012, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70193.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0767, 0.0005,  ..., 0.0000, 0.0146, 0.0000],
        [0.0000, 0.0772, 0.0005,  ..., 0.0000, 0.0147, 0.0000],
        [0.0000, 0.0774, 0.0005,  ..., 0.0000, 0.0148, 0.0000],
        ...,
        [0.0000, 0.0789, 0.0006,  ..., 0.0000, 0.0153, 0.0000],
        [0.0000, 0.0789, 0.0006,  ..., 0.0000, 0.0153, 0.0000],
        [0.0000, 0.0789, 0.0006,  ..., 0.0000, 0.0153, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559416.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2990.9243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.2020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5481.2949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1090.2861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-711.3683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6716],
        [-3.5792],
        [-3.3611],
        ...,
        [-3.5981],
        [-3.8221],
        [-3.8870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300103.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0278],
        [1.0285],
        [1.0270],
        ...,
        [0.9996],
        [0.9987],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369506.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0279],
        [1.0286],
        [1.0270],
        ...,
        [0.9996],
        [0.9986],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369510.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.8628e-03,  7.1587e-03, -6.6935e-04,  ...,  1.5359e-02,
         -7.4751e-04,  4.8650e-03],
        [ 8.4763e-05,  2.8557e-04, -6.1094e-04,  ...,  9.9195e-04,
          0.0000e+00, -8.1875e-04],
        [ 1.5019e-02,  1.1979e-02, -7.1031e-04,  ...,  2.5435e-02,
         -1.2718e-03,  8.8513e-03],
        ...,
        [ 8.4763e-05,  2.8557e-04, -6.1094e-04,  ...,  9.9195e-04,
          0.0000e+00, -8.1875e-04],
        [ 8.4763e-05,  2.8557e-04, -6.1094e-04,  ...,  9.9195e-04,
          0.0000e+00, -8.1875e-04],
        [ 8.4763e-05,  2.8557e-04, -6.1094e-04,  ...,  9.9195e-04,
          0.0000e+00, -8.1875e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3281.3071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7537, device='cuda:0')



h[100].sum tensor(106.3338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.1397, device='cuda:0')



h[200].sum tensor(47.8918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3110, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0069, 0.0000,  ..., 0.0160, 0.0000, 0.0039],
        [0.0395, 0.0318, 0.0000,  ..., 0.0682, 0.0000, 0.0220],
        [0.0223, 0.0183, 0.0000,  ..., 0.0400, 0.0000, 0.0117],
        ...,
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0012, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67399.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0254, 0.0916, 0.0707,  ..., 0.0020, 0.0363, 0.0000],
        [0.0538, 0.1078, 0.1466,  ..., 0.0115, 0.0596, 0.0000],
        [0.0552, 0.1082, 0.1491,  ..., 0.0109, 0.0615, 0.0000],
        ...,
        [0.0000, 0.0788, 0.0005,  ..., 0.0000, 0.0150, 0.0000],
        [0.0000, 0.0788, 0.0005,  ..., 0.0000, 0.0150, 0.0000],
        [0.0000, 0.0787, 0.0005,  ..., 0.0000, 0.0150, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(545333.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2914.1516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.0483, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5293.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1054.2708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-686.1368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2999],
        [-0.5189],
        [-0.0837],
        ...,
        [-3.9394],
        [-3.9336],
        [-3.9321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262660.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0279],
        [1.0286],
        [1.0270],
        ...,
        [0.9996],
        [0.9986],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369510.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0287],
        [1.0271],
        ...,
        [0.9995],
        [0.9986],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369514.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.8427e-03,  6.3591e-03, -6.6266e-04,  ...,  1.3724e-02,
         -6.5847e-04,  4.2165e-03],
        [ 6.9584e-05,  2.7291e-04, -6.1094e-04,  ...,  1.0024e-03,
          0.0000e+00, -8.1549e-04],
        [ 7.8427e-03,  6.3591e-03, -6.6266e-04,  ...,  1.3724e-02,
         -6.5847e-04,  4.2165e-03],
        ...,
        [ 6.9584e-05,  2.7291e-04, -6.1094e-04,  ...,  1.0024e-03,
          0.0000e+00, -8.1549e-04],
        [ 6.9584e-05,  2.7291e-04, -6.1094e-04,  ...,  1.0024e-03,
          0.0000e+00, -8.1549e-04],
        [ 6.9584e-05,  2.7291e-04, -6.1094e-04,  ...,  1.0024e-03,
          0.0000e+00, -8.1549e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2915.9084, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0620, device='cuda:0')



h[100].sum tensor(104.6893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0924, device='cuda:0')



h[200].sum tensor(41.7562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0068, 0.0062, 0.0000,  ..., 0.0147, 0.0000, 0.0034],
        [0.0294, 0.0239, 0.0000,  ..., 0.0517, 0.0000, 0.0155],
        [0.0068, 0.0062, 0.0000,  ..., 0.0148, 0.0000, 0.0034],
        ...,
        [0.0003, 0.0011, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0011, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0011, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59673.9414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0164, 0.0869, 0.0468,  ..., 0.0000, 0.0287, 0.0000],
        [0.0297, 0.0948, 0.0821,  ..., 0.0000, 0.0397, 0.0000],
        [0.0166, 0.0876, 0.0473,  ..., 0.0000, 0.0290, 0.0000],
        ...,
        [0.0000, 0.0790, 0.0004,  ..., 0.0000, 0.0143, 0.0000],
        [0.0000, 0.0790, 0.0004,  ..., 0.0000, 0.0143, 0.0000],
        [0.0000, 0.0790, 0.0004,  ..., 0.0000, 0.0143, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517223.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2299.1509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.0975, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5617.5864, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(938.3087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-595.7266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0025],
        [-2.8202],
        [-2.8767],
        ...,
        [-3.9708],
        [-3.9648],
        [-3.9632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342919.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0287],
        [1.0271],
        ...,
        [0.9995],
        [0.9986],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369514.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0288],
        [1.0272],
        ...,
        [0.9995],
        [0.9986],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369518.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6915e-02,  1.3469e-02, -7.2308e-04,  ...,  2.8598e-02,
         -1.4203e-03,  1.0104e-02],
        [ 1.5643e-02,  1.2473e-02, -7.1461e-04,  ...,  2.6516e-02,
         -1.3131e-03,  9.2806e-03],
        [ 2.4065e-02,  1.9068e-02, -7.7065e-04,  ...,  4.0301e-02,
         -2.0229e-03,  1.4732e-02],
        ...,
        [ 6.0587e-05,  2.7116e-04, -6.1094e-04,  ...,  1.0117e-03,
          0.0000e+00, -8.0649e-04],
        [ 6.0587e-05,  2.7116e-04, -6.1094e-04,  ...,  1.0117e-03,
          0.0000e+00, -8.0649e-04],
        [ 6.0587e-05,  2.7116e-04, -6.1094e-04,  ...,  1.0117e-03,
          0.0000e+00, -8.0649e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3156.4487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.2554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8908, device='cuda:0')



h[100].sum tensor(105.5981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.5598, device='cuda:0')



h[200].sum tensor(45.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2147, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0689, 0.0549, 0.0000,  ..., 0.1166, 0.0000, 0.0412],
        [0.0899, 0.0713, 0.0000,  ..., 0.1509, 0.0000, 0.0547],
        [0.1085, 0.0859, 0.0000,  ..., 0.1813, 0.0000, 0.0667],
        ...,
        [0.0003, 0.0011, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0011, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0011, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62889.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6021e-01, 1.7036e-01, 4.4420e-01,  ..., 7.9384e-02, 1.4115e-01,
         0.0000e+00],
        [1.9334e-01, 1.8999e-01, 5.3742e-01,  ..., 1.0123e-01, 1.6687e-01,
         0.0000e+00],
        [2.3006e-01, 2.1134e-01, 6.4142e-01,  ..., 1.2571e-01, 1.9557e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.9336e-02, 4.8435e-04,  ..., 0.0000e+00, 1.3915e-02,
         0.0000e+00],
        [0.0000e+00, 7.9350e-02, 4.8583e-04,  ..., 0.0000e+00, 1.3920e-02,
         0.0000e+00],
        [0.0000e+00, 7.9314e-02, 4.8458e-04,  ..., 0.0000e+00, 1.3912e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524851.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2418.1768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(265.2103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5496.9287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(983.8801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-630.8618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2085],
        [ 0.1963],
        [ 0.1847],
        ...,
        [-3.9867],
        [-3.9807],
        [-3.9791]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323561.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0288],
        [1.0272],
        ...,
        [0.9995],
        [0.9986],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369518.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0288],
        [1.0273],
        ...,
        [0.9995],
        [0.9986],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369523.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9970e-03,  4.9258e-03, -6.5043e-04,  ...,  1.0734e-02,
         -4.9758e-04,  3.0448e-03],
        [ 5.6078e-03,  4.6210e-03, -6.4784e-04,  ...,  1.0097e-02,
         -4.6495e-04,  2.7929e-03],
        [ 6.1726e-05,  2.7781e-04, -6.1094e-04,  ...,  1.0191e-03,
          0.0000e+00, -7.9766e-04],
        ...,
        [ 6.1726e-05,  2.7781e-04, -6.1094e-04,  ...,  1.0191e-03,
          0.0000e+00, -7.9766e-04],
        [ 6.1726e-05,  2.7781e-04, -6.1094e-04,  ...,  1.0191e-03,
          0.0000e+00, -7.9766e-04],
        [ 6.1726e-05,  2.7781e-04, -6.1094e-04,  ...,  1.0191e-03,
          0.0000e+00, -7.9766e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3996.2178, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.5303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0063, device='cuda:0')



h[100].sum tensor(109.1059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(50.8431, device='cuda:0')



h[200].sum tensor(60.0400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0473, 0.0380, 0.0000,  ..., 0.0812, 0.0000, 0.0272],
        [0.0110, 0.0096, 0.0000,  ..., 0.0218, 0.0000, 0.0053],
        [0.0236, 0.0194, 0.0000,  ..., 0.0424, 0.0000, 0.0127],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80778.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0895, 0.1290, 0.2477,  ..., 0.0337, 0.0865, 0.0000],
        [0.0524, 0.1080, 0.1451,  ..., 0.0106, 0.0577, 0.0000],
        [0.0581, 0.1116, 0.1600,  ..., 0.0134, 0.0622, 0.0000],
        ...,
        [0.0000, 0.0795, 0.0007,  ..., 0.0000, 0.0138, 0.0000],
        [0.0000, 0.0795, 0.0007,  ..., 0.0000, 0.0138, 0.0000],
        [0.0000, 0.0795, 0.0007,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602751., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3680.4780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(416.2037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4914.5518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1236.8988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-826.5562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2506],
        [ 0.2552],
        [ 0.2650],
        ...,
        [-3.8360],
        [-3.8295],
        [-3.8532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277886., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0288],
        [1.0273],
        ...,
        [0.9995],
        [0.9986],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369523.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0289],
        [1.0273],
        ...,
        [0.9995],
        [0.9985],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369528.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.6011e-05,  2.8588e-04, -6.1094e-04,  ...,  1.0227e-03,
          0.0000e+00, -7.8837e-04],
        [ 6.6011e-05,  2.8588e-04, -6.1094e-04,  ...,  1.0227e-03,
          0.0000e+00, -7.8837e-04],
        [ 6.6011e-05,  2.8588e-04, -6.1094e-04,  ...,  1.0227e-03,
          0.0000e+00, -7.8837e-04],
        ...,
        [ 6.6011e-05,  2.8588e-04, -6.1094e-04,  ...,  1.0227e-03,
          0.0000e+00, -7.8837e-04],
        [ 6.6011e-05,  2.8588e-04, -6.1094e-04,  ...,  1.0227e-03,
          0.0000e+00, -7.8837e-04],
        [ 6.6011e-05,  2.8588e-04, -6.1094e-04,  ...,  1.0227e-03,
          0.0000e+00, -7.8837e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3451.8484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.4130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8028, device='cuda:0')



h[100].sum tensor(106.2178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.2760, device='cuda:0')



h[200].sum tensor(51.2394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68846.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0774, 0.0010,  ..., 0.0000, 0.0131, 0.0000],
        [0.0000, 0.0779, 0.0010,  ..., 0.0000, 0.0132, 0.0000],
        [0.0000, 0.0781, 0.0010,  ..., 0.0000, 0.0133, 0.0000],
        ...,
        [0.0000, 0.0796, 0.0011,  ..., 0.0000, 0.0137, 0.0000],
        [0.0000, 0.0796, 0.0011,  ..., 0.0000, 0.0137, 0.0000],
        [0.0000, 0.0796, 0.0011,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549518.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2993.0435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.0971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5109.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1075.1949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-700.1742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9240],
        [-3.9405],
        [-3.8977],
        ...,
        [-3.9725],
        [-3.9379],
        [-3.8729]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257221.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0289],
        [1.0273],
        ...,
        [0.9995],
        [0.9985],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369528.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0289],
        [1.0274],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369533.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.4822e-03,  6.0959e-03, -6.6028e-04,  ...,  1.3166e-02,
         -6.1530e-04,  4.0247e-03],
        [ 7.8389e-03,  6.3753e-03, -6.6266e-04,  ...,  1.3750e-02,
         -6.4490e-04,  4.2557e-03],
        [ 7.4822e-03,  6.0959e-03, -6.6028e-04,  ...,  1.3166e-02,
         -6.1530e-04,  4.0247e-03],
        ...,
        [ 6.5977e-05,  2.8727e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.7792e-04],
        [ 6.5977e-05,  2.8727e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.7792e-04],
        [ 6.5977e-05,  2.8727e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.7792e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3755.2646, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.7267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1097, device='cuda:0')



h[100].sum tensor(107.5511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.1728, device='cuda:0')



h[200].sum tensor(56.3113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6853, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0354, 0.0287, 0.0000,  ..., 0.0616, 0.0000, 0.0195],
        [0.0345, 0.0280, 0.0000,  ..., 0.0603, 0.0000, 0.0190],
        [0.0145, 0.0123, 0.0000,  ..., 0.0275, 0.0000, 0.0076],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75722.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0504, 0.1064, 0.1387,  ..., 0.0086, 0.0553, 0.0000],
        [0.0513, 0.1073, 0.1410,  ..., 0.0088, 0.0563, 0.0000],
        [0.0394, 0.1006, 0.1088,  ..., 0.0040, 0.0471, 0.0000],
        ...,
        [0.0000, 0.0798, 0.0012,  ..., 0.0000, 0.0136, 0.0000],
        [0.0000, 0.0798, 0.0012,  ..., 0.0000, 0.0136, 0.0000],
        [0.0000, 0.0798, 0.0012,  ..., 0.0000, 0.0136, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582190.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3417.2739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.2286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5193.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1167.2609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-771.0937, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3363],
        [-0.0552],
        [-0.0189],
        ...,
        [-3.9496],
        [-3.9382],
        [-3.9422]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273830.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0289],
        [1.0274],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369533.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 20.0 event: 100 loss: tensor(488.7000, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0282],
        [1.0290],
        [1.0274],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369538.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6870e-03,  4.6893e-03, -6.4827e-04,  ...,  1.0209e-02,
         -4.6308e-04,  2.8548e-03],
        [ 7.5810e-05,  2.9435e-04, -6.1094e-04,  ...,  1.0233e-03,
          0.0000e+00, -7.7828e-04],
        [ 7.5810e-05,  2.9435e-04, -6.1094e-04,  ...,  1.0233e-03,
          0.0000e+00, -7.7828e-04],
        ...,
        [ 7.5810e-05,  2.9435e-04, -6.1094e-04,  ...,  1.0233e-03,
          0.0000e+00, -7.7828e-04],
        [ 7.5810e-05,  2.9435e-04, -6.1094e-04,  ...,  1.0233e-03,
          0.0000e+00, -7.7828e-04],
        [ 7.5810e-05,  2.9435e-04, -6.1094e-04,  ...,  1.0233e-03,
          0.0000e+00, -7.7828e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3145.0996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.9321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5040, device='cuda:0')



h[100].sum tensor(104.4095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.4035, device='cuda:0')



h[200].sum tensor(46.5478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0225, 0.0186, 0.0000,  ..., 0.0404, 0.0000, 0.0127],
        [0.0061, 0.0057, 0.0000,  ..., 0.0137, 0.0000, 0.0029],
        [0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62789.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0613, 0.1126, 0.1694,  ..., 0.0184, 0.0625, 0.0000],
        [0.0305, 0.0955, 0.0857,  ..., 0.0059, 0.0385, 0.0000],
        [0.0096, 0.0837, 0.0282,  ..., 0.0000, 0.0214, 0.0000],
        ...,
        [0.0000, 0.0796, 0.0015,  ..., 0.0000, 0.0137, 0.0000],
        [0.0000, 0.0796, 0.0015,  ..., 0.0000, 0.0137, 0.0000],
        [0.0000, 0.0796, 0.0015,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525573.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2635.9565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.7408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5580.9019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(984.6526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-630.6030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2484],
        [-0.9146],
        [-1.8197],
        ...,
        [-3.9881],
        [-3.9822],
        [-3.9807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291446.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0282],
        [1.0290],
        [1.0274],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369538.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0282],
        [1.0290],
        [1.0275],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369543.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2052e-05,  3.0708e-04, -6.1094e-04,  ...,  1.0167e-03,
          0.0000e+00, -7.8174e-04],
        [ 9.2052e-05,  3.0708e-04, -6.1094e-04,  ...,  1.0167e-03,
          0.0000e+00, -7.8174e-04],
        [ 9.2052e-05,  3.0708e-04, -6.1094e-04,  ...,  1.0167e-03,
          0.0000e+00, -7.8174e-04],
        ...,
        [ 9.2052e-05,  3.0708e-04, -6.1094e-04,  ...,  1.0167e-03,
          0.0000e+00, -7.8174e-04],
        [ 9.2052e-05,  3.0708e-04, -6.1094e-04,  ...,  1.0167e-03,
          0.0000e+00, -7.8174e-04],
        [ 9.2052e-05,  3.0708e-04, -6.1094e-04,  ...,  1.0167e-03,
          0.0000e+00, -7.8174e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2960.2571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.6328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9669, device='cuda:0')



h[100].sum tensor(103.1244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.8080, device='cuda:0')



h[200].sum tensor(43.9273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0143, 0.0122, 0.0000,  ..., 0.0270, 0.0000, 0.0082],
        ...,
        [0.0004, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58696.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0029, 0.0784, 0.0093,  ..., 0.0000, 0.0159, 0.0000],
        [0.0098, 0.0830, 0.0281,  ..., 0.0000, 0.0212, 0.0000],
        [0.0305, 0.0953, 0.0852,  ..., 0.0062, 0.0375, 0.0000],
        ...,
        [0.0003, 0.0792, 0.0017,  ..., 0.0000, 0.0139, 0.0000],
        [0.0003, 0.0792, 0.0017,  ..., 0.0000, 0.0139, 0.0000],
        [0.0003, 0.0792, 0.0017,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507856., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2432.0552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(233.1244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5841.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(922.9306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-582.8555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0139],
        [-1.9534],
        [-1.6229],
        ...,
        [-3.9793],
        [-3.9736],
        [-3.9721]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319547.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0282],
        [1.0290],
        [1.0275],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369543.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0282],
        [1.0290],
        [1.0275],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369548.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3514.1934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.1695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0332, device='cuda:0')



h[100].sum tensor(105.5046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.9649, device='cuda:0')



h[200].sum tensor(53.3416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70593.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0155, 0.0850, 0.0422,  ..., 0.0005, 0.0254, 0.0000],
        [0.0044, 0.0793, 0.0125,  ..., 0.0000, 0.0167, 0.0000],
        [0.0012, 0.0776, 0.0039,  ..., 0.0000, 0.0143, 0.0000],
        ...,
        [0.0005, 0.0787, 0.0018,  ..., 0.0000, 0.0140, 0.0000],
        [0.0005, 0.0787, 0.0019,  ..., 0.0000, 0.0140, 0.0000],
        [0.0005, 0.0787, 0.0018,  ..., 0.0000, 0.0140, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558935.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3434.1304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.3758, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5424.4473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1093.3307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-719.0255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9383],
        [-2.4469],
        [-2.5208],
        ...,
        [-3.9797],
        [-3.9741],
        [-3.9726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246536.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0282],
        [1.0290],
        [1.0275],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369548.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0291],
        [1.0276],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369553.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9180e-03,  4.0829e-03, -6.4307e-04,  ...,  8.9248e-03,
         -3.9229e-04,  2.3516e-03],
        [ 6.2358e-03,  5.1151e-03, -6.5183e-04,  ...,  1.1082e-02,
         -4.9934e-04,  3.2047e-03],
        [ 8.8814e-05,  3.0004e-04, -6.1094e-04,  ...,  1.0200e-03,
          0.0000e+00, -7.7464e-04],
        ...,
        [ 8.0167e-03,  6.5102e-03, -6.6368e-04,  ...,  1.3997e-02,
         -6.4402e-04,  4.3577e-03],
        [ 8.8814e-05,  3.0004e-04, -6.1094e-04,  ...,  1.0200e-03,
          0.0000e+00, -7.7464e-04],
        [ 8.8814e-05,  3.0004e-04, -6.1094e-04,  ...,  1.0200e-03,
          0.0000e+00, -7.7464e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3331.2576, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7046, device='cuda:0')



h[100].sum tensor(105.1227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.9928, device='cuda:0')



h[200].sum tensor(49.9899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3055, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0472, 0.0379, 0.0000,  ..., 0.0808, 0.0000, 0.0271],
        [0.0192, 0.0160, 0.0000,  ..., 0.0351, 0.0000, 0.0106],
        [0.0067, 0.0062, 0.0000,  ..., 0.0146, 0.0000, 0.0033],
        ...,
        [0.0604, 0.0482, 0.0000,  ..., 0.1024, 0.0000, 0.0364],
        [0.0535, 0.0429, 0.0000,  ..., 0.0913, 0.0000, 0.0320],
        [0.0385, 0.0311, 0.0000,  ..., 0.0667, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65276.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0830, 0.1235, 0.2248,  ..., 0.0294, 0.0789, 0.0000],
        [0.0554, 0.1083, 0.1502,  ..., 0.0151, 0.0575, 0.0000],
        [0.0280, 0.0931, 0.0764,  ..., 0.0045, 0.0360, 0.0000],
        ...,
        [0.2025, 0.1932, 0.5603,  ..., 0.1084, 0.1715, 0.0000],
        [0.1626, 0.1709, 0.4488,  ..., 0.0826, 0.1405, 0.0000],
        [0.1131, 0.1431, 0.3115,  ..., 0.0515, 0.1018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535821.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2924.6421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(286.5178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5692.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1015.6070, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-656.6993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0389],
        [-0.3504],
        [-1.0633],
        ...,
        [ 0.1749],
        [ 0.1681],
        [ 0.0234]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296423.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0291],
        [1.0276],
        ...,
        [0.9994],
        [0.9985],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369553.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0292],
        [1.0276],
        ...,
        [0.9994],
        [0.9984],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369558.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2266e-05,  2.8574e-04, -6.1094e-04,  ...,  1.0315e-03,
          0.0000e+00, -7.6454e-04],
        [ 9.4389e-03,  7.6233e-03, -6.7325e-04,  ...,  1.6365e-02,
         -7.5693e-04,  5.3002e-03],
        [ 9.4389e-03,  7.6233e-03, -6.7325e-04,  ...,  1.6365e-02,
         -7.5693e-04,  5.3002e-03],
        ...,
        [ 7.2266e-05,  2.8574e-04, -6.1094e-04,  ...,  1.0315e-03,
          0.0000e+00, -7.6454e-04],
        [ 7.2266e-05,  2.8574e-04, -6.1094e-04,  ...,  1.0315e-03,
          0.0000e+00, -7.6454e-04],
        [ 7.2266e-05,  2.8574e-04, -6.1094e-04,  ...,  1.0315e-03,
          0.0000e+00, -7.6454e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3122.0205, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0793, device='cuda:0')



h[100].sum tensor(104.5762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1337, device='cuda:0')



h[200].sum tensor(46.1995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0177, 0.0148, 0.0000,  ..., 0.0327, 0.0000, 0.0097],
        [0.0257, 0.0210, 0.0000,  ..., 0.0458, 0.0000, 0.0141],
        [0.0372, 0.0301, 0.0000,  ..., 0.0647, 0.0000, 0.0215],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62046.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.4299e-02, 1.1402e-01, 1.7662e-01,  ..., 2.0651e-02, 6.3672e-02,
         0.0000e+00],
        [9.2766e-02, 1.3058e-01, 2.5503e-01,  ..., 3.7864e-02, 8.5879e-02,
         0.0000e+00],
        [1.2748e-01, 1.5046e-01, 3.5159e-01,  ..., 6.0209e-02, 1.1275e-01,
         0.0000e+00],
        ...,
        [2.4838e-04, 7.9714e-02, 1.7521e-03,  ..., 0.0000e+00, 1.3598e-02,
         0.0000e+00],
        [2.4909e-04, 7.9727e-02, 1.7541e-03,  ..., 0.0000e+00, 1.3602e-02,
         0.0000e+00],
        [2.4842e-04, 7.9688e-02, 1.7516e-03,  ..., 0.0000e+00, 1.3594e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526200.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2624.2573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.0428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5818.1787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(969.3568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.5963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1315],
        [ 0.1188],
        [ 0.1015],
        ...,
        [-4.0300],
        [-4.0242],
        [-4.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-348785.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0292],
        [1.0276],
        ...,
        [0.9994],
        [0.9984],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369558.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0292],
        [1.0277],
        ...,
        [0.9993],
        [0.9984],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369563.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5897e-05,  2.8060e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.5561e-04],
        [ 6.5897e-05,  2.8060e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.5561e-04],
        [ 5.9151e-03,  4.8630e-03, -6.4985e-04,  ...,  1.0615e-02,
         -4.7021e-04,  3.0323e-03],
        ...,
        [ 6.5897e-05,  2.8060e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.5561e-04],
        [ 6.5897e-05,  2.8060e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.5561e-04],
        [ 6.5897e-05,  2.8060e-04, -6.1094e-04,  ...,  1.0381e-03,
          0.0000e+00, -7.5561e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3181.3589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5844, device='cuda:0')



h[100].sum tensor(105.0980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6439, device='cuda:0')



h[200].sum tensor(47.0054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0011, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0063, 0.0059, 0.0000,  ..., 0.0141, 0.0000, 0.0031],
        [0.0141, 0.0120, 0.0000,  ..., 0.0269, 0.0000, 0.0074],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64546.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0075, 0.0815, 0.0211,  ..., 0.0000, 0.0196, 0.0000],
        [0.0246, 0.0917, 0.0679,  ..., 0.0027, 0.0336, 0.0000],
        [0.0483, 0.1053, 0.1319,  ..., 0.0118, 0.0522, 0.0000],
        ...,
        [0.0003, 0.0800, 0.0017,  ..., 0.0000, 0.0135, 0.0000],
        [0.0003, 0.0800, 0.0017,  ..., 0.0000, 0.0135, 0.0000],
        [0.0003, 0.0800, 0.0017,  ..., 0.0000, 0.0135, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537178.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2919.1477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.0686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5420.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1010.1913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-649.5179, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2792],
        [-0.6209],
        [-0.1807],
        ...,
        [-4.0327],
        [-4.0269],
        [-4.0256]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292155., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0292],
        [1.0277],
        ...,
        [0.9993],
        [0.9984],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369563.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0293],
        [1.0278],
        ...,
        [0.9993],
        [0.9984],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369568.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6407e-02,  1.3081e-02, -7.1966e-04,  ...,  2.7796e-02,
         -1.3068e-03,  9.8364e-03],
        [ 1.6986e-02,  1.3535e-02, -7.2351e-04,  ...,  2.8743e-02,
         -1.3531e-03,  1.0211e-02],
        [ 1.4351e-02,  1.1470e-02, -7.0598e-04,  ...,  2.4429e-02,
         -1.1424e-03,  8.5043e-03],
        ...,
        [ 6.5905e-05,  2.7804e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.4883e-04],
        [ 6.5905e-05,  2.7804e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.4883e-04],
        [ 6.5905e-05,  2.7804e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.4883e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2996.9014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0172, device='cuda:0')



h[100].sum tensor(104.4796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.9584, device='cuda:0')



h[200].sum tensor(43.7929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0633, 0.0505, 0.0000,  ..., 0.1074, 0.0000, 0.0378],
        [0.0685, 0.0546, 0.0000,  ..., 0.1160, 0.0000, 0.0411],
        [0.0846, 0.0672, 0.0000,  ..., 0.1423, 0.0000, 0.0515],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59396.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3060e-01, 1.5164e-01, 3.5983e-01,  ..., 6.2532e-02, 1.1471e-01,
         0.0000e+00],
        [1.4329e-01, 1.5930e-01, 3.9494e-01,  ..., 7.0407e-02, 1.2457e-01,
         0.0000e+00],
        [1.5621e-01, 1.6675e-01, 4.3101e-01,  ..., 7.8821e-02, 1.3462e-01,
         0.0000e+00],
        ...,
        [3.4501e-04, 8.0092e-02, 1.7102e-03,  ..., 0.0000e+00, 1.3441e-02,
         0.0000e+00],
        [3.4575e-04, 8.0104e-02, 1.7122e-03,  ..., 0.0000e+00, 1.3445e-02,
         0.0000e+00],
        [3.4497e-04, 8.0065e-02, 1.7097e-03,  ..., 0.0000e+00, 1.3436e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(515075.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2535.2412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(235.2362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5749.6934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(935.6615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.6346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2251],
        [ 0.2216],
        [ 0.2113],
        ...,
        [-4.0541],
        [-4.0482],
        [-4.0466]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334516.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0293],
        [1.0278],
        ...,
        [0.9993],
        [0.9984],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369568.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0294],
        [1.0278],
        ...,
        [0.9993],
        [0.9984],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369573.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.8183e-05,  2.9160e-04, -6.1094e-04,  ...,  1.0340e-03,
          0.0000e+00, -7.4157e-04],
        [ 6.3649e-03,  5.2178e-03, -6.5277e-04,  ...,  1.1329e-02,
         -5.0011e-04,  3.3316e-03],
        [ 7.8183e-05,  2.9160e-04, -6.1094e-04,  ...,  1.0340e-03,
          0.0000e+00, -7.4157e-04],
        ...,
        [ 7.8183e-05,  2.9160e-04, -6.1094e-04,  ...,  1.0340e-03,
          0.0000e+00, -7.4157e-04],
        [ 7.8183e-05,  2.9160e-04, -6.1094e-04,  ...,  1.0340e-03,
          0.0000e+00, -7.4157e-04],
        [ 7.8183e-05,  2.9160e-04, -6.1094e-04,  ...,  1.0340e-03,
          0.0000e+00, -7.4157e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3202.7915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.0119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4036, device='cuda:0')



h[100].sum tensor(105.3126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1033, device='cuda:0')



h[200].sum tensor(47.3291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1604, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0283, 0.0231, 0.0000,  ..., 0.0500, 0.0000, 0.0151],
        [0.0056, 0.0053, 0.0000,  ..., 0.0129, 0.0000, 0.0027],
        [0.0068, 0.0063, 0.0000,  ..., 0.0149, 0.0000, 0.0034],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63391.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0527, 0.1065, 0.1422,  ..., 0.0123, 0.0549, 0.0000],
        [0.0320, 0.0955, 0.0865,  ..., 0.0045, 0.0387, 0.0000],
        [0.0189, 0.0884, 0.0512,  ..., 0.0006, 0.0285, 0.0000],
        ...,
        [0.0006, 0.0801, 0.0019,  ..., 0.0000, 0.0136, 0.0000],
        [0.0006, 0.0801, 0.0019,  ..., 0.0000, 0.0136, 0.0000],
        [0.0006, 0.0801, 0.0019,  ..., 0.0000, 0.0136, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529790.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2907.5422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.4852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5671.1030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(994.0468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-635.9553, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1405],
        [-0.4177],
        [-0.8795],
        ...,
        [-4.0418],
        [-4.0376],
        [-4.0360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302705.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0294],
        [1.0278],
        ...,
        [0.9993],
        [0.9984],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369573.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0285],
        [1.0294],
        [1.0279],
        ...,
        [0.9992],
        [0.9983],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369578.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2894e-05,  3.0482e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.4057e-04],
        [ 9.2894e-05,  3.0482e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.4057e-04],
        [ 9.2894e-05,  3.0482e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.4057e-04],
        ...,
        [ 9.2894e-05,  3.0482e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.4057e-04],
        [ 9.2894e-05,  3.0482e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.4057e-04],
        [ 9.2894e-05,  3.0482e-04, -6.1094e-04,  ...,  1.0252e-03,
          0.0000e+00, -7.4057e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3473.1436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.2866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3363, device='cuda:0')



h[100].sum tensor(106.4925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.8815, device='cuda:0')



h[200].sum tensor(51.9325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3760, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0117, 0.0101, 0.0000,  ..., 0.0228, 0.0000, 0.0058],
        ...,
        [0.0004, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0086, 0.0077, 0.0000,  ..., 0.0178, 0.0000, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68208.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0035, 0.0789, 0.0088,  ..., 0.0000, 0.0154, 0.0000],
        [0.0165, 0.0865, 0.0441,  ..., 0.0006, 0.0257, 0.0000],
        [0.0557, 0.1082, 0.1514,  ..., 0.0198, 0.0565, 0.0000],
        ...,
        [0.0019, 0.0804, 0.0048,  ..., 0.0000, 0.0145, 0.0000],
        [0.0127, 0.0863, 0.0341,  ..., 0.0000, 0.0229, 0.0000],
        [0.0358, 0.0990, 0.0964,  ..., 0.0085, 0.0409, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547742.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3354.8662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.0240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5595.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1061.4178, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-690.9883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4979],
        [-0.8296],
        [-0.3712],
        ...,
        [-3.2654],
        [-2.3740],
        [-1.3159]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262574.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0285],
        [1.0294],
        [1.0279],
        ...,
        [0.9992],
        [0.9983],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369578.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0295],
        [1.0280],
        ...,
        [0.9992],
        [0.9983],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369584.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2957.4309, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3959, device='cuda:0')



h[100].sum tensor(104.1948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.1008, device='cuda:0')



h[200].sum tensor(43.4705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58161.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0777, 0.0039,  ..., 0.0000, 0.0138, 0.0000],
        [0.0012, 0.0777, 0.0017,  ..., 0.0000, 0.0132, 0.0000],
        [0.0012, 0.0779, 0.0018,  ..., 0.0000, 0.0133, 0.0000],
        ...,
        [0.0013, 0.0794, 0.0018,  ..., 0.0000, 0.0137, 0.0000],
        [0.0013, 0.0794, 0.0019,  ..., 0.0000, 0.0137, 0.0000],
        [0.0013, 0.0794, 0.0018,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509238.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2774.0730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.4585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6079.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(915.1970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-577.1327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7694],
        [-3.2857],
        [-3.6160],
        ...,
        [-4.0379],
        [-4.0322],
        [-4.0307]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313914.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0295],
        [1.0280],
        ...,
        [0.9992],
        [0.9983],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369584.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 30.0 event: 150 loss: tensor(465.6509, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0295],
        [1.0280],
        ...,
        [0.9992],
        [0.9983],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369589.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0003, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3251.4722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5594, device='cuda:0')



h[100].sum tensor(105.7814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.5692, device='cuda:0')



h[200].sum tensor(48.1200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0013, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63925.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0774, 0.0034,  ..., 0.0000, 0.0141, 0.0000],
        [0.0016, 0.0778, 0.0023,  ..., 0.0000, 0.0136, 0.0000],
        [0.0014, 0.0779, 0.0018,  ..., 0.0000, 0.0134, 0.0000],
        ...,
        [0.0015, 0.0794, 0.0018,  ..., 0.0000, 0.0138, 0.0000],
        [0.0015, 0.0795, 0.0018,  ..., 0.0000, 0.0138, 0.0000],
        [0.0015, 0.0794, 0.0018,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531490.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3234.6863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(272.5490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5816.2510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(998.6994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-642.4504, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2625],
        [-2.6360],
        [-2.9442],
        ...,
        [-4.0347],
        [-4.0278],
        [-4.0254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273412.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0295],
        [1.0280],
        ...,
        [0.9992],
        [0.9983],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369589.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0286],
        [1.0296],
        [1.0281],
        ...,
        [0.9992],
        [0.9983],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369593.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2084e-05,  3.0476e-04, -6.1094e-04,  ...,  1.0138e-03,
          0.0000e+00, -7.2251e-04],
        [ 9.2084e-05,  3.0476e-04, -6.1094e-04,  ...,  1.0138e-03,
          0.0000e+00, -7.2251e-04],
        [ 6.9619e-03,  5.6897e-03, -6.5665e-04,  ...,  1.2267e-02,
         -5.3510e-04,  3.7313e-03],
        ...,
        [ 9.2084e-05,  3.0476e-04, -6.1094e-04,  ...,  1.0138e-03,
          0.0000e+00, -7.2251e-04],
        [ 9.2084e-05,  3.0476e-04, -6.1094e-04,  ...,  1.0138e-03,
          0.0000e+00, -7.2251e-04],
        [ 9.2084e-05,  3.0476e-04, -6.1094e-04,  ...,  1.0138e-03,
          0.0000e+00, -7.2251e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3454.4048, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7418, device='cuda:0')



h[100].sum tensor(107.2027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.1039, device='cuda:0')



h[200].sum tensor(50.9541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3096, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0129, 0.0111, 0.0000,  ..., 0.0247, 0.0000, 0.0066],
        [0.0318, 0.0259, 0.0000,  ..., 0.0556, 0.0000, 0.0181],
        ...,
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68539.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0195, 0.0868, 0.0500,  ..., 0.0018, 0.0280, 0.0000],
        [0.0496, 0.1036, 0.1321,  ..., 0.0159, 0.0516, 0.0000],
        [0.0924, 0.1275, 0.2503,  ..., 0.0407, 0.0846, 0.0000],
        ...,
        [0.0014, 0.0797, 0.0017,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0797, 0.0017,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0797, 0.0017,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556378.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3507.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.9180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5842.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1060.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-690.5339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1604],
        [-0.4647],
        [-0.0388],
        ...,
        [-3.9448],
        [-4.0323],
        [-4.0530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293548.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0286],
        [1.0296],
        [1.0281],
        ...,
        [0.9992],
        [0.9983],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369593.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0287],
        [1.0297],
        [1.0281],
        ...,
        [0.9992],
        [0.9983],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369598.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.3135e-05,  3.0074e-04, -6.1094e-04,  ...,  1.0159e-03,
          0.0000e+00, -7.1757e-04],
        [ 8.3135e-05,  3.0074e-04, -6.1094e-04,  ...,  1.0159e-03,
          0.0000e+00, -7.1757e-04],
        [ 8.3135e-05,  3.0074e-04, -6.1094e-04,  ...,  1.0159e-03,
          0.0000e+00, -7.1757e-04],
        ...,
        [ 8.3135e-05,  3.0074e-04, -6.1094e-04,  ...,  1.0159e-03,
          0.0000e+00, -7.1757e-04],
        [ 8.3135e-05,  3.0074e-04, -6.1094e-04,  ...,  1.0159e-03,
          0.0000e+00, -7.1757e-04],
        [ 8.3135e-05,  3.0074e-04, -6.1094e-04,  ...,  1.0159e-03,
          0.0000e+00, -7.1757e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2976.8530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3518, device='cuda:0')



h[100].sum tensor(105.2752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.9690, device='cuda:0')



h[200].sum tensor(42.9303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0013, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0013, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58545.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0775, 0.0015,  ..., 0.0000, 0.0131, 0.0000],
        [0.0013, 0.0780, 0.0015,  ..., 0.0000, 0.0132, 0.0000],
        [0.0013, 0.0783, 0.0016,  ..., 0.0000, 0.0133, 0.0000],
        ...,
        [0.0014, 0.0798, 0.0016,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0798, 0.0016,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0798, 0.0016,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512527.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2856.6257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.0183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5972.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(922.3943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-583.5303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9954],
        [-4.0834],
        [-4.1474],
        ...,
        [-4.0887],
        [-4.0824],
        [-4.0804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310279.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0287],
        [1.0297],
        [1.0281],
        ...,
        [0.9992],
        [0.9983],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369598.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0287],
        [1.0298],
        [1.0281],
        ...,
        [0.9992],
        [0.9982],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369603.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5449e-02,  2.0189e-02, -7.7979e-04,  ...,  4.2595e-02,
         -1.9557e-03,  1.5741e-02],
        [ 2.7155e-02,  2.1526e-02, -7.9114e-04,  ...,  4.5390e-02,
         -2.0872e-03,  1.6847e-02],
        [ 2.1704e-02,  1.7252e-02, -7.5487e-04,  ...,  3.6458e-02,
         -1.6670e-03,  1.3313e-02],
        ...,
        [ 7.2870e-05,  2.9490e-04, -6.1094e-04,  ...,  1.0207e-03,
          0.0000e+00, -7.1182e-04],
        [ 7.2870e-05,  2.9490e-04, -6.1094e-04,  ...,  1.0207e-03,
          0.0000e+00, -7.1182e-04],
        [ 7.2870e-05,  2.9490e-04, -6.1094e-04,  ...,  1.0207e-03,
          0.0000e+00, -7.1182e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3879.9966, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.0138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8387, device='cuda:0')



h[100].sum tensor(109.7082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(44.3627, device='cuda:0')



h[200].sum tensor(57.4210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1085, 0.0861, 0.0000,  ..., 0.1815, 0.0000, 0.0673],
        [0.1125, 0.0892, 0.0000,  ..., 0.1881, 0.0000, 0.0698],
        [0.1234, 0.0978, 0.0000,  ..., 0.2060, 0.0000, 0.0769],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77580.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2422, 0.2113, 0.6727,  ..., 0.1441, 0.1987, 0.0000],
        [0.2654, 0.2245, 0.7385,  ..., 0.1601, 0.2167, 0.0000],
        [0.2792, 0.2324, 0.7785,  ..., 0.1700, 0.2275, 0.0000],
        ...,
        [0.0014, 0.0800, 0.0015,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0800, 0.0015,  ..., 0.0000, 0.0137, 0.0000],
        [0.0014, 0.0800, 0.0015,  ..., 0.0000, 0.0136, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602695.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4135.0957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(395.0549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5582.0322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1183.3344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-786.1110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0310],
        [ 0.0044],
        [-0.0066],
        ...,
        [-4.1135],
        [-4.1075],
        [-4.1058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318978.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0287],
        [1.0298],
        [1.0281],
        ...,
        [0.9992],
        [0.9982],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369603.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0288],
        [1.0298],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369607.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7771e-03,  5.5464e-03, -6.5554e-04,  ...,  1.1998e-02,
         -5.1379e-04,  3.6150e-03],
        [ 7.3582e-05,  2.9156e-04, -6.1094e-04,  ...,  1.0185e-03,
          0.0000e+00, -7.2829e-04],
        [ 7.3582e-05,  2.9156e-04, -6.1094e-04,  ...,  1.0185e-03,
          0.0000e+00, -7.2829e-04],
        ...,
        [ 7.3582e-05,  2.9156e-04, -6.1094e-04,  ...,  1.0185e-03,
          0.0000e+00, -7.2829e-04],
        [ 7.3582e-05,  2.9156e-04, -6.1094e-04,  ...,  1.0185e-03,
          0.0000e+00, -7.2829e-04],
        [ 7.3582e-05,  2.9156e-04, -6.1094e-04,  ...,  1.0185e-03,
          0.0000e+00, -7.2829e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2995.0454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6047, device='cuda:0')



h[100].sum tensor(105.7658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.7251, device='cuda:0')



h[200].sum tensor(43.1129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0056, 0.0000,  ..., 0.0134, 0.0000, 0.0029],
        [0.0072, 0.0066, 0.0000,  ..., 0.0155, 0.0000, 0.0037],
        [0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59065.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0173, 0.0856, 0.0418,  ..., 0.0003, 0.0260, 0.0000],
        [0.0139, 0.0844, 0.0329,  ..., 0.0000, 0.0232, 0.0000],
        [0.0055, 0.0802, 0.0110,  ..., 0.0000, 0.0164, 0.0000],
        ...,
        [0.0018, 0.0796, 0.0013,  ..., 0.0000, 0.0137, 0.0000],
        [0.0018, 0.0797, 0.0013,  ..., 0.0000, 0.0137, 0.0000],
        [0.0018, 0.0796, 0.0013,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516113.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2994.2444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(233.2454, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5797.0518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(925.7148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-590.4015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1519],
        [-3.2356],
        [-3.2613],
        ...,
        [-4.1321],
        [-4.1261],
        [-4.1244]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313078.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0288],
        [1.0298],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369607.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0299],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369611.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2336e-02,  9.9031e-03, -6.9252e-04,  ...,  2.1101e-02,
         -9.3483e-04,  7.1969e-03],
        [ 1.4093e-02,  1.1280e-02, -7.0420e-04,  ...,  2.3978e-02,
         -1.0688e-03,  8.3346e-03],
        [ 3.9337e-02,  3.1068e-02, -8.7211e-04,  ...,  6.5318e-02,
         -2.9930e-03,  2.4680e-02],
        ...,
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04],
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04],
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3589.4392, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.0238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8309, device='cuda:0')



h[100].sum tensor(108.5512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.3599, device='cuda:0')



h[200].sum tensor(52.9533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0983, 0.0780, 0.0000,  ..., 0.1647, 0.0000, 0.0604],
        [0.1121, 0.0888, 0.0000,  ..., 0.1873, 0.0000, 0.0693],
        [0.0685, 0.0546, 0.0000,  ..., 0.1158, 0.0000, 0.0411],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72389.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2253, 0.1999, 0.6090,  ..., 0.1267, 0.1871, 0.0000],
        [0.2429, 0.2100, 0.6580,  ..., 0.1384, 0.2010, 0.0000],
        [0.2024, 0.1881, 0.5455,  ..., 0.1113, 0.1695, 0.0000],
        ...,
        [0.0020, 0.0793, 0.0011,  ..., 0.0000, 0.0138, 0.0000],
        [0.0020, 0.0794, 0.0011,  ..., 0.0000, 0.0138, 0.0000],
        [0.0020, 0.0793, 0.0011,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577376.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3949.8572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.1417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5500.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1105.6937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-735.0567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1560],
        [ 0.1504],
        [ 0.1597],
        ...,
        [-4.1489],
        [-4.1428],
        [-4.1411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302306.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0299],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369611.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0299],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369611.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04],
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04],
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04],
        ...,
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04],
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04],
        [ 7.2140e-05,  2.8970e-04, -6.1094e-04,  ...,  1.0176e-03,
          0.0000e+00, -7.4415e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3502.7742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4895, device='cuda:0')



h[100].sum tensor(108.1549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.3394, device='cuda:0')



h[200].sum tensor(51.5393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0175, 0.0147, 0.0000,  ..., 0.0324, 0.0000, 0.0096],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69636.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0072, 0.0800, 0.0141,  ..., 0.0000, 0.0175, 0.0000],
        [0.0127, 0.0833, 0.0289,  ..., 0.0000, 0.0222, 0.0000],
        [0.0357, 0.0958, 0.0892,  ..., 0.0067, 0.0411, 0.0000],
        ...,
        [0.0020, 0.0793, 0.0011,  ..., 0.0000, 0.0138, 0.0000],
        [0.0020, 0.0794, 0.0011,  ..., 0.0000, 0.0138, 0.0000],
        [0.0020, 0.0793, 0.0011,  ..., 0.0000, 0.0138, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561915., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3785.0212, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.2880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5360.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1071.1652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-707.5775, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2370],
        [-1.8199],
        [-1.1308],
        ...,
        [-4.1489],
        [-4.1428],
        [-4.1411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289203.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0299],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369611.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0289],
        [1.0299],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369615.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.9597e-03,  5.6869e-03, -6.5683e-04,  ...,  1.2324e-02,
         -5.2309e-04,  3.7203e-03],
        [ 4.4991e-03,  3.7582e-03, -6.4047e-04,  ...,  8.2952e-03,
         -3.3656e-04,  2.1278e-03],
        [ 1.1399e-02,  9.1668e-03, -6.8636e-04,  ...,  1.9593e-02,
         -8.5965e-04,  6.5936e-03],
        ...,
        [ 5.9599e-05,  2.7825e-04, -6.1094e-04,  ...,  1.0260e-03,
          0.0000e+00, -7.4557e-04],
        [ 5.9599e-05,  2.7825e-04, -6.1094e-04,  ...,  1.0260e-03,
          0.0000e+00, -7.4557e-04],
        [ 5.9599e-05,  2.7825e-04, -6.1094e-04,  ...,  1.0260e-03,
          0.0000e+00, -7.4557e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2980.3171, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.8160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5619, device='cuda:0')



h[100].sum tensor(106.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5973, device='cuda:0')



h[200].sum tensor(42.8984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9550, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0142, 0.0121, 0.0000,  ..., 0.0271, 0.0000, 0.0075],
        [0.0390, 0.0316, 0.0000,  ..., 0.0677, 0.0000, 0.0220],
        [0.0237, 0.0195, 0.0000,  ..., 0.0426, 0.0000, 0.0129],
        ...,
        [0.0002, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59488.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0460, 0.1014, 0.1159,  ..., 0.0118, 0.0487, 0.0000],
        [0.0682, 0.1142, 0.1747,  ..., 0.0225, 0.0662, 0.0000],
        [0.0617, 0.1107, 0.1574,  ..., 0.0186, 0.0614, 0.0000],
        ...,
        [0.0020, 0.0795, 0.0008,  ..., 0.0000, 0.0137, 0.0000],
        [0.0020, 0.0795, 0.0008,  ..., 0.0000, 0.0137, 0.0000],
        [0.0020, 0.0794, 0.0008,  ..., 0.0000, 0.0137, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(520733.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3048.4043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.9948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5658.8950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(923.8325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-594.0796, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2855],
        [-0.0128],
        [-0.0829],
        ...,
        [-4.0692],
        [-3.9964],
        [-3.9401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-356671.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0289],
        [1.0299],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369615.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0290],
        [1.0300],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369619.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1421e-05,  2.9148e-04, -6.1094e-04,  ...,  1.0375e-03,
          0.0000e+00, -7.4293e-04],
        [ 6.1421e-05,  2.9148e-04, -6.1094e-04,  ...,  1.0375e-03,
          0.0000e+00, -7.4293e-04],
        [ 6.1421e-05,  2.9148e-04, -6.1094e-04,  ...,  1.0375e-03,
          0.0000e+00, -7.4293e-04],
        ...,
        [ 6.1421e-05,  2.9148e-04, -6.1094e-04,  ...,  1.0375e-03,
          0.0000e+00, -7.4293e-04],
        [ 6.1421e-05,  2.9148e-04, -6.1094e-04,  ...,  1.0375e-03,
          0.0000e+00, -7.4293e-04],
        [ 6.1421e-05,  2.9148e-04, -6.1094e-04,  ...,  1.0375e-03,
          0.0000e+00, -7.4293e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4121.5215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.2391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7256, device='cuda:0')



h[100].sum tensor(111.3002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(50.0040, device='cuda:0')



h[200].sum tensor(61.9841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0012, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0040, 0.0042, 0.0000,  ..., 0.0105, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78893.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0773, 0.0008,  ..., 0.0000, 0.0133, 0.0000],
        [0.0020, 0.0778, 0.0008,  ..., 0.0000, 0.0134, 0.0000],
        [0.0034, 0.0788, 0.0038,  ..., 0.0000, 0.0145, 0.0000],
        ...,
        [0.0045, 0.0806, 0.0063,  ..., 0.0000, 0.0160, 0.0000],
        [0.0078, 0.0822, 0.0148,  ..., 0.0000, 0.0191, 0.0000],
        [0.0133, 0.0848, 0.0283,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593205.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4216.0093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.5695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5223.4771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1191.9231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-806.4768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8983],
        [-3.6040],
        [-3.1131],
        ...,
        [-3.8517],
        [-3.5827],
        [-3.3294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306656.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0290],
        [1.0300],
        [1.0282],
        ...,
        [0.9991],
        [0.9982],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369619.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0291],
        [1.0300],
        [1.0282],
        ...,
        [0.9990],
        [0.9981],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369624.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6234e-05,  3.0235e-04, -6.1094e-04,  ...,  1.0602e-03,
          0.0000e+00, -7.2985e-04],
        [ 5.6234e-05,  3.0235e-04, -6.1094e-04,  ...,  1.0602e-03,
          0.0000e+00, -7.2985e-04],
        [ 5.6234e-05,  3.0235e-04, -6.1094e-04,  ...,  1.0602e-03,
          0.0000e+00, -7.2985e-04],
        ...,
        [ 5.6234e-05,  3.0235e-04, -6.1094e-04,  ...,  1.0602e-03,
          0.0000e+00, -7.2985e-04],
        [ 5.6234e-05,  3.0235e-04, -6.1094e-04,  ...,  1.0602e-03,
          0.0000e+00, -7.2985e-04],
        [ 5.6234e-05,  3.0235e-04, -6.1094e-04,  ...,  1.0602e-03,
          0.0000e+00, -7.2985e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3380.4424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4740, device='cuda:0')



h[100].sum tensor(107.9583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.3034, device='cuda:0')



h[200].sum tensor(50.4422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2798, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0140, 0.0121, 0.0000,  ..., 0.0269, 0.0000, 0.0082],
        [0.0002, 0.0012, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0012, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0013, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0013, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0013, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66010.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0409, 0.0991, 0.1025,  ..., 0.0093, 0.0439, 0.0000],
        [0.0229, 0.0896, 0.0549,  ..., 0.0024, 0.0299, 0.0000],
        [0.0138, 0.0847, 0.0309,  ..., 0.0000, 0.0231, 0.0000],
        ...,
        [0.0022, 0.0798, 0.0010,  ..., 0.0000, 0.0140, 0.0000],
        [0.0022, 0.0798, 0.0010,  ..., 0.0000, 0.0140, 0.0000],
        [0.0022, 0.0798, 0.0010,  ..., 0.0000, 0.0140, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542062.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3489.7859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.0576, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5076.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1017.5558, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.9189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5402],
        [-0.9307],
        [-1.2462],
        ...,
        [-4.1700],
        [-4.1637],
        [-4.1621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320816.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0291],
        [1.0300],
        [1.0282],
        ...,
        [0.9990],
        [0.9981],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369624.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 40.0 event: 200 loss: tensor(447.9068, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0292],
        [1.0301],
        [1.0283],
        ...,
        [0.9990],
        [0.9981],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369630.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7204e-03,  3.9857e-03, -6.4195e-04,  ...,  8.7248e-03,
         -3.4783e-04,  2.3096e-03],
        [ 4.7204e-03,  3.9857e-03, -6.4195e-04,  ...,  8.7248e-03,
         -3.4783e-04,  2.3096e-03],
        [ 5.6718e-05,  3.2850e-04, -6.1094e-04,  ...,  1.0883e-03,
          0.0000e+00, -7.0725e-04],
        ...,
        [ 5.6718e-05,  3.2850e-04, -6.1094e-04,  ...,  1.0883e-03,
          0.0000e+00, -7.0725e-04],
        [ 5.6718e-05,  3.2850e-04, -6.1094e-04,  ...,  1.0883e-03,
          0.0000e+00, -7.0725e-04],
        [ 5.6718e-05,  3.2850e-04, -6.1094e-04,  ...,  1.0883e-03,
          0.0000e+00, -7.0725e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3262.6279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2401, device='cuda:0')



h[100].sum tensor(107.4059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.6144, device='cuda:0')



h[200].sum tensor(49.3858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0082, 0.0000,  ..., 0.0187, 0.0000, 0.0042],
        [0.0090, 0.0082, 0.0000,  ..., 0.0188, 0.0000, 0.0042],
        [0.0090, 0.0082, 0.0000,  ..., 0.0188, 0.0000, 0.0042],
        ...,
        [0.0002, 0.0014, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0002, 0.0014, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0002, 0.0014, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63479.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0288, 0.0911, 0.0687,  ..., 0.0007, 0.0374, 0.0000],
        [0.0226, 0.0884, 0.0526,  ..., 0.0000, 0.0322, 0.0000],
        [0.0174, 0.0861, 0.0392,  ..., 0.0000, 0.0276, 0.0000],
        ...,
        [0.0023, 0.0802, 0.0012,  ..., 0.0000, 0.0144, 0.0000],
        [0.0023, 0.0802, 0.0012,  ..., 0.0000, 0.0144, 0.0000],
        [0.0023, 0.0802, 0.0012,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526773.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3339.9470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.4406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4823.6455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(985.0425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-642.1628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0144],
        [-1.7729],
        [-2.5185],
        ...,
        [-4.1511],
        [-4.1450],
        [-4.1434]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296606.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0292],
        [1.0301],
        [1.0283],
        ...,
        [0.9990],
        [0.9981],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369630.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0302],
        [1.0284],
        ...,
        [0.9990],
        [0.9981],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369635.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.6639e-05,  3.5175e-04, -6.1094e-04,  ...,  1.1118e-03,
          0.0000e+00, -6.8408e-04],
        [ 2.1162e-02,  1.6906e-02, -7.5127e-04,  ...,  3.5677e-02,
         -1.5657e-03,  1.2970e-02],
        [ 5.6639e-05,  3.5175e-04, -6.1094e-04,  ...,  1.1118e-03,
          0.0000e+00, -6.8408e-04],
        ...,
        [ 5.6639e-05,  3.5175e-04, -6.1094e-04,  ...,  1.1118e-03,
          0.0000e+00, -6.8408e-04],
        [ 5.6639e-05,  3.5175e-04, -6.1094e-04,  ...,  1.1118e-03,
          0.0000e+00, -6.8408e-04],
        [ 5.6639e-05,  3.5175e-04, -6.1094e-04,  ...,  1.1118e-03,
          0.0000e+00, -6.8408e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3831.8088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.7681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9767, device='cuda:0')



h[100].sum tensor(109.9512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.7857, device='cuda:0')



h[200].sum tensor(59.3489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5589, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0341, 0.0280, 0.0000,  ..., 0.0601, 0.0000, 0.0205],
        [0.0429, 0.0349, 0.0000,  ..., 0.0744, 0.0000, 0.0262],
        [0.0918, 0.0733, 0.0000,  ..., 0.1546, 0.0000, 0.0564],
        ...,
        [0.0002, 0.0015, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0002, 0.0015, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0002, 0.0015, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74603.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0745, 0.1175, 0.1926,  ..., 0.0284, 0.0703, 0.0000],
        [0.0994, 0.1313, 0.2589,  ..., 0.0435, 0.0897, 0.0000],
        [0.1220, 0.1436, 0.3194,  ..., 0.0578, 0.1072, 0.0000],
        ...,
        [0.0023, 0.0806, 0.0014,  ..., 0.0000, 0.0147, 0.0000],
        [0.0023, 0.0806, 0.0014,  ..., 0.0000, 0.0147, 0.0000],
        [0.0023, 0.0806, 0.0014,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574265., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4063.0974, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(369.8108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4524.4463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1141.5953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-762.7111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6059],
        [-0.1773],
        [-0.1682],
        ...,
        [-4.1368],
        [-4.1308],
        [-4.1294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283050.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0302],
        [1.0284],
        ...,
        [0.9990],
        [0.9981],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369635.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0293],
        [1.0302],
        [1.0284],
        ...,
        [0.9990],
        [0.9981],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369640.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4504e-02,  2.7392e-02, -8.3995e-04,  ...,  5.7531e-02,
         -2.5413e-03,  2.1612e-02],
        [ 3.2625e-02,  2.5918e-02, -8.2746e-04,  ...,  5.4454e-02,
         -2.4027e-03,  2.0397e-02],
        [ 3.3007e-02,  2.6217e-02, -8.2999e-04,  ...,  5.5080e-02,
         -2.4309e-03,  2.0644e-02],
        ...,
        [ 6.0117e-05,  3.7138e-04, -6.1094e-04,  ...,  1.1202e-03,
          0.0000e+00, -6.7162e-04],
        [ 6.0117e-05,  3.7138e-04, -6.1094e-04,  ...,  1.1202e-03,
          0.0000e+00, -6.7162e-04],
        [ 6.0117e-05,  3.7138e-04, -6.1094e-04,  ...,  1.1202e-03,
          0.0000e+00, -6.7162e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3552.0576, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.5628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.9846, device='cuda:0')



h[100].sum tensor(108.5458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.8300, device='cuda:0')



h[200].sum tensor(55.2188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1335, 0.1061, 0.0000,  ..., 0.2228, 0.0000, 0.0835],
        [0.1463, 0.1161, 0.0000,  ..., 0.2438, 0.0000, 0.0917],
        [0.1436, 0.1140, 0.0000,  ..., 0.2395, 0.0000, 0.0900],
        ...,
        [0.0003, 0.0016, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68698.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3149, 0.2448, 0.8382,  ..., 0.1807, 0.2571, 0.0000],
        [0.3273, 0.2518, 0.8721,  ..., 0.1888, 0.2669, 0.0000],
        [0.3106, 0.2432, 0.8262,  ..., 0.1775, 0.2539, 0.0000],
        ...,
        [0.0024, 0.0808, 0.0015,  ..., 0.0000, 0.0149, 0.0000],
        [0.0024, 0.0808, 0.0015,  ..., 0.0000, 0.0150, 0.0000],
        [0.0024, 0.0808, 0.0015,  ..., 0.0000, 0.0149, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547430.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3721.6545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.6347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4434.9380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1062.5422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-699.8252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0688],
        [ 0.0685],
        [ 0.0762],
        ...,
        [-4.1312],
        [-4.1253],
        [-4.1238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273301., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0293],
        [1.0302],
        [1.0284],
        ...,
        [0.9990],
        [0.9981],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369640.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0302],
        [1.0285],
        ...,
        [0.9989],
        [0.9981],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369645.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.7636e-05,  4.0042e-04, -6.1094e-04,  ...,  1.1056e-03,
          0.0000e+00, -6.7114e-04],
        [ 7.7636e-05,  4.0042e-04, -6.1094e-04,  ...,  1.1056e-03,
          0.0000e+00, -6.7114e-04],
        [ 7.7636e-05,  4.0042e-04, -6.1094e-04,  ...,  1.1056e-03,
          0.0000e+00, -6.7114e-04],
        ...,
        [ 7.7636e-05,  4.0042e-04, -6.1094e-04,  ...,  1.1056e-03,
          0.0000e+00, -6.7114e-04],
        [ 7.7636e-05,  4.0042e-04, -6.1094e-04,  ...,  1.1056e-03,
          0.0000e+00, -6.7114e-04],
        [ 7.7636e-05,  4.0042e-04, -6.1094e-04,  ...,  1.1056e-03,
          0.0000e+00, -6.7114e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3792.6421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.3995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8019, device='cuda:0')



h[100].sum tensor(109.2580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.2630, device='cuda:0')



h[200].sum tensor(59.3990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0016, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0017, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72573.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0129, 0.0835, 0.0271,  ..., 0.0000, 0.0234, 0.0000],
        [0.0047, 0.0797, 0.0059,  ..., 0.0000, 0.0169, 0.0000],
        [0.0036, 0.0794, 0.0034,  ..., 0.0000, 0.0160, 0.0000],
        ...,
        [0.0028, 0.0806, 0.0013,  ..., 0.0000, 0.0153, 0.0000],
        [0.0028, 0.0806, 0.0013,  ..., 0.0000, 0.0153, 0.0000],
        [0.0028, 0.0806, 0.0013,  ..., 0.0000, 0.0153, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565062.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3993.6260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.8963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4659.4194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1108.7599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-736.3823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6747],
        [-2.3741],
        [-2.9113],
        ...,
        [-4.1196],
        [-4.1138],
        [-4.1124]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290151.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0302],
        [1.0285],
        ...,
        [0.9989],
        [0.9981],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369645.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0294],
        [1.0302],
        [1.0286],
        ...,
        [0.9989],
        [0.9980],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369650.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6834e-05,  4.2626e-04, -6.1094e-04,  ...,  1.0799e-03,
          0.0000e+00, -6.7773e-04],
        [ 9.6834e-05,  4.2626e-04, -6.1094e-04,  ...,  1.0799e-03,
          0.0000e+00, -6.7773e-04],
        [ 9.3362e-03,  7.6765e-03, -6.7237e-04,  ...,  1.6212e-02,
         -6.7436e-04,  5.3001e-03],
        ...,
        [ 9.6834e-05,  4.2626e-04, -6.1094e-04,  ...,  1.0799e-03,
          0.0000e+00, -6.7773e-04],
        [ 9.6834e-05,  4.2626e-04, -6.1094e-04,  ...,  1.0799e-03,
          0.0000e+00, -6.7773e-04],
        [ 9.6834e-05,  4.2626e-04, -6.1094e-04,  ...,  1.0799e-03,
          0.0000e+00, -6.7773e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3213.4458, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3353, device='cuda:0')



h[100].sum tensor(106.3043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.9093, device='cuda:0')



h[200].sum tensor(50.0295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0147, 0.0000,  ..., 0.0315, 0.0000, 0.0100],
        [0.0099, 0.0092, 0.0000,  ..., 0.0200, 0.0000, 0.0055],
        [0.0284, 0.0238, 0.0000,  ..., 0.0504, 0.0000, 0.0167],
        ...,
        [0.0004, 0.0018, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62227.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0614, 0.1086, 0.1519,  ..., 0.0177, 0.0606, 0.0000],
        [0.0581, 0.1073, 0.1431,  ..., 0.0154, 0.0582, 0.0000],
        [0.0802, 0.1189, 0.2014,  ..., 0.0291, 0.0754, 0.0000],
        ...,
        [0.0031, 0.0803, 0.0011,  ..., 0.0000, 0.0157, 0.0000],
        [0.0031, 0.0803, 0.0011,  ..., 0.0000, 0.0157, 0.0000],
        [0.0031, 0.0803, 0.0011,  ..., 0.0000, 0.0157, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524304.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3477.9558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(264.6452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4875.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(964.8285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.0394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2082],
        [ 0.2076],
        [ 0.2057],
        ...,
        [-4.1124],
        [-4.1066],
        [-4.1052]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282128.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0294],
        [1.0302],
        [1.0286],
        ...,
        [0.9989],
        [0.9980],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369650.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0295],
        [1.0303],
        [1.0287],
        ...,
        [0.9989],
        [0.9980],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369655.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115,  0.0094, -0.0007,  ...,  0.0198, -0.0008,  0.0067],
        [ 0.0056,  0.0048, -0.0006,  ...,  0.0101, -0.0004,  0.0029],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3963.9028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.7209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6852, device='cuda:0')



h[100].sum tensor(109.5405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.9039, device='cuda:0')



h[200].sum tensor(61.9883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0506, 0.0411, 0.0000,  ..., 0.0864, 0.0000, 0.0296],
        [0.0306, 0.0255, 0.0000,  ..., 0.0538, 0.0000, 0.0181],
        [0.0062, 0.0063, 0.0000,  ..., 0.0137, 0.0000, 0.0030],
        ...,
        [0.0005, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75460.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1167, 0.1360, 0.2935,  ..., 0.0493, 0.1042, 0.0000],
        [0.0876, 0.1216, 0.2178,  ..., 0.0321, 0.0816, 0.0000],
        [0.0560, 0.1054, 0.1351,  ..., 0.0129, 0.0572, 0.0000],
        ...,
        [0.0033, 0.0801, 0.0008,  ..., 0.0000, 0.0159, 0.0000],
        [0.0033, 0.0801, 0.0008,  ..., 0.0000, 0.0159, 0.0000],
        [0.0033, 0.0801, 0.0008,  ..., 0.0000, 0.0159, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573467.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4321.4111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.9573, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4673.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1147.8389, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-768.8925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1419],
        [ 0.1617],
        [ 0.1704],
        ...,
        [-4.1173],
        [-4.1115],
        [-4.1100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245961.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0295],
        [1.0303],
        [1.0287],
        ...,
        [0.9989],
        [0.9980],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369655.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0295],
        [1.0303],
        [1.0287],
        ...,
        [0.9989],
        [0.9980],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369660.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0004, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3234.0146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4740, device='cuda:0')



h[100].sum tensor(106.3132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.3242, device='cuda:0')



h[200].sum tensor(49.5964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0019, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0019, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0019, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61640.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0433, 0.0984, 0.1045,  ..., 0.0139, 0.0467, 0.0000],
        [0.0140, 0.0839, 0.0278,  ..., 0.0000, 0.0240, 0.0000],
        [0.0054, 0.0795, 0.0052,  ..., 0.0000, 0.0176, 0.0000],
        ...,
        [0.0031, 0.0800, 0.0004,  ..., 0.0000, 0.0159, 0.0000],
        [0.0031, 0.0800, 0.0004,  ..., 0.0000, 0.0159, 0.0000],
        [0.0031, 0.0800, 0.0004,  ..., 0.0000, 0.0159, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517715.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3383.3196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.9341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5209.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(952.4254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.8689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4838],
        [-1.0033],
        [-1.3486],
        ...,
        [-4.1324],
        [-4.1279],
        [-4.1275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272346.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0295],
        [1.0303],
        [1.0287],
        ...,
        [0.9989],
        [0.9980],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369660.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0303],
        [1.0288],
        ...,
        [0.9988],
        [0.9980],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369664.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9966e-05,  4.3582e-04, -6.1094e-04,  ...,  1.0009e-03,
          0.0000e+00, -6.9745e-04],
        [ 9.9966e-05,  4.3582e-04, -6.1094e-04,  ...,  1.0009e-03,
          0.0000e+00, -6.9745e-04],
        [ 6.5753e-03,  5.5181e-03, -6.5399e-04,  ...,  1.1604e-02,
         -4.6500e-04,  3.4924e-03],
        ...,
        [ 9.9966e-05,  4.3582e-04, -6.1094e-04,  ...,  1.0009e-03,
          0.0000e+00, -6.9745e-04],
        [ 9.9966e-05,  4.3582e-04, -6.1094e-04,  ...,  1.0009e-03,
          0.0000e+00, -6.9745e-04],
        [ 9.9966e-05,  4.3582e-04, -6.1094e-04,  ...,  1.0009e-03,
          0.0000e+00, -6.9745e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3452.3716, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.3051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2586, device='cuda:0')



h[100].sum tensor(107.7601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6594, device='cuda:0')



h[200].sum tensor(52.2373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0119, 0.0000,  ..., 0.0252, 0.0000, 0.0076],
        [0.0071, 0.0070, 0.0000,  ..., 0.0151, 0.0000, 0.0036],
        [0.0152, 0.0134, 0.0000,  ..., 0.0283, 0.0000, 0.0081],
        ...,
        [0.0004, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65381.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0342, 0.0938, 0.0784,  ..., 0.0030, 0.0402, 0.0000],
        [0.0356, 0.0946, 0.0809,  ..., 0.0026, 0.0418, 0.0000],
        [0.0508, 0.1023, 0.1192,  ..., 0.0097, 0.0540, 0.0000],
        ...,
        [0.0027, 0.0802, 0.0000,  ..., 0.0000, 0.0156, 0.0000],
        [0.0027, 0.0802, 0.0000,  ..., 0.0000, 0.0156, 0.0000],
        [0.0027, 0.0802, 0.0000,  ..., 0.0000, 0.0156, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533914.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3511.1296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.2885, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5170.9717, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1001.5467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-656.2664, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3603],
        [-0.1750],
        [ 0.0067],
        ...,
        [-4.1802],
        [-4.1739],
        [-4.1725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291207.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0303],
        [1.0288],
        ...,
        [0.9988],
        [0.9980],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369664.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0304],
        [1.0289],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369668.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        ...,
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3273.0361, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0222, device='cuda:0')



h[100].sum tensor(107.4996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9630, device='cuda:0')



h[200].sum tensor(48.2944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0004, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0105, 0.0097, 0.0000,  ..., 0.0206, 0.0000, 0.0058],
        ...,
        [0.0004, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62388.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0051, 0.0796, 0.0046,  ..., 0.0000, 0.0174, 0.0000],
        [0.0099, 0.0826, 0.0182,  ..., 0.0000, 0.0210, 0.0000],
        [0.0269, 0.0916, 0.0605,  ..., 0.0034, 0.0343, 0.0000],
        ...,
        [0.0022, 0.0804, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0022, 0.0804, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0022, 0.0804, 0.0000,  ..., 0.0000, 0.0152, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525489.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3267.2583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.3452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5311.9810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(959.5797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-624.2993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4254],
        [-2.4822],
        [-2.1853],
        ...,
        [-4.2291],
        [-4.2225],
        [-4.2206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296764.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0304],
        [1.0289],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369668.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0296],
        [1.0304],
        [1.0289],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369668.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        [ 1.3113e-02,  1.0644e-02, -6.9754e-04,  ...,  2.2306e-02,
         -9.3038e-04,  7.7255e-03],
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        ...,
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04],
        [ 8.6838e-05,  4.1952e-04, -6.1094e-04,  ...,  9.7609e-04,
          0.0000e+00, -7.0386e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3238.7441, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.0473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8102, device='cuda:0')



h[100].sum tensor(107.3445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3293, device='cuda:0')



h[200].sum tensor(47.7409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0167, 0.0000,  ..., 0.0352, 0.0000, 0.0109],
        [0.0114, 0.0104, 0.0000,  ..., 0.0220, 0.0000, 0.0064],
        [0.0634, 0.0512, 0.0000,  ..., 0.1072, 0.0000, 0.0379],
        ...,
        [0.0004, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63286.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0485, 0.1012, 0.1147,  ..., 0.0085, 0.0517, 0.0000],
        [0.0495, 0.1027, 0.1184,  ..., 0.0102, 0.0521, 0.0000],
        [0.0871, 0.1224, 0.2166,  ..., 0.0329, 0.0812, 0.0000],
        ...,
        [0.0022, 0.0804, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0022, 0.0804, 0.0000,  ..., 0.0000, 0.0152, 0.0000],
        [0.0022, 0.0804, 0.0000,  ..., 0.0000, 0.0152, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535047.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3428.1997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.4889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5229.0601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(972.4193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-634.4690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0300],
        [-0.0490],
        [-0.0844],
        ...,
        [-4.2285],
        [-4.2220],
        [-4.2203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285411.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0296],
        [1.0304],
        [1.0289],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369668.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 50.0 event: 250 loss: tensor(444.0464, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0290],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369672.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 8.0760e-03,  6.6807e-03, -6.6413e-04,  ...,  1.4055e-02,
         -5.6835e-04,  4.4644e-03],
        ...,
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5124.0273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(48.9717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.3566, device='cuda:0')



h[100].sum tensor(116.3079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(69.8285, device='cuda:0')



h[200].sum tensor(77.1733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.6051, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0131, 0.0117, 0.0000,  ..., 0.0248, 0.0000, 0.0068],
        [0.0300, 0.0250, 0.0000,  ..., 0.0526, 0.0000, 0.0170],
        ...,
        [0.0003, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(99141.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0183, 0.0861, 0.0375,  ..., 0.0002, 0.0279, 0.0000],
        [0.0450, 0.0998, 0.1055,  ..., 0.0106, 0.0495, 0.0000],
        [0.0810, 0.1183, 0.1979,  ..., 0.0274, 0.0777, 0.0000],
        ...,
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(690245.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5685.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(574.8065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4766.0645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1467.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1022.2955, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2467],
        [-0.6657],
        [-0.1663],
        ...,
        [-4.2727],
        [-4.2658],
        [-4.2638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262394.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0290],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369672.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0290],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369672.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        ...,
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04],
        [ 7.5478e-05,  4.0119e-04, -6.1094e-04,  ...,  9.5390e-04,
          0.0000e+00, -7.1323e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3572.0996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2308, device='cuda:0')



h[100].sum tensor(109.2948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.5659, device='cuda:0')



h[200].sum tensor(52.1461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71755.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0816, 0.0142,  ..., 0.0000, 0.0205, 0.0000],
        [0.0071, 0.0812, 0.0107,  ..., 0.0000, 0.0190, 0.0000],
        [0.0034, 0.0799, 0.0023,  ..., 0.0000, 0.0158, 0.0000],
        ...,
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0148, 0.0000],
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583585.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3950.5918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.3333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5340.7554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1084.0529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-720.0192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0787],
        [-2.3461],
        [-2.6465],
        ...,
        [-4.2727],
        [-4.2658],
        [-4.2638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344068.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0290],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369672.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0291],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369676.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6190e-02,  1.3041e-02, -7.1811e-04,  ...,  2.7327e-02,
         -1.1389e-03,  9.7080e-03],
        [ 8.1563e-03,  6.7357e-03, -6.6471e-04,  ...,  1.4175e-02,
         -5.7138e-04,  4.5099e-03],
        [ 8.1455e-03,  6.7272e-03, -6.6463e-04,  ...,  1.4157e-02,
         -5.7062e-04,  4.5029e-03],
        ...,
        [ 6.8844e-05,  3.8779e-04, -6.1094e-04,  ...,  9.3427e-04,
          0.0000e+00, -7.2325e-04],
        [ 6.8844e-05,  3.8779e-04, -6.1094e-04,  ...,  9.3427e-04,
          0.0000e+00, -7.2325e-04],
        [ 6.8844e-05,  3.8779e-04, -6.1094e-04,  ...,  9.3427e-04,
          0.0000e+00, -7.2325e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3428.3586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4005, device='cuda:0')



h[100].sum tensor(108.8508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.0835, device='cuda:0')



h[200].sum tensor(49.0588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0439, 0.0358, 0.0000,  ..., 0.0752, 0.0000, 0.0252],
        [0.0456, 0.0372, 0.0000,  ..., 0.0781, 0.0000, 0.0264],
        [0.0154, 0.0135, 0.0000,  ..., 0.0287, 0.0000, 0.0083],
        ...,
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67352., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0793, 0.1176, 0.1937,  ..., 0.0269, 0.0755, 0.0000],
        [0.0718, 0.1142, 0.1744,  ..., 0.0225, 0.0698, 0.0000],
        [0.0446, 0.1003, 0.1042,  ..., 0.0087, 0.0488, 0.0000],
        ...,
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555435.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3552.3601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.1398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5326.0552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1025.3961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-675.9998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1932],
        [-0.0676],
        [-0.5817],
        ...,
        [-4.2932],
        [-4.2884],
        [-4.2891]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311515.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0291],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369676.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0291],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369676.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6621e-02,  1.3380e-02, -7.2098e-04,  ...,  2.8033e-02,
         -1.1694e-03,  9.9872e-03],
        [ 1.1540e-02,  9.3914e-03, -6.8720e-04,  ...,  1.9714e-02,
         -8.1042e-04,  6.6992e-03],
        [ 1.6282e-02,  1.3114e-02, -7.1873e-04,  ...,  2.7478e-02,
         -1.1455e-03,  9.7679e-03],
        ...,
        [ 6.8844e-05,  3.8779e-04, -6.1094e-04,  ...,  9.3427e-04,
          0.0000e+00, -7.2325e-04],
        [ 6.8844e-05,  3.8779e-04, -6.1094e-04,  ...,  9.3427e-04,
          0.0000e+00, -7.2325e-04],
        [ 6.8844e-05,  3.8779e-04, -6.1094e-04,  ...,  9.3427e-04,
          0.0000e+00, -7.2325e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3378.0115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.9194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1117, device='cuda:0')



h[100].sum tensor(108.6236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2201, device='cuda:0')



h[200].sum tensor(48.2477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0424, 0.0346, 0.0000,  ..., 0.0727, 0.0000, 0.0243],
        [0.0477, 0.0388, 0.0000,  ..., 0.0815, 0.0000, 0.0277],
        [0.0299, 0.0249, 0.0000,  ..., 0.0524, 0.0000, 0.0169],
        ...,
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65550.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1073, 0.1312, 0.2648,  ..., 0.0426, 0.0980, 0.0000],
        [0.1052, 0.1308, 0.2600,  ..., 0.0417, 0.0964, 0.0000],
        [0.0877, 0.1221, 0.2146,  ..., 0.0312, 0.0828, 0.0000],
        ...,
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0144, 0.0000],
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544737.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3388.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(285.0018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5395.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1001.0743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-656.9556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2649],
        [ 0.2693],
        [ 0.2704],
        ...,
        [-4.3058],
        [-4.2986],
        [-4.2965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316638.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0297],
        [1.0304],
        [1.0291],
        ...,
        [0.9988],
        [0.9979],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369676.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0304],
        [1.0292],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369681.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.4350e-05,  3.8766e-04, -6.1094e-04,  ...,  9.1931e-04,
          0.0000e+00, -7.3194e-04],
        [ 7.4350e-05,  3.8766e-04, -6.1094e-04,  ...,  9.1931e-04,
          0.0000e+00, -7.3194e-04],
        [ 7.4350e-05,  3.8766e-04, -6.1094e-04,  ...,  9.1931e-04,
          0.0000e+00, -7.3194e-04],
        ...,
        [ 7.4350e-05,  3.8766e-04, -6.1094e-04,  ...,  9.1931e-04,
          0.0000e+00, -7.3194e-04],
        [ 7.4350e-05,  3.8766e-04, -6.1094e-04,  ...,  9.1931e-04,
          0.0000e+00, -7.3194e-04],
        [ 7.4350e-05,  3.8766e-04, -6.1094e-04,  ...,  9.1931e-04,
          0.0000e+00, -7.3194e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3186.5903, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.3614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5369, device='cuda:0')



h[100].sum tensor(107.3827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5122, device='cuda:0')



h[200].sum tensor(44.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62014.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0785, 0.0000,  ..., 0.0000, 0.0139, 0.0000],
        [0.0014, 0.0790, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        [0.0014, 0.0792, 0.0000,  ..., 0.0000, 0.0140, 0.0000],
        ...,
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0015, 0.0807, 0.0000,  ..., 0.0000, 0.0145, 0.0000],
        [0.0015, 0.0806, 0.0000,  ..., 0.0000, 0.0144, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533173.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3135.9263, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.4123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5714.9546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(949.1716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-614.0448, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9405],
        [-3.8541],
        [-3.6591],
        ...,
        [-4.3109],
        [-4.3036],
        [-4.3014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343747.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0304],
        [1.0292],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369681.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0304],
        [1.0293],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369686.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.1939e-05,  3.9776e-04, -6.1094e-04,  ...,  9.0942e-04,
          0.0000e+00, -7.4384e-04],
        [ 9.1939e-05,  3.9776e-04, -6.1094e-04,  ...,  9.0942e-04,
          0.0000e+00, -7.4384e-04],
        [ 9.1939e-05,  3.9776e-04, -6.1094e-04,  ...,  9.0942e-04,
          0.0000e+00, -7.4384e-04],
        ...,
        [ 9.1939e-05,  3.9776e-04, -6.1094e-04,  ...,  9.0942e-04,
          0.0000e+00, -7.4384e-04],
        [ 9.1939e-05,  3.9776e-04, -6.1094e-04,  ...,  9.0942e-04,
          0.0000e+00, -7.4384e-04],
        [ 9.1939e-05,  3.9776e-04, -6.1094e-04,  ...,  9.0942e-04,
          0.0000e+00, -7.4384e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3474.5481, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7214, device='cuda:0')



h[100].sum tensor(107.7048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.0432, device='cuda:0')



h[200].sum tensor(49.6294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3074, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65353.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0784, 0.0000,  ..., 0.0000, 0.0141, 0.0000],
        [0.0017, 0.0789, 0.0000,  ..., 0.0000, 0.0142, 0.0000],
        [0.0079, 0.0822, 0.0143,  ..., 0.0000, 0.0191, 0.0000],
        ...,
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0018, 0.0806, 0.0000,  ..., 0.0000, 0.0147, 0.0000],
        [0.0018, 0.0805, 0.0000,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539483.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3274.7036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.3909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5999.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(995.6240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-647.5518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2442],
        [-2.5727],
        [-2.4642],
        ...,
        [-4.2967],
        [-4.2897],
        [-4.2878]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331561.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0304],
        [1.0293],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369686.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0298],
        [1.0305],
        [1.0294],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369691.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4069.6650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.0180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8548, device='cuda:0')



h[100].sum tensor(109.3503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.4005, device='cuda:0')



h[200].sum tensor(59.4942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7684, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0017, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0005, 0.0017, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0005, 0.0017, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0005, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0005, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78388.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6429e-02, 8.4487e-02, 3.3133e-02,  ..., 0.0000e+00, 2.6513e-02,
         0.0000e+00],
        [1.1567e-02, 8.2806e-02, 2.1115e-02,  ..., 0.0000e+00, 2.2574e-02,
         0.0000e+00],
        [8.2834e-03, 8.1708e-02, 1.4156e-02,  ..., 0.0000e+00, 1.9776e-02,
         0.0000e+00],
        ...,
        [2.1874e-03, 8.0539e-02, 1.8922e-05,  ..., 0.0000e+00, 1.4955e-02,
         0.0000e+00],
        [2.1867e-03, 8.0522e-02, 1.8906e-05,  ..., 0.0000e+00, 1.4952e-02,
         0.0000e+00],
        [2.1835e-03, 8.0464e-02, 1.7106e-05,  ..., 0.0000e+00, 1.4938e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595985.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4358.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(400.4681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5629.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1183.9274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-790.7677, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9627],
        [-1.4779],
        [-1.9709],
        ...,
        [-4.2714],
        [-4.2647],
        [-4.2629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259144.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0298],
        [1.0305],
        [1.0294],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369691.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0295],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369697.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0225,  0.0180, -0.0008,  ...,  0.0375, -0.0015,  0.0137],
        [ 0.0064,  0.0053, -0.0007,  ...,  0.0111, -0.0004,  0.0033],
        [ 0.0197,  0.0158, -0.0007,  ...,  0.0329, -0.0014,  0.0119],
        ...,
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4683.4741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(43.6726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1195, device='cuda:0')



h[100].sum tensor(111.7816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(60.1506, device='cuda:0')



h[200].sum tensor(69.0313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.2441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0346, 0.0284, 0.0000,  ..., 0.0595, 0.0000, 0.0197],
        [0.0850, 0.0680, 0.0000,  ..., 0.1420, 0.0000, 0.0516],
        [0.0543, 0.0440, 0.0000,  ..., 0.0919, 0.0000, 0.0317],
        ...,
        [0.0005, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0005, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0005, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(93029.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1235e-01, 1.3244e-01, 2.8319e-01,  ..., 4.7749e-02, 1.0150e-01,
         0.0000e+00],
        [1.7034e-01, 1.6219e-01, 4.3597e-01,  ..., 8.4072e-02, 1.4636e-01,
         0.0000e+00],
        [1.7565e-01, 1.6516e-01, 4.4931e-01,  ..., 8.6905e-02, 1.5051e-01,
         0.0000e+00],
        ...,
        [2.0980e-03, 8.1063e-02, 3.3318e-04,  ..., 0.0000e+00, 1.4994e-02,
         0.0000e+00],
        [6.4986e-03, 8.3222e-02, 1.0836e-02,  ..., 0.0000e+00, 1.8439e-02,
         0.0000e+00],
        [1.9640e-02, 8.9676e-02, 4.4855e-02,  ..., 1.5137e-03, 2.8693e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(677903., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5488.7710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(530.4066, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5671.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1384.9469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-945.3069, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1631],
        [ 0.1831],
        [ 0.1754],
        ...,
        [-3.5567],
        [-2.7948],
        [-1.8048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257989.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0295],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369697.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0296],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369702.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3933.6621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.3921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6755, device='cuda:0')



h[100].sum tensor(108.6452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.8748, device='cuda:0')



h[200].sum tensor(56.1645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0017, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0017, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75220.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0802, 0.0030,  ..., 0.0000, 0.0158, 0.0000],
        [0.0018, 0.0803, 0.0008,  ..., 0.0000, 0.0146, 0.0000],
        [0.0015, 0.0805, 0.0003,  ..., 0.0000, 0.0144, 0.0000],
        ...,
        [0.0016, 0.0820, 0.0003,  ..., 0.0000, 0.0148, 0.0000],
        [0.0016, 0.0820, 0.0003,  ..., 0.0000, 0.0148, 0.0000],
        [0.0016, 0.0819, 0.0003,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587731.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4055.2571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.1168, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5892.2251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1140.9176, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-750.4415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1506],
        [-3.5886],
        [-3.8975],
        ...,
        [-4.2846],
        [-4.2778],
        [-4.2760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271889.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0296],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369702.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0296],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369707.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3346.6880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.9347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4820, device='cuda:0')



h[100].sum tensor(105.9361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.3376, device='cuda:0')



h[200].sum tensor(46.5147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1691, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0017, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0017, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65731.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0800, 0.0006,  ..., 0.0000, 0.0142, 0.0000],
        [0.0013, 0.0806, 0.0006,  ..., 0.0000, 0.0143, 0.0000],
        [0.0013, 0.0808, 0.0006,  ..., 0.0000, 0.0144, 0.0000],
        ...,
        [0.0015, 0.0824, 0.0007,  ..., 0.0000, 0.0148, 0.0000],
        [0.0015, 0.0823, 0.0007,  ..., 0.0000, 0.0148, 0.0000],
        [0.0014, 0.0823, 0.0007,  ..., 0.0000, 0.0148, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549357.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3367.8713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.9167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6365.7598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1011.3060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-646.7288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2513],
        [-4.1696],
        [-4.0178],
        ...,
        [-4.2846],
        [-4.2778],
        [-4.2760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288014.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0299],
        [1.0305],
        [1.0296],
        ...,
        [0.9988],
        [0.9979],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369707.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 60.0 event: 300 loss: tensor(391.3492, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0300],
        [1.0306],
        [1.0297],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369712.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.5345e-05,  3.9035e-04, -6.1094e-04,  ...,  8.9592e-04,
          0.0000e+00, -7.4798e-04],
        [ 9.5345e-05,  3.9035e-04, -6.1094e-04,  ...,  8.9592e-04,
          0.0000e+00, -7.4798e-04],
        [ 9.5345e-05,  3.9035e-04, -6.1094e-04,  ...,  8.9592e-04,
          0.0000e+00, -7.4798e-04],
        ...,
        [ 9.5345e-05,  3.9035e-04, -6.1094e-04,  ...,  8.9592e-04,
          0.0000e+00, -7.4798e-04],
        [ 9.5345e-05,  3.9035e-04, -6.1094e-04,  ...,  8.9592e-04,
          0.0000e+00, -7.4798e-04],
        [ 9.5345e-05,  3.9035e-04, -6.1094e-04,  ...,  8.9592e-04,
          0.0000e+00, -7.4798e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3649.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.9381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6016, device='cuda:0')



h[100].sum tensor(107.4140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.6746, device='cuda:0')



h[200].sum tensor(50.9510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4055, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0069, 0.0067, 0.0000,  ..., 0.0143, 0.0000, 0.0034],
        ...,
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69403.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0810, 0.0027,  ..., 0.0000, 0.0150, 0.0000],
        [0.0089, 0.0847, 0.0202,  ..., 0.0000, 0.0208, 0.0000],
        [0.0236, 0.0915, 0.0569,  ..., 0.0016, 0.0330, 0.0000],
        ...,
        [0.0012, 0.0829, 0.0009,  ..., 0.0000, 0.0147, 0.0000],
        [0.0012, 0.0829, 0.0009,  ..., 0.0000, 0.0147, 0.0000],
        [0.0012, 0.0828, 0.0009,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558518.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3398.0088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.5087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6442.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1061.8948, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-682.7233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8319],
        [-2.2011],
        [-1.4740],
        ...,
        [-4.2937],
        [-4.2869],
        [-4.2851]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307178.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0300],
        [1.0306],
        [1.0297],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369712.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0301],
        [1.0306],
        [1.0298],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369717.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.5039e-02,  1.2122e-02, -7.1031e-04,  ...,  2.5377e-02,
         -1.0108e-03,  8.9303e-03],
        [ 1.9844e-02,  1.5896e-02, -7.4226e-04,  ...,  3.3247e-02,
         -1.3358e-03,  1.2045e-02],
        [ 9.8551e-03,  8.0497e-03, -6.7584e-04,  ...,  1.6885e-02,
         -6.6013e-04,  5.5698e-03],
        ...,
        [ 9.6093e-05,  3.8410e-04, -6.1094e-04,  ...,  8.9868e-04,
          0.0000e+00, -7.5621e-04],
        [ 9.6093e-05,  3.8410e-04, -6.1094e-04,  ...,  8.9868e-04,
          0.0000e+00, -7.5621e-04],
        [ 9.6093e-05,  3.8410e-04, -6.1094e-04,  ...,  8.9868e-04,
          0.0000e+00, -7.5621e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3019.9639, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.0424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0306, device='cuda:0')



h[100].sum tensor(104.6133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.0089, device='cuda:0')



h[200].sum tensor(40.7163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8957, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0920, 0.0735, 0.0000,  ..., 0.1537, 0.0000, 0.0563],
        [0.0774, 0.0620, 0.0000,  ..., 0.1298, 0.0000, 0.0468],
        [0.0646, 0.0521, 0.0000,  ..., 0.1090, 0.0000, 0.0393],
        ...,
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59912.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2439, 0.2016, 0.6552,  ..., 0.1411, 0.2034, 0.0000],
        [0.2223, 0.1913, 0.5962,  ..., 0.1265, 0.1867, 0.0000],
        [0.1753, 0.1682, 0.4692,  ..., 0.0960, 0.1502, 0.0000],
        ...,
        [0.0010, 0.0831, 0.0011,  ..., 0.0000, 0.0147, 0.0000],
        [0.0010, 0.0831, 0.0011,  ..., 0.0000, 0.0147, 0.0000],
        [0.0010, 0.0830, 0.0011,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528444.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2921.0334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.5399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6658.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(933.4896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-581.3810, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0639],
        [-0.0595],
        [-0.0714],
        ...,
        [-4.3023],
        [-4.2959],
        [-4.2945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294037.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0301],
        [1.0306],
        [1.0298],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369717.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0302],
        [1.0307],
        [1.0299],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369722.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8726e-03,  4.1263e-03, -6.4271e-04,  ...,  8.7280e-03,
         -3.2142e-04,  2.3317e-03],
        [ 1.1817e-02,  9.5810e-03, -6.8889e-04,  ...,  2.0105e-02,
         -7.8858e-04,  6.8334e-03],
        [ 9.4752e-05,  3.7326e-04, -6.1094e-04,  ...,  9.0031e-04,
          0.0000e+00, -7.6576e-04],
        ...,
        [ 9.4752e-05,  3.7326e-04, -6.1094e-04,  ...,  9.0031e-04,
          0.0000e+00, -7.6576e-04],
        [ 9.4752e-05,  3.7326e-04, -6.1094e-04,  ...,  9.0031e-04,
          0.0000e+00, -7.6576e-04],
        [ 9.4752e-05,  3.7326e-04, -6.1094e-04,  ...,  9.0031e-04,
          0.0000e+00, -7.6576e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4248.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.5612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.2256, device='cuda:0')



h[100].sum tensor(110.2265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(51.4989, device='cuda:0')



h[200].sum tensor(60.1348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0434, 0.0353, 0.0000,  ..., 0.0742, 0.0000, 0.0248],
        [0.0194, 0.0164, 0.0000,  ..., 0.0348, 0.0000, 0.0099],
        [0.0165, 0.0142, 0.0000,  ..., 0.0301, 0.0000, 0.0089],
        ...,
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78042.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0702, 0.1137, 0.1808,  ..., 0.0238, 0.0697, 0.0000],
        [0.0540, 0.1060, 0.1377,  ..., 0.0140, 0.0573, 0.0000],
        [0.0404, 0.1000, 0.1028,  ..., 0.0090, 0.0464, 0.0000],
        ...,
        [0.0010, 0.0832, 0.0012,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0832, 0.0012,  ..., 0.0000, 0.0145, 0.0000],
        [0.0010, 0.0831, 0.0012,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591271.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3703.0107, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(404.1963, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6740.7974, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1176.1035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-770.4081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1556],
        [-0.2776],
        [-0.7589],
        ...,
        [-4.3024],
        [-4.2490],
        [-4.1598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335082.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0302],
        [1.0307],
        [1.0299],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369722.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0303],
        [1.0308],
        [1.0300],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369725.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0004, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3167.6279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.2866, device='cuda:0')



h[100].sum tensor(105.2255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.7638, device='cuda:0')



h[200].sum tensor(43.2060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0358, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0015, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0015, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0004, 0.0015, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0004, 0.0016, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61502.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0814, 0.0062,  ..., 0.0000, 0.0159, 0.0000],
        [0.0013, 0.0812, 0.0019,  ..., 0.0000, 0.0143, 0.0000],
        [0.0011, 0.0813, 0.0013,  ..., 0.0000, 0.0141, 0.0000],
        ...,
        [0.0012, 0.0829, 0.0014,  ..., 0.0000, 0.0146, 0.0000],
        [0.0012, 0.0828, 0.0014,  ..., 0.0000, 0.0146, 0.0000],
        [0.0012, 0.0828, 0.0014,  ..., 0.0000, 0.0145, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531255.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2982.6924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(256.3211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6790.3389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(952.4619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-597.7902, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9933],
        [-3.1779],
        [-3.2965],
        ...,
        [-4.3336],
        [-4.3268],
        [-4.3250]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304427.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0303],
        [1.0308],
        [1.0300],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369725.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0308],
        [1.0301],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369729.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.6295e-05,  3.5884e-04, -6.1094e-04,  ...,  9.0309e-04,
          0.0000e+00, -8.0675e-04],
        [ 2.8947e-02,  2.3018e-02, -8.0270e-04,  ...,  4.8142e-02,
         -1.9187e-03,  1.7864e-02],
        [ 1.3688e-02,  1.1034e-02, -7.0128e-04,  ...,  2.3158e-02,
         -9.0392e-04,  7.9894e-03],
        ...,
        [ 1.0480e-02,  8.5139e-03, -6.7996e-04,  ...,  1.7904e-02,
         -6.9053e-04,  5.9130e-03],
        [ 7.3847e-03,  6.0831e-03, -6.5938e-04,  ...,  1.2837e-02,
         -4.8470e-04,  3.9100e-03],
        [ 1.0480e-02,  8.5139e-03, -6.7996e-04,  ...,  1.7904e-02,
         -6.9053e-04,  5.9130e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3443.5298, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.6503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5207, device='cuda:0')



h[100].sum tensor(106.7345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.4431, device='cuda:0')



h[200].sum tensor(47.4432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0996, 0.0794, 0.0000,  ..., 0.1662, 0.0000, 0.0609],
        [0.0388, 0.0316, 0.0000,  ..., 0.0665, 0.0000, 0.0223],
        [0.1011, 0.0806, 0.0000,  ..., 0.1687, 0.0000, 0.0619],
        ...,
        [0.0169, 0.0145, 0.0000,  ..., 0.0308, 0.0000, 0.0090],
        [0.0461, 0.0374, 0.0000,  ..., 0.0787, 0.0000, 0.0262],
        [0.0370, 0.0302, 0.0000,  ..., 0.0637, 0.0000, 0.0203]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64850.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1944, 0.1747, 0.5098,  ..., 0.1043, 0.1665, 0.0000],
        [0.1496, 0.1535, 0.3895,  ..., 0.0748, 0.1313, 0.0000],
        [0.2015, 0.1790, 0.5281,  ..., 0.1084, 0.1721, 0.0000],
        ...,
        [0.0421, 0.1024, 0.1049,  ..., 0.0088, 0.0473, 0.0000],
        [0.0678, 0.1149, 0.1714,  ..., 0.0220, 0.0677, 0.0000],
        [0.0743, 0.1177, 0.1882,  ..., 0.0256, 0.0731, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541061.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3067.7964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.1069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6836.4194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(993.0383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-630.7668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1095],
        [ 0.1173],
        [ 0.1030],
        ...,
        [-1.9003],
        [-1.0895],
        [-0.5188]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332916.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0308],
        [1.0301],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369729.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0304],
        [1.0309],
        [1.0302],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369733.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.2012e-05,  3.4076e-04, -6.1094e-04,  ...,  9.0150e-04,
          0.0000e+00, -8.1759e-04],
        [ 8.2012e-05,  3.4076e-04, -6.1094e-04,  ...,  9.0150e-04,
          0.0000e+00, -8.1759e-04],
        [ 8.2012e-05,  3.4076e-04, -6.1094e-04,  ...,  9.0150e-04,
          0.0000e+00, -8.1759e-04],
        ...,
        [ 8.2012e-05,  3.4076e-04, -6.1094e-04,  ...,  9.0150e-04,
          0.0000e+00, -8.1759e-04],
        [ 8.2012e-05,  3.4076e-04, -6.1094e-04,  ...,  9.0150e-04,
          0.0000e+00, -8.1759e-04],
        [ 8.2012e-05,  3.4076e-04, -6.1094e-04,  ...,  9.0150e-04,
          0.0000e+00, -8.1759e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3147.7212, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3766, device='cuda:0')



h[100].sum tensor(106.0166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0329, device='cuda:0')



h[200].sum tensor(42.2403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0014, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61956.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0221, 0.0912, 0.0545,  ..., 0.0027, 0.0307, 0.0000],
        [0.0083, 0.0850, 0.0191,  ..., 0.0000, 0.0200, 0.0000],
        [0.0040, 0.0829, 0.0078,  ..., 0.0000, 0.0168, 0.0000],
        ...,
        [0.0010, 0.0834, 0.0010,  ..., 0.0000, 0.0142, 0.0000],
        [0.0010, 0.0833, 0.0010,  ..., 0.0000, 0.0142, 0.0000],
        [0.0010, 0.0833, 0.0010,  ..., 0.0000, 0.0142, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537672.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2959.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.0191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6842.2080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(952.0737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-600.4860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1513],
        [-1.7588],
        [-1.9992],
        ...,
        [-4.3971],
        [-4.3900],
        [-4.3881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347284.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0304],
        [1.0309],
        [1.0302],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369733.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0305],
        [1.0310],
        [1.0303],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369737.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7410e-05,  3.3428e-04, -6.1094e-04,  ...,  9.0534e-04,
          0.0000e+00, -8.1840e-04],
        [ 6.7410e-05,  3.3428e-04, -6.1094e-04,  ...,  9.0534e-04,
          0.0000e+00, -8.1840e-04],
        [ 6.7410e-05,  3.3428e-04, -6.1094e-04,  ...,  9.0534e-04,
          0.0000e+00, -8.1840e-04],
        ...,
        [ 6.7410e-05,  3.3428e-04, -6.1094e-04,  ...,  9.0534e-04,
          0.0000e+00, -8.1840e-04],
        [ 6.7410e-05,  3.3428e-04, -6.1094e-04,  ...,  9.0534e-04,
          0.0000e+00, -8.1840e-04],
        [ 6.7410e-05,  3.3428e-04, -6.1094e-04,  ...,  9.0534e-04,
          0.0000e+00, -8.1840e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3347.5952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.5798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8476, device='cuda:0')



h[100].sum tensor(107.3795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.4307, device='cuda:0')



h[200].sum tensor(45.0834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0014, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66023.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0125, 0.0873, 0.0307,  ..., 0.0000, 0.0229, 0.0000],
        [0.0039, 0.0836, 0.0085,  ..., 0.0000, 0.0162, 0.0000],
        [0.0048, 0.0842, 0.0105,  ..., 0.0000, 0.0170, 0.0000],
        ...,
        [0.0008, 0.0838, 0.0009,  ..., 0.0000, 0.0141, 0.0000],
        [0.0008, 0.0838, 0.0009,  ..., 0.0000, 0.0141, 0.0000],
        [0.0008, 0.0838, 0.0009,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552645.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3227.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6455.1514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1011.6767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-648.0230, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3713],
        [-2.5534],
        [-2.2089],
        ...,
        [-4.4170],
        [-4.4115],
        [-4.4109]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314338.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0305],
        [1.0310],
        [1.0303],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369737.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0307],
        [1.0312],
        [1.0304],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369741.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.9437e-05,  3.2968e-04, -6.1094e-04,  ...,  9.1474e-04,
          0.0000e+00, -8.0932e-04],
        [ 4.9437e-05,  3.2968e-04, -6.1094e-04,  ...,  9.1474e-04,
          0.0000e+00, -8.0932e-04],
        [ 4.9437e-05,  3.2968e-04, -6.1094e-04,  ...,  9.1474e-04,
          0.0000e+00, -8.0932e-04],
        ...,
        [ 4.9437e-05,  3.2968e-04, -6.1094e-04,  ...,  9.1474e-04,
          0.0000e+00, -8.0932e-04],
        [ 4.9437e-05,  3.2968e-04, -6.1094e-04,  ...,  9.1474e-04,
          0.0000e+00, -8.0932e-04],
        [ 4.9437e-05,  3.2968e-04, -6.1094e-04,  ...,  9.1474e-04,
          0.0000e+00, -8.0932e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2887.7417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.4096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.5254, device='cuda:0')



h[100].sum tensor(105.7318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.4983, device='cuda:0')



h[200].sum tensor(37.3386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0045, 0.0047, 0.0000,  ..., 0.0108, 0.0000, 0.0019],
        [0.0087, 0.0081, 0.0000,  ..., 0.0178, 0.0000, 0.0038],
        ...,
        [0.0002, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0002, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0002, 0.0014, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56341.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0133, 0.0878, 0.0316,  ..., 0.0000, 0.0248, 0.0000],
        [0.0224, 0.0925, 0.0538,  ..., 0.0010, 0.0329, 0.0000],
        [0.0306, 0.0965, 0.0740,  ..., 0.0033, 0.0399, 0.0000],
        ...,
        [0.0007, 0.0845, 0.0007,  ..., 0.0000, 0.0139, 0.0000],
        [0.0007, 0.0845, 0.0007,  ..., 0.0000, 0.0139, 0.0000],
        [0.0007, 0.0844, 0.0007,  ..., 0.0000, 0.0139, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(514023.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2410.9448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.5006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6816.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(870.4915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-536.7757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3850],
        [-0.8853],
        [-0.5463],
        ...,
        [-4.4465],
        [-4.4391],
        [-4.4372]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-395942.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0307],
        [1.0312],
        [1.0304],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369741.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0308],
        [1.0313],
        [1.0306],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369746.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1374e-02,  9.2457e-03, -6.8624e-04,  ...,  1.9476e-02,
         -7.3689e-04,  6.5239e-03],
        [ 4.7212e-03,  4.0193e-03, -6.4204e-04,  ...,  8.5871e-03,
         -3.0433e-04,  2.2251e-03],
        [ 6.6934e-03,  5.5686e-03, -6.5514e-04,  ...,  1.1815e-02,
         -4.3256e-04,  3.4994e-03],
        ...,
        [ 4.0443e-05,  3.4222e-04, -6.1094e-04,  ...,  9.2594e-04,
          0.0000e+00, -7.9932e-04],
        [ 4.0443e-05,  3.4222e-04, -6.1094e-04,  ...,  9.2594e-04,
          0.0000e+00, -7.9932e-04],
        [ 4.0443e-05,  3.4222e-04, -6.1094e-04,  ...,  9.2594e-04,
          0.0000e+00, -7.9932e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3380.8586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.8786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9639, device='cuda:0')



h[100].sum tensor(107.8667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.7783, device='cuda:0')



h[200].sum tensor(45.1441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0165, 0.0000,  ..., 0.0353, 0.0000, 0.0100],
        [0.0439, 0.0358, 0.0000,  ..., 0.0754, 0.0000, 0.0249],
        [0.0196, 0.0167, 0.0000,  ..., 0.0356, 0.0000, 0.0109],
        ...,
        [0.0002, 0.0014, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0014, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0014, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64699.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0544, 0.1085, 0.1353,  ..., 0.0125, 0.0592, 0.0000],
        [0.0720, 0.1180, 0.1802,  ..., 0.0218, 0.0730, 0.0000],
        [0.0583, 0.1115, 0.1459,  ..., 0.0149, 0.0619, 0.0000],
        ...,
        [0.0006, 0.0851, 0.0008,  ..., 0.0000, 0.0141, 0.0000],
        [0.0006, 0.0850, 0.0008,  ..., 0.0000, 0.0141, 0.0000],
        [0.0006, 0.0850, 0.0008,  ..., 0.0000, 0.0141, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(545085.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2883.7751, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.0824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6487.3076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(986.7999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-626.5271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0578],
        [ 0.1506],
        [ 0.2075],
        ...,
        [-4.4454],
        [-4.4397],
        [-4.4394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-394229.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0308],
        [1.0313],
        [1.0306],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369746.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0309],
        [1.0314],
        [1.0307],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369751.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.7334e-05,  3.7949e-04, -6.1094e-04,  ...,  9.4181e-04,
          0.0000e+00, -7.8775e-04],
        [ 4.7334e-05,  3.7949e-04, -6.1094e-04,  ...,  9.4181e-04,
          0.0000e+00, -7.8775e-04],
        [ 4.7334e-05,  3.7949e-04, -6.1094e-04,  ...,  9.4181e-04,
          0.0000e+00, -7.8775e-04],
        ...,
        [ 4.7334e-05,  3.7949e-04, -6.1094e-04,  ...,  9.4181e-04,
          0.0000e+00, -7.8775e-04],
        [ 4.7334e-05,  3.7949e-04, -6.1094e-04,  ...,  9.4181e-04,
          0.0000e+00, -7.8775e-04],
        [ 4.7334e-05,  3.7949e-04, -6.1094e-04,  ...,  9.4181e-04,
          0.0000e+00, -7.8775e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3327.7078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4094, device='cuda:0')



h[100].sum tensor(107.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1207, device='cuda:0')



h[200].sum tensor(44.9249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1610, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0016, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65712.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0103, 0.0873, 0.0250,  ..., 0.0000, 0.0227, 0.0000],
        [0.0032, 0.0846, 0.0073,  ..., 0.0000, 0.0165, 0.0000],
        [0.0007, 0.0837, 0.0014,  ..., 0.0000, 0.0143, 0.0000],
        ...,
        [0.0008, 0.0853, 0.0014,  ..., 0.0000, 0.0147, 0.0000],
        [0.0008, 0.0852, 0.0014,  ..., 0.0000, 0.0147, 0.0000],
        [0.0008, 0.0852, 0.0014,  ..., 0.0000, 0.0147, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550792.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3197.9636, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.5104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5986.3936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1011.4096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-644.5554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4808],
        [-2.2402],
        [-2.8763],
        ...,
        [-4.4217],
        [-4.4145],
        [-4.4126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302382.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0309],
        [1.0314],
        [1.0307],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369751.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 70.0 event: 350 loss: tensor(483.4464, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0310],
        [1.0315],
        [1.0308],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369756.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6813e-02,  1.3585e-02, -7.2229e-04,  ...,  2.8397e-02,
         -1.0776e-03,  1.0057e-02],
        [ 2.2417e-02,  1.7990e-02, -7.5951e-04,  ...,  3.7571e-02,
         -1.4379e-03,  1.3677e-02],
        [ 2.8875e-02,  2.3066e-02, -8.0241e-04,  ...,  4.8144e-02,
         -1.8531e-03,  1.7850e-02],
        ...,
        [ 5.2290e-05,  4.1152e-04, -6.1094e-04,  ...,  9.5652e-04,
          0.0000e+00, -7.7273e-04],
        [ 5.2290e-05,  4.1152e-04, -6.1094e-04,  ...,  9.5652e-04,
          0.0000e+00, -7.7273e-04],
        [ 5.2290e-05,  4.1152e-04, -6.1094e-04,  ...,  9.5652e-04,
          0.0000e+00, -7.7273e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3930.0107, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.2341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4501, device='cuda:0')



h[100].sum tensor(109.3537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.2010, device='cuda:0')



h[200].sum tensor(55.0099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0774, 0.0624, 0.0000,  ..., 0.1303, 0.0000, 0.0467],
        [0.0852, 0.0685, 0.0000,  ..., 0.1431, 0.0000, 0.0517],
        [0.1014, 0.0812, 0.0000,  ..., 0.1696, 0.0000, 0.0622],
        ...,
        [0.0002, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0002, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0002, 0.0017, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74314.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1551, 0.1586, 0.3974,  ..., 0.0723, 0.1382, 0.0000],
        [0.1865, 0.1743, 0.4792,  ..., 0.0915, 0.1634, 0.0000],
        [0.2132, 0.1872, 0.5485,  ..., 0.1076, 0.1849, 0.0000],
        ...,
        [0.0009, 0.0855, 0.0020,  ..., 0.0000, 0.0152, 0.0000],
        [0.0009, 0.0855, 0.0020,  ..., 0.0000, 0.0152, 0.0000],
        [0.0008, 0.0854, 0.0020,  ..., 0.0000, 0.0151, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576839.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3646.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.6887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5781.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1132.8407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-736.6739, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1964],
        [ 0.2044],
        [ 0.2027],
        ...,
        [-4.3963],
        [-4.3893],
        [-4.3875]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286011.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0310],
        [1.0315],
        [1.0308],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369756.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0311],
        [1.0316],
        [1.0309],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369761.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7746e-05,  4.3233e-04, -6.1094e-04,  ...,  9.7007e-04,
          0.0000e+00, -7.6361e-04],
        [ 5.7746e-05,  4.3233e-04, -6.1094e-04,  ...,  9.7007e-04,
          0.0000e+00, -7.6361e-04],
        [ 5.7746e-05,  4.3233e-04, -6.1094e-04,  ...,  9.7007e-04,
          0.0000e+00, -7.6361e-04],
        ...,
        [ 5.7746e-05,  4.3233e-04, -6.1094e-04,  ...,  9.7007e-04,
          0.0000e+00, -7.6361e-04],
        [ 5.7746e-05,  4.3233e-04, -6.1094e-04,  ...,  9.7007e-04,
          0.0000e+00, -7.6361e-04],
        [ 5.7746e-05,  4.3233e-04, -6.1094e-04,  ...,  9.7007e-04,
          0.0000e+00, -7.6361e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3499.9307, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.1145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3225, device='cuda:0')



h[100].sum tensor(107.2646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.8506, device='cuda:0')



h[200].sum tensor(48.5853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0151, 0.0135, 0.0000,  ..., 0.0284, 0.0000, 0.0081],
        [0.0002, 0.0018, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67869.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0389, 0.1008, 0.0977,  ..., 0.0045, 0.0475, 0.0000],
        [0.0180, 0.0916, 0.0452,  ..., 0.0000, 0.0300, 0.0000],
        [0.0079, 0.0872, 0.0202,  ..., 0.0000, 0.0214, 0.0000],
        ...,
        [0.0009, 0.0856, 0.0028,  ..., 0.0000, 0.0155, 0.0000],
        [0.0009, 0.0855, 0.0028,  ..., 0.0000, 0.0155, 0.0000],
        [0.0009, 0.0855, 0.0028,  ..., 0.0000, 0.0155, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554928.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3360.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.1380, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5808.9111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1046.1344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-667.7825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7666],
        [-1.4638],
        [-2.1933],
        ...,
        [-4.3846],
        [-4.3775],
        [-4.3727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273146.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0311],
        [1.0316],
        [1.0309],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369761.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0317],
        [1.0310],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369766.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.3048e-05,  4.4318e-04, -6.1094e-04,  ...,  9.8283e-04,
          0.0000e+00, -7.5915e-04],
        [ 9.3909e-03,  7.7767e-03, -6.7291e-04,  ...,  1.6262e-02,
         -5.9308e-04,  5.2686e-03],
        [ 6.8084e-03,  5.7463e-03, -6.5575e-04,  ...,  1.2032e-02,
         -4.2888e-04,  3.5998e-03],
        ...,
        [ 6.3048e-05,  4.4318e-04, -6.1094e-04,  ...,  9.8283e-04,
          0.0000e+00, -7.5915e-04],
        [ 6.3048e-05,  4.4318e-04, -6.1094e-04,  ...,  9.8283e-04,
          0.0000e+00, -7.5915e-04],
        [ 6.3048e-05,  4.4318e-04, -6.1094e-04,  ...,  9.8283e-04,
          0.0000e+00, -7.5915e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3510.5024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.3392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3530, device='cuda:0')



h[100].sum tensor(107.4377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.9415, device='cuda:0')



h[200].sum tensor(49.0434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0229, 0.0196, 0.0000,  ..., 0.0412, 0.0000, 0.0131],
        [0.0208, 0.0180, 0.0000,  ..., 0.0377, 0.0000, 0.0117],
        [0.0480, 0.0393, 0.0000,  ..., 0.0822, 0.0000, 0.0277],
        ...,
        [0.0003, 0.0019, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0019, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0019, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69282.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0536, 0.1083, 0.1365,  ..., 0.0113, 0.0581, 0.0000],
        [0.0579, 0.1108, 0.1471,  ..., 0.0129, 0.0621, 0.0000],
        [0.0831, 0.1230, 0.2108,  ..., 0.0263, 0.0826, 0.0000],
        ...,
        [0.0010, 0.0854, 0.0034,  ..., 0.0000, 0.0158, 0.0000],
        [0.0010, 0.0854, 0.0034,  ..., 0.0000, 0.0158, 0.0000],
        [0.0010, 0.0853, 0.0034,  ..., 0.0000, 0.0158, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563925.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3503.0972, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.7242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5841.2324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1066.3654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.1091, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1803],
        [ 0.1898],
        [ 0.2171],
        ...,
        [-4.3845],
        [-4.3776],
        [-4.3759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273109.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0317],
        [1.0310],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369766.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0312],
        [1.0319],
        [1.0311],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369770.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.5890e-05,  4.4511e-04, -6.1094e-04,  ...,  9.9435e-04,
          0.0000e+00, -7.5820e-04],
        [ 4.4773e-03,  3.9134e-03, -6.4025e-04,  ...,  8.2223e-03,
         -2.7892e-04,  2.0929e-03],
        [ 4.4773e-03,  3.9134e-03, -6.4025e-04,  ...,  8.2223e-03,
         -2.7892e-04,  2.0929e-03],
        ...,
        [ 6.5890e-05,  4.4511e-04, -6.1094e-04,  ...,  9.9435e-04,
          0.0000e+00, -7.5820e-04],
        [ 6.5890e-05,  4.4511e-04, -6.1094e-04,  ...,  9.9435e-04,
          0.0000e+00, -7.5820e-04],
        [ 6.5890e-05,  4.4511e-04, -6.1094e-04,  ...,  9.9435e-04,
          0.0000e+00, -7.5820e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3584.5554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.5030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.9948, device='cuda:0')



h[100].sum tensor(108.1768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.8604, device='cuda:0')



h[200].sum tensor(50.3721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0085, 0.0083, 0.0000,  ..., 0.0175, 0.0000, 0.0038],
        [0.0085, 0.0083, 0.0000,  ..., 0.0177, 0.0000, 0.0038],
        [0.0086, 0.0084, 0.0000,  ..., 0.0177, 0.0000, 0.0038],
        ...,
        [0.0003, 0.0019, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0019, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0019, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68433.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0161, 0.0886, 0.0400,  ..., 0.0000, 0.0295, 0.0000],
        [0.0251, 0.0931, 0.0624,  ..., 0.0003, 0.0374, 0.0000],
        [0.0450, 0.1028, 0.1134,  ..., 0.0084, 0.0534, 0.0000],
        ...,
        [0.0011, 0.0851, 0.0037,  ..., 0.0000, 0.0159, 0.0000],
        [0.0011, 0.0851, 0.0037,  ..., 0.0000, 0.0159, 0.0000],
        [0.0011, 0.0850, 0.0037,  ..., 0.0000, 0.0159, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556128.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3405.8413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.7477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5839.7007, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1054.6688, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-676.9304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1904],
        [-1.2303],
        [-0.4425],
        ...,
        [-4.3968],
        [-4.3899],
        [-4.3881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263968.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0312],
        [1.0319],
        [1.0311],
        ...,
        [0.9988],
        [0.9979],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369770.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0313],
        [1.0319],
        [1.0312],
        ...,
        [0.9988],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369775.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3229e-05,  4.3302e-04, -6.1094e-04,  ...,  1.0031e-03,
          0.0000e+00, -7.5361e-04],
        [ 5.3229e-05,  4.3302e-04, -6.1094e-04,  ...,  1.0031e-03,
          0.0000e+00, -7.5361e-04],
        [ 5.3229e-05,  4.3302e-04, -6.1094e-04,  ...,  1.0031e-03,
          0.0000e+00, -7.5361e-04],
        ...,
        [ 5.3229e-05,  4.3302e-04, -6.1094e-04,  ...,  1.0031e-03,
          0.0000e+00, -7.5361e-04],
        [ 5.3229e-05,  4.3302e-04, -6.1094e-04,  ...,  1.0031e-03,
          0.0000e+00, -7.5361e-04],
        [ 5.3229e-05,  4.3302e-04, -6.1094e-04,  ...,  1.0031e-03,
          0.0000e+00, -7.5361e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3207.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1953, device='cuda:0')



h[100].sum tensor(107.2660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.4908, device='cuda:0')



h[200].sum tensor(44.0669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61283.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0829, 0.0033,  ..., 0.0000, 0.0152, 0.0000],
        [0.0008, 0.0835, 0.0033,  ..., 0.0000, 0.0154, 0.0000],
        [0.0009, 0.0837, 0.0034,  ..., 0.0000, 0.0155, 0.0000],
        ...,
        [0.0009, 0.0852, 0.0036,  ..., 0.0000, 0.0159, 0.0000],
        [0.0009, 0.0852, 0.0036,  ..., 0.0000, 0.0159, 0.0000],
        [0.0009, 0.0851, 0.0036,  ..., 0.0000, 0.0159, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528200.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2741.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.5100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6308.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(948.2459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-595.4328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4288],
        [-4.3778],
        [-4.2755],
        ...,
        [-4.4292],
        [-4.4221],
        [-4.4203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-347449.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0313],
        [1.0319],
        [1.0312],
        ...,
        [0.9988],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369775.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0314],
        [1.0320],
        [1.0313],
        ...,
        [0.9987],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369779.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.8306e-05,  4.2014e-04, -6.1094e-04,  ...,  1.0105e-03,
          0.0000e+00, -7.4831e-04],
        [ 3.8306e-05,  4.2014e-04, -6.1094e-04,  ...,  1.0105e-03,
          0.0000e+00, -7.4831e-04],
        [ 3.8306e-05,  4.2014e-04, -6.1094e-04,  ...,  1.0105e-03,
          0.0000e+00, -7.4831e-04],
        ...,
        [ 3.8306e-05,  4.2014e-04, -6.1094e-04,  ...,  1.0105e-03,
          0.0000e+00, -7.4831e-04],
        [ 3.8306e-05,  4.2014e-04, -6.1094e-04,  ...,  1.0105e-03,
          0.0000e+00, -7.4831e-04],
        [ 3.8306e-05,  4.2014e-04, -6.1094e-04,  ...,  1.0105e-03,
          0.0000e+00, -7.4831e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3440.4617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9728, device='cuda:0')



h[100].sum tensor(109.0476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.8048, device='cuda:0')



h[200].sum tensor(47.3361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0187, 0.0163, 0.0000,  ..., 0.0345, 0.0000, 0.0104],
        [0.0002, 0.0017, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0017, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0018, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69517.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0453, 0.1029, 0.1158,  ..., 0.0091, 0.0534, 0.0000],
        [0.0176, 0.0908, 0.0459,  ..., 0.0000, 0.0306, 0.0000],
        [0.0065, 0.0861, 0.0177,  ..., 0.0000, 0.0211, 0.0000],
        ...,
        [0.0007, 0.0854, 0.0035,  ..., 0.0000, 0.0158, 0.0000],
        [0.0007, 0.0854, 0.0035,  ..., 0.0000, 0.0158, 0.0000],
        [0.0007, 0.0853, 0.0035,  ..., 0.0000, 0.0157, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580587.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3489.6938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.5624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6083.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1064.3212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-687.5760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0588],
        [-1.9500],
        [-2.7590],
        ...,
        [-4.4604],
        [-4.4530],
        [-4.4508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317908.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0314],
        [1.0320],
        [1.0313],
        ...,
        [0.9987],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369779.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0314],
        [1.0321],
        [1.0313],
        ...,
        [0.9987],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369783.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5124e-05,  4.0830e-04, -6.1094e-04,  ...,  1.0187e-03,
          0.0000e+00, -7.4580e-04],
        [ 2.5124e-05,  4.0830e-04, -6.1094e-04,  ...,  1.0187e-03,
          0.0000e+00, -7.4580e-04],
        [ 2.5124e-05,  4.0830e-04, -6.1094e-04,  ...,  1.0187e-03,
          0.0000e+00, -7.4580e-04],
        ...,
        [ 2.5124e-05,  4.0830e-04, -6.1094e-04,  ...,  1.0187e-03,
          0.0000e+00, -7.4580e-04],
        [ 2.5124e-05,  4.0830e-04, -6.1094e-04,  ...,  1.0187e-03,
          0.0000e+00, -7.4580e-04],
        [ 2.5124e-05,  4.0830e-04, -6.1094e-04,  ...,  1.0187e-03,
          0.0000e+00, -7.4580e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3269.2444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7788, device='cuda:0')



h[100].sum tensor(108.9377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.2352, device='cuda:0')



h[200].sum tensor(44.2592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0907, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0090, 0.0087, 0.0000,  ..., 0.0188, 0.0000, 0.0050],
        [0.0001, 0.0017, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0001, 0.0017, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0017, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0001, 0.0017, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0001, 0.0017, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64642.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0249, 0.0947, 0.0661,  ..., 0.0035, 0.0354, 0.0000],
        [0.0067, 0.0868, 0.0193,  ..., 0.0000, 0.0205, 0.0000],
        [0.0015, 0.0846, 0.0062,  ..., 0.0000, 0.0162, 0.0000],
        ...,
        [0.0005, 0.0856, 0.0034,  ..., 0.0000, 0.0157, 0.0000],
        [0.0005, 0.0856, 0.0034,  ..., 0.0000, 0.0157, 0.0000],
        [0.0005, 0.0855, 0.0034,  ..., 0.0000, 0.0156, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552127.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3020.0076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.5302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6055.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1001.0076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-637.6567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8048],
        [-2.7974],
        [-3.5081],
        ...,
        [-4.4941],
        [-4.4867],
        [-4.4848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319690.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0314],
        [1.0321],
        [1.0313],
        ...,
        [0.9987],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369783.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0315],
        [1.0322],
        [1.0314],
        ...,
        [0.9987],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369788.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4574e-05,  4.0158e-04, -6.1094e-04,  ...,  1.0321e-03,
          0.0000e+00, -7.3783e-04],
        [ 1.4574e-05,  4.0158e-04, -6.1094e-04,  ...,  1.0321e-03,
          0.0000e+00, -7.3783e-04],
        [ 1.4574e-05,  4.0158e-04, -6.1094e-04,  ...,  1.0321e-03,
          0.0000e+00, -7.3783e-04],
        ...,
        [ 1.4574e-05,  4.0158e-04, -6.1094e-04,  ...,  1.0321e-03,
          0.0000e+00, -7.3783e-04],
        [ 1.4574e-05,  4.0158e-04, -6.1094e-04,  ...,  1.0321e-03,
          0.0000e+00, -7.3783e-04],
        [ 1.4574e-05,  4.0158e-04, -6.1094e-04,  ...,  1.0321e-03,
          0.0000e+00, -7.3783e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3112.6982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.3861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6657, device='cuda:0')



h[100].sum tensor(108.8365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.9074, device='cuda:0')



h[200].sum tensor(41.5442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.9840e-05, 1.6489e-03, 0.0000e+00,  ..., 4.2377e-03, 0.0000e+00,
         0.0000e+00],
        [6.0250e-05, 1.6601e-03, 0.0000e+00,  ..., 4.2667e-03, 0.0000e+00,
         0.0000e+00],
        [2.0413e-02, 1.7664e-02, 0.0000e+00,  ..., 3.7656e-02, 0.0000e+00,
         1.1638e-02],
        ...,
        [6.1105e-05, 1.6837e-03, 0.0000e+00,  ..., 4.3272e-03, 0.0000e+00,
         0.0000e+00],
        [6.1088e-05, 1.6832e-03, 0.0000e+00,  ..., 4.3260e-03, 0.0000e+00,
         0.0000e+00],
        [6.1049e-05, 1.6822e-03, 0.0000e+00,  ..., 4.3233e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61134.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0852, 0.0132,  ..., 0.0000, 0.0184, 0.0000],
        [0.0181, 0.0923, 0.0490,  ..., 0.0011, 0.0300, 0.0000],
        [0.0526, 0.1086, 0.1375,  ..., 0.0143, 0.0588, 0.0000],
        ...,
        [0.0004, 0.0858, 0.0035,  ..., 0.0000, 0.0156, 0.0000],
        [0.0004, 0.0858, 0.0035,  ..., 0.0000, 0.0156, 0.0000],
        [0.0004, 0.0857, 0.0035,  ..., 0.0000, 0.0156, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535024.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2677.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.8933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6087.8262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(952.4869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-598.8136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8755],
        [-1.8484],
        [-0.8054],
        ...,
        [-4.5170],
        [-4.5095],
        [-4.5075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336755.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0315],
        [1.0322],
        [1.0314],
        ...,
        [0.9987],
        [0.9978],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369788.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0316],
        [1.0323],
        [1.0315],
        ...,
        [0.9987],
        [0.9977],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369792.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7208e-03,  4.8908e-03, -6.4888e-04,  ...,  1.0410e-02,
         -3.5114e-04,  2.9593e-03],
        [ 1.0723e-05,  4.0107e-04, -6.1094e-04,  ...,  1.0407e-03,
          0.0000e+00, -7.3468e-04],
        [ 1.0723e-05,  4.0107e-04, -6.1094e-04,  ...,  1.0407e-03,
          0.0000e+00, -7.3468e-04],
        ...,
        [ 1.0723e-05,  4.0107e-04, -6.1094e-04,  ...,  1.0407e-03,
          0.0000e+00, -7.3468e-04],
        [ 1.0723e-05,  4.0107e-04, -6.1094e-04,  ...,  1.0407e-03,
          0.0000e+00, -7.3468e-04],
        [ 1.0723e-05,  4.0107e-04, -6.1094e-04,  ...,  1.0407e-03,
          0.0000e+00, -7.3468e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3116.3354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.2662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6126, device='cuda:0')



h[100].sum tensor(109.2922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.7488, device='cuda:0')



h[200].sum tensor(41.5104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.9380e-02, 1.6850e-02, 0.0000e+00,  ..., 3.5998e-02, 0.0000e+00,
         1.0999e-02],
        [5.9347e-03, 6.2897e-03, 0.0000e+00,  ..., 1.3967e-02, 0.0000e+00,
         3.0528e-03],
        [3.2224e-02, 2.6962e-02, 0.0000e+00,  ..., 5.7105e-02, 0.0000e+00,
         1.9298e-02],
        ...,
        [4.4959e-05, 1.6816e-03, 0.0000e+00,  ..., 4.3636e-03, 0.0000e+00,
         0.0000e+00],
        [4.4946e-05, 1.6811e-03, 0.0000e+00,  ..., 4.3623e-03, 0.0000e+00,
         0.0000e+00],
        [4.4918e-05, 1.6801e-03, 0.0000e+00,  ..., 4.3595e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61048.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0688, 0.1153, 0.1796,  ..., 0.0207, 0.0722, 0.0000],
        [0.0571, 0.1105, 0.1502,  ..., 0.0149, 0.0625, 0.0000],
        [0.0848, 0.1240, 0.2227,  ..., 0.0323, 0.0843, 0.0000],
        ...,
        [0.0003, 0.0858, 0.0037,  ..., 0.0000, 0.0158, 0.0000],
        [0.0003, 0.0858, 0.0037,  ..., 0.0000, 0.0157, 0.0000],
        [0.0003, 0.0857, 0.0037,  ..., 0.0000, 0.0157, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533002.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2612.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(261.3050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6055.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(953.4230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-599.3394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2170],
        [ 0.2123],
        [ 0.1921],
        ...,
        [-4.5197],
        [-4.5126],
        [-4.5121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335179.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0316],
        [1.0323],
        [1.0315],
        ...,
        [0.9987],
        [0.9977],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369792.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0324],
        [1.0316],
        ...,
        [0.9987],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369797.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3574e-06,  3.9542e-04, -6.1094e-04,  ...,  1.0497e-03,
          0.0000e+00, -7.2790e-04],
        [ 1.3574e-06,  3.9542e-04, -6.1094e-04,  ...,  1.0497e-03,
          0.0000e+00, -7.2790e-04],
        [ 1.3574e-06,  3.9542e-04, -6.1094e-04,  ...,  1.0497e-03,
          0.0000e+00, -7.2790e-04],
        ...,
        [ 1.3574e-06,  3.9542e-04, -6.1094e-04,  ...,  1.0497e-03,
          0.0000e+00, -7.2790e-04],
        [ 1.3574e-06,  3.9542e-04, -6.1094e-04,  ...,  1.0497e-03,
          0.0000e+00, -7.2790e-04],
        [ 1.3574e-06,  3.9542e-04, -6.1094e-04,  ...,  1.0497e-03,
          0.0000e+00, -7.2790e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3185.7239, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(17.7854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0842, device='cuda:0')



h[100].sum tensor(109.9943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1587, device='cuda:0')



h[200].sum tensor(42.2550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[5.5738e-06, 1.6237e-03, 0.0000e+00,  ..., 4.3106e-03, 0.0000e+00,
         0.0000e+00],
        [5.6122e-06, 1.6349e-03, 0.0000e+00,  ..., 4.3403e-03, 0.0000e+00,
         0.0000e+00],
        [5.6163e-06, 1.6361e-03, 0.0000e+00,  ..., 4.3434e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [5.6914e-06, 1.6580e-03, 0.0000e+00,  ..., 4.4016e-03, 0.0000e+00,
         0.0000e+00],
        [5.6898e-06, 1.6575e-03, 0.0000e+00,  ..., 4.4003e-03, 0.0000e+00,
         0.0000e+00],
        [5.6862e-06, 1.6565e-03, 0.0000e+00,  ..., 4.3975e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62800.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.4843e-05, 8.3959e-02, 3.6780e-03,  ..., 0.0000e+00, 1.5293e-02,
         0.0000e+00],
        [5.7953e-04, 8.4945e-02, 5.9178e-03,  ..., 0.0000e+00, 1.6168e-02,
         0.0000e+00],
        [8.1721e-03, 8.8548e-02, 2.5180e-02,  ..., 0.0000e+00, 2.2699e-02,
         0.0000e+00],
        ...,
        [1.1653e-04, 8.6341e-02, 4.0175e-03,  ..., 0.0000e+00, 1.5927e-02,
         0.0000e+00],
        [1.1639e-04, 8.6314e-02, 4.0151e-03,  ..., 0.0000e+00, 1.5922e-02,
         0.0000e+00],
        [1.1535e-04, 8.6250e-02, 4.0088e-03,  ..., 0.0000e+00, 1.5907e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543501.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2581.1960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.0846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6204.1494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(977.7159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-615.1307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9181],
        [-3.3932],
        [-2.5964],
        ...,
        [-4.5562],
        [-4.5485],
        [-4.5464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-375837.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0324],
        [1.0316],
        ...,
        [0.9987],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369797.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 80.0 event: 400 loss: tensor(477.3766, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0317],
        [1.0324],
        [1.0317],
        ...,
        [0.9987],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369802.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4248e-05,  4.1309e-04, -6.1094e-04,  ...,  1.0518e-03,
          0.0000e+00, -7.3078e-04],
        [ 1.1999e-02,  9.8373e-03, -6.9058e-04,  ...,  2.0723e-02,
         -7.2879e-04,  7.0241e-03],
        [ 6.0727e-03,  5.1772e-03, -6.5120e-04,  ...,  1.0996e-02,
         -3.6841e-04,  3.1894e-03],
        ...,
        [ 1.4248e-05,  4.1309e-04, -6.1094e-04,  ...,  1.0518e-03,
          0.0000e+00, -7.3078e-04],
        [ 1.4248e-05,  4.1309e-04, -6.1094e-04,  ...,  1.0518e-03,
          0.0000e+00, -7.3078e-04],
        [ 1.4248e-05,  4.1309e-04, -6.1094e-04,  ...,  1.0518e-03,
          0.0000e+00, -7.3078e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3481.4502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.2873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0597, device='cuda:0')



h[100].sum tensor(111.1382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.0648, device='cuda:0')



h[200].sum tensor(47.3556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.7426e-02, 1.5353e-02, 0.0000e+00,  ..., 3.2824e-02, 0.0000e+00,
         9.7389e-03],
        [2.0024e-02, 1.7408e-02, 0.0000e+00,  ..., 3.7118e-02, 0.0000e+00,
         1.0651e-02],
        [7.1995e-02, 5.8276e-02, 0.0000e+00,  ..., 1.2242e-01, 0.0000e+00,
         4.3523e-02],
        ...,
        [5.9742e-05, 1.7321e-03, 0.0000e+00,  ..., 4.4102e-03, 0.0000e+00,
         0.0000e+00],
        [5.9725e-05, 1.7316e-03, 0.0000e+00,  ..., 4.4089e-03, 0.0000e+00,
         0.0000e+00],
        [5.9687e-05, 1.7305e-03, 0.0000e+00,  ..., 4.4061e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65018.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5743e-02, 1.0405e-01, 1.2200e-01,  ..., 1.1328e-02, 5.4507e-02,
         0.0000e+00],
        [7.9989e-02, 1.2039e-01, 2.1050e-01,  ..., 2.7727e-02, 8.2738e-02,
         0.0000e+00],
        [1.4757e-01, 1.5252e-01, 3.8712e-01,  ..., 6.7831e-02, 1.3681e-01,
         0.0000e+00],
        ...,
        [2.4645e-04, 8.5648e-02, 4.3665e-03,  ..., 0.0000e+00, 1.6112e-02,
         0.0000e+00],
        [2.4629e-04, 8.5621e-02, 4.3640e-03,  ..., 0.0000e+00, 1.6106e-02,
         0.0000e+00],
        [2.4506e-04, 8.5558e-02, 4.3574e-03,  ..., 0.0000e+00, 1.6091e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541817.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2719.7554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.6334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5966.7681, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1009.9927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-643.2294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8080],
        [-0.1949],
        [ 0.1029],
        ...,
        [-4.5453],
        [-4.5377],
        [-4.5357]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323732.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0317],
        [1.0324],
        [1.0317],
        ...,
        [0.9987],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369802.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0318],
        [1.0325],
        [1.0318],
        ...,
        [0.9987],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369806.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3937e-05,  4.3400e-04, -6.1094e-04,  ...,  1.0500e-03,
          0.0000e+00, -7.3737e-04],
        [ 3.3937e-05,  4.3400e-04, -6.1094e-04,  ...,  1.0500e-03,
          0.0000e+00, -7.3737e-04],
        [ 3.3937e-05,  4.3400e-04, -6.1094e-04,  ...,  1.0500e-03,
          0.0000e+00, -7.3737e-04],
        ...,
        [ 3.3937e-05,  4.3400e-04, -6.1094e-04,  ...,  1.0500e-03,
          0.0000e+00, -7.3737e-04],
        [ 3.3937e-05,  4.3400e-04, -6.1094e-04,  ...,  1.0500e-03,
          0.0000e+00, -7.3737e-04],
        [ 3.3937e-05,  4.3400e-04, -6.1094e-04,  ...,  1.0500e-03,
          0.0000e+00, -7.3737e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3756.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0701, device='cuda:0')



h[100].sum tensor(112.1790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.0752, device='cuda:0')



h[200].sum tensor(52.1681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4578, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0018, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0001, 0.0018, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0001, 0.0018, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0018, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0001, 0.0018, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0001, 0.0018, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74082.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0827, 0.0043,  ..., 0.0000, 0.0159, 0.0000],
        [0.0003, 0.0833, 0.0043,  ..., 0.0000, 0.0160, 0.0000],
        [0.0004, 0.0835, 0.0044,  ..., 0.0000, 0.0161, 0.0000],
        ...,
        [0.0004, 0.0850, 0.0046,  ..., 0.0000, 0.0165, 0.0000],
        [0.0004, 0.0850, 0.0046,  ..., 0.0000, 0.0165, 0.0000],
        [0.0004, 0.0850, 0.0046,  ..., 0.0000, 0.0165, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587371.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3501.2705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(378.3524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5770.3481, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1136.4637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-741.8538, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6410],
        [-3.8144],
        [-3.9512],
        ...,
        [-4.5353],
        [-4.5280],
        [-4.5260]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289405., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0318],
        [1.0325],
        [1.0318],
        ...,
        [0.9987],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369806.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0325],
        [1.0319],
        ...,
        [0.9986],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369811.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6076e-05,  4.5223e-04, -6.1094e-04,  ...,  1.0559e-03,
          0.0000e+00, -7.3649e-04],
        [ 4.6076e-05,  4.5223e-04, -6.1094e-04,  ...,  1.0559e-03,
          0.0000e+00, -7.3649e-04],
        [ 4.6076e-05,  4.5223e-04, -6.1094e-04,  ...,  1.0559e-03,
          0.0000e+00, -7.3649e-04],
        ...,
        [ 4.6076e-05,  4.5223e-04, -6.1094e-04,  ...,  1.0559e-03,
          0.0000e+00, -7.3649e-04],
        [ 4.6076e-05,  4.5223e-04, -6.1094e-04,  ...,  1.0559e-03,
          0.0000e+00, -7.3649e-04],
        [ 4.6076e-05,  4.5223e-04, -6.1094e-04,  ...,  1.0559e-03,
          0.0000e+00, -7.3649e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4083.9182, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.6221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2753, device='cuda:0')



h[100].sum tensor(113.4520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.6680, device='cuda:0')



h[200].sum tensor(57.8361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7038, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0053, 0.0000,  ..., 0.0116, 0.0000, 0.0021],
        [0.0091, 0.0089, 0.0000,  ..., 0.0191, 0.0000, 0.0043],
        [0.0047, 0.0054, 0.0000,  ..., 0.0117, 0.0000, 0.0021],
        ...,
        [0.0002, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77090.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0904, 0.0548,  ..., 0.0000, 0.0350, 0.0000],
        [0.0232, 0.0922, 0.0627,  ..., 0.0000, 0.0380, 0.0000],
        [0.0224, 0.0922, 0.0607,  ..., 0.0000, 0.0372, 0.0000],
        ...,
        [0.0005, 0.0848, 0.0050,  ..., 0.0000, 0.0169, 0.0000],
        [0.0005, 0.0848, 0.0050,  ..., 0.0000, 0.0169, 0.0000],
        [0.0005, 0.0847, 0.0050,  ..., 0.0000, 0.0169, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594828.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3592.7012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.0030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5873.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1175.7468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-772.5133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6674],
        [-0.7614],
        [-0.6669],
        ...,
        [-4.5276],
        [-4.5205],
        [-4.5186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304087.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0325],
        [1.0319],
        ...,
        [0.9986],
        [0.9977],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369811.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0326],
        [1.0320],
        ...,
        [0.9986],
        [0.9976],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369815.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04],
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04],
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04],
        ...,
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04],
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04],
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3365.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.5344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2794, device='cuda:0')



h[100].sum tensor(110.5230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.7318, device='cuda:0')



h[200].sum tensor(46.6806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65568.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0338, 0.0982, 0.0920,  ..., 0.0055, 0.0446, 0.0000],
        [0.0206, 0.0926, 0.0574,  ..., 0.0003, 0.0336, 0.0000],
        [0.0145, 0.0902, 0.0426,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0004, 0.0849, 0.0053,  ..., 0.0000, 0.0169, 0.0000],
        [0.0004, 0.0849, 0.0053,  ..., 0.0000, 0.0169, 0.0000],
        [0.0004, 0.0849, 0.0053,  ..., 0.0000, 0.0169, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549308.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2887.3494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(311.5619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5859.2705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1018.1624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-650.1166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0784],
        [-0.1656],
        [-0.1977],
        ...,
        [-4.5136],
        [-4.5055],
        [-4.4942]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303927.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0326],
        [1.0320],
        ...,
        [0.9986],
        [0.9976],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369815.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0319],
        [1.0326],
        [1.0320],
        ...,
        [0.9986],
        [0.9976],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369815.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.3905e-02,  2.7090e-02, -8.3593e-04,  ...,  5.6651e-02,
         -2.0243e-03,  2.1175e-02],
        [ 2.4357e-02,  1.9580e-02, -7.7249e-04,  ...,  4.0978e-02,
         -1.4535e-03,  1.4997e-02],
        [ 1.2872e-02,  1.0546e-02, -6.9618e-04,  ...,  2.2123e-02,
         -7.6688e-04,  7.5656e-03],
        ...,
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04],
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04],
        [ 4.4441e-05,  4.5697e-04, -6.1094e-04,  ...,  1.0651e-03,
          0.0000e+00, -7.3478e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3340.5461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.1930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9912, device='cuda:0')



h[100].sum tensor(110.4151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.8703, device='cuda:0')



h[200].sum tensor(46.2960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1067, 0.0856, 0.0000,  ..., 0.1792, 0.0000, 0.0659],
        [0.0987, 0.0794, 0.0000,  ..., 0.1661, 0.0000, 0.0607],
        [0.0817, 0.0660, 0.0000,  ..., 0.1382, 0.0000, 0.0497],
        ...,
        [0.0002, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0002, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63610.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.1438e-01, 1.8236e-01, 5.6453e-01,  ..., 1.1125e-01, 1.9045e-01,
         0.0000e+00],
        [1.8878e-01, 1.7103e-01, 4.9726e-01,  ..., 9.5033e-02, 1.7007e-01,
         0.0000e+00],
        [1.5344e-01, 1.5481e-01, 4.0371e-01,  ..., 7.2085e-02, 1.4195e-01,
         0.0000e+00],
        ...,
        [3.9817e-04, 8.4939e-02, 5.2635e-03,  ..., 0.0000e+00, 1.6944e-02,
         0.0000e+00],
        [3.9819e-04, 8.4925e-02, 5.2619e-03,  ..., 0.0000e+00, 1.6941e-02,
         0.0000e+00],
        [3.9661e-04, 8.4851e-02, 5.2536e-03,  ..., 0.0000e+00, 1.6923e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537708.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2675.9690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.8256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5976.3403, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(988.0955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-626.7899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1568],
        [ 0.1753],
        [ 0.1967],
        ...,
        [-4.5369],
        [-4.5299],
        [-4.5279]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319266.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0319],
        [1.0326],
        [1.0320],
        ...,
        [0.9986],
        [0.9976],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369815.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0326],
        [1.0320],
        ...,
        [0.9986],
        [0.9976],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369820.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.8068e-02,  3.0374e-02, -8.6368e-04,  ...,  6.3523e-02,
         -2.2611e-03,  2.3885e-02],
        [ 1.1620e-02,  9.5692e-03, -6.8793e-04,  ...,  2.0102e-02,
         -6.8882e-04,  6.7693e-03],
        [ 3.2283e-05,  4.5450e-04, -6.1094e-04,  ...,  1.0794e-03,
          0.0000e+00, -7.2890e-04],
        ...,
        [ 3.2283e-05,  4.5450e-04, -6.1094e-04,  ...,  1.0794e-03,
          0.0000e+00, -7.2890e-04],
        [ 3.2283e-05,  4.5450e-04, -6.1094e-04,  ...,  1.0794e-03,
          0.0000e+00, -7.2890e-04],
        [ 3.2283e-05,  4.5450e-04, -6.1094e-04,  ...,  1.0794e-03,
          0.0000e+00, -7.2890e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3327.2905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.6067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0097, device='cuda:0')



h[100].sum tensor(110.7245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9257, device='cuda:0')



h[200].sum tensor(46.1214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1165, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0657, 0.0534, 0.0000,  ..., 0.1121, 0.0000, 0.0394],
        [0.0784, 0.0635, 0.0000,  ..., 0.1330, 0.0000, 0.0484],
        [0.0315, 0.0265, 0.0000,  ..., 0.0559, 0.0000, 0.0188],
        ...,
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64533.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5581e-01, 1.5660e-01, 4.1188e-01,  ..., 7.3961e-02, 1.4377e-01,
         0.0000e+00],
        [1.5679e-01, 1.5746e-01, 4.1493e-01,  ..., 7.4903e-02, 1.4477e-01,
         0.0000e+00],
        [1.1502e-01, 1.3810e-01, 3.0529e-01,  ..., 4.9289e-02, 1.1109e-01,
         0.0000e+00],
        ...,
        [2.2011e-04, 8.5355e-02, 5.6122e-03,  ..., 0.0000e+00, 1.6875e-02,
         0.0000e+00],
        [2.2017e-04, 8.5344e-02, 5.6110e-03,  ..., 0.0000e+00, 1.6872e-02,
         0.0000e+00],
        [2.1885e-04, 8.5267e-02, 5.6021e-03,  ..., 0.0000e+00, 1.6854e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543766.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2713.0364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(305.2432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5766.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1005.4456, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-641.0159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1407],
        [ 0.1767],
        [ 0.1912],
        ...,
        [-4.5536],
        [-4.5465],
        [-4.5444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313045.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0326],
        [1.0320],
        ...,
        [0.9986],
        [0.9976],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369820.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0327],
        [1.0321],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369824.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6686e-05,  4.5712e-04, -6.1094e-04,  ...,  1.0842e-03,
          0.0000e+00, -7.2601e-04],
        [ 2.6686e-05,  4.5712e-04, -6.1094e-04,  ...,  1.0842e-03,
          0.0000e+00, -7.2601e-04],
        [ 2.6686e-05,  4.5712e-04, -6.1094e-04,  ...,  1.0842e-03,
          0.0000e+00, -7.2601e-04],
        ...,
        [ 2.6686e-05,  4.5712e-04, -6.1094e-04,  ...,  1.0842e-03,
          0.0000e+00, -7.2601e-04],
        [ 2.6686e-05,  4.5712e-04, -6.1094e-04,  ...,  1.0842e-03,
          0.0000e+00, -7.2601e-04],
        [ 2.6686e-05,  4.5712e-04, -6.1094e-04,  ...,  1.0842e-03,
          0.0000e+00, -7.2601e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4078.4326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.8414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5141, device='cuda:0')



h[100].sum tensor(114.3775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.3819, device='cuda:0')



h[200].sum tensor(57.8721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77366.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.2399e-05, 8.3219e-02, 5.3437e-03,  ..., 0.0000e+00, 1.6297e-02,
         0.0000e+00],
        [8.3730e-05, 8.3832e-02, 5.3945e-03,  ..., 0.0000e+00, 1.6427e-02,
         0.0000e+00],
        [1.0985e-04, 8.4041e-02, 5.4704e-03,  ..., 0.0000e+00, 1.6520e-02,
         0.0000e+00],
        ...,
        [1.2400e-04, 8.5567e-02, 5.7377e-03,  ..., 0.0000e+00, 1.6962e-02,
         0.0000e+00],
        [1.2407e-04, 8.5559e-02, 5.7368e-03,  ..., 0.0000e+00, 1.6960e-02,
         0.0000e+00],
        [1.2289e-04, 8.5479e-02, 5.7275e-03,  ..., 0.0000e+00, 1.6941e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598875.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3529.3079, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(417.3210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5471.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1186.0858, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-781.7705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3396],
        [-4.4217],
        [-4.4446],
        ...,
        [-4.5532],
        [-4.5456],
        [-4.5430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291641.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0327],
        [1.0321],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369824.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0327],
        [1.0322],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369829.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3790e-02,  1.1291e-02, -7.0240e-04,  ...,  2.3679e-02,
         -8.0901e-04,  8.1803e-03],
        [ 2.8708e-02,  2.3027e-02, -8.0152e-04,  ...,  4.8170e-02,
         -1.6858e-03,  1.7835e-02],
        [ 2.2943e-02,  1.8492e-02, -7.6322e-04,  ...,  3.8706e-02,
         -1.3470e-03,  1.4104e-02],
        ...,
        [ 2.6342e-05,  4.6260e-04, -6.1094e-04,  ...,  1.0817e-03,
          0.0000e+00, -7.2776e-04],
        [ 2.6342e-05,  4.6260e-04, -6.1094e-04,  ...,  1.0817e-03,
          0.0000e+00, -7.2776e-04],
        [ 2.6342e-05,  4.6260e-04, -6.1094e-04,  ...,  1.0817e-03,
          0.0000e+00, -7.2776e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3871.3396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.9199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6950, device='cuda:0')



h[100].sum tensor(113.7520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.9436, device='cuda:0')



h[200].sum tensor(54.5695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5275, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.2456e-02, 5.8819e-02, 0.0000e+00,  ..., 1.2322e-01, 0.0000e+00,
         4.3834e-02],
        [7.7503e-02, 6.2802e-02, 0.0000e+00,  ..., 1.3154e-01, 0.0000e+00,
         4.7079e-02],
        [9.5840e-02, 7.7229e-02, 0.0000e+00,  ..., 1.6165e-01, 0.0000e+00,
         5.8944e-02],
        ...,
        [1.1048e-04, 1.9401e-03, 0.0000e+00,  ..., 4.5366e-03, 0.0000e+00,
         0.0000e+00],
        [1.1047e-04, 1.9400e-03, 0.0000e+00,  ..., 4.5363e-03, 0.0000e+00,
         0.0000e+00],
        [1.1038e-04, 1.9384e-03, 0.0000e+00,  ..., 4.5325e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74518.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.2741e-01, 1.4421e-01, 3.3888e-01,  ..., 5.7040e-02, 1.2139e-01,
         0.0000e+00],
        [1.5712e-01, 1.5893e-01, 4.1642e-01,  ..., 7.4479e-02, 1.4568e-01,
         0.0000e+00],
        [1.7784e-01, 1.6888e-01, 4.7082e-01,  ..., 8.7160e-02, 1.6254e-01,
         0.0000e+00],
        ...,
        [7.7363e-05, 8.5638e-02, 5.7084e-03,  ..., 0.0000e+00, 1.7087e-02,
         0.0000e+00],
        [7.7460e-05, 8.5633e-02, 5.7078e-03,  ..., 0.0000e+00, 1.7086e-02,
         0.0000e+00],
        [7.6341e-05, 8.5551e-02, 5.6983e-03,  ..., 0.0000e+00, 1.7066e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582317.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3296.9504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(393.4417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5372.7744, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1149.1940, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-751.9437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0201],
        [ 0.1017],
        [ 0.0994],
        ...,
        [-4.5751],
        [-4.5680],
        [-4.5658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276313.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0327],
        [1.0322],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369829.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0323],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369832.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.3992e-03,  4.6858e-03, -6.4665e-04,  ...,  9.8869e-03,
         -3.1406e-04,  2.7309e-03],
        [ 1.7833e-02,  1.4467e-02, -7.2926e-04,  ...,  3.0293e-02,
         -1.0406e-03,  1.0775e-02],
        [ 2.4926e-05,  4.5786e-04, -6.1094e-04,  ...,  1.0664e-03,
          0.0000e+00, -7.4594e-04],
        ...,
        [ 2.4926e-05,  4.5786e-04, -6.1094e-04,  ...,  1.0664e-03,
          0.0000e+00, -7.4594e-04],
        [ 2.4926e-05,  4.5786e-04, -6.1094e-04,  ...,  1.0664e-03,
          0.0000e+00, -7.4594e-04],
        [ 2.4926e-05,  4.5786e-04, -6.1094e-04,  ...,  1.0664e-03,
          0.0000e+00, -7.4594e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4403.5225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.4393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6057, device='cuda:0')



h[100].sum tensor(116.1875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(52.6352, device='cuda:0')



h[200].sum tensor(62.6190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0588, 0.0480, 0.0000,  ..., 0.1007, 0.0000, 0.0349],
        [0.0244, 0.0210, 0.0000,  ..., 0.0442, 0.0000, 0.0134],
        [0.0398, 0.0332, 0.0000,  ..., 0.0696, 0.0000, 0.0226],
        ...,
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82309.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.2306e-01, 1.4245e-01, 3.2559e-01,  ..., 5.1636e-02, 1.1929e-01,
         0.0000e+00],
        [9.5316e-02, 1.2973e-01, 2.5328e-01,  ..., 3.4861e-02, 9.6980e-02,
         0.0000e+00],
        [8.3430e-02, 1.2411e-01, 2.2254e-01,  ..., 2.8180e-02, 8.7342e-02,
         0.0000e+00],
        ...,
        [2.3175e-05, 8.5531e-02, 5.5059e-03,  ..., 0.0000e+00, 1.6986e-02,
         0.0000e+00],
        [2.3286e-05, 8.5528e-02, 5.5056e-03,  ..., 0.0000e+00, 1.6985e-02,
         0.0000e+00],
        [2.2247e-05, 8.5444e-02, 5.4961e-03,  ..., 0.0000e+00, 1.6965e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611539.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3714.6157, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(463.3238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5255.9463, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1254.9677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-836.3652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1478],
        [ 0.1324],
        [ 0.0693],
        ...,
        [-4.6012],
        [-4.5940],
        [-4.5918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282246.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0323],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369832.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0323],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369835.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 5.7434e-03,  4.9531e-03, -6.4895e-04,  ...,  1.0441e-02,
         -3.3245e-04,  2.9384e-03],
        ...,
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4248.8936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.3362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4847, device='cuda:0')



h[100].sum tensor(115.7037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(49.2838, device='cuda:0')



h[200].sum tensor(59.8645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.8742e-05, 1.8556e-03, 0.0000e+00,  ..., 4.3233e-03, 0.0000e+00,
         0.0000e+00],
        [5.9982e-03, 6.5172e-03, 0.0000e+00,  ..., 1.4049e-02, 0.0000e+00,
         3.0345e-03],
        [2.0099e-02, 1.7612e-02, 0.0000e+00,  ..., 3.7190e-02, 0.0000e+00,
         1.1364e-02],
        ...,
        [9.0614e-05, 1.8947e-03, 0.0000e+00,  ..., 4.4145e-03, 0.0000e+00,
         0.0000e+00],
        [9.0615e-05, 1.8948e-03, 0.0000e+00,  ..., 4.4146e-03, 0.0000e+00,
         0.0000e+00],
        [9.0538e-05, 1.8931e-03, 0.0000e+00,  ..., 4.4108e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81533.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0091, 0.0877, 0.0294,  ..., 0.0000, 0.0246, 0.0000],
        [0.0261, 0.0966, 0.0739,  ..., 0.0022, 0.0401, 0.0000],
        [0.0498, 0.1082, 0.1347,  ..., 0.0096, 0.0603, 0.0000],
        ...,
        [0.0000, 0.0855, 0.0053,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0855, 0.0053,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0855, 0.0053,  ..., 0.0000, 0.0168, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615369.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3728.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.9177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5354.2002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1240.7573, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-827.4033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3224],
        [-1.3372],
        [-0.5281],
        ...,
        [-4.6284],
        [-4.6212],
        [-4.6190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293713.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0323],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369835.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0323],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369835.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        ...,
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04],
        [ 2.1605e-05,  4.5175e-04, -6.1094e-04,  ...,  1.0525e-03,
          0.0000e+00, -7.6235e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3474.3545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0276, device='cuda:0')



h[100].sum tensor(112.3056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9688, device='cuda:0')



h[200].sum tensor(47.7589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[8.8742e-05, 1.8556e-03, 0.0000e+00,  ..., 4.3233e-03, 0.0000e+00,
         0.0000e+00],
        [8.9371e-05, 1.8687e-03, 0.0000e+00,  ..., 4.3539e-03, 0.0000e+00,
         0.0000e+00],
        [8.9430e-05, 1.8700e-03, 0.0000e+00,  ..., 4.3568e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [9.0614e-05, 1.8947e-03, 0.0000e+00,  ..., 4.4145e-03, 0.0000e+00,
         0.0000e+00],
        [9.0615e-05, 1.8948e-03, 0.0000e+00,  ..., 4.4146e-03, 0.0000e+00,
         0.0000e+00],
        [9.0538e-05, 1.8931e-03, 0.0000e+00,  ..., 4.4108e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70229.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0849, 0.0146,  ..., 0.0000, 0.0199, 0.0000],
        [0.0021, 0.0847, 0.0104,  ..., 0.0000, 0.0187, 0.0000],
        [0.0082, 0.0880, 0.0269,  ..., 0.0000, 0.0243, 0.0000],
        ...,
        [0.0000, 0.0855, 0.0053,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0855, 0.0053,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0855, 0.0053,  ..., 0.0000, 0.0168, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581099.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3193.9431, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.5310, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5450.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1084.4812, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-705.9712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9836],
        [-2.1547],
        [-1.8380],
        ...,
        [-4.6284],
        [-4.6212],
        [-4.6190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299869.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0323],
        ...,
        [0.9986],
        [0.9976],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369835.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0324],
        ...,
        [0.9986],
        [0.9975],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369839., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6442e-05,  4.4316e-04, -6.1094e-04,  ...,  1.0419e-03,
          0.0000e+00, -7.7513e-04],
        [ 1.6442e-05,  4.4316e-04, -6.1094e-04,  ...,  1.0419e-03,
          0.0000e+00, -7.7513e-04],
        [ 1.6442e-05,  4.4316e-04, -6.1094e-04,  ...,  1.0419e-03,
          0.0000e+00, -7.7513e-04],
        ...,
        [ 1.6442e-05,  4.4316e-04, -6.1094e-04,  ...,  1.0419e-03,
          0.0000e+00, -7.7513e-04],
        [ 1.6442e-05,  4.4316e-04, -6.1094e-04,  ...,  1.0419e-03,
          0.0000e+00, -7.7513e-04],
        [ 1.6442e-05,  4.4316e-04, -6.1094e-04,  ...,  1.0419e-03,
          0.0000e+00, -7.7513e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3262.9829, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.5985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8018, device='cuda:0')



h[100].sum tensor(111.6535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3041, device='cuda:0')



h[200].sum tensor(44.0794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0933, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[6.7538e-05, 1.8203e-03, 0.0000e+00,  ..., 4.2795e-03, 0.0000e+00,
         0.0000e+00],
        [6.8017e-05, 1.8332e-03, 0.0000e+00,  ..., 4.3099e-03, 0.0000e+00,
         0.0000e+00],
        [6.8062e-05, 1.8344e-03, 0.0000e+00,  ..., 4.3128e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [6.8963e-05, 1.8587e-03, 0.0000e+00,  ..., 4.3698e-03, 0.0000e+00,
         0.0000e+00],
        [6.8967e-05, 1.8588e-03, 0.0000e+00,  ..., 4.3701e-03, 0.0000e+00,
         0.0000e+00],
        [6.8908e-05, 1.8572e-03, 0.0000e+00,  ..., 4.3664e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63129.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0891, 0.0348,  ..., 0.0000, 0.0255, 0.0000],
        [0.0059, 0.0872, 0.0224,  ..., 0.0000, 0.0220, 0.0000],
        [0.0157, 0.0919, 0.0464,  ..., 0.0000, 0.0306, 0.0000],
        ...,
        [0.0000, 0.0857, 0.0051,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0857, 0.0051,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0856, 0.0051,  ..., 0.0000, 0.0165, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538177.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2427.1108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.4209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5814.7349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(978.6386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.7261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7383],
        [-1.6806],
        [-1.1281],
        ...,
        [-4.6454],
        [-4.6338],
        [-4.6012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345421.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0324],
        ...,
        [0.9986],
        [0.9975],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369839., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0324],
        ...,
        [0.9985],
        [0.9975],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369843.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2734e-03,  5.3687e-03, -6.5244e-04,  ...,  1.1281e-02,
         -3.5884e-04,  3.2565e-03],
        [ 4.9944e-03,  4.3624e-03, -6.4395e-04,  ...,  9.1833e-03,
         -2.8537e-04,  2.4293e-03],
        [ 1.1242e-02,  9.2775e-03, -6.8545e-04,  ...,  1.9431e-02,
         -6.4421e-04,  6.4694e-03],
        ...,
        [ 2.5939e-05,  4.5357e-04, -6.1094e-04,  ...,  1.0335e-03,
          0.0000e+00, -7.8360e-04],
        [ 2.5939e-05,  4.5357e-04, -6.1094e-04,  ...,  1.0335e-03,
          0.0000e+00, -7.8360e-04],
        [ 2.5939e-05,  4.5357e-04, -6.1094e-04,  ...,  1.0335e-03,
          0.0000e+00, -7.8360e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3052.0789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.9494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2255, device='cuda:0')



h[100].sum tensor(110.4487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.5914, device='cuda:0')



h[200].sum tensor(41.1126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0169, 0.0000,  ..., 0.0356, 0.0000, 0.0107],
        [0.0422, 0.0350, 0.0000,  ..., 0.0733, 0.0000, 0.0240],
        [0.0194, 0.0170, 0.0000,  ..., 0.0359, 0.0000, 0.0100],
        ...,
        [0.0001, 0.0019, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0001, 0.0019, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59967.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0476, 0.1069, 0.1293,  ..., 0.0088, 0.0583, 0.0000],
        [0.0685, 0.1179, 0.1830,  ..., 0.0179, 0.0759, 0.0000],
        [0.0523, 0.1100, 0.1413,  ..., 0.0101, 0.0628, 0.0000],
        ...,
        [0.0000, 0.0857, 0.0054,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0857, 0.0054,  ..., 0.0000, 0.0165, 0.0000],
        [0.0000, 0.0857, 0.0054,  ..., 0.0000, 0.0165, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528318., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2299.7751, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.7945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5837.9380, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(934.5843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-592.1208, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4403],
        [-0.1550],
        [-0.4071],
        ...,
        [-4.6224],
        [-4.5784],
        [-4.4468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332231.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0328],
        [1.0324],
        ...,
        [0.9985],
        [0.9975],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369843.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0325],
        ...,
        [0.9985],
        [0.9975],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369847.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9541e-05,  4.5390e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8466e-04],
        [ 1.9541e-05,  4.5390e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8466e-04],
        [ 6.8788e-03,  5.8506e-03, -6.5651e-04,  ...,  1.2289e-02,
         -3.9173e-04,  3.6512e-03],
        ...,
        [ 1.9541e-05,  4.5390e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8466e-04],
        [ 1.9541e-05,  4.5390e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8466e-04],
        [ 1.9541e-05,  4.5390e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -7.8466e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3730.9111, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.1079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1592, device='cuda:0')



h[100].sum tensor(113.4035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.3415, device='cuda:0')



h[200].sum tensor(51.8777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[2.0762e-02, 1.8136e-02, 0.0000e+00,  ..., 3.8185e-02, 0.0000e+00,
         1.2568e-02],
        [7.1652e-03, 7.4515e-03, 0.0000e+00,  ..., 1.5913e-02, 0.0000e+00,
         3.7710e-03],
        [1.2370e-02, 1.1548e-02, 0.0000e+00,  ..., 2.4452e-02, 0.0000e+00,
         6.3230e-03],
        ...,
        [8.1963e-05, 1.9039e-03, 0.0000e+00,  ..., 4.3521e-03, 0.0000e+00,
         0.0000e+00],
        [8.1974e-05, 1.9041e-03, 0.0000e+00,  ..., 4.3527e-03, 0.0000e+00,
         0.0000e+00],
        [8.1905e-05, 1.9025e-03, 0.0000e+00,  ..., 4.3491e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71324.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0532, 0.1104, 0.1467,  ..., 0.0163, 0.0605, 0.0000],
        [0.0333, 0.1013, 0.0946,  ..., 0.0032, 0.0450, 0.0000],
        [0.0407, 0.1051, 0.1133,  ..., 0.0073, 0.0521, 0.0000],
        ...,
        [0.0000, 0.0861, 0.0058,  ..., 0.0000, 0.0163, 0.0000],
        [0.0000, 0.0861, 0.0058,  ..., 0.0000, 0.0163, 0.0000],
        [0.0000, 0.0860, 0.0058,  ..., 0.0000, 0.0163, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573014.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2910.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(375.0792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5622.8447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1089.4604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-713.1762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3252],
        [-0.4713],
        [-0.3266],
        ...,
        [-4.6613],
        [-4.6543],
        [-4.6521]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340602.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0325],
        ...,
        [0.9985],
        [0.9975],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369847.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0325],
        ...,
        [0.9985],
        [0.9975],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369852.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2859e-05,  4.6664e-04, -6.1094e-04,  ...,  1.0426e-03,
          0.0000e+00, -7.8562e-04],
        [ 2.2859e-05,  4.6664e-04, -6.1094e-04,  ...,  1.0426e-03,
          0.0000e+00, -7.8562e-04],
        [ 7.1853e-03,  6.1022e-03, -6.5852e-04,  ...,  1.2791e-02,
         -4.0671e-04,  3.8467e-03],
        ...,
        [ 2.2859e-05,  4.6664e-04, -6.1094e-04,  ...,  1.0426e-03,
          0.0000e+00, -7.8562e-04],
        [ 2.2859e-05,  4.6664e-04, -6.1094e-04,  ...,  1.0426e-03,
          0.0000e+00, -7.8562e-04],
        [ 2.2859e-05,  4.6664e-04, -6.1094e-04,  ...,  1.0426e-03,
          0.0000e+00, -7.8562e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3885.4951, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0046, device='cuda:0')



h[100].sum tensor(113.6945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.8690, device='cuda:0')



h[200].sum tensor(54.8993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.3905e-05, 1.9169e-03, 0.0000e+00,  ..., 4.2831e-03, 0.0000e+00,
         0.0000e+00],
        [7.4922e-03, 7.7512e-03, 0.0000e+00,  ..., 1.6448e-02, 0.0000e+00,
         3.9730e-03],
        [6.1576e-03, 6.7024e-03, 0.0000e+00,  ..., 1.4261e-02, 0.0000e+00,
         3.1068e-03],
        ...,
        [9.5884e-05, 1.9573e-03, 0.0000e+00,  ..., 4.3734e-03, 0.0000e+00,
         0.0000e+00],
        [9.5899e-05, 1.9577e-03, 0.0000e+00,  ..., 4.3741e-03, 0.0000e+00,
         0.0000e+00],
        [9.5820e-05, 1.9560e-03, 0.0000e+00,  ..., 4.3705e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75817.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0864, 0.0192,  ..., 0.0000, 0.0207, 0.0000],
        [0.0134, 0.0917, 0.0439,  ..., 0.0000, 0.0294, 0.0000],
        [0.0211, 0.0956, 0.0635,  ..., 0.0000, 0.0364, 0.0000],
        ...,
        [0.0000, 0.0863, 0.0065,  ..., 0.0000, 0.0163, 0.0000],
        [0.0000, 0.0863, 0.0065,  ..., 0.0000, 0.0164, 0.0000],
        [0.0000, 0.0862, 0.0065,  ..., 0.0000, 0.0163, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595687.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3257.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(412.8426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5345.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1153.7628, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-763.6577, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9742],
        [-2.1894],
        [-1.3645],
        ...,
        [-4.6516],
        [-4.6448],
        [-4.6427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316923.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0325],
        ...,
        [0.9985],
        [0.9975],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369852.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0326],
        ...,
        [0.9985],
        [0.9975],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369857.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.2787e-05,  4.9810e-04, -6.1094e-04,  ...,  1.0443e-03,
          0.0000e+00, -7.8410e-04],
        [ 4.2787e-05,  4.9810e-04, -6.1094e-04,  ...,  1.0443e-03,
          0.0000e+00, -7.8410e-04],
        [ 4.2787e-05,  4.9810e-04, -6.1094e-04,  ...,  1.0443e-03,
          0.0000e+00, -7.8410e-04],
        ...,
        [ 4.2787e-05,  4.9810e-04, -6.1094e-04,  ...,  1.0443e-03,
          0.0000e+00, -7.8410e-04],
        [ 4.2787e-05,  4.9810e-04, -6.1094e-04,  ...,  1.0443e-03,
          0.0000e+00, -7.8410e-04],
        [ 4.2787e-05,  4.9810e-04, -6.1094e-04,  ...,  1.0443e-03,
          0.0000e+00, -7.8410e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4310.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.4962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.6487, device='cuda:0')



h[100].sum tensor(114.8451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(49.7739, device='cuda:0')



h[200].sum tensor(62.6065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8569, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0020, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0021, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0002, 0.0021, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0021, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0021, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0021, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81093.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0839, 0.0069,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0845, 0.0069,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0847, 0.0070,  ..., 0.0000, 0.0162, 0.0000],
        ...,
        [0.0000, 0.0862, 0.0073,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0863, 0.0073,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0862, 0.0073,  ..., 0.0000, 0.0166, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607172.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3487.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(458.2139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5280.9790, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1228.0287, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-821.9774, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5508],
        [-4.5038],
        [-4.3881],
        ...,
        [-4.5994],
        [-4.5990],
        [-4.5987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279530.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0326],
        ...,
        [0.9985],
        [0.9975],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369857.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0326],
        ...,
        [0.9985],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369862.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9461e-03,  5.1564e-03, -6.5007e-04,  ...,  1.0705e-02,
         -3.3065e-04,  3.0273e-03],
        [ 5.5754e-05,  5.2051e-04, -6.1094e-04,  ...,  1.0441e-03,
          0.0000e+00, -7.8315e-04],
        [ 5.5754e-05,  5.2051e-04, -6.1094e-04,  ...,  1.0441e-03,
          0.0000e+00, -7.8315e-04],
        ...,
        [ 5.5754e-05,  5.2051e-04, -6.1094e-04,  ...,  1.0441e-03,
          0.0000e+00, -7.8315e-04],
        [ 5.5754e-05,  5.2051e-04, -6.1094e-04,  ...,  1.0441e-03,
          0.0000e+00, -7.8315e-04],
        [ 5.5754e-05,  5.2051e-04, -6.1094e-04,  ...,  1.0441e-03,
          0.0000e+00, -7.8315e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3470.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8855, device='cuda:0')



h[100].sum tensor(110.6532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.5440, device='cuda:0')



h[200].sum tensor(50.3808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2141, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0205, 0.0181, 0.0000,  ..., 0.0375, 0.0000, 0.0115],
        [0.0063, 0.0069, 0.0000,  ..., 0.0143, 0.0000, 0.0031],
        [0.0002, 0.0022, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0022, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0022, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0002, 0.0022, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66818.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0505, 0.1093, 0.1401,  ..., 0.0105, 0.0601, 0.0000],
        [0.0260, 0.0978, 0.0770,  ..., 0.0025, 0.0395, 0.0000],
        [0.0096, 0.0899, 0.0349,  ..., 0.0000, 0.0253, 0.0000],
        ...,
        [0.0000, 0.0862, 0.0079,  ..., 0.0000, 0.0167, 0.0000],
        [0.0000, 0.0862, 0.0079,  ..., 0.0000, 0.0167, 0.0000],
        [0.0000, 0.0862, 0.0079,  ..., 0.0000, 0.0167, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548258.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2585.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.5536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5621.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1026.9375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-667.3908, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5036],
        [-1.3085],
        [-2.1841],
        ...,
        [-4.5999],
        [-4.5936],
        [-4.5917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290577.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0329],
        [1.0326],
        ...,
        [0.9985],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369862.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0330],
        [1.0326],
        ...,
        [0.9984],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369867.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.4310e-02,  2.7491e-02, -8.3838e-04,  ...,  5.7190e-02,
         -1.9109e-03,  2.1366e-02],
        [ 7.1306e-05,  5.4106e-04, -6.1094e-04,  ...,  1.0358e-03,
          0.0000e+00, -7.8452e-04],
        [ 2.0363e-02,  1.6513e-02, -7.4573e-04,  ...,  3.4316e-02,
         -1.1325e-03,  1.2343e-02],
        ...,
        [ 7.1306e-05,  5.4106e-04, -6.1094e-04,  ...,  1.0358e-03,
          0.0000e+00, -7.8452e-04],
        [ 7.1306e-05,  5.4106e-04, -6.1094e-04,  ...,  1.0358e-03,
          0.0000e+00, -7.8452e-04],
        [ 7.1306e-05,  5.4106e-04, -6.1094e-04,  ...,  1.0358e-03,
          0.0000e+00, -7.8452e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3696.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2439, device='cuda:0')



h[100].sum tensor(111.1812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.6051, device='cuda:0')



h[200].sum tensor(54.5271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0622, 0.0509, 0.0000,  ..., 0.1057, 0.0000, 0.0376],
        [0.0909, 0.0736, 0.0000,  ..., 0.1529, 0.0000, 0.0554],
        [0.0175, 0.0158, 0.0000,  ..., 0.0325, 0.0000, 0.0103],
        ...,
        [0.0003, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71102.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1677, 0.1654, 0.4482,  ..., 0.0836, 0.1530, 0.0000],
        [0.1363, 0.1511, 0.3659,  ..., 0.0645, 0.1275, 0.0000],
        [0.0630, 0.1158, 0.1743,  ..., 0.0213, 0.0681, 0.0000],
        ...,
        [0.0000, 0.0861, 0.0082,  ..., 0.0000, 0.0167, 0.0000],
        [0.0000, 0.0861, 0.0082,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0860, 0.0082,  ..., 0.0000, 0.0167, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570918., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2949.4521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.8997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5645.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1085.1078, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-711.5164, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1110],
        [-0.0938],
        [-0.6961],
        ...,
        [-4.5861],
        [-4.5800],
        [-4.5782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301635.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0330],
        [1.0326],
        ...,
        [0.9984],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369867.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0330],
        [1.0326],
        ...,
        [0.9984],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369871.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.8972e-05,  5.6255e-04, -6.1094e-04,  ...,  1.0222e-03,
          0.0000e+00, -7.8918e-04],
        [ 8.8972e-05,  5.6255e-04, -6.1094e-04,  ...,  1.0222e-03,
          0.0000e+00, -7.8918e-04],
        [ 9.6885e-03,  8.1193e-03, -6.7470e-04,  ...,  1.6764e-02,
         -5.3267e-04,  5.4211e-03],
        ...,
        [ 8.8972e-05,  5.6255e-04, -6.1094e-04,  ...,  1.0222e-03,
          0.0000e+00, -7.8918e-04],
        [ 8.8972e-05,  5.6255e-04, -6.1094e-04,  ...,  1.0222e-03,
          0.0000e+00, -7.8918e-04],
        [ 8.8972e-05,  5.6255e-04, -6.1094e-04,  ...,  1.0222e-03,
          0.0000e+00, -7.8918e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4009.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.4436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2294, device='cuda:0')



h[100].sum tensor(112.1263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.5411, device='cuda:0')



h[200].sum tensor(59.9647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0103, 0.0101, 0.0000,  ..., 0.0205, 0.0000, 0.0056],
        [0.0349, 0.0295, 0.0000,  ..., 0.0609, 0.0000, 0.0207],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78512.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9333e-02, 9.2969e-02, 5.8308e-02,  ..., 7.7126e-04, 3.2499e-02,
         0.0000e+00],
        [5.8073e-02, 1.1264e-01, 1.5986e-01,  ..., 1.9744e-02, 6.4273e-02,
         0.0000e+00],
        [1.2456e-01, 1.4490e-01, 3.3329e-01,  ..., 5.6079e-02, 1.1779e-01,
         0.0000e+00],
        ...,
        [8.6604e-05, 8.5734e-02, 8.3710e-03,  ..., 0.0000e+00, 1.6845e-02,
         0.0000e+00],
        [8.7196e-05, 8.5758e-02, 8.3750e-03,  ..., 0.0000e+00, 1.6851e-02,
         0.0000e+00],
        [8.6199e-05, 8.5678e-02, 8.3634e-03,  ..., 0.0000e+00, 1.6832e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600934.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3502.5439, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(436.6691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5576.4712, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1185.9447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-790.7138, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6569],
        [-0.2578],
        [-0.0689],
        ...,
        [-4.5724],
        [-4.5665],
        [-4.5647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265701.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0330],
        [1.0326],
        ...,
        [0.9984],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369871.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0320],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369876.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.8312e-05,  5.7606e-04, -6.1094e-04,  ...,  1.0129e-03,
          0.0000e+00, -7.9329e-04],
        [ 9.8312e-05,  5.7606e-04, -6.1094e-04,  ...,  1.0129e-03,
          0.0000e+00, -7.9329e-04],
        [ 9.8312e-05,  5.7606e-04, -6.1094e-04,  ...,  1.0129e-03,
          0.0000e+00, -7.9329e-04],
        ...,
        [ 9.8312e-05,  5.7606e-04, -6.1094e-04,  ...,  1.0129e-03,
          0.0000e+00, -7.9329e-04],
        [ 9.8312e-05,  5.7606e-04, -6.1094e-04,  ...,  1.0129e-03,
          0.0000e+00, -7.9329e-04],
        [ 9.8312e-05,  5.7606e-04, -6.1094e-04,  ...,  1.0129e-03,
          0.0000e+00, -7.9329e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3085.8904, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.9733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0187, device='cuda:0')



h[100].sum tensor(107.7773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.9731, device='cuda:0')



h[200].sum tensor(46.0392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8944, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0219, 0.0193, 0.0000,  ..., 0.0394, 0.0000, 0.0131],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60401.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0846, 0.0155,  ..., 0.0000, 0.0185, 0.0000],
        [0.0122, 0.0899, 0.0404,  ..., 0.0000, 0.0263, 0.0000],
        [0.0422, 0.1048, 0.1191,  ..., 0.0115, 0.0509, 0.0000],
        ...,
        [0.0002, 0.0856, 0.0085,  ..., 0.0000, 0.0168, 0.0000],
        [0.0002, 0.0856, 0.0085,  ..., 0.0000, 0.0168, 0.0000],
        [0.0002, 0.0855, 0.0085,  ..., 0.0000, 0.0168, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(524369.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2325.0142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.0775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5996.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(931.5755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-592.8116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3336],
        [-2.4383],
        [-1.3582],
        ...,
        [-4.2642],
        [-4.4821],
        [-4.5417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289667.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0320],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369876.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 100.0 event: 500 loss: tensor(872.9244, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0321],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369880.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2003e-05,  5.7880e-04, -6.1094e-04,  ...,  1.0087e-03,
          0.0000e+00, -7.9175e-04],
        [ 9.2003e-05,  5.7880e-04, -6.1094e-04,  ...,  1.0087e-03,
          0.0000e+00, -7.9175e-04],
        [ 5.9182e-03,  5.1659e-03, -6.4964e-04,  ...,  1.0559e-02,
         -3.1956e-04,  2.9771e-03],
        ...,
        [ 9.2003e-05,  5.7880e-04, -6.1094e-04,  ...,  1.0087e-03,
          0.0000e+00, -7.9175e-04],
        [ 9.2003e-05,  5.7880e-04, -6.1094e-04,  ...,  1.0087e-03,
          0.0000e+00, -7.9175e-04],
        [ 9.2003e-05,  5.7880e-04, -6.1094e-04,  ...,  1.0087e-03,
          0.0000e+00, -7.9175e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3169.9343, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.8936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5287, device='cuda:0')



h[100].sum tensor(108.1618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.4978, device='cuda:0')



h[200].sum tensor(47.4329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0064, 0.0071, 0.0000,  ..., 0.0140, 0.0000, 0.0031],
        [0.0053, 0.0063, 0.0000,  ..., 0.0123, 0.0000, 0.0024],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61653.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.1084e-03, 8.5166e-02, 1.6922e-02,  ..., 0.0000e+00, 1.9051e-02,
         0.0000e+00],
        [1.0415e-02, 8.9450e-02, 3.6372e-02,  ..., 0.0000e+00, 2.5970e-02,
         0.0000e+00],
        [1.4312e-02, 9.1434e-02, 4.6125e-02,  ..., 0.0000e+00, 2.9636e-02,
         0.0000e+00],
        ...,
        [5.9180e-05, 8.5871e-02, 8.6732e-03,  ..., 0.0000e+00, 1.6624e-02,
         0.0000e+00],
        [5.9787e-05, 8.5897e-02, 8.6776e-03,  ..., 0.0000e+00, 1.6630e-02,
         0.0000e+00],
        [4.0687e-04, 8.5971e-02, 9.5310e-03,  ..., 0.0000e+00, 1.6943e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528999.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2372.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.8157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5909.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(949.6420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-605.8359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2711],
        [-2.7427],
        [-2.0905],
        ...,
        [-4.5357],
        [-4.4512],
        [-4.3413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286376.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0321],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369880.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369883.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.3932e-03,  5.5460e-03, -6.5291e-04,  ...,  1.1366e-02,
         -3.4459e-04,  3.2986e-03],
        [ 7.4229e-05,  5.7079e-04, -6.1094e-04,  ...,  1.0086e-03,
          0.0000e+00, -7.8899e-04],
        [ 6.3932e-03,  5.5460e-03, -6.5291e-04,  ...,  1.1366e-02,
         -3.4459e-04,  3.2986e-03],
        ...,
        [ 7.4229e-05,  5.7079e-04, -6.1094e-04,  ...,  1.0086e-03,
          0.0000e+00, -7.8899e-04],
        [ 7.4229e-05,  5.7079e-04, -6.1094e-04,  ...,  1.0086e-03,
          0.0000e+00, -7.8899e-04],
        [ 7.4229e-05,  5.7079e-04, -6.1094e-04,  ...,  1.0086e-03,
          0.0000e+00, -7.8899e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4221.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.7956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0841, device='cuda:0')



h[100].sum tensor(113.0873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.0861, device='cuda:0')



h[200].sum tensor(63.5144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7940, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0203, 0.0181, 0.0000,  ..., 0.0369, 0.0000, 0.0113],
        [0.0336, 0.0286, 0.0000,  ..., 0.0588, 0.0000, 0.0183],
        [0.0104, 0.0103, 0.0000,  ..., 0.0208, 0.0000, 0.0057],
        ...,
        [0.0003, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81840.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0555, 0.1123, 0.1527,  ..., 0.0105, 0.0634, 0.0000],
        [0.0556, 0.1129, 0.1528,  ..., 0.0097, 0.0640, 0.0000],
        [0.0331, 0.1018, 0.0953,  ..., 0.0028, 0.0452, 0.0000],
        ...,
        [0.0000, 0.0864, 0.0087,  ..., 0.0000, 0.0163, 0.0000],
        [0.0000, 0.0864, 0.0087,  ..., 0.0000, 0.0163, 0.0000],
        [0.0000, 0.0863, 0.0087,  ..., 0.0000, 0.0163, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612480.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3677.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(460.5621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5173.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1236.3177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-825.9633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1476],
        [-0.1392],
        [-0.7432],
        ...,
        [-4.6138],
        [-4.6074],
        [-4.6053]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235108.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369883.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0322],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369887.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8605e-05,  5.5320e-04, -6.1094e-04,  ...,  1.0057e-03,
          0.0000e+00, -7.8506e-04],
        [ 4.8605e-05,  5.5320e-04, -6.1094e-04,  ...,  1.0057e-03,
          0.0000e+00, -7.8506e-04],
        [ 4.8605e-05,  5.5320e-04, -6.1094e-04,  ...,  1.0057e-03,
          0.0000e+00, -7.8506e-04],
        ...,
        [ 4.8605e-05,  5.5320e-04, -6.1094e-04,  ...,  1.0057e-03,
          0.0000e+00, -7.8506e-04],
        [ 4.8605e-05,  5.5320e-04, -6.1094e-04,  ...,  1.0057e-03,
          0.0000e+00, -7.8506e-04],
        [ 4.8605e-05,  5.5320e-04, -6.1094e-04,  ...,  1.0057e-03,
          0.0000e+00, -7.8506e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3799.7058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.2311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0289, device='cuda:0')



h[100].sum tensor(111.9808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.9519, device='cuda:0')



h[200].sum tensor(56.4039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4532, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0023, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0057, 0.0066, 0.0000,  ..., 0.0132, 0.0000, 0.0028],
        ...,
        [0.0002, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72521.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.3317e-03, 8.7223e-02, 2.2732e-02,  ..., 0.0000e+00, 2.1209e-02,
         0.0000e+00],
        [6.4946e-03, 8.9057e-02, 2.8496e-02,  ..., 0.0000e+00, 2.2734e-02,
         0.0000e+00],
        [1.6349e-02, 9.4292e-02, 5.3923e-02,  ..., 3.2352e-06, 3.0794e-02,
         0.0000e+00],
        ...,
        [4.8024e-04, 8.7481e-02, 1.1127e-02,  ..., 0.0000e+00, 1.6655e-02,
         0.0000e+00],
        [0.0000e+00, 8.6984e-02, 8.5020e-03,  ..., 0.0000e+00, 1.5793e-02,
         0.0000e+00],
        [0.0000e+00, 8.6903e-02, 8.4904e-03,  ..., 0.0000e+00, 1.5776e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573534., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2845.9395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(380.0873, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5683.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1100.4514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-722.3307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4416],
        [-2.2665],
        [-1.8511],
        ...,
        [-4.4413],
        [-4.5786],
        [-4.6347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307080.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0322],
        [1.0330],
        [1.0327],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369887.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0330],
        [1.0328],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369890.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.5244e-05,  5.4300e-04, -6.1094e-04,  ...,  9.9571e-04,
          0.0000e+00, -7.8662e-04],
        [ 8.8995e-03,  7.5225e-03, -6.6981e-04,  ...,  1.5520e-02,
         -4.7781e-04,  4.9468e-03],
        [ 7.6487e-03,  6.5377e-03, -6.6151e-04,  ...,  1.3470e-02,
         -4.1039e-04,  4.1378e-03],
        ...,
        [ 3.5244e-05,  5.4300e-04, -6.1094e-04,  ...,  9.9571e-04,
          0.0000e+00, -7.8662e-04],
        [ 3.5244e-05,  5.4300e-04, -6.1094e-04,  ...,  9.9571e-04,
          0.0000e+00, -7.8662e-04],
        [ 3.5244e-05,  5.4300e-04, -6.1094e-04,  ...,  9.9571e-04,
          0.0000e+00, -7.8662e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3002.9644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.9735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.5126, device='cuda:0')



h[100].sum tensor(108.8642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.4601, device='cuda:0')



h[200].sum tensor(43.6895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8379, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0092, 0.0094, 0.0000,  ..., 0.0190, 0.0000, 0.0051],
        [0.0220, 0.0194, 0.0000,  ..., 0.0399, 0.0000, 0.0125],
        [0.0541, 0.0447, 0.0000,  ..., 0.0924, 0.0000, 0.0316],
        ...,
        [0.0001, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0001, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0001, 0.0023, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59208.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.1072, 0.1200,  ..., 0.0065, 0.0503, 0.0000],
        [0.0627, 0.1184, 0.1738,  ..., 0.0177, 0.0675, 0.0000],
        [0.0990, 0.1369, 0.2677,  ..., 0.0381, 0.0970, 0.0000],
        ...,
        [0.0000, 0.0871, 0.0082,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0872, 0.0082,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0871, 0.0082,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523629.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1997.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(264.5732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6063.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(911.2758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-576.1663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0553],
        [ 0.1376],
        [ 0.1700],
        ...,
        [-4.6975],
        [-4.6884],
        [-4.6685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-341922.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0330],
        [1.0328],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369890.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0323],
        [1.0331],
        [1.0329],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369894.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1403e-02,  9.4962e-03, -6.8639e-04,  ...,  1.9594e-02,
         -6.0882e-04,  6.5524e-03],
        [ 1.9327e-02,  1.5736e-02, -7.3902e-04,  ...,  3.2574e-02,
         -1.0335e-03,  1.1677e-02],
        [ 2.0118e-02,  1.6358e-02, -7.4427e-04,  ...,  3.3870e-02,
         -1.0758e-03,  1.2188e-02],
        ...,
        [ 4.1713e-05,  5.5017e-04, -6.1094e-04,  ...,  9.8238e-04,
          0.0000e+00, -7.9506e-04],
        [ 4.1713e-05,  5.5017e-04, -6.1094e-04,  ...,  9.8238e-04,
          0.0000e+00, -7.9506e-04],
        [ 4.1713e-05,  5.5017e-04, -6.1094e-04,  ...,  9.8238e-04,
          0.0000e+00, -7.9506e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3314.4631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.5491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8252, device='cuda:0')



h[100].sum tensor(110.0109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3740, device='cuda:0')



h[200].sum tensor(48.6776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0617, 0.0507, 0.0000,  ..., 0.1049, 0.0000, 0.0365],
        [0.0828, 0.0674, 0.0000,  ..., 0.1395, 0.0000, 0.0502],
        [0.0733, 0.0598, 0.0000,  ..., 0.1238, 0.0000, 0.0440],
        ...,
        [0.0002, 0.0023, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65620.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1535, 0.1631, 0.4074,  ..., 0.0697, 0.1398, 0.0000],
        [0.1645, 0.1689, 0.4367,  ..., 0.0776, 0.1481, 0.0000],
        [0.1385, 0.1560, 0.3697,  ..., 0.0626, 0.1276, 0.0000],
        ...,
        [0.0000, 0.0869, 0.0082,  ..., 0.0000, 0.0153, 0.0000],
        [0.0000, 0.0869, 0.0082,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0869, 0.0082,  ..., 0.0000, 0.0153, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555082.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2555.7842, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.3419, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5968.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(999.6569, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-643.5255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1902],
        [ 0.1743],
        [ 0.1127],
        ...,
        [-4.7086],
        [-4.7021],
        [-4.7001]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327975.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0323],
        [1.0331],
        [1.0329],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369894.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0324],
        [1.0331],
        [1.0329],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369899.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.4731e-02,  1.2120e-02, -7.0840e-04,  ...,  2.5004e-02,
         -7.8187e-04,  8.6851e-03],
        [ 8.6713e-03,  7.3477e-03, -6.6816e-04,  ...,  1.5079e-02,
         -4.5902e-04,  4.7665e-03],
        [ 3.0545e-02,  2.4572e-02, -8.1342e-04,  ...,  5.0903e-02,
         -1.6243e-03,  1.8911e-02],
        ...,
        [ 5.5128e-05,  5.6274e-04, -6.1094e-04,  ...,  9.6735e-04,
          0.0000e+00, -8.0495e-04],
        [ 5.5128e-05,  5.6274e-04, -6.1094e-04,  ...,  9.6735e-04,
          0.0000e+00, -8.0495e-04],
        [ 5.5128e-05,  5.6274e-04, -6.1094e-04,  ...,  9.6735e-04,
          0.0000e+00, -8.0495e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3389.6724, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.0808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3223, device='cuda:0')



h[100].sum tensor(109.9801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8603, device='cuda:0')



h[200].sum tensor(50.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0447, 0.0373, 0.0000,  ..., 0.0768, 0.0000, 0.0254],
        [0.0790, 0.0644, 0.0000,  ..., 0.1330, 0.0000, 0.0476],
        [0.0350, 0.0297, 0.0000,  ..., 0.0609, 0.0000, 0.0200],
        ...,
        [0.0002, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0002, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65920.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0800, 0.1253, 0.2160,  ..., 0.0272, 0.0804, 0.0000],
        [0.1122, 0.1418, 0.2999,  ..., 0.0471, 0.1059, 0.0000],
        [0.0902, 0.1313, 0.2428,  ..., 0.0337, 0.0884, 0.0000],
        ...,
        [0.0000, 0.0866, 0.0084,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0867, 0.0084,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0866, 0.0083,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550208.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2605.1292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(322.2587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6029.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1004.7363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-645.8549, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6807],
        [-0.1483],
        [ 0.1128],
        ...,
        [-4.7106],
        [-4.7042],
        [-4.7022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303588.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0324],
        [1.0331],
        [1.0329],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369899.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0324],
        [1.0331],
        [1.0329],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369903.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.1179e-05,  5.7479e-04, -6.1094e-04,  ...,  9.5208e-04,
          0.0000e+00, -8.1747e-04],
        [ 7.1179e-05,  5.7479e-04, -6.1094e-04,  ...,  9.5208e-04,
          0.0000e+00, -8.1747e-04],
        [ 7.1179e-05,  5.7479e-04, -6.1094e-04,  ...,  9.5208e-04,
          0.0000e+00, -8.1747e-04],
        ...,
        [ 7.1179e-05,  5.7479e-04, -6.1094e-04,  ...,  9.5208e-04,
          0.0000e+00, -8.1747e-04],
        [ 7.1179e-05,  5.7479e-04, -6.1094e-04,  ...,  9.5208e-04,
          0.0000e+00, -8.1747e-04],
        [ 7.1179e-05,  5.7479e-04, -6.1094e-04,  ...,  9.5208e-04,
          0.0000e+00, -8.1747e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3259.2729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.9249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4176, device='cuda:0')



h[100].sum tensor(108.9799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1553, device='cuda:0')



h[200].sum tensor(48.4001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0079, 0.0000,  ..., 0.0154, 0.0000, 0.0037],
        [0.0003, 0.0024, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63552.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0202, 0.0943, 0.0600,  ..., 0.0003, 0.0328, 0.0000],
        [0.0074, 0.0883, 0.0272,  ..., 0.0000, 0.0218, 0.0000],
        [0.0014, 0.0855, 0.0121,  ..., 0.0000, 0.0167, 0.0000],
        ...,
        [0.0001, 0.0863, 0.0084,  ..., 0.0000, 0.0155, 0.0000],
        [0.0001, 0.0863, 0.0084,  ..., 0.0000, 0.0155, 0.0000],
        [0.0001, 0.0862, 0.0084,  ..., 0.0000, 0.0155, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539602.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2557.1392, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.0732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6264.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(971.5859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.3250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4241],
        [-2.1710],
        [-2.6740],
        ...,
        [-4.7129],
        [-4.7066],
        [-4.7046]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292972.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0324],
        [1.0331],
        [1.0329],
        ...,
        [0.9984],
        [0.9974],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369903.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0325],
        [1.0332],
        [1.0330],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369907.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8553e-03,  4.3429e-03, -6.4271e-04,  ...,  8.7760e-03,
         -2.5194e-04,  2.2692e-03],
        [ 4.8553e-03,  4.3429e-03, -6.4271e-04,  ...,  8.7760e-03,
         -2.5194e-04,  2.2692e-03],
        [ 7.0495e-05,  5.7468e-04, -6.1094e-04,  ...,  9.4183e-04,
          0.0000e+00, -8.2422e-04],
        ...,
        [ 7.0495e-05,  5.7468e-04, -6.1094e-04,  ...,  9.4183e-04,
          0.0000e+00, -8.2422e-04],
        [ 7.0495e-05,  5.7468e-04, -6.1094e-04,  ...,  9.4183e-04,
          0.0000e+00, -8.2422e-04],
        [ 7.0495e-05,  5.7468e-04, -6.1094e-04,  ...,  9.4183e-04,
          0.0000e+00, -8.2422e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4560.1064, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.7993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.5716, device='cuda:0')



h[100].sum tensor(114.7046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.5228, device='cuda:0')



h[200].sum tensor(68.2789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0316, 0.0270, 0.0000,  ..., 0.0551, 0.0000, 0.0168],
        [0.0189, 0.0171, 0.0000,  ..., 0.0344, 0.0000, 0.0095],
        [0.0375, 0.0317, 0.0000,  ..., 0.0648, 0.0000, 0.0207],
        ...,
        [0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88689.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0749, 0.1218, 0.1971,  ..., 0.0186, 0.0775, 0.0000],
        [0.0678, 0.1189, 0.1793,  ..., 0.0149, 0.0719, 0.0000],
        [0.0797, 0.1253, 0.2100,  ..., 0.0222, 0.0810, 0.0000],
        ...,
        [0.0003, 0.0862, 0.0082,  ..., 0.0000, 0.0154, 0.0000],
        [0.0003, 0.0863, 0.0082,  ..., 0.0000, 0.0154, 0.0000],
        [0.0003, 0.0862, 0.0082,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652558.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4275.4707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(522.8350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6058.7275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1315.2104, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-885.0643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3060],
        [ 0.3323],
        [ 0.2802],
        ...,
        [-4.7319],
        [-4.7255],
        [-4.7235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301449.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0325],
        [1.0332],
        [1.0330],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369907.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0325],
        [1.0332],
        [1.0330],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369911.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2349e-05,  5.6964e-04, -6.1094e-04,  ...,  9.3397e-04,
          0.0000e+00, -8.2793e-04],
        [ 6.2349e-05,  5.6964e-04, -6.1094e-04,  ...,  9.3397e-04,
          0.0000e+00, -8.2793e-04],
        [ 6.2349e-05,  5.6964e-04, -6.1094e-04,  ...,  9.3397e-04,
          0.0000e+00, -8.2793e-04],
        ...,
        [ 6.2349e-05,  5.6964e-04, -6.1094e-04,  ...,  9.3397e-04,
          0.0000e+00, -8.2793e-04],
        [ 6.2349e-05,  5.6964e-04, -6.1094e-04,  ...,  9.3397e-04,
          0.0000e+00, -8.2793e-04],
        [ 6.2349e-05,  5.6964e-04, -6.1094e-04,  ...,  9.3397e-04,
          0.0000e+00, -8.2793e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3698.4702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.8224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5896, device='cuda:0')



h[100].sum tensor(111.3984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.6385, device='cuda:0')



h[200].sum tensor(54.6217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0023, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0222, 0.0196, 0.0000,  ..., 0.0397, 0.0000, 0.0133],
        ...,
        [0.0003, 0.0024, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71522.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0095, 0.0890, 0.0326,  ..., 0.0000, 0.0228, 0.0000],
        [0.0214, 0.0954, 0.0624,  ..., 0.0019, 0.0321, 0.0000],
        [0.0535, 0.1117, 0.1459,  ..., 0.0164, 0.0579, 0.0000],
        ...,
        [0.0002, 0.0864, 0.0079,  ..., 0.0000, 0.0152, 0.0000],
        [0.0002, 0.0864, 0.0079,  ..., 0.0000, 0.0152, 0.0000],
        [0.0002, 0.0863, 0.0079,  ..., 0.0000, 0.0152, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573118., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3111.0176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(369.6811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6266.0322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1080.7162, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-701.4607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1304],
        [-0.8165],
        [-0.3424],
        ...,
        [-4.5756],
        [-4.6304],
        [-4.6819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297359.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0325],
        [1.0332],
        [1.0330],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369911.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0326],
        [1.0333],
        [1.0331],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369915.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.5616e-05,  5.5713e-04, -6.1094e-04,  ...,  9.3797e-04,
          0.0000e+00, -8.2671e-04],
        [ 4.5616e-05,  5.5713e-04, -6.1094e-04,  ...,  9.3797e-04,
          0.0000e+00, -8.2671e-04],
        [ 4.5616e-05,  5.5713e-04, -6.1094e-04,  ...,  9.3797e-04,
          0.0000e+00, -8.2671e-04],
        ...,
        [ 1.2970e-02,  1.0737e-02, -6.9677e-04,  ...,  2.2101e-02,
         -6.7269e-04,  7.5315e-03],
        [ 4.5616e-05,  5.5713e-04, -6.1094e-04,  ...,  9.3797e-04,
          0.0000e+00, -8.2671e-04],
        [ 4.5616e-05,  5.5713e-04, -6.1094e-04,  ...,  9.3797e-04,
          0.0000e+00, -8.2671e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4077.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.4827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0756, device='cuda:0')



h[100].sum tensor(113.7478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.0710, device='cuda:0')



h[200].sum tensor(59.8891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6815, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0002, 0.0023, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0221, 0.0196, 0.0000,  ..., 0.0398, 0.0000, 0.0115],
        [0.0193, 0.0174, 0.0000,  ..., 0.0352, 0.0000, 0.0106],
        [0.0002, 0.0023, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80570.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.2046e-03, 8.6131e-02, 1.5953e-02,  ..., 0.0000e+00, 1.7244e-02,
         0.0000e+00],
        [1.0830e-04, 8.5007e-02, 7.2962e-03,  ..., 0.0000e+00, 1.4474e-02,
         0.0000e+00],
        [1.3416e-04, 8.5216e-02, 7.3788e-03,  ..., 0.0000e+00, 1.4559e-02,
         0.0000e+00],
        ...,
        [6.4961e-02, 1.2032e-01, 1.7395e-01,  ..., 1.6438e-02, 6.8678e-02,
         0.0000e+00],
        [4.7313e-02, 1.1128e-01, 1.2935e-01,  ..., 1.0394e-02, 5.4007e-02,
         0.0000e+00],
        [1.5792e-02, 9.4874e-02, 4.8267e-02,  ..., 0.0000e+00, 2.8026e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619785.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3768.8704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(446.3748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6061.6553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1204.7296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-798.4471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7719],
        [-3.3556],
        [-3.6615],
        ...,
        [-0.4964],
        [-1.4161],
        [-2.6763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303301.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0326],
        [1.0333],
        [1.0331],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369915.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 110.0 event: 550 loss: tensor(464.9203, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0334],
        [1.0331],
        ...,
        [0.9983],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369920.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8221e-03,  4.3222e-03, -6.4271e-04,  ...,  8.7841e-03,
         -2.4757e-04,  2.2727e-03],
        [ 6.1735e-03,  5.3867e-03, -6.5169e-04,  ...,  1.0997e-02,
         -3.1750e-04,  3.1468e-03],
        [ 3.7841e-05,  5.5406e-04, -6.1094e-04,  ...,  9.4942e-04,
          0.0000e+00, -8.2192e-04],
        ...,
        [ 3.7841e-05,  5.5406e-04, -6.1094e-04,  ...,  9.4942e-04,
          0.0000e+00, -8.2192e-04],
        [ 3.7841e-05,  5.5406e-04, -6.1094e-04,  ...,  9.4942e-04,
          0.0000e+00, -8.2192e-04],
        [ 3.7841e-05,  5.5406e-04, -6.1094e-04,  ...,  9.4942e-04,
          0.0000e+00, -8.2192e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3695.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6601, device='cuda:0')



h[100].sum tensor(112.4162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.8493, device='cuda:0')



h[200].sum tensor(53.7990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0489, 0.0407, 0.0000,  ..., 0.0837, 0.0000, 0.0282],
        [0.0203, 0.0182, 0.0000,  ..., 0.0369, 0.0000, 0.0113],
        [0.0065, 0.0073, 0.0000,  ..., 0.0143, 0.0000, 0.0033],
        ...,
        [0.0002, 0.0023, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0002, 0.0023, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71044.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.6814e-02, 1.3011e-01, 2.3187e-01,  ..., 2.9369e-02, 8.5375e-02,
         0.0000e+00],
        [5.7961e-02, 1.1574e-01, 1.5781e-01,  ..., 1.5202e-02, 6.2269e-02,
         0.0000e+00],
        [2.9157e-02, 1.0103e-01, 8.3907e-02,  ..., 4.6149e-03, 3.9041e-02,
         0.0000e+00],
        ...,
        [6.2676e-05, 8.7123e-02, 7.7801e-03,  ..., 0.0000e+00, 1.4891e-02,
         0.0000e+00],
        [6.3279e-05, 8.7150e-02, 7.7844e-03,  ..., 0.0000e+00, 1.4897e-02,
         0.0000e+00],
        [6.2330e-05, 8.7069e-02, 7.7734e-03,  ..., 0.0000e+00, 1.4880e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572616.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3011.6692, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.5805, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6130.0747, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1075.5602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-695.4504, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0247],
        [-0.4690],
        [-1.2948],
        ...,
        [-4.8010],
        [-4.7942],
        [-4.7920]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318679.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0334],
        [1.0331],
        ...,
        [0.9983],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369920.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0327],
        [1.0335],
        [1.0332],
        ...,
        [0.9983],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369925.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8733e-05,  5.6375e-04, -6.1094e-04,  ...,  9.5879e-04,
          0.0000e+00, -8.2320e-04],
        [ 4.8733e-05,  5.6375e-04, -6.1094e-04,  ...,  9.5879e-04,
          0.0000e+00, -8.2320e-04],
        [ 4.8417e-03,  4.3391e-03, -6.4277e-04,  ...,  8.8087e-03,
         -2.4658e-04,  2.2775e-03],
        ...,
        [ 4.8733e-05,  5.6375e-04, -6.1094e-04,  ...,  9.5879e-04,
          0.0000e+00, -8.2320e-04],
        [ 4.8733e-05,  5.6375e-04, -6.1094e-04,  ...,  9.5879e-04,
          0.0000e+00, -8.2320e-04],
        [ 4.8733e-05,  5.6375e-04, -6.1094e-04,  ...,  9.5879e-04,
          0.0000e+00, -8.2320e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4194.8989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.0078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9019, device='cuda:0')



h[100].sum tensor(114.1806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.5412, device='cuda:0')



h[200].sum tensor(61.7723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0042, 0.0055, 0.0000,  ..., 0.0105, 0.0000, 0.0017],
        [0.0211, 0.0188, 0.0000,  ..., 0.0381, 0.0000, 0.0109],
        [0.0363, 0.0307, 0.0000,  ..., 0.0630, 0.0000, 0.0208],
        ...,
        [0.0002, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0002, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0002, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82065.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6560e-02, 9.8701e-02, 7.6831e-02,  ..., 2.1323e-03, 3.7704e-02,
         0.0000e+00],
        [5.5453e-02, 1.1436e-01, 1.5086e-01,  ..., 1.1880e-02, 6.1439e-02,
         0.0000e+00],
        [7.8529e-02, 1.2676e-01, 2.1036e-01,  ..., 2.2905e-02, 7.9603e-02,
         0.0000e+00],
        ...,
        [1.3231e-04, 8.7217e-02, 8.0920e-03,  ..., 0.0000e+00, 1.5098e-02,
         0.0000e+00],
        [1.3290e-04, 8.7242e-02, 8.0959e-03,  ..., 0.0000e+00, 1.5103e-02,
         0.0000e+00],
        [1.3181e-04, 8.7161e-02, 8.0845e-03,  ..., 0.0000e+00, 1.5086e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621188., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3830.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(457.5278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5822.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1233.9728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-814.6617, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4007],
        [-0.5152],
        [ 0.0081],
        ...,
        [-4.7948],
        [-4.7880],
        [-4.7858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283530.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0327],
        [1.0335],
        [1.0332],
        ...,
        [0.9983],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369925.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0335],
        [1.0332],
        ...,
        [0.9983],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369930.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.2517e-05,  5.8091e-04, -6.1094e-04,  ...,  9.6603e-04,
          0.0000e+00, -8.2789e-04],
        [ 7.2517e-05,  5.8091e-04, -6.1094e-04,  ...,  9.6603e-04,
          0.0000e+00, -8.2789e-04],
        [ 7.2517e-05,  5.8091e-04, -6.1094e-04,  ...,  9.6603e-04,
          0.0000e+00, -8.2789e-04],
        ...,
        [ 7.2517e-05,  5.8091e-04, -6.1094e-04,  ...,  9.6603e-04,
          0.0000e+00, -8.2789e-04],
        [ 7.2517e-05,  5.8091e-04, -6.1094e-04,  ...,  9.6603e-04,
          0.0000e+00, -8.2789e-04],
        [ 7.2517e-05,  5.8091e-04, -6.1094e-04,  ...,  9.6603e-04,
          0.0000e+00, -8.2789e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3343.2070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0240, device='cuda:0')



h[100].sum tensor(109.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9684, device='cuda:0')



h[200].sum tensor(49.3226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66005.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0847, 0.0080,  ..., 0.0000, 0.0148, 0.0000],
        [0.0003, 0.0853, 0.0081,  ..., 0.0000, 0.0149, 0.0000],
        [0.0003, 0.0855, 0.0082,  ..., 0.0000, 0.0150, 0.0000],
        ...,
        [0.0003, 0.0871, 0.0085,  ..., 0.0000, 0.0154, 0.0000],
        [0.0003, 0.0871, 0.0085,  ..., 0.0000, 0.0154, 0.0000],
        [0.0003, 0.0870, 0.0085,  ..., 0.0000, 0.0154, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553532.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2766.2278, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.8232, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6436.5522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1007.4542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-636.5101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5037],
        [-4.2622],
        [-3.8860],
        ...,
        [-4.7774],
        [-4.7708],
        [-4.7685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318462.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0335],
        [1.0332],
        ...,
        [0.9983],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369930.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0336],
        [1.0333],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369934.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.2655e-05,  5.9844e-04, -6.1094e-04,  ...,  9.8148e-04,
          0.0000e+00, -8.2861e-04],
        [ 9.2655e-05,  5.9844e-04, -6.1094e-04,  ...,  9.8148e-04,
          0.0000e+00, -8.2861e-04],
        [ 5.9142e-03,  5.1844e-03, -6.4960e-04,  ...,  1.0518e-02,
         -2.9599e-04,  2.9381e-03],
        ...,
        [ 9.2655e-05,  5.9844e-04, -6.1094e-04,  ...,  9.8148e-04,
          0.0000e+00, -8.2861e-04],
        [ 9.2655e-05,  5.9844e-04, -6.1094e-04,  ...,  9.8148e-04,
          0.0000e+00, -8.2861e-04],
        [ 9.2655e-05,  5.9844e-04, -6.1094e-04,  ...,  9.8148e-04,
          0.0000e+00, -8.2861e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3678.4255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.2232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3575, device='cuda:0')



h[100].sum tensor(110.1011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.9448, device='cuda:0')



h[200].sum tensor(55.1497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0025, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0064, 0.0072, 0.0000,  ..., 0.0139, 0.0000, 0.0030],
        [0.0094, 0.0096, 0.0000,  ..., 0.0188, 0.0000, 0.0050],
        ...,
        [0.0004, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72141.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0877, 0.0234,  ..., 0.0000, 0.0202, 0.0000],
        [0.0200, 0.0955, 0.0595,  ..., 0.0004, 0.0323, 0.0000],
        [0.0331, 0.1023, 0.0929,  ..., 0.0022, 0.0434, 0.0000],
        ...,
        [0.0004, 0.0871, 0.0090,  ..., 0.0000, 0.0158, 0.0000],
        [0.0004, 0.0871, 0.0090,  ..., 0.0000, 0.0158, 0.0000],
        [0.0004, 0.0870, 0.0089,  ..., 0.0000, 0.0158, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576662.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3279.5300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.5367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6026.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1103.3292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-705.2656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1675],
        [-2.2506],
        [-1.3929],
        ...,
        [-4.7590],
        [-4.7523],
        [-4.7501]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255046., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0336],
        [1.0333],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369934.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0336],
        [1.0334],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369939., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4788e-05,  5.9931e-04, -6.1094e-04,  ...,  9.9704e-04,
          0.0000e+00, -8.3013e-04],
        [ 9.4788e-05,  5.9931e-04, -6.1094e-04,  ...,  9.9704e-04,
          0.0000e+00, -8.3013e-04],
        [ 9.4788e-05,  5.9931e-04, -6.1094e-04,  ...,  9.9704e-04,
          0.0000e+00, -8.3013e-04],
        ...,
        [ 9.4788e-05,  5.9931e-04, -6.1094e-04,  ...,  9.9704e-04,
          0.0000e+00, -8.3013e-04],
        [ 9.4788e-05,  5.9931e-04, -6.1094e-04,  ...,  9.9704e-04,
          0.0000e+00, -8.3013e-04],
        [ 9.4788e-05,  5.9931e-04, -6.1094e-04,  ...,  9.9704e-04,
          0.0000e+00, -8.3013e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3546.8555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4069, device='cuda:0')



h[100].sum tensor(109.2199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.1028, device='cuda:0')



h[200].sum tensor(53.2666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0004, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0117, 0.0114, 0.0000,  ..., 0.0226, 0.0000, 0.0056],
        ...,
        [0.0004, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67970.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0863, 0.0150,  ..., 0.0000, 0.0174, 0.0000],
        [0.0103, 0.0909, 0.0347,  ..., 0.0000, 0.0240, 0.0000],
        [0.0295, 0.1011, 0.0842,  ..., 0.0014, 0.0405, 0.0000],
        ...,
        [0.0004, 0.0874, 0.0091,  ..., 0.0000, 0.0158, 0.0000],
        [0.0004, 0.0874, 0.0092,  ..., 0.0000, 0.0158, 0.0000],
        [0.0004, 0.0873, 0.0091,  ..., 0.0000, 0.0158, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555784., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2914.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.5584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6106.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1045.7780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-657.4027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4213],
        [-2.6017],
        [-1.6975],
        ...,
        [-4.7154],
        [-4.6970],
        [-4.6894]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279528.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0336],
        [1.0334],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369939., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0328],
        [1.0336],
        [1.0334],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369943.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.1219e-02,  2.5122e-02, -8.1768e-04,  ...,  5.2014e-02,
         -1.5643e-03,  1.9315e-02],
        [ 7.8863e-03,  6.7385e-03, -6.6273e-04,  ...,  1.3792e-02,
         -3.9186e-04,  4.2181e-03],
        [ 2.5389e-02,  2.0528e-02, -7.7896e-04,  ...,  4.2463e-02,
         -1.2714e-03,  1.5543e-02],
        ...,
        [ 8.7895e-05,  5.9458e-04, -6.1094e-04,  ...,  1.0172e-03,
          0.0000e+00, -8.2749e-04],
        [ 8.7895e-05,  5.9458e-04, -6.1094e-04,  ...,  1.0172e-03,
          0.0000e+00, -8.2749e-04],
        [ 8.7895e-05,  5.9458e-04, -6.1094e-04,  ...,  1.0172e-03,
          0.0000e+00, -8.2749e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4544.2349, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.7365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.2221, device='cuda:0')



h[100].sum tensor(113.5743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(54.4778, device='cuda:0')



h[200].sum tensor(68.5211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0577, 0.0476, 0.0000,  ..., 0.0981, 0.0000, 0.0337],
        [0.1302, 0.1047, 0.0000,  ..., 0.2169, 0.0000, 0.0806],
        [0.0828, 0.0674, 0.0000,  ..., 0.1392, 0.0000, 0.0499],
        ...,
        [0.0004, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0004, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87500.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6686e-01, 1.7006e-01, 4.4501e-01,  ..., 7.8817e-02, 1.4899e-01,
         0.0000e+00],
        [2.2734e-01, 2.0041e-01, 6.0481e-01,  ..., 1.1814e-01, 1.9610e-01,
         0.0000e+00],
        [2.0815e-01, 1.9148e-01, 5.5385e-01,  ..., 1.0567e-01, 1.8090e-01,
         0.0000e+00],
        ...,
        [2.6572e-04, 8.7838e-02, 9.4196e-03,  ..., 0.0000e+00, 1.5863e-02,
         0.0000e+00],
        [2.6618e-04, 8.7856e-02, 9.4224e-03,  ..., 0.0000e+00, 1.5867e-02,
         0.0000e+00],
        [2.6479e-04, 8.7773e-02, 9.4090e-03,  ..., 0.0000e+00, 1.5849e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637409.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4167.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(513.9590, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5431.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1323.5684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-868.4332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1177],
        [ 0.0972],
        [ 0.0718],
        ...,
        [-4.7435],
        [-4.7141],
        [-4.6886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250189.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0328],
        [1.0336],
        [1.0334],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369943.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0335],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369947.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.2376e-05,  5.8648e-04, -6.1094e-04,  ...,  1.0311e-03,
          0.0000e+00, -8.2733e-04],
        [ 1.8073e-02,  1.4761e-02, -7.3041e-04,  ...,  3.0500e-02,
         -8.9869e-04,  1.0813e-02],
        [ 8.2376e-05,  5.8648e-04, -6.1094e-04,  ...,  1.0311e-03,
          0.0000e+00, -8.2733e-04],
        ...,
        [ 8.2376e-05,  5.8648e-04, -6.1094e-04,  ...,  1.0311e-03,
          0.0000e+00, -8.2733e-04],
        [ 8.2376e-05,  5.8648e-04, -6.1094e-04,  ...,  1.0311e-03,
          0.0000e+00, -8.2733e-04],
        [ 8.2376e-05,  5.8648e-04, -6.1094e-04,  ...,  1.0311e-03,
          0.0000e+00, -8.2733e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3403.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.0300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4223, device='cuda:0')



h[100].sum tensor(108.8285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1590, device='cuda:0')



h[200].sum tensor(50.9355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1625, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0188, 0.0170, 0.0000,  ..., 0.0345, 0.0000, 0.0111],
        [0.0156, 0.0144, 0.0000,  ..., 0.0293, 0.0000, 0.0090],
        [0.0680, 0.0557, 0.0000,  ..., 0.1151, 0.0000, 0.0404],
        ...,
        [0.0003, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66491.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.3676e-02, 1.0346e-01, 9.8153e-02,  ..., 6.3485e-03, 4.2430e-02,
         0.0000e+00],
        [4.3605e-02, 1.0930e-01, 1.2398e-01,  ..., 8.5200e-03, 5.0464e-02,
         0.0000e+00],
        [7.5070e-02, 1.2575e-01, 2.0589e-01,  ..., 2.5141e-02, 7.5509e-02,
         0.0000e+00],
        ...,
        [1.5753e-04, 8.8143e-02, 9.5596e-03,  ..., 0.0000e+00, 1.5858e-02,
         0.0000e+00],
        [1.5790e-04, 8.8159e-02, 9.5621e-03,  ..., 0.0000e+00, 1.5861e-02,
         0.0000e+00],
        [1.5668e-04, 8.8076e-02, 9.5484e-03,  ..., 0.0000e+00, 1.5843e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551093.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2710.2417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.9365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5974.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1029.7090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-639.7982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3868],
        [-1.7659],
        [-1.2909],
        ...,
        [-4.7935],
        [-4.7866],
        [-4.7841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296641.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0335],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369947.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0335],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369951.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.3045e-05,  5.7554e-04, -6.1094e-04,  ...,  1.0453e-03,
          0.0000e+00, -8.2436e-04],
        [ 7.3045e-05,  5.7554e-04, -6.1094e-04,  ...,  1.0453e-03,
          0.0000e+00, -8.2436e-04],
        [ 7.3045e-05,  5.7554e-04, -6.1094e-04,  ...,  1.0453e-03,
          0.0000e+00, -8.2436e-04],
        ...,
        [ 7.3045e-05,  5.7554e-04, -6.1094e-04,  ...,  1.0453e-03,
          0.0000e+00, -8.2436e-04],
        [ 7.3045e-05,  5.7554e-04, -6.1094e-04,  ...,  1.0453e-03,
          0.0000e+00, -8.2436e-04],
        [ 7.3045e-05,  5.7554e-04, -6.1094e-04,  ...,  1.0453e-03,
          0.0000e+00, -8.2436e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3454.2349, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6018, device='cuda:0')



h[100].sum tensor(109.3980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6957, device='cuda:0')



h[200].sum tensor(51.4444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66405.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 8.6169e-02, 9.0881e-03,  ..., 0.0000e+00, 1.5184e-02,
         0.0000e+00],
        [2.7715e-04, 8.6943e-02, 9.8927e-03,  ..., 0.0000e+00, 1.5650e-02,
         0.0000e+00],
        [1.0132e-03, 8.7514e-02, 1.1795e-02,  ..., 0.0000e+00, 1.6602e-02,
         0.0000e+00],
        ...,
        [1.0757e-05, 8.8569e-02, 9.5966e-03,  ..., 0.0000e+00, 1.5798e-02,
         0.0000e+00],
        [1.1026e-05, 8.8584e-02, 9.5989e-03,  ..., 0.0000e+00, 1.5801e-02,
         0.0000e+00],
        [1.0025e-05, 8.8500e-02, 9.5851e-03,  ..., 0.0000e+00, 1.5782e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549348.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2622.2212, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.4065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5880.1650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1032.9166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-639.8801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8509],
        [-3.9680],
        [-3.9481],
        ...,
        [-4.7259],
        [-4.7911],
        [-4.8024]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299532.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0335],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369951.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0336],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369955.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.1240e-05,  5.7443e-04, -6.1094e-04,  ...,  1.0482e-03,
          0.0000e+00, -8.2860e-04],
        [ 8.1240e-05,  5.7443e-04, -6.1094e-04,  ...,  1.0482e-03,
          0.0000e+00, -8.2860e-04],
        [ 4.8790e-03,  4.3544e-03, -6.4280e-04,  ...,  8.9070e-03,
         -2.3685e-04,  2.2756e-03],
        ...,
        [ 8.1240e-05,  5.7443e-04, -6.1094e-04,  ...,  1.0482e-03,
          0.0000e+00, -8.2860e-04],
        [ 8.1240e-05,  5.7443e-04, -6.1094e-04,  ...,  1.0482e-03,
          0.0000e+00, -8.2860e-04],
        [ 8.1240e-05,  5.7443e-04, -6.1094e-04,  ...,  1.0482e-03,
          0.0000e+00, -8.2860e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3309.0261, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7901, device='cuda:0')



h[100].sum tensor(108.9504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.2690, device='cuda:0')



h[200].sum tensor(49.2146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0003, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0094, 0.0095, 0.0000,  ..., 0.0191, 0.0000, 0.0041],
        [0.0132, 0.0125, 0.0000,  ..., 0.0254, 0.0000, 0.0057],
        ...,
        [0.0003, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0003, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63402.8945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.1822e-03, 9.0484e-02, 3.0759e-02,  ..., 0.0000e+00, 2.2698e-02,
         0.0000e+00],
        [2.4285e-02, 9.9792e-02, 7.3048e-02,  ..., 0.0000e+00, 3.7039e-02,
         0.0000e+00],
        [3.5122e-02, 1.0565e-01, 1.0076e-01,  ..., 7.6640e-04, 4.6668e-02,
         0.0000e+00],
        ...,
        [6.0333e-05, 8.8405e-02, 9.4185e-03,  ..., 0.0000e+00, 1.5885e-02,
         0.0000e+00],
        [6.0594e-05, 8.8420e-02, 9.4205e-03,  ..., 0.0000e+00, 1.5888e-02,
         0.0000e+00],
        [5.9501e-05, 8.8336e-02, 9.4067e-03,  ..., 0.0000e+00, 1.5869e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(536588.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2363.1848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.5078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6176.0098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(984.5212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-603.1022, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5775],
        [-1.7161],
        [-1.0380],
        ...,
        [-4.8252],
        [-4.8182],
        [-4.8157]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338118.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0336],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369955.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0336],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369959.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.8359e-05,  5.7890e-04, -6.1094e-04,  ...,  1.0461e-03,
          0.0000e+00, -8.3671e-04],
        [ 9.8359e-05,  5.7890e-04, -6.1094e-04,  ...,  1.0461e-03,
          0.0000e+00, -8.3671e-04],
        [ 5.8771e-03,  5.1316e-03, -6.4931e-04,  ...,  1.0511e-02,
         -2.8359e-04,  2.9019e-03],
        ...,
        [ 9.8359e-05,  5.7890e-04, -6.1094e-04,  ...,  1.0461e-03,
          0.0000e+00, -8.3671e-04],
        [ 1.5577e-02,  1.2774e-02, -7.1373e-04,  ...,  2.6399e-02,
         -7.5960e-04,  9.1773e-03],
        [ 9.8359e-05,  5.7890e-04, -6.1094e-04,  ...,  1.0461e-03,
          0.0000e+00, -8.3671e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3828.4229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.3208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3268, device='cuda:0')



h[100].sum tensor(111.1204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.8427, device='cuda:0')



h[200].sum tensor(57.3025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0064, 0.0071, 0.0000,  ..., 0.0141, 0.0000, 0.0030],
        [0.0204, 0.0181, 0.0000,  ..., 0.0370, 0.0000, 0.0112],
        ...,
        [0.0166, 0.0152, 0.0000,  ..., 0.0310, 0.0000, 0.0096],
        [0.0137, 0.0129, 0.0000,  ..., 0.0261, 0.0000, 0.0077],
        [0.0594, 0.0489, 0.0000,  ..., 0.1010, 0.0000, 0.0346]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74478.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0170, 0.0949, 0.0538,  ..., 0.0000, 0.0299, 0.0000],
        [0.0295, 0.1021, 0.0858,  ..., 0.0018, 0.0407, 0.0000],
        [0.0509, 0.1135, 0.1407,  ..., 0.0088, 0.0584, 0.0000],
        ...,
        [0.0303, 0.1041, 0.0887,  ..., 0.0046, 0.0405, 0.0000],
        [0.0390, 0.1088, 0.1113,  ..., 0.0059, 0.0475, 0.0000],
        [0.0672, 0.1233, 0.1842,  ..., 0.0193, 0.0700, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586510.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3328.5518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(398.0242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5676.2749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1148.9750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-729.7759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1893],
        [-0.1060],
        [-0.0065],
        ...,
        [-2.9405],
        [-2.2477],
        [-1.8722]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256068.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0336],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369959.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 120.0 event: 600 loss: tensor(500.5688, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0337],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369962.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.3599e-05,  5.6946e-04, -6.1094e-04,  ...,  1.0624e-03,
          0.0000e+00, -8.3342e-04],
        [ 9.3599e-05,  5.6946e-04, -6.1094e-04,  ...,  1.0624e-03,
          0.0000e+00, -8.3342e-04],
        [ 9.3599e-05,  5.6946e-04, -6.1094e-04,  ...,  1.0624e-03,
          0.0000e+00, -8.3342e-04],
        ...,
        [ 9.3599e-05,  5.6946e-04, -6.1094e-04,  ...,  1.0624e-03,
          0.0000e+00, -8.3342e-04],
        [ 9.3599e-05,  5.6946e-04, -6.1094e-04,  ...,  1.0624e-03,
          0.0000e+00, -8.3342e-04],
        [ 9.3599e-05,  5.6946e-04, -6.1094e-04,  ...,  1.0624e-03,
          0.0000e+00, -8.3342e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3498.5791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9727, device='cuda:0')



h[100].sum tensor(109.8637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.8047, device='cuda:0')



h[200].sum tensor(52.1233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69700.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9592e-03, 8.7268e-02, 1.4754e-02,  ..., 0.0000e+00, 1.7404e-02,
         0.0000e+00],
        [1.6457e-04, 8.6756e-02, 9.2466e-03,  ..., 0.0000e+00, 1.5672e-02,
         0.0000e+00],
        [7.1602e-05, 8.6904e-02, 9.0435e-03,  ..., 0.0000e+00, 1.5605e-02,
         0.0000e+00],
        ...,
        [8.4390e-05, 8.8464e-02, 9.3814e-03,  ..., 0.0000e+00, 1.6016e-02,
         0.0000e+00],
        [8.4612e-05, 8.8477e-02, 9.3830e-03,  ..., 0.0000e+00, 1.6019e-02,
         0.0000e+00],
        [8.3458e-05, 8.8392e-02, 9.3690e-03,  ..., 0.0000e+00, 1.6000e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570298.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2925.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.3423, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5845.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1077.2281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-673.2426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6061],
        [-4.2181],
        [-4.5661],
        ...,
        [-4.8429],
        [-4.8357],
        [-4.8330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311253.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0336],
        [1.0337],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369962.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0335],
        [1.0338],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369966.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.4027e-05,  5.6811e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.2883e-04],
        [ 9.4027e-05,  5.6811e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.2883e-04],
        [ 9.4027e-05,  5.6811e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.2883e-04],
        ...,
        [ 9.4027e-05,  5.6811e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.2883e-04],
        [ 9.4027e-05,  5.6811e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.2883e-04],
        [ 9.4027e-05,  5.6811e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.2883e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3280.6729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.5560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3301, device='cuda:0')



h[100].sum tensor(109.1020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8937, device='cuda:0')



h[200].sum tensor(48.8338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0070, 0.0000,  ..., 0.0142, 0.0000, 0.0030],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0124, 0.0118, 0.0000,  ..., 0.0241, 0.0000, 0.0069],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65206.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1742e-02, 9.3104e-02, 4.1462e-02,  ..., 0.0000e+00, 2.6425e-02,
         0.0000e+00],
        [9.5696e-03, 9.2572e-02, 3.6011e-02,  ..., 0.0000e+00, 2.4287e-02,
         0.0000e+00],
        [2.2460e-02, 9.9779e-02, 6.9570e-02,  ..., 1.8404e-03, 3.4544e-02,
         0.0000e+00],
        ...,
        [1.9406e-05, 8.8717e-02, 9.5774e-03,  ..., 0.0000e+00, 1.6152e-02,
         0.0000e+00],
        [1.9582e-05, 8.8730e-02, 9.5789e-03,  ..., 0.0000e+00, 1.6154e-02,
         0.0000e+00],
        [1.8527e-05, 8.8644e-02, 9.5647e-03,  ..., 0.0000e+00, 1.6135e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550529.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2587.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.7211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5853.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1017.6378, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.7585, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5661],
        [-3.5469],
        [-3.2297],
        ...,
        [-4.8465],
        [-4.8388],
        [-4.8356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304924., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0335],
        [1.0338],
        ...,
        [0.9984],
        [0.9973],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369966.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0335],
        [1.0338],
        ...,
        [0.9983],
        [0.9973],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369970.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9344e-05,  5.7022e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.2519e-04],
        [ 9.9344e-05,  5.7022e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.2519e-04],
        [ 9.9344e-05,  5.7022e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.2519e-04],
        ...,
        [ 9.9344e-05,  5.7022e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.2519e-04],
        [ 9.9344e-05,  5.7022e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.2519e-04],
        [ 9.9344e-05,  5.7022e-04, -6.1094e-04,  ...,  1.0814e-03,
          0.0000e+00, -8.2519e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3785.6035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.4443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.7286, device='cuda:0')



h[100].sum tensor(111.3618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.0543, device='cuda:0')



h[200].sum tensor(56.5826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71802.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.0536e-04, 8.6865e-02, 1.1495e-02,  ..., 0.0000e+00, 1.6534e-02,
         0.0000e+00],
        [1.0371e-03, 8.7787e-02, 1.2933e-02,  ..., 0.0000e+00, 1.7332e-02,
         0.0000e+00],
        [5.0479e-03, 9.0172e-02, 2.3317e-02,  ..., 0.0000e+00, 2.1200e-02,
         0.0000e+00],
        ...,
        [3.1720e-05, 8.8779e-02, 9.7015e-03,  ..., 0.0000e+00, 1.6327e-02,
         0.0000e+00],
        [3.1883e-05, 8.8790e-02, 9.7028e-03,  ..., 0.0000e+00, 1.6329e-02,
         0.0000e+00],
        [3.0801e-05, 8.8704e-02, 9.6884e-03,  ..., 0.0000e+00, 1.6309e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(569507.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2989.3306, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(376.9308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5397.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1117.7031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-700.4752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7123],
        [-3.3103],
        [-2.6297],
        ...,
        [-4.8485],
        [-4.8417],
        [-4.8396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256809.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0335],
        [1.0338],
        ...,
        [0.9983],
        [0.9973],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369970.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0334],
        [1.0339],
        ...,
        [0.9983],
        [0.9973],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369974.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.3919e-05,  5.6027e-04, -6.1094e-04,  ...,  1.0911e-03,
          0.0000e+00, -8.2019e-04],
        [ 9.3919e-05,  5.6027e-04, -6.1094e-04,  ...,  1.0911e-03,
          0.0000e+00, -8.2019e-04],
        [ 9.3919e-05,  5.6027e-04, -6.1094e-04,  ...,  1.0911e-03,
          0.0000e+00, -8.2019e-04],
        ...,
        [ 9.3919e-05,  5.6027e-04, -6.1094e-04,  ...,  1.0911e-03,
          0.0000e+00, -8.2019e-04],
        [ 9.3919e-05,  5.6027e-04, -6.1094e-04,  ...,  1.0911e-03,
          0.0000e+00, -8.2019e-04],
        [ 9.3919e-05,  5.6027e-04, -6.1094e-04,  ...,  1.0911e-03,
          0.0000e+00, -8.2019e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3431.8535, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3224, device='cuda:0')



h[100].sum tensor(109.8518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8605, device='cuda:0')



h[200].sum tensor(50.9995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0522, 0.0431, 0.0000,  ..., 0.0894, 0.0000, 0.0310],
        [0.0088, 0.0090, 0.0000,  ..., 0.0184, 0.0000, 0.0046],
        [0.0004, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66023.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1084, 0.1456, 0.2959,  ..., 0.0440, 0.1035, 0.0000],
        [0.0662, 0.1240, 0.1853,  ..., 0.0205, 0.0700, 0.0000],
        [0.0472, 0.1138, 0.1356,  ..., 0.0123, 0.0547, 0.0000],
        ...,
        [0.0000, 0.0893, 0.0099,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0893, 0.0099,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0892, 0.0099,  ..., 0.0000, 0.0162, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548303.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2519.8696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.0877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5641.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1034.0656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-634.3546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1812],
        [ 0.1251],
        [ 0.0299],
        ...,
        [-4.8700],
        [-4.8626],
        [-4.8599]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301004.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0334],
        [1.0339],
        ...,
        [0.9983],
        [0.9973],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369974.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0333],
        [1.0339],
        ...,
        [0.9983],
        [0.9973],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369977.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.7057e-05,  5.4897e-04, -6.1094e-04,  ...,  1.0948e-03,
          0.0000e+00, -8.1404e-04],
        [ 8.7057e-05,  5.4897e-04, -6.1094e-04,  ...,  1.0948e-03,
          0.0000e+00, -8.1404e-04],
        [ 8.7057e-05,  5.4897e-04, -6.1094e-04,  ...,  1.0948e-03,
          0.0000e+00, -8.1404e-04],
        ...,
        [ 8.7057e-05,  5.4897e-04, -6.1094e-04,  ...,  1.0948e-03,
          0.0000e+00, -8.1404e-04],
        [ 8.7057e-05,  5.4897e-04, -6.1094e-04,  ...,  1.0948e-03,
          0.0000e+00, -8.1404e-04],
        [ 8.7057e-05,  5.4897e-04, -6.1094e-04,  ...,  1.0948e-03,
          0.0000e+00, -8.1404e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3452.0610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4500, device='cuda:0')



h[100].sum tensor(110.1697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2421, device='cuda:0')



h[200].sum tensor(50.9067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0023, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0023, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0023, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67589.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0044, 0.0901, 0.0225,  ..., 0.0000, 0.0199, 0.0000],
        [0.0006, 0.0885, 0.0126,  ..., 0.0000, 0.0166, 0.0000],
        [0.0000, 0.0880, 0.0095,  ..., 0.0000, 0.0157, 0.0000],
        ...,
        [0.0000, 0.0896, 0.0099,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0896, 0.0099,  ..., 0.0000, 0.0161, 0.0000],
        [0.0000, 0.0895, 0.0099,  ..., 0.0000, 0.0161, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558813.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2653.1870, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.6406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5431.8213, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1056.6686, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-649.5134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4138],
        [-4.1010],
        [-4.5348],
        ...,
        [-4.8929],
        [-4.8860],
        [-4.8834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299760.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0333],
        [1.0339],
        ...,
        [0.9983],
        [0.9973],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369977.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0333],
        [1.0339],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369981.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.0602e-05,  5.4314e-04, -6.1094e-04,  ...,  1.0885e-03,
          0.0000e+00, -8.1533e-04],
        [ 9.0602e-05,  5.4314e-04, -6.1094e-04,  ...,  1.0885e-03,
          0.0000e+00, -8.1533e-04],
        [ 9.0602e-05,  5.4314e-04, -6.1094e-04,  ...,  1.0885e-03,
          0.0000e+00, -8.1533e-04],
        ...,
        [ 9.0602e-05,  5.4314e-04, -6.1094e-04,  ...,  1.0885e-03,
          0.0000e+00, -8.1533e-04],
        [ 9.0602e-05,  5.4314e-04, -6.1094e-04,  ...,  1.0885e-03,
          0.0000e+00, -8.1533e-04],
        [ 9.0602e-05,  5.4314e-04, -6.1094e-04,  ...,  1.0885e-03,
          0.0000e+00, -8.1533e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3508.9370, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7897, device='cuda:0')



h[100].sum tensor(110.7003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.2575, device='cuda:0')



h[200].sum tensor(51.4520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0022, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0022, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0004, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0023, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0023, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0004, 0.0023, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67054.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0145, 0.0961, 0.0495,  ..., 0.0000, 0.0286, 0.0000],
        [0.0106, 0.0943, 0.0384,  ..., 0.0000, 0.0252, 0.0000],
        [0.0061, 0.0919, 0.0267,  ..., 0.0000, 0.0215, 0.0000],
        ...,
        [0.0000, 0.0896, 0.0095,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0896, 0.0095,  ..., 0.0000, 0.0160, 0.0000],
        [0.0000, 0.0895, 0.0095,  ..., 0.0000, 0.0160, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551862.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2573.0579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.1891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5539.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1049.7495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-645.1893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3239],
        [-1.4726],
        [-1.6810],
        ...,
        [-4.9046],
        [-4.8992],
        [-4.8992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293589.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0333],
        [1.0339],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369981.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0339],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369985.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3471.4255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5156, device='cuda:0')



h[100].sum tensor(110.6200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.4380, device='cuda:0')



h[200].sum tensor(50.9162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1729, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66030.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0867, 0.0084,  ..., 0.0000, 0.0156, 0.0000],
        [0.0001, 0.0873, 0.0085,  ..., 0.0000, 0.0158, 0.0000],
        [0.0002, 0.0875, 0.0086,  ..., 0.0000, 0.0158, 0.0000],
        ...,
        [0.0002, 0.0891, 0.0089,  ..., 0.0000, 0.0163, 0.0000],
        [0.0002, 0.0891, 0.0089,  ..., 0.0000, 0.0163, 0.0000],
        [0.0002, 0.0890, 0.0089,  ..., 0.0000, 0.0162, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550427.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2572.8665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.9025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6000.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1026.1552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-629.5438, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2010],
        [-4.2599],
        [-4.2676],
        ...,
        [-4.9188],
        [-4.9114],
        [-4.9087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-338633.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0339],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369985.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0339],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369989.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3473.3647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4955, device='cuda:0')



h[100].sum tensor(110.6596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.3781, device='cuda:0')



h[200].sum tensor(51.0698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67057.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6679e-03, 8.7494e-02, 1.4130e-02,  ..., 0.0000e+00, 1.8020e-02,
         0.0000e+00],
        [1.0898e-02, 9.2559e-02, 3.5510e-02,  ..., 0.0000e+00, 2.4893e-02,
         0.0000e+00],
        [2.0408e-02, 9.8017e-02, 6.0685e-02,  ..., 7.0863e-05, 3.3002e-02,
         0.0000e+00],
        ...,
        [4.7534e-04, 8.8530e-02, 8.2840e-03,  ..., 0.0000e+00, 1.6615e-02,
         0.0000e+00],
        [4.7558e-04, 8.8539e-02, 8.2847e-03,  ..., 0.0000e+00, 1.6617e-02,
         0.0000e+00],
        [4.7388e-04, 8.8457e-02, 8.2724e-03,  ..., 0.0000e+00, 1.6598e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557930.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2883.9575, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(337.6488, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5964.3540, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1041.3348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-642.2491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6780],
        [-1.1523],
        [-0.6354],
        ...,
        [-4.9165],
        [-4.9091],
        [-4.9065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290106.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0339],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369989.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369993.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3479.5400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.6101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6027, device='cuda:0')



h[100].sum tensor(111.1349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6985, device='cuda:0')



h[200].sum tensor(51.0026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66450.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0870, 0.0112,  ..., 0.0000, 0.0178, 0.0000],
        [0.0019, 0.0876, 0.0110,  ..., 0.0000, 0.0178, 0.0000],
        [0.0030, 0.0885, 0.0143,  ..., 0.0000, 0.0188, 0.0000],
        ...,
        [0.0041, 0.0907, 0.0175,  ..., 0.0000, 0.0197, 0.0000],
        [0.0115, 0.0947, 0.0366,  ..., 0.0000, 0.0258, 0.0000],
        [0.0150, 0.0967, 0.0461,  ..., 0.0000, 0.0289, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552147.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2784.2612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.9543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6151.6572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1028.7003, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-634.1284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7539],
        [-3.5432],
        [-3.1403],
        ...,
        [-4.0150],
        [-3.4132],
        [-3.0162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312730.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369993.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369998.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0147,  0.0120, -0.0007,  ...,  0.0249, -0.0007,  0.0086],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4646.8364, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(40.8154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.4523, device='cuda:0')



h[100].sum tensor(116.7627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.1662, device='cuda:0')



h[200].sum tensor(68.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0581, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0279, 0.0239, 0.0000,  ..., 0.0492, 0.0000, 0.0160],
        [0.0279, 0.0239, 0.0000,  ..., 0.0492, 0.0000, 0.0160],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85118.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.0039e-02, 1.0340e-01, 8.5694e-02,  ..., 1.1143e-04, 4.0964e-02,
         0.0000e+00],
        [5.8829e-02, 1.1948e-01, 1.6106e-01,  ..., 1.5764e-02, 6.3676e-02,
         0.0000e+00],
        [7.4417e-02, 1.2804e-01, 2.0185e-01,  ..., 2.4919e-02, 7.6012e-02,
         0.0000e+00],
        ...,
        [4.5768e-04, 8.9116e-02, 7.8327e-03,  ..., 0.0000e+00, 1.6548e-02,
         0.0000e+00],
        [4.5789e-04, 8.9124e-02, 7.8332e-03,  ..., 0.0000e+00, 1.6549e-02,
         0.0000e+00],
        [4.5627e-04, 8.9043e-02, 7.8217e-03,  ..., 0.0000e+00, 1.6531e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629026.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3931.1108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(494.4053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5622.5576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1289.9562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-838.0978, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.8947e-03],
        [ 7.6366e-02],
        [ 1.3160e-01],
        ...,
        [-4.9314e+00],
        [-4.9237e+00],
        [-4.9208e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287655.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0332],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369998.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 130.0 event: 650 loss: tensor(507.7948, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370002.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0044, -0.0006,  ...,  0.0090, -0.0002,  0.0023],
        [ 0.0214,  0.0173, -0.0008,  ...,  0.0359, -0.0010,  0.0130],
        [ 0.0332,  0.0266, -0.0008,  ...,  0.0552, -0.0015,  0.0206],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4029.1270, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.2383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1768, device='cuda:0')



h[100].sum tensor(114.7258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.3837, device='cuda:0')



h[200].sum tensor(58.5117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5812, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0722, 0.0588, 0.0000,  ..., 0.1218, 0.0000, 0.0431],
        [0.0814, 0.0660, 0.0000,  ..., 0.1368, 0.0000, 0.0490],
        [0.1078, 0.0868, 0.0000,  ..., 0.1801, 0.0000, 0.0662],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77406.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8676e-01, 1.8679e-01, 4.9854e-01,  ..., 9.3544e-02, 1.6555e-01,
         0.0000e+00],
        [2.0367e-01, 1.9577e-01, 5.4345e-01,  ..., 1.0431e-01, 1.7918e-01,
         0.0000e+00],
        [2.2928e-01, 2.0892e-01, 6.1114e-01,  ..., 1.2106e-01, 1.9922e-01,
         0.0000e+00],
        ...,
        [4.0917e-04, 8.9419e-02, 7.6499e-03,  ..., 0.0000e+00, 1.6493e-02,
         0.0000e+00],
        [4.0937e-04, 8.9427e-02, 7.6504e-03,  ..., 0.0000e+00, 1.6495e-02,
         0.0000e+00],
        [4.0783e-04, 8.9346e-02, 7.6391e-03,  ..., 0.0000e+00, 1.6476e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603395.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3490.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(426.3704, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5721.9360, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1182.9181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-756.0223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1483],
        [ 0.1504],
        [ 0.1433],
        ...,
        [-4.9510],
        [-4.9435],
        [-4.9409]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304796.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370002.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370006.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0124,  0.0102, -0.0007,  ...,  0.0210, -0.0006,  0.0071],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0336,  0.0269, -0.0008,  ...,  0.0558, -0.0015,  0.0209],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4567.8467, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(39.7156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6867, device='cuda:0')



h[100].sum tensor(117.2116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(52.8774, device='cuda:0')



h[200].sum tensor(66.7114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9727, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0108, 0.0104, 0.0000,  ..., 0.0211, 0.0000, 0.0058],
        [0.0766, 0.0622, 0.0000,  ..., 0.1288, 0.0000, 0.0459],
        [0.0715, 0.0581, 0.0000,  ..., 0.1204, 0.0000, 0.0434],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81908.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0546, 0.1166, 0.1503,  ..., 0.0165, 0.0605, 0.0000],
        [0.1258, 0.1548, 0.3379,  ..., 0.0561, 0.1174, 0.0000],
        [0.1590, 0.1721, 0.4254,  ..., 0.0769, 0.1437, 0.0000],
        ...,
        [0.0005, 0.0894, 0.0076,  ..., 0.0000, 0.0167, 0.0000],
        [0.0042, 0.0917, 0.0178,  ..., 0.0000, 0.0201, 0.0000],
        [0.0099, 0.0948, 0.0327,  ..., 0.0000, 0.0250, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605023.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3533.0098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(465.7244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5882.8340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1242.9283, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-804.0591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6868],
        [-0.0580],
        [ 0.1857],
        ...,
        [-4.6664],
        [-4.2369],
        [-3.6020]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312572.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0340],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370006.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0341],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370010.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0120,  0.0099, -0.0007,  ...,  0.0205, -0.0005,  0.0069],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3184.9653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2854, device='cuda:0')



h[100].sum tensor(111.3737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.7705, device='cuda:0')



h[200].sum tensor(46.1624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0174, 0.0156, 0.0000,  ..., 0.0318, 0.0000, 0.0092],
        [0.0162, 0.0146, 0.0000,  ..., 0.0298, 0.0000, 0.0076],
        ...,
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61675.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0928, 0.0348,  ..., 0.0000, 0.0255, 0.0000],
        [0.0335, 0.1060, 0.0934,  ..., 0.0030, 0.0449, 0.0000],
        [0.0422, 0.1111, 0.1162,  ..., 0.0033, 0.0531, 0.0000],
        ...,
        [0.0007, 0.0893, 0.0077,  ..., 0.0000, 0.0170, 0.0000],
        [0.0007, 0.0893, 0.0077,  ..., 0.0000, 0.0170, 0.0000],
        [0.0007, 0.0892, 0.0077,  ..., 0.0000, 0.0170, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(536073.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2531.7417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.1056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6336.9507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(960.7421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-585.6204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4613],
        [-1.5138],
        [-0.7706],
        ...,
        [-4.9488],
        [-4.9415],
        [-4.9391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327309.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0341],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370010.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0341],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370014.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0216,  0.0174, -0.0008,  ...,  0.0361, -0.0010,  0.0130],
        [ 0.0217,  0.0176, -0.0008,  ...,  0.0363, -0.0010,  0.0131],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3961.1707, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.7392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6020, device='cuda:0')



h[100].sum tensor(114.6013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.6655, device='cuda:0')



h[200].sum tensor(58.2885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0910, 0.0735, 0.0000,  ..., 0.1521, 0.0000, 0.0551],
        [0.0662, 0.0540, 0.0000,  ..., 0.1114, 0.0000, 0.0399],
        [0.0609, 0.0498, 0.0000,  ..., 0.1027, 0.0000, 0.0356],
        ...,
        [0.0008, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75153.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1624, 0.1715, 0.4303,  ..., 0.0766, 0.1459, 0.0000],
        [0.1641, 0.1733, 0.4348,  ..., 0.0768, 0.1478, 0.0000],
        [0.1560, 0.1697, 0.4130,  ..., 0.0704, 0.1420, 0.0000],
        ...,
        [0.0009, 0.0891, 0.0080,  ..., 0.0000, 0.0175, 0.0000],
        [0.0009, 0.0891, 0.0080,  ..., 0.0000, 0.0175, 0.0000],
        [0.0009, 0.0890, 0.0080,  ..., 0.0000, 0.0175, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591964.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3496.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(405.7523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6134.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1151.2538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-733.1365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2054],
        [ 0.2327],
        [ 0.2426],
        ...,
        [-4.9389],
        [-4.9316],
        [-4.9294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284929.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0331],
        [1.0341],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370014.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0331],
        [1.0341],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370018.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0042, -0.0006,  ...,  0.0086, -0.0002,  0.0022],
        [ 0.0065,  0.0056, -0.0007,  ...,  0.0114, -0.0003,  0.0033],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3364.3354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3285, device='cuda:0')



h[100].sum tensor(112.4557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8891, device='cuda:0')



h[200].sum tensor(49.1183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0433, 0.0359, 0.0000,  ..., 0.0739, 0.0000, 0.0242],
        [0.0148, 0.0134, 0.0000,  ..., 0.0272, 0.0000, 0.0074],
        [0.0073, 0.0075, 0.0000,  ..., 0.0149, 0.0000, 0.0034],
        ...,
        [0.0008, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64634.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0890, 0.1352, 0.2376,  ..., 0.0288, 0.0890, 0.0000],
        [0.0491, 0.1141, 0.1335,  ..., 0.0102, 0.0572, 0.0000],
        [0.0230, 0.1000, 0.0654,  ..., 0.0011, 0.0358, 0.0000],
        ...,
        [0.0009, 0.0894, 0.0081,  ..., 0.0000, 0.0175, 0.0000],
        [0.0009, 0.0894, 0.0081,  ..., 0.0000, 0.0175, 0.0000],
        [0.0009, 0.0893, 0.0081,  ..., 0.0000, 0.0175, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548416., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2803.9028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.2234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6401.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1004.8966, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-620.2537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0133],
        [-0.4628],
        [-1.2709],
        ...,
        [-4.9478],
        [-4.9406],
        [-4.9384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302652.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0331],
        [1.0341],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370018.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0331],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370023.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3489.6802, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.5764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2825, device='cuda:0')



h[100].sum tensor(113.4989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.7411, device='cuda:0')



h[200].sum tensor(50.5928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0089, 0.0088, 0.0000,  ..., 0.0177, 0.0000, 0.0045],
        ...,
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67105.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0122, 0.0941, 0.0383,  ..., 0.0000, 0.0265, 0.0000],
        [0.0203, 0.0992, 0.0595,  ..., 0.0001, 0.0333, 0.0000],
        [0.0354, 0.1079, 0.0994,  ..., 0.0043, 0.0460, 0.0000],
        ...,
        [0.0007, 0.0899, 0.0082,  ..., 0.0000, 0.0174, 0.0000],
        [0.0007, 0.0899, 0.0082,  ..., 0.0000, 0.0174, 0.0000],
        [0.0007, 0.0898, 0.0082,  ..., 0.0000, 0.0174, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562771.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2950.9900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.7176, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6220.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1038.3082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-646.5999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9990],
        [-0.6937],
        [-0.3477],
        ...,
        [-4.9628],
        [-4.9556],
        [-4.9534]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312107.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0331],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370023.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370027.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3485.4976, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9861, device='cuda:0')



h[100].sum tensor(114.0265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.8552, device='cuda:0')



h[200].sum tensor(50.1086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66147.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0896, 0.0153,  ..., 0.0000, 0.0195, 0.0000],
        [0.0015, 0.0891, 0.0104,  ..., 0.0000, 0.0179, 0.0000],
        [0.0007, 0.0888, 0.0084,  ..., 0.0000, 0.0171, 0.0000],
        ...,
        [0.0006, 0.0904, 0.0084,  ..., 0.0000, 0.0174, 0.0000],
        [0.0006, 0.0904, 0.0084,  ..., 0.0000, 0.0174, 0.0000],
        [0.0006, 0.0903, 0.0083,  ..., 0.0000, 0.0174, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552457.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2742.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.2136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6152.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1025.7074, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-637.2858, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2109],
        [-2.9657],
        [-3.5859],
        ...,
        [-4.9470],
        [-4.9539],
        [-4.9585]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321734.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370027.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370027.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0089, -0.0007,  ...,  0.0184, -0.0005,  0.0061],
        [ 0.0121,  0.0100, -0.0007,  ...,  0.0206, -0.0005,  0.0070],
        [ 0.0124,  0.0102, -0.0007,  ...,  0.0211, -0.0005,  0.0071],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3536.6714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.6150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4263, device='cuda:0')



h[100].sum tensor(114.2422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1710, device='cuda:0')



h[200].sum tensor(50.8777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0404, 0.0336, 0.0000,  ..., 0.0694, 0.0000, 0.0226],
        [0.0472, 0.0390, 0.0000,  ..., 0.0806, 0.0000, 0.0270],
        [0.0595, 0.0487, 0.0000,  ..., 0.1008, 0.0000, 0.0349],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67349.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1406, 0.1659, 0.3789,  ..., 0.0614, 0.1299, 0.0000],
        [0.1288, 0.1603, 0.3476,  ..., 0.0534, 0.1211, 0.0000],
        [0.1143, 0.1528, 0.3091,  ..., 0.0446, 0.1096, 0.0000],
        ...,
        [0.0006, 0.0904, 0.0084,  ..., 0.0000, 0.0174, 0.0000],
        [0.0006, 0.0904, 0.0084,  ..., 0.0000, 0.0174, 0.0000],
        [0.0006, 0.0903, 0.0083,  ..., 0.0000, 0.0174, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560050.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2859.4038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.2861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6103.6460, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1043.0740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-650.9062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1326],
        [ 0.1607],
        [ 0.1404],
        ...,
        [-4.9780],
        [-4.9704],
        [-4.9675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313504.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370027.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0329],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370031.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0114,  0.0094, -0.0007,  ...,  0.0194, -0.0005,  0.0065],
        [ 0.0049,  0.0043, -0.0006,  ...,  0.0088, -0.0002,  0.0023],
        [ 0.0066,  0.0057, -0.0007,  ...,  0.0117, -0.0003,  0.0034],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3399.9121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3857, device='cuda:0')



h[100].sum tensor(113.9286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0600, device='cuda:0')



h[200].sum tensor(48.8022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0197, 0.0173, 0.0000,  ..., 0.0355, 0.0000, 0.0100],
        [0.0435, 0.0362, 0.0000,  ..., 0.0747, 0.0000, 0.0246],
        [0.0198, 0.0175, 0.0000,  ..., 0.0358, 0.0000, 0.0109],
        ...,
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63922.9492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0523, 0.1181, 0.1456,  ..., 0.0090, 0.0608, 0.0000],
        [0.0691, 0.1282, 0.1902,  ..., 0.0171, 0.0742, 0.0000],
        [0.0482, 0.1164, 0.1351,  ..., 0.0083, 0.0573, 0.0000],
        ...,
        [0.0006, 0.0905, 0.0085,  ..., 0.0000, 0.0175, 0.0000],
        [0.0006, 0.0905, 0.0085,  ..., 0.0000, 0.0175, 0.0000],
        [0.0006, 0.0904, 0.0085,  ..., 0.0000, 0.0175, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544137.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2534.0381, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(308.7360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6455.9399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(990.4421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-611.7897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4090],
        [-0.2959],
        [-0.8046],
        ...,
        [-4.9843],
        [-4.9770],
        [-4.9750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-357333.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0329],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370031.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370035.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0046, -0.0006,  ...,  0.0095, -0.0002,  0.0026],
        [ 0.0118,  0.0098, -0.0007,  ...,  0.0202, -0.0005,  0.0068],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4316.3945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.4479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.6799, device='cuda:0')



h[100].sum tensor(117.6810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.8776, device='cuda:0')



h[200].sum tensor(63.0481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7489, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0423, 0.0352, 0.0000,  ..., 0.0725, 0.0000, 0.0238],
        [0.0198, 0.0174, 0.0000,  ..., 0.0356, 0.0000, 0.0100],
        [0.0171, 0.0153, 0.0000,  ..., 0.0312, 0.0000, 0.0091],
        ...,
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84659.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0690, 0.1271, 0.1891,  ..., 0.0166, 0.0742, 0.0000],
        [0.0522, 0.1182, 0.1449,  ..., 0.0087, 0.0610, 0.0000],
        [0.0372, 0.1097, 0.1052,  ..., 0.0050, 0.0482, 0.0000],
        ...,
        [0.0008, 0.0903, 0.0087,  ..., 0.0000, 0.0179, 0.0000],
        [0.0008, 0.0903, 0.0087,  ..., 0.0000, 0.0179, 0.0000],
        [0.0007, 0.0903, 0.0087,  ..., 0.0000, 0.0179, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642635.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4160.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(486.9268, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5750.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1283.9097, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-836.0443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0766],
        [-0.3819],
        [-1.1135],
        ...,
        [-4.9714],
        [-4.9635],
        [-4.9604]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284657.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370035.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 140.0 event: 700 loss: tensor(415.6805, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0330],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370039.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0240,  0.0193, -0.0008,  ...,  0.0400, -0.0010,  0.0146],
        [ 0.0478,  0.0381, -0.0009,  ...,  0.0790, -0.0021,  0.0301],
        [ 0.0241,  0.0195, -0.0008,  ...,  0.0403, -0.0010,  0.0147],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3361.8765, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1503, device='cuda:0')



h[100].sum tensor(113.4323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.3562, device='cuda:0')



h[200].sum tensor(49.4331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0206, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1202, 0.0965, 0.0000,  ..., 0.1999, 0.0000, 0.0742],
        [0.1127, 0.0906, 0.0000,  ..., 0.1877, 0.0000, 0.0693],
        [0.1225, 0.0984, 0.0000,  ..., 0.2037, 0.0000, 0.0756],
        ...,
        [0.0008, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63595.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2065, 0.1950, 0.5522,  ..., 0.1056, 0.1817, 0.0000],
        [0.2211, 0.2033, 0.5906,  ..., 0.1144, 0.1931, 0.0000],
        [0.2206, 0.2035, 0.5893,  ..., 0.1138, 0.1929, 0.0000],
        ...,
        [0.0010, 0.0899, 0.0088,  ..., 0.0000, 0.0183, 0.0000],
        [0.0010, 0.0899, 0.0088,  ..., 0.0000, 0.0183, 0.0000],
        [0.0010, 0.0898, 0.0088,  ..., 0.0000, 0.0183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542536., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2673.2407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(306.0106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6547.0347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(989.3051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.0066, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1949],
        [ 0.1794],
        [ 0.1851],
        ...,
        [-4.9585],
        [-4.9516],
        [-4.9497]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313578.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0330],
        [1.0330],
        [1.0342],
        ...,
        [0.9983],
        [0.9972],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370039.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0331],
        [1.0330],
        [1.0343],
        ...,
        [0.9982],
        [0.9972],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370042.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0061,  0.0052, -0.0006,  ...,  0.0106, -0.0003,  0.0030],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3476.1577, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.5413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9164, device='cuda:0')



h[100].sum tensor(113.9193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.6468, device='cuda:0')



h[200].sum tensor(51.5257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0322, 0.0271, 0.0000,  ..., 0.0556, 0.0000, 0.0171],
        [0.0058, 0.0064, 0.0000,  ..., 0.0124, 0.0000, 0.0024],
        [0.0069, 0.0072, 0.0000,  ..., 0.0142, 0.0000, 0.0031],
        ...,
        [0.0009, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65403.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0477, 0.1135, 0.1301,  ..., 0.0048, 0.0575, 0.0000],
        [0.0240, 0.1008, 0.0683,  ..., 0.0000, 0.0377, 0.0000],
        [0.0213, 0.0994, 0.0615,  ..., 0.0000, 0.0353, 0.0000],
        ...,
        [0.0012, 0.0897, 0.0089,  ..., 0.0000, 0.0186, 0.0000],
        [0.0012, 0.0897, 0.0089,  ..., 0.0000, 0.0186, 0.0000],
        [0.0012, 0.0897, 0.0089,  ..., 0.0000, 0.0186, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547790.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2784.8062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(322.8674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6592.8936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1012.7061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.9770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6876],
        [-2.2497],
        [-2.4931],
        ...,
        [-4.9572],
        [-4.9503],
        [-4.9485]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308581.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0331],
        [1.0330],
        [1.0343],
        ...,
        [0.9982],
        [0.9972],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370042.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0331],
        [1.0330],
        [1.0343],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370045.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0153,  0.0125, -0.0007,  ...,  0.0257, -0.0006,  0.0090],
        [ 0.0227,  0.0183, -0.0008,  ...,  0.0380, -0.0010,  0.0138],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3748.3210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.3319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7776, device='cuda:0')



h[100].sum tensor(115.2964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2109, device='cuda:0')



h[200].sum tensor(55.7240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0302, 0.0255, 0.0000,  ..., 0.0523, 0.0000, 0.0165],
        [0.0431, 0.0357, 0.0000,  ..., 0.0734, 0.0000, 0.0249],
        [0.0654, 0.0533, 0.0000,  ..., 0.1099, 0.0000, 0.0385],
        ...,
        [0.0009, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70214.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1212, 0.1519, 0.3233,  ..., 0.0483, 0.1151, 0.0000],
        [0.1449, 0.1649, 0.3854,  ..., 0.0628, 0.1336, 0.0000],
        [0.1728, 0.1800, 0.4585,  ..., 0.0795, 0.1554, 0.0000],
        ...,
        [0.0013, 0.0896, 0.0089,  ..., 0.0000, 0.0187, 0.0000],
        [0.0013, 0.0897, 0.0089,  ..., 0.0000, 0.0187, 0.0000],
        [0.0013, 0.0896, 0.0089,  ..., 0.0000, 0.0187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(567319.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3142.4856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.3094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6378.5947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1084.5880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-680.8066, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0092],
        [-0.0063],
        [-0.0057],
        ...,
        [-4.9625],
        [-4.9550],
        [-4.9518]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278824.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0331],
        [1.0330],
        [1.0343],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370045.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0331],
        [1.0331],
        [1.0344],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370049.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0065,  0.0055, -0.0007,  ...,  0.0113, -0.0003,  0.0033],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3488.9539, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.3989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0663, device='cuda:0')



h[100].sum tensor(114.8533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.0950, device='cuda:0')



h[200].sum tensor(51.5665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0073, 0.0075, 0.0000,  ..., 0.0150, 0.0000, 0.0034],
        [0.0362, 0.0303, 0.0000,  ..., 0.0624, 0.0000, 0.0213],
        ...,
        [0.0008, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66281.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8763e-02, 9.7566e-02, 5.5299e-02,  ..., 1.8204e-04, 3.2635e-02,
         0.0000e+00],
        [4.2908e-02, 1.1141e-01, 1.1877e-01,  ..., 8.5965e-03, 5.2637e-02,
         0.0000e+00],
        [8.8285e-02, 1.3578e-01, 2.3829e-01,  ..., 2.9567e-02, 8.9068e-02,
         0.0000e+00],
        ...,
        [1.0676e-03, 9.0005e-02, 9.0910e-03,  ..., 0.0000e+00, 1.8638e-02,
         0.0000e+00],
        [1.0677e-03, 9.0009e-02, 9.0914e-03,  ..., 0.0000e+00, 1.8638e-02,
         0.0000e+00],
        [1.0658e-03, 8.9936e-02, 9.0814e-03,  ..., 0.0000e+00, 1.8621e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555466.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2866.6667, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.6579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6441.0225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1028.8043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-638.0109, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1759],
        [-0.4552],
        [-0.0246],
        ...,
        [-4.9886],
        [-4.9817],
        [-4.9799]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306333.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0331],
        [1.0331],
        [1.0344],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370049.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0332],
        [1.0331],
        [1.0344],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370052.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3063.4565, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.9701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.1059, device='cuda:0')



h[100].sum tensor(113.9450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.2441, device='cuda:0')



h[200].sum tensor(44.7693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0301, 0.0254, 0.0000,  ..., 0.0525, 0.0000, 0.0174],
        [0.0007, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60008.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0533, 0.1175, 0.1476,  ..., 0.0129, 0.0600, 0.0000],
        [0.0183, 0.0985, 0.0552,  ..., 0.0000, 0.0320, 0.0000],
        [0.0072, 0.0925, 0.0260,  ..., 0.0000, 0.0232, 0.0000],
        ...,
        [0.0008, 0.0905, 0.0093,  ..., 0.0000, 0.0185, 0.0000],
        [0.0008, 0.0905, 0.0093,  ..., 0.0000, 0.0185, 0.0000],
        [0.0008, 0.0904, 0.0093,  ..., 0.0000, 0.0185, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533103.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2442.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.8160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6420.5879, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(943.8674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-572.7437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0035],
        [-2.2030],
        [-3.3041],
        ...,
        [-5.0165],
        [-5.0094],
        [-5.0076]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319776.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0332],
        [1.0331],
        [1.0344],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370052.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0332],
        [1.0331],
        [1.0344],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370055.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0125,  0.0102, -0.0007,  ...,  0.0212, -0.0005,  0.0072],
        [ 0.0250,  0.0201, -0.0008,  ...,  0.0417, -0.0010,  0.0153],
        [ 0.0361,  0.0289, -0.0008,  ...,  0.0600, -0.0015,  0.0225],
        ...,
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3176.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.8536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9540, device='cuda:0')



h[100].sum tensor(115.1742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.7798, device='cuda:0')



h[200].sum tensor(46.1183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8872, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0834, 0.0674, 0.0000,  ..., 0.1400, 0.0000, 0.0504],
        [0.1190, 0.0955, 0.0000,  ..., 0.1984, 0.0000, 0.0735],
        [0.1754, 0.1399, 0.0000,  ..., 0.2909, 0.0000, 0.1101],
        ...,
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60549.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2246e-01, 2.0789e-01, 5.9751e-01,  ..., 1.1467e-01, 1.9479e-01,
         0.0000e+00],
        [3.1744e-01, 2.5591e-01, 8.5064e-01,  ..., 1.7778e-01, 2.6931e-01,
         0.0000e+00],
        [4.1484e-01, 3.0409e-01, 1.1105e+00,  ..., 2.4289e-01, 3.4615e-01,
         0.0000e+00],
        ...,
        [7.1578e-04, 9.0864e-02, 9.4067e-03,  ..., 0.0000e+00, 1.8324e-02,
         0.0000e+00],
        [7.1600e-04, 9.0867e-02, 9.4071e-03,  ..., 0.0000e+00, 1.8325e-02,
         0.0000e+00],
        [7.1452e-04, 9.0794e-02, 9.3971e-03,  ..., 0.0000e+00, 1.8308e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531309.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2329.4248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.4048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6433.6821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(947.8976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-577.3564, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0262],
        [-0.0579],
        [-0.1345],
        ...,
        [-5.0414],
        [-5.0341],
        [-5.0323]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-353682.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0332],
        [1.0331],
        [1.0344],
        ...,
        [0.9982],
        [0.9971],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370055.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0332],
        [1.0345],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370058.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3514.8982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.0639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3136, device='cuda:0')



h[100].sum tensor(116.8637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8343, device='cuda:0')



h[200].sum tensor(51.1610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1504, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0079, 0.0000,  ..., 0.0163, 0.0000, 0.0039],
        [0.0006, 0.0022, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0095, 0.0092, 0.0000,  ..., 0.0191, 0.0000, 0.0050],
        ...,
        [0.0006, 0.0022, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0022, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0022, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67448.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0145, 0.0972, 0.0467,  ..., 0.0000, 0.0298, 0.0000],
        [0.0104, 0.0953, 0.0359,  ..., 0.0000, 0.0263, 0.0000],
        [0.0226, 0.1026, 0.0681,  ..., 0.0014, 0.0362, 0.0000],
        ...,
        [0.0007, 0.0911, 0.0097,  ..., 0.0000, 0.0183, 0.0000],
        [0.0007, 0.0911, 0.0097,  ..., 0.0000, 0.0183, 0.0000],
        [0.0007, 0.0910, 0.0096,  ..., 0.0000, 0.0183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563311.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2812.1460, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.3982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6040.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1047.2676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-652.3732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0427],
        [-2.7223],
        [-2.0300],
        ...,
        [-5.0538],
        [-5.0465],
        [-5.0447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-339608.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0332],
        [1.0345],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370058.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0332],
        [1.0346],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370061.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0042, -0.0006,  ...,  0.0086, -0.0002,  0.0022],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4051.2488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.2657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6609, device='cuda:0')



h[100].sum tensor(119.2544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.8414, device='cuda:0')



h[200].sum tensor(59.2577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0152, 0.0000,  ..., 0.0315, 0.0000, 0.0091],
        [0.0310, 0.0261, 0.0000,  ..., 0.0543, 0.0000, 0.0173],
        [0.0511, 0.0420, 0.0000,  ..., 0.0873, 0.0000, 0.0303],
        ...,
        [0.0006, 0.0022, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0022, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0022, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76110.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0869, 0.1381, 0.2381,  ..., 0.0288, 0.0882, 0.0000],
        [0.1169, 0.1551, 0.3174,  ..., 0.0471, 0.1118, 0.0000],
        [0.1498, 0.1726, 0.4042,  ..., 0.0684, 0.1372, 0.0000],
        ...,
        [0.0007, 0.0910, 0.0096,  ..., 0.0000, 0.0183, 0.0000],
        [0.0007, 0.0910, 0.0096,  ..., 0.0000, 0.0183, 0.0000],
        [0.0007, 0.0910, 0.0096,  ..., 0.0000, 0.0183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595516.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3326.7778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(406.1835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5896.1401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1167.6864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-745.9918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0801],
        [ 0.0651],
        [ 0.0477],
        ...,
        [-5.0524],
        [-5.0450],
        [-5.0430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324028.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0332],
        [1.0346],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370061.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0333],
        [1.0332],
        [1.0346],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370064.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3241.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3566, device='cuda:0')



h[100].sum tensor(115.1748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.9833, device='cuda:0')



h[200].sum tensor(48.0118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9321, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0083, 0.0082, 0.0000,  ..., 0.0168, 0.0000, 0.0033],
        [0.0045, 0.0053, 0.0000,  ..., 0.0107, 0.0000, 0.0016],
        [0.0008, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63350.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0154, 0.0969, 0.0480,  ..., 0.0000, 0.0324, 0.0000],
        [0.0113, 0.0949, 0.0369,  ..., 0.0000, 0.0284, 0.0000],
        [0.0063, 0.0920, 0.0236,  ..., 0.0000, 0.0235, 0.0000],
        ...,
        [0.0011, 0.0904, 0.0097,  ..., 0.0000, 0.0188, 0.0000],
        [0.0011, 0.0904, 0.0097,  ..., 0.0000, 0.0188, 0.0000],
        [0.0011, 0.0903, 0.0097,  ..., 0.0000, 0.0188, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(547168.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2724.0376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(296.9478, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6078.2251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(992.1434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.4856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5993],
        [-3.7533],
        [-3.9773],
        ...,
        [-5.0419],
        [-5.0348],
        [-5.0331]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300899.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0333],
        [1.0332],
        [1.0346],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370064.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0347],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370068., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0060,  0.0051, -0.0006,  ...,  0.0105, -0.0002,  0.0030],
        [ 0.0058,  0.0050, -0.0006,  ...,  0.0103, -0.0002,  0.0029],
        [ 0.0116,  0.0096, -0.0007,  ...,  0.0198, -0.0005,  0.0066],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3643.0496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.7306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0479, device='cuda:0')



h[100].sum tensor(116.1879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.0296, device='cuda:0')



h[200].sum tensor(54.7568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0176, 0.0000,  ..., 0.0361, 0.0000, 0.0109],
        [0.0413, 0.0343, 0.0000,  ..., 0.0707, 0.0000, 0.0229],
        [0.0204, 0.0178, 0.0000,  ..., 0.0365, 0.0000, 0.0102],
        ...,
        [0.0009, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68684.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0558, 0.1188, 0.1524,  ..., 0.0112, 0.0641, 0.0000],
        [0.0805, 0.1335, 0.2171,  ..., 0.0216, 0.0844, 0.0000],
        [0.0720, 0.1288, 0.1948,  ..., 0.0166, 0.0779, 0.0000],
        ...,
        [0.0014, 0.0899, 0.0100,  ..., 0.0000, 0.0192, 0.0000],
        [0.0014, 0.0899, 0.0100,  ..., 0.0000, 0.0192, 0.0000],
        [0.0014, 0.0898, 0.0099,  ..., 0.0000, 0.0192, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562039.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3054.5476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.7551, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6020.9087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1067.5924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-665.0236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0794],
        [ 0.2123],
        [ 0.2767],
        ...,
        [-5.0201],
        [-5.0134],
        [-5.0122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276999.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0347],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370068., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 150.0 event: 750 loss: tensor(451.1758, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0347],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370071.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3533.7964, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.1930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4645, device='cuda:0')



h[100].sum tensor(115.8208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2854, device='cuda:0')



h[200].sum tensor(53.1716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1672, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0024, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67414.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0875, 0.0095,  ..., 0.0000, 0.0185, 0.0000],
        [0.0015, 0.0883, 0.0103,  ..., 0.0000, 0.0190, 0.0000],
        [0.0022, 0.0890, 0.0123,  ..., 0.0000, 0.0199, 0.0000],
        ...,
        [0.0013, 0.0900, 0.0100,  ..., 0.0000, 0.0192, 0.0000],
        [0.0013, 0.0900, 0.0100,  ..., 0.0000, 0.0192, 0.0000],
        [0.0013, 0.0899, 0.0100,  ..., 0.0000, 0.0192, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562069.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2937.6831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.6976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6323.8276, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1043.2294, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-648.5157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9003],
        [-3.9275],
        [-3.8997],
        ...,
        [-5.0213],
        [-5.0171],
        [-5.0180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318168.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0347],
        ...,
        [0.9981],
        [0.9970],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370071.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0348],
        ...,
        [0.9980],
        [0.9970],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370074.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3223.3572, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2586, device='cuda:0')



h[100].sum tensor(115.0938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.6903, device='cuda:0')



h[200].sum tensor(48.1506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0053, 0.0000,  ..., 0.0106, 0.0000, 0.0016],
        [0.0084, 0.0083, 0.0000,  ..., 0.0169, 0.0000, 0.0033],
        [0.0046, 0.0053, 0.0000,  ..., 0.0107, 0.0000, 0.0016],
        ...,
        [0.0008, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63547.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0155, 0.0968, 0.0483,  ..., 0.0000, 0.0321, 0.0000],
        [0.0197, 0.0999, 0.0594,  ..., 0.0000, 0.0362, 0.0000],
        [0.0147, 0.0971, 0.0461,  ..., 0.0000, 0.0316, 0.0000],
        ...,
        [0.0011, 0.0903, 0.0100,  ..., 0.0000, 0.0190, 0.0000],
        [0.0011, 0.0903, 0.0100,  ..., 0.0000, 0.0190, 0.0000],
        [0.0011, 0.0902, 0.0100,  ..., 0.0000, 0.0190, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551836.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2751.7761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.1767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6058.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(994.1547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-609.9338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4155],
        [-2.1017],
        [-2.0004],
        ...,
        [-5.0561],
        [-5.0491],
        [-5.0474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306931.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0348],
        ...,
        [0.9980],
        [0.9970],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370074.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0348],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370077., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0123,  0.0101, -0.0007,  ...,  0.0209, -0.0005,  0.0071],
        ...,
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3954.4075, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.4318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1338, device='cuda:0')



h[100].sum tensor(118.9145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.2655, device='cuda:0')



h[200].sum tensor(58.4213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0075, 0.0000,  ..., 0.0155, 0.0000, 0.0036],
        [0.0377, 0.0314, 0.0000,  ..., 0.0651, 0.0000, 0.0215],
        [0.0565, 0.0462, 0.0000,  ..., 0.0959, 0.0000, 0.0337],
        ...,
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73677.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0497, 0.1164, 0.1385,  ..., 0.0131, 0.0585, 0.0000],
        [0.1105, 0.1502, 0.2983,  ..., 0.0442, 0.1068, 0.0000],
        [0.1624, 0.1780, 0.4348,  ..., 0.0765, 0.1474, 0.0000],
        ...,
        [0.0008, 0.0907, 0.0098,  ..., 0.0000, 0.0187, 0.0000],
        [0.0008, 0.0907, 0.0098,  ..., 0.0000, 0.0187, 0.0000],
        [0.0008, 0.0906, 0.0097,  ..., 0.0000, 0.0187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583136.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3159.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(385.9083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5847.1450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1133.3910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-719.8239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3944],
        [ 0.0063],
        [ 0.1299],
        ...,
        [-5.0872],
        [-5.0802],
        [-5.0789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313699.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0348],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370077., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0349],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370079.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0082,  0.0069, -0.0007,  ...,  0.0143, -0.0003,  0.0044],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3231.2117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.3932, device='cuda:0')



h[100].sum tensor(116.1196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.0928, device='cuda:0')



h[200].sum tensor(47.5317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0271, 0.0230, 0.0000,  ..., 0.0477, 0.0000, 0.0155],
        [0.0090, 0.0088, 0.0000,  ..., 0.0180, 0.0000, 0.0046],
        [0.0007, 0.0022, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0022, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0022, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0022, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62243.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0798, 0.1331, 0.2164,  ..., 0.0278, 0.0816, 0.0000],
        [0.0379, 0.1103, 0.1069,  ..., 0.0083, 0.0485, 0.0000],
        [0.0127, 0.0962, 0.0409,  ..., 0.0000, 0.0283, 0.0000],
        ...,
        [0.0008, 0.0907, 0.0096,  ..., 0.0000, 0.0185, 0.0000],
        [0.0008, 0.0907, 0.0096,  ..., 0.0000, 0.0185, 0.0000],
        [0.0008, 0.0906, 0.0096,  ..., 0.0000, 0.0185, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541123.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2487.0503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.3575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6082.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(971.3510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.5623, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0964],
        [-0.6005],
        [-1.4250],
        ...,
        [-5.1072],
        [-5.0999],
        [-5.0982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336353.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0349],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370079.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0349],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370082.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0217,  0.0175, -0.0008,  ...,  0.0363, -0.0009,  0.0132],
        [ 0.0254,  0.0204, -0.0008,  ...,  0.0424, -0.0010,  0.0156],
        ...,
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0005, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3287.2549, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.6907, device='cuda:0')



h[100].sum tensor(116.2264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.9824, device='cuda:0')



h[200].sum tensor(48.5156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0228, 0.0197, 0.0000,  ..., 0.0406, 0.0000, 0.0135],
        [0.0498, 0.0410, 0.0000,  ..., 0.0849, 0.0000, 0.0302],
        [0.1171, 0.0940, 0.0000,  ..., 0.1950, 0.0000, 0.0722],
        ...,
        [0.0007, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63840.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0650, 0.1229, 0.1776,  ..., 0.0228, 0.0694, 0.0000],
        [0.1204, 0.1531, 0.3229,  ..., 0.0527, 0.1137, 0.0000],
        [0.1867, 0.1883, 0.4967,  ..., 0.0926, 0.1665, 0.0000],
        ...,
        [0.0008, 0.0906, 0.0097,  ..., 0.0000, 0.0186, 0.0000],
        [0.0008, 0.0906, 0.0097,  ..., 0.0000, 0.0186, 0.0000],
        [0.0008, 0.0906, 0.0097,  ..., 0.0000, 0.0185, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550587.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2592.7905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(304.3712, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6188.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(989.7107, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-611.9995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3893],
        [ 0.0651],
        [ 0.2395],
        ...,
        [-5.1115],
        [-5.1042],
        [-5.1025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-356275.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0333],
        [1.0349],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370082.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0334],
        [1.0349],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370086.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3957.0200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2135, device='cuda:0')



h[100].sum tensor(118.5210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.5038, device='cuda:0')



h[200].sum tensor(58.8887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72101.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0153, 0.0966, 0.0474,  ..., 0.0000, 0.0308, 0.0000],
        [0.0052, 0.0913, 0.0212,  ..., 0.0000, 0.0225, 0.0000],
        [0.0014, 0.0892, 0.0111,  ..., 0.0000, 0.0190, 0.0000],
        ...,
        [0.0009, 0.0904, 0.0099,  ..., 0.0000, 0.0188, 0.0000],
        [0.0009, 0.0904, 0.0099,  ..., 0.0000, 0.0188, 0.0000],
        [0.0009, 0.0903, 0.0099,  ..., 0.0000, 0.0188, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572474.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3042.2329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(374.8929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5814.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1109.7455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-702.4550, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4800],
        [-2.4219],
        [-3.1981],
        ...,
        [-5.0822],
        [-5.0764],
        [-5.0767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298321.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0334],
        [1.0349],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370086.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0334],
        [1.0350],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370090.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3410.7876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.1558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4802, device='cuda:0')



h[100].sum tensor(115.9968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.3425, device='cuda:0')



h[200].sum tensor(51.0165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0023, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64974.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0881, 0.0097,  ..., 0.0000, 0.0181, 0.0000],
        [0.0031, 0.0900, 0.0159,  ..., 0.0000, 0.0202, 0.0000],
        [0.0062, 0.0919, 0.0240,  ..., 0.0000, 0.0227, 0.0000],
        ...,
        [0.0008, 0.0905, 0.0102,  ..., 0.0000, 0.0188, 0.0000],
        [0.0008, 0.0905, 0.0102,  ..., 0.0000, 0.0188, 0.0000],
        [0.0008, 0.0905, 0.0102,  ..., 0.0000, 0.0188, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549472.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2661.9924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.3235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5896.0068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1011.3553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-625.2923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4092],
        [-4.1036],
        [-3.4674],
        ...,
        [-5.0993],
        [-5.0922],
        [-5.0906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300311.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0334],
        [1.0350],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370090.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0334],
        [1.0334],
        [1.0350],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370093.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0068,  0.0058, -0.0007,  ...,  0.0119, -0.0003,  0.0035],
        [ 0.0159,  0.0130, -0.0007,  ...,  0.0268, -0.0006,  0.0094],
        [ 0.0200,  0.0162, -0.0007,  ...,  0.0335, -0.0008,  0.0121],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4972.1025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(44.3627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.3363, device='cuda:0')



h[100].sum tensor(122.5399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(57.8089, device='cuda:0')



h[200].sum tensor(74.1020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.1567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0265, 0.0000,  ..., 0.0543, 0.0000, 0.0182],
        [0.0695, 0.0567, 0.0000,  ..., 0.1170, 0.0000, 0.0414],
        [0.0838, 0.0680, 0.0000,  ..., 0.1404, 0.0000, 0.0506],
        ...,
        [0.0007, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0024, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91562.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0955, 0.1409, 0.2569,  ..., 0.0337, 0.0944, 0.0000],
        [0.1568, 0.1740, 0.4167,  ..., 0.0699, 0.1428, 0.0000],
        [0.1997, 0.1966, 0.5285,  ..., 0.0963, 0.1762, 0.0000],
        ...,
        [0.0007, 0.0906, 0.0102,  ..., 0.0000, 0.0188, 0.0000],
        [0.0007, 0.0906, 0.0102,  ..., 0.0000, 0.0188, 0.0000],
        [0.0007, 0.0906, 0.0102,  ..., 0.0000, 0.0187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661129.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4235.8311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(549.1816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5807.6987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1373.1488, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-908.6039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2013],
        [ 0.1599],
        [ 0.1128],
        ...,
        [-5.0695],
        [-5.0827],
        [-5.0873]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312063.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0334],
        [1.0334],
        [1.0350],
        ...,
        [0.9980],
        [0.9969],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370093.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0335],
        [1.0351],
        ...,
        [0.9979],
        [0.9969],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370097.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0189,  0.0154, -0.0007,  ...,  0.0317, -0.0007,  0.0114],
        [ 0.0331,  0.0265, -0.0008,  ...,  0.0549, -0.0013,  0.0205],
        [ 0.0180,  0.0146, -0.0007,  ...,  0.0301, -0.0007,  0.0108],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3379.5005, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1992, device='cuda:0')



h[100].sum tensor(115.7268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.5026, device='cuda:0')



h[200].sum tensor(50.6745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0952, 0.0770, 0.0000,  ..., 0.1589, 0.0000, 0.0580],
        [0.0991, 0.0801, 0.0000,  ..., 0.1653, 0.0000, 0.0605],
        [0.1047, 0.0845, 0.0000,  ..., 0.1744, 0.0000, 0.0641],
        ...,
        [0.0007, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64016.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1626, 0.1746, 0.4319,  ..., 0.0754, 0.1466, 0.0000],
        [0.1910, 0.1905, 0.5058,  ..., 0.0911, 0.1695, 0.0000],
        [0.1953, 0.1931, 0.5173,  ..., 0.0930, 0.1734, 0.0000],
        ...,
        [0.0007, 0.0906, 0.0103,  ..., 0.0000, 0.0188, 0.0000],
        [0.0007, 0.0906, 0.0103,  ..., 0.0000, 0.0188, 0.0000],
        [0.0007, 0.0905, 0.0103,  ..., 0.0000, 0.0188, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(544417.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2512.1904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(308.5729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6073.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(995.1405, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.7518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0568],
        [ 0.2357],
        [ 0.2791],
        ...,
        [-5.1059],
        [-5.0989],
        [-5.0974]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312863.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0335],
        [1.0351],
        ...,
        [0.9979],
        [0.9969],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370097.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0335],
        [1.0351],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370101.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0240,  0.0194, -0.0008,  ...,  0.0400, -0.0009,  0.0147],
        [ 0.0466,  0.0372, -0.0009,  ...,  0.0769, -0.0018,  0.0293],
        [ 0.0456,  0.0364, -0.0009,  ...,  0.0753, -0.0018,  0.0286],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3435.7407, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.9240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4463, device='cuda:0')



h[100].sum tensor(115.9335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.2412, device='cuda:0')



h[200].sum tensor(51.5949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1213, 0.0977, 0.0000,  ..., 0.2016, 0.0000, 0.0750],
        [0.1615, 0.1294, 0.0000,  ..., 0.2673, 0.0000, 0.1010],
        [0.1690, 0.1353, 0.0000,  ..., 0.2796, 0.0000, 0.1058],
        ...,
        [0.0007, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65923.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.3619e-01, 2.1032e-01, 6.2578e-01,  ..., 1.2331e-01, 2.0423e-01,
         0.0000e+00],
        [3.0819e-01, 2.4732e-01, 8.1437e-01,  ..., 1.6765e-01, 2.6124e-01,
         0.0000e+00],
        [3.1899e-01, 2.5330e-01, 8.4270e-01,  ..., 1.7359e-01, 2.7013e-01,
         0.0000e+00],
        ...,
        [6.6911e-04, 9.0672e-02, 1.0440e-02,  ..., 0.0000e+00, 1.8853e-02,
         0.0000e+00],
        [6.6944e-04, 9.0677e-02, 1.0441e-02,  ..., 0.0000e+00, 1.8854e-02,
         0.0000e+00],
        [6.6819e-04, 9.0611e-02, 1.0432e-02,  ..., 0.0000e+00, 1.8839e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555846.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2664.7043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.0513, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5992.3799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1024.1786, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-633.9951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0552],
        [ 0.1670],
        [ 0.1935],
        ...,
        [-5.1029],
        [-5.0952],
        [-5.0909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302015.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0335],
        [1.0351],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370101.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0335],
        [1.0335],
        [1.0351],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370101.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0100,  0.0084, -0.0007,  ...,  0.0171, -0.0004,  0.0056],
        [ 0.0238,  0.0193, -0.0008,  ...,  0.0398, -0.0009,  0.0146],
        [ 0.0205,  0.0166, -0.0007,  ...,  0.0342, -0.0008,  0.0124],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4057.3159, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.0784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6314, device='cuda:0')



h[100].sum tensor(118.5061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.7533, device='cuda:0')



h[200].sum tensor(60.7842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0686, 0.0561, 0.0000,  ..., 0.1153, 0.0000, 0.0409],
        [0.0686, 0.0561, 0.0000,  ..., 0.1153, 0.0000, 0.0408],
        [0.0714, 0.0583, 0.0000,  ..., 0.1199, 0.0000, 0.0427],
        ...,
        [0.0007, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74870.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1148, 0.1509, 0.3070,  ..., 0.0452, 0.1086, 0.0000],
        [0.1294, 0.1594, 0.3450,  ..., 0.0532, 0.1204, 0.0000],
        [0.1283, 0.1591, 0.3423,  ..., 0.0525, 0.1198, 0.0000],
        ...,
        [0.0007, 0.0907, 0.0104,  ..., 0.0000, 0.0189, 0.0000],
        [0.0007, 0.0907, 0.0104,  ..., 0.0000, 0.0189, 0.0000],
        [0.0007, 0.0906, 0.0104,  ..., 0.0000, 0.0188, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591347., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3218.2300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(402.8401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5817.4507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1148.9187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-729.5604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2244],
        [ 0.2792],
        [ 0.2691],
        ...,
        [-5.0981],
        [-5.0953],
        [-5.0957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294873.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0335],
        [1.0335],
        [1.0351],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370101.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0336],
        [1.0352],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370104.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0134,  0.0111, -0.0007,  ...,  0.0227, -0.0005,  0.0078],
        [ 0.0121,  0.0100, -0.0007,  ...,  0.0205, -0.0005,  0.0070],
        [ 0.0131,  0.0109, -0.0007,  ...,  0.0223, -0.0005,  0.0077],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0150,  0.0123, -0.0007,  ...,  0.0253, -0.0006,  0.0089],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3861.0305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.1594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3711, device='cuda:0')



h[100].sum tensor(117.8852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.9855, device='cuda:0')



h[200].sum tensor(57.7787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3798, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0476, 0.0396, 0.0000,  ..., 0.0810, 0.0000, 0.0273],
        [0.0492, 0.0408, 0.0000,  ..., 0.0836, 0.0000, 0.0283],
        [0.0476, 0.0396, 0.0000,  ..., 0.0811, 0.0000, 0.0273],
        ...,
        [0.0163, 0.0149, 0.0000,  ..., 0.0298, 0.0000, 0.0093],
        [0.0134, 0.0126, 0.0000,  ..., 0.0251, 0.0000, 0.0075],
        [0.0574, 0.0473, 0.0000,  ..., 0.0970, 0.0000, 0.0336]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71354.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0966, 0.1434, 0.2600,  ..., 0.0307, 0.0954, 0.0000],
        [0.0970, 0.1440, 0.2610,  ..., 0.0313, 0.0956, 0.0000],
        [0.0954, 0.1434, 0.2571,  ..., 0.0305, 0.0945, 0.0000],
        ...,
        [0.0300, 0.1073, 0.0871,  ..., 0.0038, 0.0421, 0.0000],
        [0.0384, 0.1122, 0.1089,  ..., 0.0047, 0.0488, 0.0000],
        [0.0655, 0.1272, 0.1793,  ..., 0.0164, 0.0702, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573141.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2965.8640, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(369.5318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5703.6504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1104.5001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-693.4537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3571],
        [ 0.3683],
        [ 0.3697],
        ...,
        [-2.7563],
        [-2.3488],
        [-2.1332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270342.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0336],
        [1.0352],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370104.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0336],
        [1.0336],
        [1.0352],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370108.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4145.3271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.0176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0878, device='cuda:0')



h[100].sum tensor(119.2030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.1179, device='cuda:0')



h[200].sum tensor(61.9038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5713, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75965.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0937, 0.0341,  ..., 0.0000, 0.0262, 0.0000],
        [0.0046, 0.0912, 0.0199,  ..., 0.0000, 0.0218, 0.0000],
        [0.0028, 0.0902, 0.0149,  ..., 0.0000, 0.0203, 0.0000],
        ...,
        [0.0008, 0.0906, 0.0100,  ..., 0.0000, 0.0188, 0.0000],
        [0.0008, 0.0906, 0.0100,  ..., 0.0000, 0.0188, 0.0000],
        [0.0008, 0.0905, 0.0100,  ..., 0.0000, 0.0188, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590348.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3290.6772, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(409.4867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5699.9395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1168.9369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-740.7296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7522],
        [-2.5166],
        [-3.1473],
        ...,
        [-5.1261],
        [-5.1190],
        [-5.1172]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269051.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0336],
        [1.0336],
        [1.0352],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370108.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0337],
        [1.0353],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370111.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0119,  0.0099, -0.0007,  ...,  0.0202, -0.0004,  0.0069],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4024.3330, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.9757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3098, device='cuda:0')



h[100].sum tensor(119.0407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.7917, device='cuda:0')



h[200].sum tensor(59.8401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4845, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0228, 0.0200, 0.0000,  ..., 0.0403, 0.0000, 0.0128],
        [0.0312, 0.0267, 0.0000,  ..., 0.0542, 0.0000, 0.0175],
        ...,
        [0.0006, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0168, 0.0153, 0.0000,  ..., 0.0307, 0.0000, 0.0090]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72552.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0199, 0.0993, 0.0594,  ..., 0.0000, 0.0334, 0.0000],
        [0.0554, 0.1198, 0.1516,  ..., 0.0129, 0.0618, 0.0000],
        [0.0849, 0.1365, 0.2282,  ..., 0.0250, 0.0857, 0.0000],
        ...,
        [0.0062, 0.0941, 0.0242,  ..., 0.0000, 0.0232, 0.0000],
        [0.0175, 0.1006, 0.0534,  ..., 0.0000, 0.0326, 0.0000],
        [0.0440, 0.1158, 0.1225,  ..., 0.0054, 0.0543, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574984.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3020.7205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(380.6956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5922.4399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1120.1483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-703.0477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3980],
        [-0.6397],
        [-0.0632],
        ...,
        [-3.7534],
        [-2.5608],
        [-1.3384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279484.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0337],
        [1.0353],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370111.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0337],
        [1.0353],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370115.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0109,  0.0091, -0.0007,  ...,  0.0185, -0.0004,  0.0062],
        [ 0.0287,  0.0231, -0.0008,  ...,  0.0477, -0.0011,  0.0178],
        [ 0.0050,  0.0044, -0.0006,  ...,  0.0089, -0.0002,  0.0024],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3421.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3143, device='cuda:0')



h[100].sum tensor(117.0697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.8465, device='cuda:0')



h[200].sum tensor(50.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1048, 0.0847, 0.0000,  ..., 0.1745, 0.0000, 0.0645],
        [0.0535, 0.0443, 0.0000,  ..., 0.0907, 0.0000, 0.0313],
        [0.0595, 0.0490, 0.0000,  ..., 0.1005, 0.0000, 0.0351],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63123.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1781, 0.1837, 0.4713,  ..., 0.0819, 0.1581, 0.0000],
        [0.1393, 0.1651, 0.3705,  ..., 0.0568, 0.1284, 0.0000],
        [0.1161, 0.1530, 0.3101,  ..., 0.0433, 0.1103, 0.0000],
        ...,
        [0.0007, 0.0911, 0.0096,  ..., 0.0000, 0.0184, 0.0000],
        [0.0007, 0.0911, 0.0096,  ..., 0.0000, 0.0184, 0.0000],
        [0.0007, 0.0911, 0.0095,  ..., 0.0000, 0.0184, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539241.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2378.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.5934, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6378.5571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(985.7773, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-599.5167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2483],
        [ 0.2351],
        [ 0.1525],
        ...,
        [-5.1682],
        [-5.1612],
        [-5.1596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337275.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0337],
        [1.0353],
        ...,
        [0.9979],
        [0.9968],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370115.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0337],
        [1.0354],
        ...,
        [0.9979],
        [0.9968],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370119.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0096,  0.0080, -0.0007,  ...,  0.0164, -0.0004,  0.0054],
        [ 0.0190,  0.0155, -0.0007,  ...,  0.0319, -0.0007,  0.0115],
        [ 0.0096,  0.0081, -0.0007,  ...,  0.0165, -0.0004,  0.0054],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4188.2236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.5496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2624, device='cuda:0')



h[100].sum tensor(120.5574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.6396, device='cuda:0')



h[200].sum tensor(61.7415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0646, 0.0531, 0.0000,  ..., 0.1089, 0.0000, 0.0393],
        [0.0651, 0.0534, 0.0000,  ..., 0.1096, 0.0000, 0.0388],
        [0.0645, 0.0529, 0.0000,  ..., 0.1086, 0.0000, 0.0391],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78188.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2013, 0.1940, 0.5322,  ..., 0.0988, 0.1755, 0.0000],
        [0.2219, 0.2056, 0.5858,  ..., 0.1104, 0.1919, 0.0000],
        [0.2126, 0.2008, 0.5616,  ..., 0.1054, 0.1845, 0.0000],
        ...,
        [0.0008, 0.0912, 0.0094,  ..., 0.0000, 0.0184, 0.0000],
        [0.0008, 0.0912, 0.0094,  ..., 0.0000, 0.0184, 0.0000],
        [0.0008, 0.0911, 0.0094,  ..., 0.0000, 0.0184, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604373.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3423.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(428.4403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6038.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1197.9125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-761.0751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0241],
        [ 0.0170],
        [ 0.0089],
        ...,
        [-5.1745],
        [-5.1676],
        [-5.1661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303307.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0337],
        [1.0354],
        ...,
        [0.9979],
        [0.9968],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370119.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0355],
        ...,
        [0.9978],
        [0.9967],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370123.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3516.9229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8657, device='cuda:0')



h[100].sum tensor(117.8091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.4950, device='cuda:0')



h[200].sum tensor(52.0126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1004, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0025, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66568.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0895, 0.0129,  ..., 0.0000, 0.0195, 0.0000],
        [0.0011, 0.0893, 0.0096,  ..., 0.0000, 0.0183, 0.0000],
        [0.0009, 0.0894, 0.0090,  ..., 0.0000, 0.0181, 0.0000],
        ...,
        [0.0010, 0.0910, 0.0093,  ..., 0.0000, 0.0186, 0.0000],
        [0.0010, 0.0910, 0.0093,  ..., 0.0000, 0.0186, 0.0000],
        [0.0010, 0.0909, 0.0093,  ..., 0.0000, 0.0186, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559213.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2818.4299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.3670, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6207.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1039.7177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-635.2948, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3463],
        [-3.9838],
        [-4.4266],
        ...,
        [-5.1711],
        [-5.1643],
        [-5.1629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298032.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0355],
        ...,
        [0.9978],
        [0.9967],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370123.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0356],
        ...,
        [0.9978],
        [0.9967],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370127.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3390.2268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9144, device='cuda:0')



h[100].sum tensor(117.3754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.6511, device='cuda:0')



h[200].sum tensor(50.2537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0025, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63771.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0883, 0.0087,  ..., 0.0000, 0.0180, 0.0000],
        [0.0012, 0.0889, 0.0087,  ..., 0.0000, 0.0181, 0.0000],
        [0.0012, 0.0892, 0.0088,  ..., 0.0000, 0.0182, 0.0000],
        ...,
        [0.0013, 0.0907, 0.0092,  ..., 0.0000, 0.0187, 0.0000],
        [0.0013, 0.0907, 0.0092,  ..., 0.0000, 0.0187, 0.0000],
        [0.0013, 0.0907, 0.0092,  ..., 0.0000, 0.0187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546099.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2666.4507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.2564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6405.2046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(999.9811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-603.2516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8473],
        [-4.9813],
        [-5.0732],
        ...,
        [-5.1693],
        [-5.1626],
        [-5.1613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306065.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0338],
        [1.0356],
        ...,
        [0.9978],
        [0.9967],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370127.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0357],
        ...,
        [0.9978],
        [0.9967],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370131.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0135,  0.0112, -0.0007,  ...,  0.0228, -0.0005,  0.0079],
        [ 0.0226,  0.0184, -0.0008,  ...,  0.0377, -0.0008,  0.0138],
        [ 0.0155,  0.0127, -0.0007,  ...,  0.0260, -0.0006,  0.0092],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5366.6982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(48.4295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.0682, device='cuda:0')



h[100].sum tensor(126.0063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(65.9763, device='cuda:0')



h[200].sum tensor(79.1551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.4614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0386, 0.0325, 0.0000,  ..., 0.0661, 0.0000, 0.0224],
        [0.0567, 0.0468, 0.0000,  ..., 0.0957, 0.0000, 0.0333],
        [0.1080, 0.0874, 0.0000,  ..., 0.1796, 0.0000, 0.0666],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(103065.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1183, 0.1505, 0.3120,  ..., 0.0445, 0.1113, 0.0000],
        [0.1672, 0.1760, 0.4392,  ..., 0.0742, 0.1495, 0.0000],
        [0.2357, 0.2098, 0.6175,  ..., 0.1180, 0.2024, 0.0000],
        ...,
        [0.0014, 0.0907, 0.0090,  ..., 0.0000, 0.0187, 0.0000],
        [0.0015, 0.0907, 0.0090,  ..., 0.0000, 0.0187, 0.0000],
        [0.0014, 0.0907, 0.0090,  ..., 0.0000, 0.0187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(745329., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5698.7192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(641.8293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5759.0679, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1547.7693, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1026.2610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2239],
        [ 0.1690],
        [ 0.1138],
        ...,
        [-5.1769],
        [-5.1703],
        [-5.1690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267397.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0337],
        [1.0339],
        [1.0357],
        ...,
        [0.9978],
        [0.9967],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370131.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0340],
        [1.0358],
        ...,
        [0.9978],
        [0.9967],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370134.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0119,  0.0099, -0.0007,  ...,  0.0201, -0.0004,  0.0069],
        [ 0.0055,  0.0048, -0.0006,  ...,  0.0097, -0.0002,  0.0027],
        [ 0.0206,  0.0167, -0.0007,  ...,  0.0343, -0.0007,  0.0125],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4041.3833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.1818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1967, device='cuda:0')



h[100].sum tensor(121.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.4537, device='cuda:0')



h[200].sum tensor(59.6021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4719, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0276, 0.0239, 0.0000,  ..., 0.0482, 0.0000, 0.0145],
        [0.0492, 0.0409, 0.0000,  ..., 0.0835, 0.0000, 0.0285],
        [0.0280, 0.0241, 0.0000,  ..., 0.0488, 0.0000, 0.0154],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77615.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0949, 0.1387, 0.2484,  ..., 0.0272, 0.0942, 0.0000],
        [0.0948, 0.1395, 0.2481,  ..., 0.0275, 0.0939, 0.0000],
        [0.0788, 0.1313, 0.2070,  ..., 0.0191, 0.0811, 0.0000],
        ...,
        [0.0017, 0.0906, 0.0088,  ..., 0.0000, 0.0188, 0.0000],
        [0.0017, 0.0907, 0.0089,  ..., 0.0000, 0.0188, 0.0000],
        [0.0017, 0.0906, 0.0088,  ..., 0.0000, 0.0188, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616170., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3883.4043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.2625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5939.0659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1196.5325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-754.1151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3369],
        [ 0.3625],
        [ 0.3704],
        ...,
        [-5.1879],
        [-5.1813],
        [-5.1801]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265740.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0340],
        [1.0358],
        ...,
        [0.9978],
        [0.9967],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370134.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 170.0 event: 850 loss: tensor(936.1046, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0341],
        [1.0359],
        ...,
        [0.9977],
        [0.9966],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370138.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0088,  0.0075, -0.0007,  ...,  0.0152, -0.0003,  0.0049],
        [ 0.0090,  0.0076, -0.0007,  ...,  0.0155, -0.0003,  0.0050],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3990.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.8632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9992, device='cuda:0')



h[100].sum tensor(121.8383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.8632, device='cuda:0')



h[200].sum tensor(58.2843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0500, 0.0415, 0.0000,  ..., 0.0849, 0.0000, 0.0290],
        [0.0171, 0.0155, 0.0000,  ..., 0.0311, 0.0000, 0.0092],
        [0.0097, 0.0097, 0.0000,  ..., 0.0191, 0.0000, 0.0052],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75723.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0824, 0.1324, 0.2156,  ..., 0.0238, 0.0825, 0.0000],
        [0.0467, 0.1141, 0.1243,  ..., 0.0085, 0.0545, 0.0000],
        [0.0252, 0.1027, 0.0690,  ..., 0.0019, 0.0373, 0.0000],
        ...,
        [0.0016, 0.0912, 0.0087,  ..., 0.0000, 0.0186, 0.0000],
        [0.0016, 0.0912, 0.0087,  ..., 0.0000, 0.0186, 0.0000],
        [0.0016, 0.0912, 0.0087,  ..., 0.0000, 0.0186, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608256.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3690.2251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(401.0443, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6093.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1165.8905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-734.0150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6479],
        [-1.7034],
        [-2.8692],
        ...,
        [-5.2141],
        [-5.2074],
        [-5.2064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297826.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0341],
        [1.0359],
        ...,
        [0.9977],
        [0.9966],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370138.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0341],
        [1.0360],
        ...,
        [0.9977],
        [0.9966],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370141.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0267,  0.0216, -0.0008,  ...,  0.0445, -0.0010,  0.0165],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3943.2222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.5753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6377, device='cuda:0')



h[100].sum tensor(122.7165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.7825, device='cuda:0')



h[200].sum tensor(56.8203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4096, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0627, 0.0515, 0.0000,  ..., 0.1057, 0.0000, 0.0373],
        [0.0231, 0.0202, 0.0000,  ..., 0.0410, 0.0000, 0.0131],
        [0.0657, 0.0539, 0.0000,  ..., 0.1107, 0.0000, 0.0392],
        ...,
        [0.0005, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74181.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0839, 0.1334, 0.2188,  ..., 0.0259, 0.0830, 0.0000],
        [0.0678, 0.1257, 0.1776,  ..., 0.0167, 0.0704, 0.0000],
        [0.0870, 0.1359, 0.2267,  ..., 0.0276, 0.0856, 0.0000],
        ...,
        [0.0015, 0.0917, 0.0084,  ..., 0.0000, 0.0183, 0.0000],
        [0.0015, 0.0917, 0.0084,  ..., 0.0000, 0.0183, 0.0000],
        [0.0015, 0.0917, 0.0084,  ..., 0.0000, 0.0183, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593415.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3406.2354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(384.9536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6256.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1142.1410, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-718.5363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9144],
        [-0.9320],
        [-1.0859],
        ...,
        [-5.2189],
        [-5.2133],
        [-5.2161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329738.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0341],
        [1.0360],
        ...,
        [0.9977],
        [0.9966],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370141.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0338],
        [1.0342],
        [1.0360],
        ...,
        [0.9976],
        [0.9965],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370145.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070,  0.0060, -0.0007,  ...,  0.0122, -0.0002,  0.0037],
        [ 0.0206,  0.0168, -0.0007,  ...,  0.0345, -0.0007,  0.0125],
        [ 0.0254,  0.0205, -0.0008,  ...,  0.0423, -0.0009,  0.0156],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3654.0049, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.7590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7434, device='cuda:0')



h[100].sum tensor(121.8757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.1192, device='cuda:0')



h[200].sum tensor(52.3900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0265, 0.0000,  ..., 0.0541, 0.0000, 0.0183],
        [0.0732, 0.0598, 0.0000,  ..., 0.1229, 0.0000, 0.0441],
        [0.1208, 0.0974, 0.0000,  ..., 0.2007, 0.0000, 0.0749],
        ...,
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0004, 0.0024, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68857.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1019, 0.1419, 0.2639,  ..., 0.0357, 0.0977, 0.0000],
        [0.1723, 0.1778, 0.4436,  ..., 0.0769, 0.1530, 0.0000],
        [0.2487, 0.2153, 0.6384,  ..., 0.1233, 0.2125, 0.0000],
        ...,
        [0.0015, 0.0919, 0.0084,  ..., 0.0000, 0.0184, 0.0000],
        [0.0015, 0.0919, 0.0084,  ..., 0.0000, 0.0184, 0.0000],
        [0.0015, 0.0918, 0.0084,  ..., 0.0000, 0.0184, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572002.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3120.5125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.8129, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6237.9917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1072.2784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-663.8073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2907],
        [ 0.2632],
        [ 0.2347],
        ...,
        [-5.2613],
        [-5.2546],
        [-5.2538]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309927.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0338],
        [1.0342],
        [1.0360],
        ...,
        [0.9976],
        [0.9965],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370145.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0342],
        [1.0361],
        ...,
        [0.9975],
        [0.9964],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370149.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106,  0.0089, -0.0007,  ...,  0.0181, -0.0004,  0.0060],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3593.5659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.4212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4590, device='cuda:0')



h[100].sum tensor(121.3454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2689, device='cuda:0')



h[200].sum tensor(51.7635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0223, 0.0196, 0.0000,  ..., 0.0396, 0.0000, 0.0126],
        [0.0113, 0.0110, 0.0000,  ..., 0.0218, 0.0000, 0.0063],
        [0.0005, 0.0024, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66933.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0599, 0.1200, 0.1559,  ..., 0.0141, 0.0644, 0.0000],
        [0.0340, 0.1073, 0.0906,  ..., 0.0039, 0.0444, 0.0000],
        [0.0138, 0.0970, 0.0396,  ..., 0.0000, 0.0288, 0.0000],
        ...,
        [0.0016, 0.0917, 0.0086,  ..., 0.0000, 0.0186, 0.0000],
        [0.0016, 0.0918, 0.0086,  ..., 0.0000, 0.0186, 0.0000],
        [0.0016, 0.0917, 0.0086,  ..., 0.0000, 0.0186, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561369.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2942.6619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.9132, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6544.8838, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1041.4967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-639.0933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7334],
        [-1.5443],
        [-2.3392],
        ...,
        [-5.2528],
        [-5.2463],
        [-5.2457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330131., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0342],
        [1.0361],
        ...,
        [0.9975],
        [0.9964],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370149.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0339],
        [1.0343],
        [1.0361],
        ...,
        [0.9975],
        [0.9963],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370153.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115,  0.0096, -0.0007,  ...,  0.0196, -0.0004,  0.0066],
        [ 0.0051,  0.0045, -0.0006,  ...,  0.0091, -0.0002,  0.0025],
        [ 0.0066,  0.0057, -0.0007,  ...,  0.0115, -0.0002,  0.0034],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3479.5791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.2872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5573, device='cuda:0')



h[100].sum tensor(120.3183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.5732, device='cuda:0')



h[200].sum tensor(50.4864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0199, 0.0178, 0.0000,  ..., 0.0357, 0.0000, 0.0103],
        [0.0436, 0.0365, 0.0000,  ..., 0.0743, 0.0000, 0.0248],
        [0.0201, 0.0179, 0.0000,  ..., 0.0360, 0.0000, 0.0112],
        ...,
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65076.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0559, 0.1184, 0.1460,  ..., 0.0076, 0.0629, 0.0000],
        [0.0737, 0.1285, 0.1910,  ..., 0.0148, 0.0771, 0.0000],
        [0.0526, 0.1175, 0.1379,  ..., 0.0072, 0.0605, 0.0000],
        ...,
        [0.0017, 0.0918, 0.0089,  ..., 0.0000, 0.0189, 0.0000],
        [0.0017, 0.0918, 0.0089,  ..., 0.0000, 0.0189, 0.0000],
        [0.0017, 0.0917, 0.0089,  ..., 0.0000, 0.0189, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555288.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2834.7483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(305.7708, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6749.2666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1017.5786, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.4113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3874],
        [-0.0668],
        [-0.3347],
        ...,
        [-5.2362],
        [-5.2299],
        [-5.2294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331180.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0339],
        [1.0343],
        [1.0361],
        ...,
        [0.9975],
        [0.9963],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370153.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0343],
        [1.0361],
        ...,
        [0.9974],
        [0.9963],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370157.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0051, -0.0006,  ...,  0.0103, -0.0002,  0.0030],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3604.7148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3245, device='cuda:0')



h[100].sum tensor(120.5644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.8669, device='cuda:0')



h[200].sum tensor(52.4409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0204, 0.0182, 0.0000,  ..., 0.0365, 0.0000, 0.0114],
        [0.0065, 0.0072, 0.0000,  ..., 0.0137, 0.0000, 0.0031],
        [0.0006, 0.0025, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67737.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0537, 0.1177, 0.1415,  ..., 0.0077, 0.0612, 0.0000],
        [0.0295, 0.1055, 0.0803,  ..., 0.0011, 0.0420, 0.0000],
        [0.0123, 0.0965, 0.0366,  ..., 0.0000, 0.0280, 0.0000],
        ...,
        [0.0015, 0.0921, 0.0094,  ..., 0.0000, 0.0191, 0.0000],
        [0.0015, 0.0922, 0.0094,  ..., 0.0000, 0.0191, 0.0000],
        [0.0015, 0.0921, 0.0094,  ..., 0.0000, 0.0191, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563918.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2989.5771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.5189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6497.8306, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1060.9015, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-647.3815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3734],
        [-1.1341],
        [-1.9858],
        ...,
        [-5.1553],
        [-5.1962],
        [-5.2095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300786.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0343],
        [1.0361],
        ...,
        [0.9974],
        [0.9963],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370157.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0340],
        [1.0343],
        [1.0362],
        ...,
        [0.9973],
        [0.9962],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370161.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0049, -0.0006,  ...,  0.0099, -0.0002,  0.0028],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3654.3110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4318, device='cuda:0')



h[100].sum tensor(121.0081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.1875, device='cuda:0')



h[200].sum tensor(52.8210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1635, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0144, 0.0135, 0.0000,  ..., 0.0269, 0.0000, 0.0069],
        [0.0107, 0.0106, 0.0000,  ..., 0.0208, 0.0000, 0.0052],
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70141.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0397, 0.1117, 0.1082,  ..., 0.0006, 0.0513, 0.0000],
        [0.0290, 0.1065, 0.0809,  ..., 0.0000, 0.0424, 0.0000],
        [0.0119, 0.0973, 0.0372,  ..., 0.0000, 0.0282, 0.0000],
        ...,
        [0.0012, 0.0928, 0.0097,  ..., 0.0000, 0.0190, 0.0000],
        [0.0012, 0.0929, 0.0097,  ..., 0.0000, 0.0190, 0.0000],
        [0.0012, 0.0928, 0.0097,  ..., 0.0000, 0.0190, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581163.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3126.3167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(349.3352, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6450.7314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1097.6779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-674.0389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7407],
        [-1.3276],
        [-2.1822],
        ...,
        [-5.2487],
        [-5.2426],
        [-5.2423]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316092.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0340],
        [1.0343],
        [1.0362],
        ...,
        [0.9973],
        [0.9962],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370161.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0341],
        [1.0344],
        [1.0362],
        ...,
        [0.9973],
        [0.9962],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370165.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0241,  0.0195, -0.0008,  ...,  0.0402, -0.0008,  0.0148],
        [ 0.0386,  0.0310, -0.0009,  ...,  0.0640, -0.0013,  0.0242],
        [ 0.0275,  0.0222, -0.0008,  ...,  0.0457, -0.0010,  0.0170],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3268.9805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9222, device='cuda:0')



h[100].sum tensor(119.7775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6847, device='cuda:0')



h[200].sum tensor(46.9317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1069, 0.0866, 0.0000,  ..., 0.1782, 0.0000, 0.0660],
        [0.1380, 0.1111, 0.0000,  ..., 0.2289, 0.0000, 0.0861],
        [0.1460, 0.1175, 0.0000,  ..., 0.2421, 0.0000, 0.0913],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61827.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2151, 0.1980, 0.5589,  ..., 0.1049, 0.1876, 0.0000],
        [0.2724, 0.2265, 0.7057,  ..., 0.1391, 0.2329, 0.0000],
        [0.2809, 0.2313, 0.7277,  ..., 0.1433, 0.2399, 0.0000],
        ...,
        [0.0009, 0.0935, 0.0101,  ..., 0.0000, 0.0190, 0.0000],
        [0.0009, 0.0935, 0.0101,  ..., 0.0000, 0.0190, 0.0000],
        [0.0009, 0.0934, 0.0101,  ..., 0.0000, 0.0190, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541735.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2402.0029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(276.6341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6694.1006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(982.8696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-587.4554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2130],
        [ 0.2332],
        [ 0.2346],
        ...,
        [-5.2652],
        [-5.2591],
        [-5.2589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-343445., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0341],
        [1.0344],
        [1.0362],
        ...,
        [0.9973],
        [0.9962],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370165.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0342],
        [1.0344],
        [1.0363],
        ...,
        [0.9973],
        [0.9961],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370169.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115,  0.0096, -0.0007,  ...,  0.0197, -0.0004,  0.0067],
        [ 0.0056,  0.0050, -0.0006,  ...,  0.0100, -0.0002,  0.0029],
        [ 0.0060,  0.0053, -0.0007,  ...,  0.0107, -0.0002,  0.0031],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3827.3882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5078, device='cuda:0')



h[100].sum tensor(121.6849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.4046, device='cuda:0')



h[200].sum tensor(55.4142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2835, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0178, 0.0000,  ..., 0.0359, 0.0000, 0.0104],
        [0.0414, 0.0348, 0.0000,  ..., 0.0711, 0.0000, 0.0236],
        [0.0200, 0.0180, 0.0000,  ..., 0.0363, 0.0000, 0.0112],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71842.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0526, 0.1198, 0.1440,  ..., 0.0064, 0.0614, 0.0000],
        [0.0683, 0.1289, 0.1842,  ..., 0.0127, 0.0739, 0.0000],
        [0.0484, 0.1182, 0.1331,  ..., 0.0065, 0.0579, 0.0000],
        ...,
        [0.0008, 0.0936, 0.0106,  ..., 0.0000, 0.0192, 0.0000],
        [0.0008, 0.0936, 0.0106,  ..., 0.0000, 0.0192, 0.0000],
        [0.0008, 0.0936, 0.0106,  ..., 0.0000, 0.0192, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581847.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3059.7534, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.9258, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6195.0498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1131.2550, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-697.7493, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4980],
        [-0.4360],
        [-1.0206],
        ...,
        [-5.2554],
        [-5.2495],
        [-5.2495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289910.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0342],
        [1.0344],
        [1.0363],
        ...,
        [0.9973],
        [0.9961],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370169.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0345],
        [1.0364],
        ...,
        [0.9972],
        [0.9961],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370174.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0186,  0.0152, -0.0007,  ...,  0.0312, -0.0006,  0.0113],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3949.8953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.8395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2740, device='cuda:0')



h[100].sum tensor(121.6020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.6951, device='cuda:0')



h[200].sum tensor(57.6549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0706, 0.0579, 0.0000,  ..., 0.1187, 0.0000, 0.0432],
        [0.0484, 0.0404, 0.0000,  ..., 0.0824, 0.0000, 0.0288],
        [0.0292, 0.0252, 0.0000,  ..., 0.0510, 0.0000, 0.0164],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73283.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1799, 0.1819, 0.4698,  ..., 0.0816, 0.1604, 0.0000],
        [0.1518, 0.1693, 0.3977,  ..., 0.0638, 0.1386, 0.0000],
        [0.1231, 0.1559, 0.3242,  ..., 0.0454, 0.1166, 0.0000],
        ...,
        [0.0008, 0.0933, 0.0107,  ..., 0.0000, 0.0196, 0.0000],
        [0.0008, 0.0933, 0.0107,  ..., 0.0000, 0.0196, 0.0000],
        [0.0008, 0.0933, 0.0107,  ..., 0.0000, 0.0196, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585654.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3151.7195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(378.3549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6145.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1152.7726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-711.3450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2077],
        [ 0.2160],
        [ 0.2260],
        ...,
        [-5.2377],
        [-5.2320],
        [-5.2321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-270044.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0345],
        [1.0364],
        ...,
        [0.9972],
        [0.9961],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370174.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0343],
        [1.0345],
        [1.0364],
        ...,
        [0.9972],
        [0.9961],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370174.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3805.5396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4948, device='cuda:0')



h[100].sum tensor(121.0122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.3654, device='cuda:0')



h[200].sum tensor(55.5489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0026, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0068, 0.0075, 0.0000,  ..., 0.0144, 0.0000, 0.0033],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71084.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0919, 0.0151,  ..., 0.0000, 0.0208, 0.0000],
        [0.0078, 0.0955, 0.0287,  ..., 0.0000, 0.0253, 0.0000],
        [0.0229, 0.1039, 0.0676,  ..., 0.0000, 0.0379, 0.0000],
        ...,
        [0.0008, 0.0933, 0.0107,  ..., 0.0000, 0.0196, 0.0000],
        [0.0008, 0.0933, 0.0107,  ..., 0.0000, 0.0196, 0.0000],
        [0.0008, 0.0933, 0.0107,  ..., 0.0000, 0.0196, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580038.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3000.7783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(361.5282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6399.7671, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1117.2931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-685.2892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2048],
        [-2.3687],
        [-1.4405],
        ...,
        [-5.2377],
        [-5.2320],
        [-5.2321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306152.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0343],
        [1.0345],
        [1.0364],
        ...,
        [0.9972],
        [0.9961],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370174.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0344],
        [1.0346],
        [1.0365],
        ...,
        [0.9972],
        [0.9960],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370178.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072,  0.0062, -0.0007,  ...,  0.0125, -0.0002,  0.0039],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0072,  0.0062, -0.0007,  ...,  0.0125, -0.0002,  0.0039],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5395.7158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(48.0086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.4076, device='cuda:0')



h[100].sum tensor(127.1406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(64.0015, device='cuda:0')



h[200].sum tensor(79.0197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.3877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0145, 0.0000,  ..., 0.0286, 0.0000, 0.0083],
        [0.0316, 0.0271, 0.0000,  ..., 0.0549, 0.0000, 0.0172],
        [0.0066, 0.0074, 0.0000,  ..., 0.0140, 0.0000, 0.0031],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(102696.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0464, 0.1152, 0.1276,  ..., 0.0052, 0.0571, 0.0000],
        [0.0533, 0.1197, 0.1455,  ..., 0.0052, 0.0630, 0.0000],
        [0.0326, 0.1088, 0.0923,  ..., 0.0015, 0.0462, 0.0000],
        ...,
        [0.0008, 0.0930, 0.0106,  ..., 0.0000, 0.0199, 0.0000],
        [0.0009, 0.0930, 0.0106,  ..., 0.0000, 0.0199, 0.0000],
        [0.0009, 0.0930, 0.0106,  ..., 0.0000, 0.0199, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(736157.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5244.3950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(642.3598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6451.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1547.8464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1022.7784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1734],
        [-0.1817],
        [-0.6222],
        ...,
        [-5.2311],
        [-5.2254],
        [-5.2257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-299578., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0344],
        [1.0346],
        [1.0365],
        ...,
        [0.9972],
        [0.9960],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370178.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0345],
        [1.0347],
        [1.0365],
        ...,
        [0.9971],
        [0.9960],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370181.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0050, -0.0006,  ...,  0.0099, -0.0002,  0.0028],
        [ 0.0056,  0.0050, -0.0006,  ...,  0.0099, -0.0002,  0.0028],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3870.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.6519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7981, device='cuda:0')



h[100].sum tensor(120.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2724, device='cuda:0')



h[200].sum tensor(56.9876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3159, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0186, 0.0168, 0.0000,  ..., 0.0334, 0.0000, 0.0094],
        [0.0148, 0.0139, 0.0000,  ..., 0.0273, 0.0000, 0.0070],
        [0.0110, 0.0108, 0.0000,  ..., 0.0210, 0.0000, 0.0052],
        ...,
        [0.0007, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71348.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0498, 0.1168, 0.1355,  ..., 0.0024, 0.0611, 0.0000],
        [0.0423, 0.1134, 0.1163,  ..., 0.0003, 0.0552, 0.0000],
        [0.0316, 0.1079, 0.0889,  ..., 0.0000, 0.0462, 0.0000],
        ...,
        [0.0009, 0.0928, 0.0105,  ..., 0.0000, 0.0201, 0.0000],
        [0.0009, 0.0928, 0.0105,  ..., 0.0000, 0.0201, 0.0000],
        [0.0009, 0.0928, 0.0105,  ..., 0.0000, 0.0201, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578852.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3042.5254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(364.8842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6404.4980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1125.8414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-690.1019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0281],
        [-0.2706],
        [-0.8399],
        ...,
        [-5.2301],
        [-5.2239],
        [-5.2235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265697.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0345],
        [1.0347],
        [1.0365],
        ...,
        [0.9971],
        [0.9960],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370181.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0346],
        [1.0348],
        [1.0366],
        ...,
        [0.9971],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370184.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0143,  0.0118, -0.0007,  ...,  0.0242, -0.0005,  0.0085],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3554.2393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.1300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7323, device='cuda:0')



h[100].sum tensor(119.8892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0964, device='cuda:0')



h[200].sum tensor(51.9221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0268, 0.0000,  ..., 0.0542, 0.0000, 0.0169],
        [0.0126, 0.0122, 0.0000,  ..., 0.0239, 0.0000, 0.0063],
        [0.0379, 0.0321, 0.0000,  ..., 0.0651, 0.0000, 0.0212],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66737.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0592, 0.1223, 0.1595,  ..., 0.0072, 0.0681, 0.0000],
        [0.0535, 0.1200, 0.1451,  ..., 0.0061, 0.0635, 0.0000],
        [0.0706, 0.1292, 0.1885,  ..., 0.0127, 0.0772, 0.0000],
        ...,
        [0.0007, 0.0932, 0.0103,  ..., 0.0000, 0.0199, 0.0000],
        [0.0007, 0.0932, 0.0103,  ..., 0.0000, 0.0199, 0.0000],
        [0.0007, 0.0931, 0.0103,  ..., 0.0000, 0.0199, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562421.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2688.7622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.9207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6471.5264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1060.4695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-642.7053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2977],
        [ 0.2949],
        [ 0.3074],
        ...,
        [-5.2667],
        [-5.2587],
        [-5.2542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290893.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0346],
        [1.0348],
        [1.0366],
        ...,
        [0.9971],
        [0.9960],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370184.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0347],
        [1.0349],
        [1.0367],
        ...,
        [0.9970],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370187.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0131,  0.0108, -0.0007,  ...,  0.0222, -0.0004,  0.0077],
        [ 0.0115,  0.0096, -0.0007,  ...,  0.0197, -0.0004,  0.0067],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3833.6416, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7797, device='cuda:0')



h[100].sum tensor(121.9055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2172, device='cuda:0')



h[200].sum tensor(55.3208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0542, 0.0450, 0.0000,  ..., 0.0919, 0.0000, 0.0318],
        [0.0236, 0.0208, 0.0000,  ..., 0.0420, 0.0000, 0.0135],
        [0.0123, 0.0119, 0.0000,  ..., 0.0236, 0.0000, 0.0069],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69600.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0811, 0.1337, 0.2149,  ..., 0.0231, 0.0834, 0.0000],
        [0.0543, 0.1206, 0.1473,  ..., 0.0108, 0.0624, 0.0000],
        [0.0315, 0.1088, 0.0895,  ..., 0.0036, 0.0445, 0.0000],
        ...,
        [0.0005, 0.0936, 0.0101,  ..., 0.0000, 0.0196, 0.0000],
        [0.0005, 0.0936, 0.0101,  ..., 0.0000, 0.0196, 0.0000],
        [0.0005, 0.0936, 0.0101,  ..., 0.0000, 0.0196, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568366.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2637.5347, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.3033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6552.9326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1094.9634, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-674.1907, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9096],
        [-1.3113],
        [-1.8958],
        ...,
        [-5.3202],
        [-5.3144],
        [-5.3148]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332432.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0347],
        [1.0349],
        [1.0367],
        ...,
        [0.9970],
        [0.9959],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370187.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0350],
        [1.0367],
        ...,
        [0.9970],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370190.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0053,  0.0047, -0.0006,  ...,  0.0095, -0.0002,  0.0026],
        [ 0.0127,  0.0106, -0.0007,  ...,  0.0216, -0.0004,  0.0075],
        [ 0.0069,  0.0060, -0.0007,  ...,  0.0121, -0.0002,  0.0037],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3434.4299, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.8312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0537, device='cuda:0')



h[100].sum tensor(120.7934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0674, device='cuda:0')



h[200].sum tensor(49.3396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0098, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0380, 0.0322, 0.0000,  ..., 0.0655, 0.0000, 0.0214],
        [0.0236, 0.0208, 0.0000,  ..., 0.0420, 0.0000, 0.0121],
        [0.0388, 0.0329, 0.0000,  ..., 0.0669, 0.0000, 0.0219],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64948.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0599, 0.1235, 0.1614,  ..., 0.0080, 0.0684, 0.0000],
        [0.0622, 0.1255, 0.1675,  ..., 0.0075, 0.0710, 0.0000],
        [0.0775, 0.1338, 0.2061,  ..., 0.0159, 0.0829, 0.0000],
        ...,
        [0.0015, 0.0942, 0.0127,  ..., 0.0000, 0.0205, 0.0000],
        [0.0004, 0.0936, 0.0100,  ..., 0.0000, 0.0197, 0.0000],
        [0.0004, 0.0936, 0.0100,  ..., 0.0000, 0.0197, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557857.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2466.7788, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.3786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6508.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1032.8317, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-627.5941, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3490],
        [-0.7119],
        [-0.3671],
        ...,
        [-4.4693],
        [-4.8838],
        [-5.1555]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317053.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0350],
        [1.0367],
        ...,
        [0.9970],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370190.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0350],
        [1.0368],
        ...,
        [0.9970],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370193.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3256.8013, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.3885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9041, device='cuda:0')



h[100].sum tensor(120.3424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6307, device='cuda:0')



h[200].sum tensor(46.7441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8816, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0025, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62553.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0913, 0.0096,  ..., 0.0000, 0.0192, 0.0000],
        [0.0009, 0.0924, 0.0118,  ..., 0.0000, 0.0200, 0.0000],
        [0.0022, 0.0933, 0.0150,  ..., 0.0000, 0.0212, 0.0000],
        ...,
        [0.0003, 0.0937, 0.0101,  ..., 0.0000, 0.0198, 0.0000],
        [0.0003, 0.0937, 0.0101,  ..., 0.0000, 0.0199, 0.0000],
        [0.0003, 0.0937, 0.0101,  ..., 0.0000, 0.0198, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548768.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2275.7725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(279.2075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6537.8345, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(998.7827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-601.6857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6342],
        [-4.1765],
        [-3.5367],
        ...,
        [-5.3227],
        [-5.2990],
        [-5.2813]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328513.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0350],
        [1.0368],
        ...,
        [0.9970],
        [0.9959],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370193.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0351],
        [1.0368],
        ...,
        [0.9969],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370197., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3407.7148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8491, device='cuda:0')



h[100].sum tensor(121.2181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.4558, device='cuda:0')



h[200].sum tensor(48.9329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9870, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64440.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0912, 0.0096,  ..., 0.0000, 0.0193, 0.0000],
        [0.0003, 0.0919, 0.0097,  ..., 0.0000, 0.0195, 0.0000],
        [0.0003, 0.0922, 0.0098,  ..., 0.0000, 0.0196, 0.0000],
        ...,
        [0.0003, 0.0936, 0.0101,  ..., 0.0000, 0.0200, 0.0000],
        [0.0003, 0.0937, 0.0101,  ..., 0.0000, 0.0200, 0.0000],
        [0.0003, 0.0936, 0.0101,  ..., 0.0000, 0.0200, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555373.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2332.9070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.4835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6691.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1022.4948, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-621.8035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6139],
        [-4.8832],
        [-5.0800],
        ...,
        [-5.3505],
        [-5.3451],
        [-5.3464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328900.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0351],
        [1.0368],
        ...,
        [0.9969],
        [0.9958],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370197., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0351],
        [1.0368],
        ...,
        [0.9969],
        [0.9958],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370200.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0121,  0.0101, -0.0007,  ...,  0.0207, -0.0004,  0.0071],
        [ 0.0181,  0.0148, -0.0007,  ...,  0.0303, -0.0006,  0.0109],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3257.4155, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(19.5584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.8779, device='cuda:0')



h[100].sum tensor(120.6740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.5521, device='cuda:0')



h[200].sum tensor(46.9101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0353, 0.0301, 0.0000,  ..., 0.0610, 0.0000, 0.0196],
        [0.0293, 0.0254, 0.0000,  ..., 0.0513, 0.0000, 0.0171],
        [0.0282, 0.0245, 0.0000,  ..., 0.0495, 0.0000, 0.0165],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62131.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0588, 0.1222, 0.1588,  ..., 0.0089, 0.0679, 0.0000],
        [0.0699, 0.1282, 0.1866,  ..., 0.0151, 0.0764, 0.0000],
        [0.0787, 0.1331, 0.2088,  ..., 0.0190, 0.0835, 0.0000],
        ...,
        [0.0003, 0.0934, 0.0101,  ..., 0.0000, 0.0203, 0.0000],
        [0.0003, 0.0935, 0.0101,  ..., 0.0000, 0.0203, 0.0000],
        [0.0003, 0.0934, 0.0101,  ..., 0.0000, 0.0203, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546935.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2207.6550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(275.4062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6775.9521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(989.6918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.0504, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2215],
        [ 0.1780],
        [ 0.3327],
        ...,
        [-5.3568],
        [-5.3509],
        [-5.3515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332253., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0351],
        [1.0368],
        ...,
        [0.9969],
        [0.9958],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370200.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0351],
        [1.0368],
        ...,
        [0.9969],
        [0.9958],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370203.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3523.3716, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5326, device='cuda:0')



h[100].sum tensor(121.8177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.4992, device='cuda:0')



h[200].sum tensor(50.9460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0632, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0026, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68631.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0919, 0.0150,  ..., 0.0000, 0.0216, 0.0000],
        [0.0067, 0.0950, 0.0264,  ..., 0.0000, 0.0256, 0.0000],
        [0.0088, 0.0965, 0.0323,  ..., 0.0000, 0.0277, 0.0000],
        ...,
        [0.0004, 0.0932, 0.0102,  ..., 0.0000, 0.0205, 0.0000],
        [0.0004, 0.0932, 0.0102,  ..., 0.0000, 0.0205, 0.0000],
        [0.0004, 0.0932, 0.0102,  ..., 0.0000, 0.0205, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579894.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2729.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.5650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6675.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1079.0364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-666.3018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6332],
        [-2.6052],
        [-2.3837],
        ...,
        [-5.3558],
        [-5.3499],
        [-5.3506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314216.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0351],
        [1.0368],
        ...,
        [0.9969],
        [0.9958],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370203.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 190.0 event: 950 loss: tensor(916.4442, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0352],
        [1.0368],
        ...,
        [0.9968],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370206.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0168,  0.0139, -0.0007,  ...,  0.0283, -0.0005,  0.0101],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3557.8020, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6971, device='cuda:0')



h[100].sum tensor(122.2031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.9909, device='cuda:0')



h[200].sum tensor(51.3994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0816, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0542, 0.0451, 0.0000,  ..., 0.0919, 0.0000, 0.0332],
        [0.0320, 0.0276, 0.0000,  ..., 0.0557, 0.0000, 0.0189],
        [0.0005, 0.0027, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68182.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1274e-01, 1.4640e-01, 2.9517e-01,  ..., 4.2933e-02, 1.0968e-01,
         0.0000e+00],
        [7.8884e-02, 1.3122e-01, 2.0943e-01,  ..., 2.2800e-02, 8.2924e-02,
         0.0000e+00],
        [3.9744e-02, 1.1210e-01, 1.1068e-01,  ..., 2.6976e-03, 5.1975e-02,
         0.0000e+00],
        ...,
        [2.7466e-04, 9.3298e-02, 1.0277e-02,  ..., 0.0000e+00, 2.0644e-02,
         0.0000e+00],
        [2.7537e-04, 9.3319e-02, 1.0283e-02,  ..., 0.0000e+00, 2.0651e-02,
         0.0000e+00],
        [2.7561e-04, 9.3273e-02, 1.0281e-02,  ..., 0.0000e+00, 2.0643e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575416., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2649.6333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.6422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6540.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1075.4867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-662.4028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1258],
        [ 0.0434],
        [-0.0437],
        ...,
        [-5.3676],
        [-5.3617],
        [-5.3624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304641.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0352],
        [1.0368],
        ...,
        [0.9968],
        [0.9957],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370206.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9968],
        [0.9957],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370209.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0191,  0.0157, -0.0007,  ...,  0.0321, -0.0006,  0.0116],
        [ 0.0087,  0.0074, -0.0007,  ...,  0.0150, -0.0003,  0.0048],
        [ 0.0111,  0.0093, -0.0007,  ...,  0.0190, -0.0004,  0.0064],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3985.4116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.4877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5569, device='cuda:0')



h[100].sum tensor(124.3670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5410, device='cuda:0')



h[200].sum tensor(57.4793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4006, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0569, 0.0473, 0.0000,  ..., 0.0964, 0.0000, 0.0336],
        [0.0574, 0.0477, 0.0000,  ..., 0.0973, 0.0000, 0.0339],
        [0.0186, 0.0170, 0.0000,  ..., 0.0341, 0.0000, 0.0103],
        ...,
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74760.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1708, 0.1779, 0.4430,  ..., 0.0711, 0.1566, 0.0000],
        [0.1382, 0.1630, 0.3603,  ..., 0.0518, 0.1309, 0.0000],
        [0.0889, 0.1387, 0.2358,  ..., 0.0244, 0.0921, 0.0000],
        ...,
        [0.0015, 0.0944, 0.0144,  ..., 0.0000, 0.0221, 0.0000],
        [0.0045, 0.0961, 0.0224,  ..., 0.0000, 0.0249, 0.0000],
        [0.0058, 0.0969, 0.0263,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597949., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2972.0295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(382.7290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6306.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1171.8998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-734.6535, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3004],
        [ 0.3033],
        [ 0.3020],
        ...,
        [-4.9694],
        [-4.7060],
        [-4.5393]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293082.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9968],
        [0.9957],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370209.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9968],
        [0.9957],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370213.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3609.9136, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0868, device='cuda:0')



h[100].sum tensor(123.2190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1561, device='cuda:0')



h[200].sum tensor(51.9570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1250, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67308.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0237, 0.1037, 0.0708,  ..., 0.0009, 0.0397, 0.0000],
        [0.0068, 0.0957, 0.0279,  ..., 0.0000, 0.0263, 0.0000],
        [0.0017, 0.0931, 0.0142,  ..., 0.0000, 0.0218, 0.0000],
        ...,
        [0.0001, 0.0937, 0.0105,  ..., 0.0000, 0.0209, 0.0000],
        [0.0001, 0.0937, 0.0105,  ..., 0.0000, 0.0209, 0.0000],
        [0.0002, 0.0937, 0.0105,  ..., 0.0000, 0.0209, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564590.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2365.6650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.7899, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6759.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1064.3109, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-651.6407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2147],
        [-2.2539],
        [-2.9850],
        ...,
        [-5.3891],
        [-5.3832],
        [-5.3839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325632., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9968],
        [0.9957],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370213.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9968],
        [0.9956],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370217.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0101,  0.0086, -0.0007,  ...,  0.0174, -0.0003,  0.0058],
        [ 0.0074,  0.0064, -0.0007,  ...,  0.0130, -0.0002,  0.0041],
        [ 0.0052,  0.0047, -0.0006,  ...,  0.0094, -0.0002,  0.0026],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3422.4365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.3642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8355, device='cuda:0')



h[100].sum tensor(122.4578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.4151, device='cuda:0')



h[200].sum tensor(49.5138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0364, 0.0311, 0.0000,  ..., 0.0629, 0.0000, 0.0204],
        [0.0267, 0.0234, 0.0000,  ..., 0.0472, 0.0000, 0.0142],
        [0.0215, 0.0193, 0.0000,  ..., 0.0388, 0.0000, 0.0108],
        ...,
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64301.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0650, 0.1258, 0.1774,  ..., 0.0105, 0.0744, 0.0000],
        [0.0649, 0.1267, 0.1777,  ..., 0.0087, 0.0752, 0.0000],
        [0.0590, 0.1239, 0.1630,  ..., 0.0058, 0.0710, 0.0000],
        ...,
        [0.0002, 0.0937, 0.0106,  ..., 0.0000, 0.0213, 0.0000],
        [0.0002, 0.0937, 0.0106,  ..., 0.0000, 0.0213, 0.0000],
        [0.0002, 0.0936, 0.0106,  ..., 0.0000, 0.0213, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554642., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2247.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.3912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6762.6001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1027.1825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-618.6616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1430],
        [ 0.3081],
        [ 0.2575],
        ...,
        [-5.3762],
        [-5.3704],
        [-5.3712]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-311127.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9968],
        [0.9956],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370217.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9967],
        [0.9956],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370221.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0146,  0.0121, -0.0007,  ...,  0.0246, -0.0005,  0.0087],
        ...,
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4203.6782, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.6050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6839, device='cuda:0')



h[100].sum tensor(125.8293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.9104, device='cuda:0')



h[200].sum tensor(60.9734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0155, 0.0146, 0.0000,  ..., 0.0288, 0.0000, 0.0090],
        [0.0501, 0.0419, 0.0000,  ..., 0.0853, 0.0000, 0.0306],
        ...,
        [0.0005, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78708.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7171e-02, 9.9871e-02, 5.4764e-02,  ..., 1.0592e-03, 3.4973e-02,
         0.0000e+00],
        [6.4402e-02, 1.2333e-01, 1.7603e-01,  ..., 2.3421e-02, 7.2520e-02,
         0.0000e+00],
        [1.4353e-01, 1.6136e-01, 3.7934e-01,  ..., 6.4358e-02, 1.3550e-01,
         0.0000e+00],
        ...,
        [3.1042e-04, 9.3560e-02, 1.0641e-02,  ..., 0.0000e+00, 2.1634e-02,
         0.0000e+00],
        [3.1109e-04, 9.3581e-02, 1.0646e-02,  ..., 0.0000e+00, 2.1641e-02,
         0.0000e+00],
        [3.1134e-04, 9.3537e-02, 1.0645e-02,  ..., 0.0000e+00, 2.1634e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618779.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3247.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(417.4492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6597.1914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1228.6263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-771.4973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3441],
        [-0.6351],
        [-0.0596],
        ...,
        [-5.3699],
        [-5.3641],
        [-5.3650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293438.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9967],
        [0.9956],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370221.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9967],
        [0.9956],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370225.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3629.8379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.3526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1997, device='cuda:0')



h[100].sum tensor(123.8808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.4937, device='cuda:0')



h[200].sum tensor(52.7844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1376, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68229.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0911, 0.0102,  ..., 0.0000, 0.0212, 0.0000],
        [0.0006, 0.0920, 0.0110,  ..., 0.0000, 0.0217, 0.0000],
        [0.0013, 0.0926, 0.0128,  ..., 0.0000, 0.0225, 0.0000],
        ...,
        [0.0004, 0.0935, 0.0107,  ..., 0.0000, 0.0220, 0.0000],
        [0.0004, 0.0936, 0.0107,  ..., 0.0000, 0.0220, 0.0000],
        [0.0004, 0.0935, 0.0107,  ..., 0.0000, 0.0220, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572684.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2624.8076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.0688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6491.2202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1089.4855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-658.6498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8165],
        [-4.4913],
        [-3.9732],
        ...,
        [-5.3604],
        [-5.3520],
        [-5.3506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275840.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0369],
        ...,
        [0.9967],
        [0.9956],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370225.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0370],
        ...,
        [0.9967],
        [0.9956],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370229.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0172,  0.0142, -0.0007,  ...,  0.0290, -0.0005,  0.0104],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3186.8306, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(18.7383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.1413, device='cuda:0')



h[100].sum tensor(122.9528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.3501, device='cuda:0')



h[200].sum tensor(46.2639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7965, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0501, 0.0420, 0.0000,  ..., 0.0854, 0.0000, 0.0300],
        [0.0328, 0.0282, 0.0000,  ..., 0.0571, 0.0000, 0.0195],
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61238.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1106, 0.1458, 0.2957,  ..., 0.0430, 0.1100, 0.0000],
        [0.0723, 0.1278, 0.1965,  ..., 0.0231, 0.0792, 0.0000],
        [0.0232, 0.1035, 0.0693,  ..., 0.0022, 0.0401, 0.0000],
        ...,
        [0.0005, 0.0934, 0.0104,  ..., 0.0000, 0.0222, 0.0000],
        [0.0005, 0.0934, 0.0104,  ..., 0.0000, 0.0222, 0.0000],
        [0.0005, 0.0934, 0.0104,  ..., 0.0000, 0.0222, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542970., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2119.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.3294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7055.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(986.1008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-580.9506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0172],
        [-0.6174],
        [-1.6889],
        ...,
        [-5.3792],
        [-5.3735],
        [-5.3746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310565.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0353],
        [1.0370],
        ...,
        [0.9967],
        [0.9956],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370229.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0353],
        [1.0370],
        ...,
        [0.9966],
        [0.9955],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370232.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0059,  0.0052, -0.0006,  ...,  0.0105, -0.0002,  0.0031],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5332.7021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(46.3127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2674, device='cuda:0')



h[100].sum tensor(132.3557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(63.5822, device='cuda:0')



h[200].sum tensor(76.9828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.3721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0394, 0.0335, 0.0000,  ..., 0.0679, 0.0000, 0.0224],
        [0.0055, 0.0066, 0.0000,  ..., 0.0125, 0.0000, 0.0025],
        [0.0382, 0.0325, 0.0000,  ..., 0.0659, 0.0000, 0.0223],
        ...,
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(95822.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1042, 0.1428, 0.2789,  ..., 0.0372, 0.1059, 0.0000],
        [0.0931, 0.1380, 0.2502,  ..., 0.0315, 0.0969, 0.0000],
        [0.1327, 0.1573, 0.3524,  ..., 0.0556, 0.1282, 0.0000],
        ...,
        [0.0005, 0.0931, 0.0100,  ..., 0.0000, 0.0223, 0.0000],
        [0.0005, 0.0932, 0.0100,  ..., 0.0000, 0.0224, 0.0000],
        [0.0005, 0.0931, 0.0100,  ..., 0.0000, 0.0224, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682455.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4329.4214, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(561.9561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6302.5146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1472.3771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-953.3058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1145],
        [ 0.0989],
        [ 0.0800],
        ...,
        [-5.3879],
        [-5.3834],
        [-5.3860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267358.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0353],
        [1.0370],
        ...,
        [0.9966],
        [0.9955],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370232.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0354],
        [1.0371],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370235.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0222,  0.0181, -0.0008,  ...,  0.0371, -0.0007,  0.0136],
        [ 0.0240,  0.0196, -0.0008,  ...,  0.0401, -0.0007,  0.0148],
        [ 0.0224,  0.0183, -0.0008,  ...,  0.0374, -0.0007,  0.0137],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3961.6980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.3770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3415, device='cuda:0')



h[100].sum tensor(127.2720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.8969, device='cuda:0')



h[200].sum tensor(57.2390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3765, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0867, 0.0708, 0.0000,  ..., 0.1450, 0.0000, 0.0528],
        [0.1005, 0.0818, 0.0000,  ..., 0.1676, 0.0000, 0.0618],
        [0.1046, 0.0850, 0.0000,  ..., 0.1742, 0.0000, 0.0644],
        ...,
        [0.0006, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75094.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2357, 0.2038, 0.6160,  ..., 0.1200, 0.2089, 0.0000],
        [0.2427, 0.2076, 0.6340,  ..., 0.1243, 0.2146, 0.0000],
        [0.2306, 0.2021, 0.6028,  ..., 0.1166, 0.2053, 0.0000],
        ...,
        [0.0007, 0.0925, 0.0096,  ..., 0.0000, 0.0226, 0.0000],
        [0.0007, 0.0925, 0.0096,  ..., 0.0000, 0.0226, 0.0000],
        [0.0007, 0.0925, 0.0096,  ..., 0.0000, 0.0226, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602724., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3206.4233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(377.3342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6620.2158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1185.5184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-730.7928, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1172],
        [ 0.1227],
        [ 0.1350],
        ...,
        [-5.4095],
        [-5.4038],
        [-5.4049]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264598.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0354],
        [1.0371],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370235.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0354],
        [1.0371],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370238.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0254,  0.0206, -0.0008,  ...,  0.0423, -0.0008,  0.0156],
        [ 0.0178,  0.0146, -0.0007,  ...,  0.0299, -0.0005,  0.0107],
        [ 0.0060,  0.0053, -0.0007,  ...,  0.0107, -0.0002,  0.0031],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3754.2107, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.5216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1055, device='cuda:0')



h[100].sum tensor(127.7161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2016, device='cuda:0')



h[200].sum tensor(53.7323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0949, 0.0772, 0.0000,  ..., 0.1585, 0.0000, 0.0581],
        [0.0536, 0.0446, 0.0000,  ..., 0.0910, 0.0000, 0.0314],
        [0.0460, 0.0386, 0.0000,  ..., 0.0787, 0.0000, 0.0265],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70644.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1912, 0.1830, 0.5014,  ..., 0.0917, 0.1747, 0.0000],
        [0.1457, 0.1626, 0.3837,  ..., 0.0615, 0.1394, 0.0000],
        [0.1167, 0.1493, 0.3088,  ..., 0.0426, 0.1169, 0.0000],
        ...,
        [0.0007, 0.0925, 0.0092,  ..., 0.0000, 0.0224, 0.0000],
        [0.0007, 0.0925, 0.0092,  ..., 0.0000, 0.0224, 0.0000],
        [0.0007, 0.0925, 0.0092,  ..., 0.0000, 0.0224, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583340.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2897.9060, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.0751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6718.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1122.6964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.1913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2718],
        [ 0.2953],
        [ 0.2944],
        ...,
        [-5.4472],
        [-5.4412],
        [-5.4424]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285037.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0354],
        [1.0371],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370238.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 200.0 event: 1000 loss: tensor(480.8932, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0354],
        [1.0372],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370241., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0069,  0.0060, -0.0007,  ...,  0.0122, -0.0002,  0.0037],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3544.4783, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8774, device='cuda:0')



h[100].sum tensor(128.6709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.5302, device='cuda:0')



h[200].sum tensor(49.9296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1017, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0005, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0075, 0.0082, 0.0000,  ..., 0.0160, 0.0000, 0.0038],
        [0.0148, 0.0139, 0.0000,  ..., 0.0279, 0.0000, 0.0078],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69328.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0116, 0.0960, 0.0371,  ..., 0.0000, 0.0311, 0.0000],
        [0.0222, 0.1023, 0.0647,  ..., 0.0000, 0.0401, 0.0000],
        [0.0379, 0.1107, 0.1055,  ..., 0.0030, 0.0535, 0.0000],
        ...,
        [0.0007, 0.0927, 0.0088,  ..., 0.0000, 0.0222, 0.0000],
        [0.0007, 0.0927, 0.0088,  ..., 0.0000, 0.0222, 0.0000],
        [0.0007, 0.0927, 0.0088,  ..., 0.0000, 0.0222, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584478.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2853.4951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.2369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6873.1772, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1100.9791, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-671.0084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1486],
        [-1.3643],
        [-0.5790],
        ...,
        [-5.5013],
        [-5.4950],
        [-5.4960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318810.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0354],
        [1.0372],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370241., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0372],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370244.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0191,  0.0157, -0.0007,  ...,  0.0322, -0.0006,  0.0116],
        [ 0.0177,  0.0146, -0.0007,  ...,  0.0299, -0.0005,  0.0107],
        [ 0.0046,  0.0041, -0.0006,  ...,  0.0084, -0.0001,  0.0022],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3873.3870, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.3353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0477, device='cuda:0')



h[100].sum tensor(131.0508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.0185, device='cuda:0')



h[200].sum tensor(54.2874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0890, 0.0725, 0.0000,  ..., 0.1492, 0.0000, 0.0544],
        [0.0659, 0.0543, 0.0000,  ..., 0.1116, 0.0000, 0.0394],
        [0.0835, 0.0682, 0.0000,  ..., 0.1402, 0.0000, 0.0508],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72209.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2028, 0.1907, 0.5330,  ..., 0.1012, 0.1829, 0.0000],
        [0.1898, 0.1855, 0.4992,  ..., 0.0923, 0.1728, 0.0000],
        [0.1905, 0.1864, 0.5010,  ..., 0.0926, 0.1734, 0.0000],
        ...,
        [0.0007, 0.0929, 0.0086,  ..., 0.0000, 0.0221, 0.0000],
        [0.0007, 0.0929, 0.0086,  ..., 0.0000, 0.0221, 0.0000],
        [0.0007, 0.0929, 0.0086,  ..., 0.0000, 0.0221, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588926.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2909.7332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.9108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6791.2158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1141.1449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-702.7104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1453],
        [ 0.1384],
        [ 0.1367],
        ...,
        [-5.5197],
        [-5.5130],
        [-5.5139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328850.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0372],
        ...,
        [0.9966],
        [0.9955],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370244.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0373],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370247.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0073,  0.0063, -0.0007,  ...,  0.0128, -0.0002,  0.0039],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3686.9138, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.4311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0026, device='cuda:0')



h[100].sum tensor(130.2302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.8940, device='cuda:0')



h[200].sum tensor(51.8298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0079, 0.0084, 0.0000,  ..., 0.0165, 0.0000, 0.0040],
        [0.0066, 0.0074, 0.0000,  ..., 0.0145, 0.0000, 0.0032],
        [0.0317, 0.0272, 0.0000,  ..., 0.0555, 0.0000, 0.0172],
        ...,
        [0.0005, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0005, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71794.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0189, 0.1002, 0.0561,  ..., 0.0000, 0.0372, 0.0000],
        [0.0287, 0.1060, 0.0817,  ..., 0.0009, 0.0458, 0.0000],
        [0.0484, 0.1165, 0.1329,  ..., 0.0038, 0.0623, 0.0000],
        ...,
        [0.0007, 0.0928, 0.0086,  ..., 0.0000, 0.0222, 0.0000],
        [0.0007, 0.0928, 0.0086,  ..., 0.0000, 0.0222, 0.0000],
        [0.0007, 0.0927, 0.0086,  ..., 0.0000, 0.0222, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595850.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2980.8257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.7919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6948.6890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1132.9342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-697.8191, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0225],
        [-1.2800],
        [-0.7973],
        ...,
        [-5.5403],
        [-5.5342],
        [-5.5355]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336738.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0355],
        [1.0373],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370247.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0355],
        [1.0373],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370251.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3834.6570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.9356, device='cuda:0')



h[100].sum tensor(129.8902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.6836, device='cuda:0')



h[200].sum tensor(54.7899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70507.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0905, 0.0105,  ..., 0.0000, 0.0224, 0.0000],
        [0.0011, 0.0910, 0.0093,  ..., 0.0000, 0.0222, 0.0000],
        [0.0019, 0.0916, 0.0115,  ..., 0.0000, 0.0232, 0.0000],
        ...,
        [0.0008, 0.0924, 0.0088,  ..., 0.0000, 0.0225, 0.0000],
        [0.0008, 0.0925, 0.0088,  ..., 0.0000, 0.0225, 0.0000],
        [0.0008, 0.0924, 0.0089,  ..., 0.0000, 0.0225, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578220.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2769.3171, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(330.9335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7031.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1117.1875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-685.4099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0127],
        [-4.3514],
        [-4.4161],
        ...,
        [-5.5155],
        [-5.5112],
        [-5.5147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320391.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0355],
        [1.0373],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370251.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0355],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370254.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0058,  0.0051, -0.0006,  ...,  0.0103, -0.0002,  0.0029],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3478.2117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.2226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6001, device='cuda:0')



h[100].sum tensor(127.8761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.7012, device='cuda:0')



h[200].sum tensor(50.2871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0065, 0.0073, 0.0000,  ..., 0.0139, 0.0000, 0.0030],
        [0.0202, 0.0182, 0.0000,  ..., 0.0364, 0.0000, 0.0111],
        ...,
        [0.0007, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67944.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0440, 0.1122, 0.1210,  ..., 0.0090, 0.0573, 0.0000],
        [0.0525, 0.1177, 0.1428,  ..., 0.0105, 0.0647, 0.0000],
        [0.0668, 0.1257, 0.1799,  ..., 0.0157, 0.0766, 0.0000],
        ...,
        [0.0008, 0.0923, 0.0089,  ..., 0.0000, 0.0226, 0.0000],
        [0.0060, 0.0951, 0.0225,  ..., 0.0000, 0.0270, 0.0000],
        [0.0178, 0.1013, 0.0532,  ..., 0.0000, 0.0368, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576769.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2729.9426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(309.1347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6953.0254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1085.1830, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-660.1703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0686],
        [ 0.1264],
        [ 0.1644],
        ...,
        [-4.8028],
        [-3.8867],
        [-2.6920]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298788., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0355],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370254.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0355],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370258.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0054, -0.0007,  ...,  0.0108, -0.0002,  0.0031],
        [ 0.0058,  0.0051, -0.0006,  ...,  0.0103, -0.0002,  0.0029],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3586.6877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.2494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2577, device='cuda:0')



h[100].sum tensor(128.8260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.6669, device='cuda:0')



h[200].sum tensor(51.6735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0328, 0.0281, 0.0000,  ..., 0.0571, 0.0000, 0.0178],
        [0.0166, 0.0153, 0.0000,  ..., 0.0307, 0.0000, 0.0088],
        [0.0064, 0.0073, 0.0000,  ..., 0.0140, 0.0000, 0.0030],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69146.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.1191, 0.1459,  ..., 0.0062, 0.0660, 0.0000],
        [0.0384, 0.1119, 0.1078,  ..., 0.0031, 0.0541, 0.0000],
        [0.0201, 0.1023, 0.0604,  ..., 0.0000, 0.0391, 0.0000],
        ...,
        [0.0005, 0.0928, 0.0088,  ..., 0.0000, 0.0224, 0.0000],
        [0.0005, 0.0928, 0.0088,  ..., 0.0000, 0.0224, 0.0000],
        [0.0005, 0.0928, 0.0088,  ..., 0.0000, 0.0224, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580430.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2660.8416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.4746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6919.3467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1098.8585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-672.6862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5884],
        [-1.0041],
        [-1.7911],
        ...,
        [-5.5165],
        [-5.5122],
        [-5.5158]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330100.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0355],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370258.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370262.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3483.8088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6286, device='cuda:0')



h[100].sum tensor(128.7917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.7863, device='cuda:0')



h[200].sum tensor(50.1178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67504.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0946, 0.0257,  ..., 0.0000, 0.0278, 0.0000],
        [0.0056, 0.0948, 0.0236,  ..., 0.0000, 0.0272, 0.0000],
        [0.0064, 0.0955, 0.0257,  ..., 0.0000, 0.0279, 0.0000],
        ...,
        [0.0004, 0.0932, 0.0088,  ..., 0.0000, 0.0223, 0.0000],
        [0.0004, 0.0933, 0.0088,  ..., 0.0000, 0.0223, 0.0000],
        [0.0004, 0.0932, 0.0088,  ..., 0.0000, 0.0223, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572730.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2473.4731, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(304.2991, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6795.2920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1077.8394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-657.9376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4422],
        [-2.5618],
        [-2.4813],
        ...,
        [-5.5434],
        [-5.5373],
        [-5.5391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329979.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370262.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370262.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4427.0098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.6080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0574, device='cuda:0')



h[100].sum tensor(132.5836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.0062, device='cuda:0')



h[200].sum tensor(63.6166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83435.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0909, 0.0083,  ..., 0.0000, 0.0216, 0.0000],
        [0.0003, 0.0916, 0.0084,  ..., 0.0000, 0.0217, 0.0000],
        [0.0004, 0.0918, 0.0085,  ..., 0.0000, 0.0219, 0.0000],
        ...,
        [0.0004, 0.0932, 0.0088,  ..., 0.0000, 0.0223, 0.0000],
        [0.0004, 0.0933, 0.0088,  ..., 0.0000, 0.0223, 0.0000],
        [0.0004, 0.0932, 0.0088,  ..., 0.0000, 0.0223, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(642958.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3495.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(444.7654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6731.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1296.7246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-828.3839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9333],
        [-5.2346],
        [-5.4124],
        ...,
        [-5.5395],
        [-5.5359],
        [-5.5396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330221.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0374],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370262.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370265.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0069,  0.0059, -0.0007,  ...,  0.0120, -0.0002,  0.0036],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3409.7954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.2199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.1652, device='cuda:0')



h[100].sum tensor(127.6607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.4009, device='cuda:0')



h[200].sum tensor(49.7348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0223, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0151, 0.0141, 0.0000,  ..., 0.0281, 0.0000, 0.0078],
        [0.0076, 0.0082, 0.0000,  ..., 0.0159, 0.0000, 0.0037],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65654.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0398, 0.1130, 0.1123,  ..., 0.0044, 0.0545, 0.0000],
        [0.0210, 0.1033, 0.0634,  ..., 0.0003, 0.0392, 0.0000],
        [0.0062, 0.0952, 0.0244,  ..., 0.0000, 0.0270, 0.0000],
        ...,
        [0.0004, 0.0933, 0.0091,  ..., 0.0000, 0.0225, 0.0000],
        [0.0004, 0.0933, 0.0091,  ..., 0.0000, 0.0225, 0.0000],
        [0.0004, 0.0933, 0.0091,  ..., 0.0000, 0.0225, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563527.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2328.5818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.3141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6955.8364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1052.6654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-637.9169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3068],
        [-2.4461],
        [-3.6015],
        ...,
        [-5.5311],
        [-5.5250],
        [-5.5263]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324643.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370265.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370269.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0124,  0.0103, -0.0007,  ...,  0.0211, -0.0004,  0.0071],
        [ 0.0108,  0.0091, -0.0007,  ...,  0.0184, -0.0003,  0.0061],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3600.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.3008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3688, device='cuda:0')



h[100].sum tensor(127.3322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.9992, device='cuda:0')



h[200].sum tensor(53.3936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0127, 0.0000,  ..., 0.0250, 0.0000, 0.0074],
        [0.0221, 0.0197, 0.0000,  ..., 0.0395, 0.0000, 0.0123],
        [0.0678, 0.0559, 0.0000,  ..., 0.1141, 0.0000, 0.0402],
        ...,
        [0.0007, 0.0028, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67539.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0324, 0.1083, 0.0932,  ..., 0.0049, 0.0478, 0.0000],
        [0.0601, 0.1236, 0.1653,  ..., 0.0159, 0.0700, 0.0000],
        [0.1123, 0.1504, 0.3012,  ..., 0.0448, 0.1113, 0.0000],
        ...,
        [0.0159, 0.1019, 0.0503,  ..., 0.0000, 0.0354, 0.0000],
        [0.0122, 0.0998, 0.0403,  ..., 0.0000, 0.0323, 0.0000],
        [0.0043, 0.0955, 0.0199,  ..., 0.0000, 0.0259, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565545.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2431.5884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.6431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6918.5747, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1080.1674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-656.2979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0577],
        [-0.3840],
        [ 0.0823],
        ...,
        [-3.1818],
        [-3.6826],
        [-4.3982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300219.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370269.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 210.0 event: 1050 loss: tensor(416.5703, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370272.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3079.7876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.2712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-6.9707, device='cuda:0')



h[100].sum tensor(124.2793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.8399, device='cuda:0')



h[200].sum tensor(46.7797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0029, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61611.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0029, 0.0922, 0.0160,  ..., 0.0000, 0.0243, 0.0000],
        [0.0032, 0.0931, 0.0171,  ..., 0.0000, 0.0247, 0.0000],
        [0.0066, 0.0952, 0.0260,  ..., 0.0000, 0.0274, 0.0000],
        ...,
        [0.0006, 0.0933, 0.0103,  ..., 0.0000, 0.0230, 0.0000],
        [0.0006, 0.0933, 0.0103,  ..., 0.0000, 0.0230, 0.0000],
        [0.0006, 0.0933, 0.0103,  ..., 0.0000, 0.0230, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548462.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2203.5017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.7745, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7088.7886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(999.8295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-593.3698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7410],
        [-2.8201],
        [-2.5479],
        ...,
        [-5.4784],
        [-5.4725],
        [-5.4739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288575.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370272.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370276.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0059,  0.0052, -0.0006,  ...,  0.0105, -0.0002,  0.0029],
        [ 0.0060,  0.0053, -0.0006,  ...,  0.0107, -0.0002,  0.0030],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3694.0093, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9496, device='cuda:0')



h[100].sum tensor(127.0481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.7356, device='cuda:0')



h[200].sum tensor(55.5412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0075, 0.0000,  ..., 0.0141, 0.0000, 0.0030],
        [0.0206, 0.0185, 0.0000,  ..., 0.0369, 0.0000, 0.0112],
        [0.0480, 0.0402, 0.0000,  ..., 0.0817, 0.0000, 0.0273],
        ...,
        [0.0008, 0.0029, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71192.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0262, 0.1065, 0.0788,  ..., 0.0009, 0.0440, 0.0000],
        [0.0532, 0.1221, 0.1493,  ..., 0.0090, 0.0657, 0.0000],
        [0.0819, 0.1381, 0.2243,  ..., 0.0200, 0.0888, 0.0000],
        ...,
        [0.0004, 0.0939, 0.0106,  ..., 0.0000, 0.0229, 0.0000],
        [0.0004, 0.0939, 0.0106,  ..., 0.0000, 0.0229, 0.0000],
        [0.0004, 0.0939, 0.0106,  ..., 0.0000, 0.0229, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582388.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2653.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.8601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6712.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1134.3685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-694.1615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0046],
        [-0.3258],
        [ 0.0935],
        ...,
        [-5.4932],
        [-5.4873],
        [-5.4887]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284350.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0356],
        [1.0375],
        ...,
        [0.9965],
        [0.9954],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370276.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370278.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3757.6255, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.7907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.4798, device='cuda:0')



h[100].sum tensor(127.8643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.3208, device='cuda:0')



h[200].sum tensor(56.1934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72874.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0008, 0.0926, 0.0127,  ..., 0.0000, 0.0225, 0.0000],
        [0.0002, 0.0929, 0.0105,  ..., 0.0000, 0.0220, 0.0000],
        [0.0002, 0.0931, 0.0106,  ..., 0.0000, 0.0221, 0.0000],
        ...,
        [0.0002, 0.0946, 0.0109,  ..., 0.0000, 0.0226, 0.0000],
        [0.0002, 0.0946, 0.0109,  ..., 0.0000, 0.0226, 0.0000],
        [0.0002, 0.0945, 0.0109,  ..., 0.0000, 0.0226, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596938.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2760.7930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.7508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6609.7363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1157.7052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-713.2294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0965],
        [-4.7293],
        [-5.0963],
        ...,
        [-5.5219],
        [-5.5157],
        [-5.5171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308922.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370278.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370282., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0115,  0.0096, -0.0007,  ...,  0.0197, -0.0003,  0.0066],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3573.5322, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.0302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2077, device='cuda:0')



h[100].sum tensor(127.5182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.5176, device='cuda:0')



h[200].sum tensor(53.4322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0119, 0.0000,  ..., 0.0237, 0.0000, 0.0067],
        [0.0195, 0.0177, 0.0000,  ..., 0.0357, 0.0000, 0.0114],
        [0.0903, 0.0737, 0.0000,  ..., 0.1513, 0.0000, 0.0548],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68398.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.3922e-02, 1.3399e-01, 2.0633e-01,  ..., 2.2913e-02, 8.0742e-02,
         0.0000e+00],
        [1.1376e-01, 1.5552e-01, 3.1111e-01,  ..., 4.5161e-02, 1.1224e-01,
         0.0000e+00],
        [1.8420e-01, 1.9275e-01, 4.9647e-01,  ..., 8.7812e-02, 1.6813e-01,
         0.0000e+00],
        ...,
        [8.6836e-05, 9.5059e-02, 1.1203e-02,  ..., 0.0000e+00, 2.2252e-02,
         0.0000e+00],
        [8.7143e-05, 9.5070e-02, 1.1207e-02,  ..., 0.0000e+00, 2.2256e-02,
         0.0000e+00],
        [8.7791e-05, 9.5040e-02, 1.1208e-02,  ..., 0.0000e+00, 2.2253e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570137.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2335.1340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.3113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6551.9697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1099.8884, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-667.9708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1953],
        [ 0.1841],
        [ 0.1732],
        ...,
        [-5.5431],
        [-5.5368],
        [-5.5382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307878.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370282., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370282., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0543,  0.0435, -0.0010,  ...,  0.0896, -0.0015,  0.0342],
        [ 0.0216,  0.0176, -0.0008,  ...,  0.0362, -0.0006,  0.0131],
        [ 0.0114,  0.0096, -0.0007,  ...,  0.0195, -0.0003,  0.0065],
        ...,
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3713.4468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0149, device='cuda:0')



h[100].sum tensor(128.0776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9310, device='cuda:0')



h[200].sum tensor(55.4268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0946, 0.0770, 0.0000,  ..., 0.1583, 0.0000, 0.0576],
        [0.1212, 0.0981, 0.0000,  ..., 0.2017, 0.0000, 0.0748],
        [0.0707, 0.0582, 0.0000,  ..., 0.1193, 0.0000, 0.0422],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72111.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4309e-01, 2.2128e-01, 6.5176e-01,  ..., 1.2539e-01, 2.1473e-01,
         0.0000e+00],
        [2.4910e-01, 2.2512e-01, 6.6797e-01,  ..., 1.2841e-01, 2.2001e-01,
         0.0000e+00],
        [1.8737e-01, 1.9570e-01, 5.0508e-01,  ..., 8.6821e-02, 1.7132e-01,
         0.0000e+00],
        ...,
        [8.6836e-05, 9.5059e-02, 1.1203e-02,  ..., 0.0000e+00, 2.2252e-02,
         0.0000e+00],
        [8.7143e-05, 9.5070e-02, 1.1207e-02,  ..., 0.0000e+00, 2.2256e-02,
         0.0000e+00],
        [8.7791e-05, 9.5040e-02, 1.1208e-02,  ..., 0.0000e+00, 2.2253e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588992.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2635.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.7570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6333.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1153.3514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-707.5649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1627],
        [ 0.1714],
        [ 0.1969],
        ...,
        [-5.5431],
        [-5.5368],
        [-5.5382]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297948.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0356],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370282., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370284.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0306,  0.0247, -0.0008,  ...,  0.0509, -0.0008,  0.0189],
        [ 0.0220,  0.0180, -0.0008,  ...,  0.0369, -0.0006,  0.0134],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0115,  0.0096, -0.0007,  ...,  0.0197, -0.0003,  0.0066]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4517.2412, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.2637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4098, device='cuda:0')



h[100].sum tensor(131.2456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(49.0599, device='cuda:0')



h[200].sum tensor(67.1223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8303, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1194, 0.0967, 0.0000,  ..., 0.1987, 0.0000, 0.0736],
        [0.0585, 0.0485, 0.0000,  ..., 0.0992, 0.0000, 0.0350],
        [0.0321, 0.0276, 0.0000,  ..., 0.0561, 0.0000, 0.0187],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0126, 0.0122, 0.0000,  ..., 0.0242, 0.0000, 0.0069],
        [0.0222, 0.0198, 0.0000,  ..., 0.0400, 0.0000, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(86408.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2480, 0.2222, 0.6633,  ..., 0.1294, 0.2190, 0.0000],
        [0.1528, 0.1747, 0.4130,  ..., 0.0694, 0.1436, 0.0000],
        [0.0803, 0.1373, 0.2223,  ..., 0.0279, 0.0860, 0.0000],
        ...,
        [0.0119, 0.1020, 0.0426,  ..., 0.0000, 0.0319, 0.0000],
        [0.0318, 0.1133, 0.0952,  ..., 0.0041, 0.0478, 0.0000],
        [0.0583, 0.1281, 0.1645,  ..., 0.0138, 0.0689, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656827.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3726.6138, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(480.0117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6064.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1353.8260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-861.8946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1531],
        [-0.1563],
        [-1.0128],
        ...,
        [-2.9849],
        [-2.0482],
        [-0.9992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283471.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0375],
        ...,
        [0.9964],
        [0.9953],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370284.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0376],
        ...,
        [0.9964],
        [0.9952],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370287.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4145.1553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.1930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0407, device='cuda:0')



h[100].sum tensor(129.5734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.9771, device='cuda:0')



h[200].sum tensor(62.1208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5661, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76108.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0118, 0.0993, 0.0416,  ..., 0.0000, 0.0317, 0.0000],
        [0.0119, 0.1001, 0.0421,  ..., 0.0000, 0.0320, 0.0000],
        [0.0118, 0.1003, 0.0418,  ..., 0.0000, 0.0320, 0.0000],
        ...,
        [0.0005, 0.0947, 0.0117,  ..., 0.0000, 0.0227, 0.0000],
        [0.0005, 0.0947, 0.0117,  ..., 0.0000, 0.0227, 0.0000],
        [0.0005, 0.0947, 0.0117,  ..., 0.0000, 0.0227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596611.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2886.8608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(392.8372, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6641.4561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1201.8737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-749.9333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9776],
        [-1.7542],
        [-1.5169],
        ...,
        [-5.5574],
        [-5.5511],
        [-5.5524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301847.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0376],
        ...,
        [0.9964],
        [0.9952],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370287.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0357],
        [1.0376],
        ...,
        [0.9963],
        [0.9952],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370290.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0152,  0.0126, -0.0007,  ...,  0.0257, -0.0004,  0.0089],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4368.2979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.7277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0896, device='cuda:0')



h[100].sum tensor(130.2855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.1127, device='cuda:0')



h[200].sum tensor(65.5737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6830, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0572, 0.0473, 0.0000,  ..., 0.0965, 0.0000, 0.0331],
        [0.0135, 0.0129, 0.0000,  ..., 0.0254, 0.0000, 0.0074],
        [0.0164, 0.0151, 0.0000,  ..., 0.0300, 0.0000, 0.0092],
        ...,
        [0.0008, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83889.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0645, 0.1276, 0.1770,  ..., 0.0137, 0.0728, 0.0000],
        [0.0382, 0.1137, 0.1089,  ..., 0.0037, 0.0522, 0.0000],
        [0.0301, 0.1093, 0.0879,  ..., 0.0032, 0.0459, 0.0000],
        ...,
        [0.0007, 0.0943, 0.0119,  ..., 0.0000, 0.0230, 0.0000],
        [0.0007, 0.0944, 0.0119,  ..., 0.0000, 0.0230, 0.0000],
        [0.0007, 0.0943, 0.0119,  ..., 0.0000, 0.0230, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645247.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3750.0181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(459.2912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6518.1313, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1312.3789, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-835.6035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5976],
        [-2.0408],
        [-2.7057],
        ...,
        [-5.5568],
        [-5.5500],
        [-5.5508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265009.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0357],
        [1.0376],
        ...,
        [0.9963],
        [0.9952],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370290.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0358],
        [1.0376],
        ...,
        [0.9963],
        [0.9952],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370293.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0067,  0.0058, -0.0007,  ...,  0.0117, -0.0002,  0.0034],
        [ 0.0130,  0.0108, -0.0007,  ...,  0.0220, -0.0003,  0.0075],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3234.6685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.0998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2918, device='cuda:0')



h[100].sum tensor(126.5861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.7898, device='cuda:0')



h[200].sum tensor(49.0103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0344, 0.0293, 0.0000,  ..., 0.0595, 0.0000, 0.0192],
        [0.0183, 0.0166, 0.0000,  ..., 0.0332, 0.0000, 0.0097],
        [0.0490, 0.0409, 0.0000,  ..., 0.0834, 0.0000, 0.0278],
        ...,
        [0.0007, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63987.8945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0999, 0.1479, 0.2701,  ..., 0.0318, 0.1016, 0.0000],
        [0.0674, 0.1309, 0.1853,  ..., 0.0142, 0.0759, 0.0000],
        [0.0650, 0.1301, 0.1789,  ..., 0.0125, 0.0738, 0.0000],
        ...,
        [0.0007, 0.0946, 0.0120,  ..., 0.0000, 0.0229, 0.0000],
        [0.0007, 0.0946, 0.0120,  ..., 0.0000, 0.0229, 0.0000],
        [0.0007, 0.0946, 0.0120,  ..., 0.0000, 0.0229, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559112.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2382.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.4335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7027.7783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1028.7856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.5586, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3036],
        [ 0.1842],
        [-0.1192],
        ...,
        [-5.5956],
        [-5.5891],
        [-5.5904]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328643.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0358],
        [1.0376],
        ...,
        [0.9963],
        [0.9952],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370293.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0358],
        [1.0377],
        ...,
        [0.9963],
        [0.9952],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370296.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3846.6487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.1181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0517, device='cuda:0')



h[100].sum tensor(129.9597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.0306, device='cuda:0')



h[200].sum tensor(57.1818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75639.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0926, 0.0117,  ..., 0.0000, 0.0220, 0.0000],
        [0.0006, 0.0933, 0.0118,  ..., 0.0000, 0.0222, 0.0000],
        [0.0007, 0.0936, 0.0119,  ..., 0.0000, 0.0223, 0.0000],
        ...,
        [0.0014, 0.0955, 0.0142,  ..., 0.0000, 0.0234, 0.0000],
        [0.0007, 0.0951, 0.0122,  ..., 0.0000, 0.0228, 0.0000],
        [0.0007, 0.0950, 0.0122,  ..., 0.0000, 0.0228, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610723.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3242.4954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(378.5348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6199.5317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1203.5380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-750.3739, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6105],
        [-5.5725],
        [-5.4592],
        ...,
        [-5.4293],
        [-5.5523],
        [-5.6064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272623.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0358],
        [1.0377],
        ...,
        [0.9963],
        [0.9952],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370296.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 220.0 event: 1100 loss: tensor(436.6419, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0357],
        [1.0377],
        ...,
        [0.9963],
        [0.9952],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370299.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0124,  0.0103, -0.0007,  ...,  0.0211, -0.0003,  0.0071],
        [ 0.0126,  0.0105, -0.0007,  ...,  0.0215, -0.0003,  0.0073],
        [ 0.0167,  0.0137, -0.0007,  ...,  0.0282, -0.0004,  0.0099],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3652.2827, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0188, device='cuda:0')



h[100].sum tensor(130.1685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9424, device='cuda:0')



h[200].sum tensor(53.7587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0578, 0.0477, 0.0000,  ..., 0.0980, 0.0000, 0.0337],
        [0.0552, 0.0457, 0.0000,  ..., 0.0939, 0.0000, 0.0320],
        [0.0590, 0.0487, 0.0000,  ..., 0.1001, 0.0000, 0.0353],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69949.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1052, 0.1530, 0.2886,  ..., 0.0394, 0.1049, 0.0000],
        [0.1250, 0.1645, 0.3411,  ..., 0.0510, 0.1208, 0.0000],
        [0.1341, 0.1686, 0.3655,  ..., 0.0582, 0.1281, 0.0000],
        ...,
        [0.0006, 0.0955, 0.0123,  ..., 0.0000, 0.0226, 0.0000],
        [0.0006, 0.0955, 0.0123,  ..., 0.0000, 0.0226, 0.0000],
        [0.0006, 0.0955, 0.0123,  ..., 0.0000, 0.0226, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580941.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2611.8057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(329.7421, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6805.7446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1108.6023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.2478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2603],
        [ 0.3275],
        [ 0.3311],
        ...,
        [-5.6613],
        [-5.6546],
        [-5.6559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-357010.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0357],
        [1.0377],
        ...,
        [0.9963],
        [0.9952],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370299.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0377],
        ...,
        [0.9963],
        [0.9951],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370303.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0203,  0.0165, -0.0007,  ...,  0.0340, -0.0005,  0.0123],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0087,  0.0074, -0.0007,  ...,  0.0151, -0.0002,  0.0047],
        [ 0.0191,  0.0156, -0.0007,  ...,  0.0320, -0.0005,  0.0115],
        [ 0.0135,  0.0112, -0.0007,  ...,  0.0230, -0.0004,  0.0079]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3846.0864, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.0532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.1249, device='cuda:0')



h[100].sum tensor(131.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.2493, device='cuda:0')



h[200].sum tensor(56.2147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0774, 0.0633, 0.0000,  ..., 0.1302, 0.0000, 0.0474],
        [0.0505, 0.0420, 0.0000,  ..., 0.0863, 0.0000, 0.0299],
        [0.0231, 0.0203, 0.0000,  ..., 0.0414, 0.0000, 0.0130],
        ...,
        [0.0370, 0.0314, 0.0000,  ..., 0.0642, 0.0000, 0.0219],
        [0.0646, 0.0532, 0.0000,  ..., 0.1094, 0.0000, 0.0382],
        [0.0759, 0.0621, 0.0000,  ..., 0.1277, 0.0000, 0.0454]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72643.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2187, 0.2106, 0.5924,  ..., 0.1160, 0.1945, 0.0000],
        [0.1678, 0.1857, 0.4565,  ..., 0.0815, 0.1544, 0.0000],
        [0.1359, 0.1695, 0.3718,  ..., 0.0609, 0.1294, 0.0000],
        ...,
        [0.0844, 0.1434, 0.2350,  ..., 0.0283, 0.0895, 0.0000],
        [0.1185, 0.1628, 0.3254,  ..., 0.0473, 0.1165, 0.0000],
        [0.1216, 0.1640, 0.3336,  ..., 0.0502, 0.1187, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592685.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2755.4915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.3574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6891.6504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1145.8888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-714.1094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0231],
        [ 0.0413],
        [ 0.0539],
        ...,
        [-0.7615],
        [ 0.0192],
        [-0.0566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-369280.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0377],
        ...,
        [0.9963],
        [0.9951],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370303.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0377],
        ...,
        [0.9963],
        [0.9951],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370303.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0270,  0.0219, -0.0008,  ...,  0.0451, -0.0007,  0.0166],
        [ 0.0292,  0.0236, -0.0008,  ...,  0.0486, -0.0008,  0.0180],
        [ 0.0268,  0.0217, -0.0008,  ...,  0.0447, -0.0007,  0.0165],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3521.6465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.9717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1234, device='cuda:0')



h[100].sum tensor(130.0422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.2657, device='cuda:0')



h[200].sum tensor(51.6131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0986, 0.0800, 0.0000,  ..., 0.1647, 0.0000, 0.0602],
        [0.1018, 0.0826, 0.0000,  ..., 0.1701, 0.0000, 0.0623],
        [0.0941, 0.0765, 0.0000,  ..., 0.1574, 0.0000, 0.0573],
        ...,
        [0.0005, 0.0025, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0005, 0.0025, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68217.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1944, 0.1978, 0.5279,  ..., 0.1004, 0.1755, 0.0000],
        [0.1884, 0.1952, 0.5117,  ..., 0.0970, 0.1706, 0.0000],
        [0.1662, 0.1841, 0.4527,  ..., 0.0826, 0.1531, 0.0000],
        ...,
        [0.0006, 0.0956, 0.0123,  ..., 0.0000, 0.0226, 0.0000],
        [0.0006, 0.0957, 0.0123,  ..., 0.0000, 0.0226, 0.0000],
        [0.0006, 0.0956, 0.0123,  ..., 0.0000, 0.0226, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576659.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2553.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(309.8931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6729.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1088.3873, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.1879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2481],
        [ 0.2227],
        [ 0.0874],
        ...,
        [-5.6778],
        [-5.6712],
        [-5.6725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344178.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0377],
        ...,
        [0.9963],
        [0.9951],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370303.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0378],
        ...,
        [0.9962],
        [0.9951],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370306.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0071,  0.0061, -0.0007,  ...,  0.0124, -0.0002,  0.0037],
        [ 0.0110,  0.0092, -0.0007,  ...,  0.0188, -0.0003,  0.0062],
        [ 0.0232,  0.0188, -0.0008,  ...,  0.0387, -0.0006,  0.0141],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3696.0225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2392, device='cuda:0')



h[100].sum tensor(130.5553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6013, device='cuda:0')



h[200].sum tensor(54.2785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0263, 0.0229, 0.0000,  ..., 0.0466, 0.0000, 0.0142],
        [0.0600, 0.0496, 0.0000,  ..., 0.1017, 0.0000, 0.0352],
        [0.0760, 0.0622, 0.0000,  ..., 0.1277, 0.0000, 0.0455],
        ...,
        [0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71445.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0786, 0.1381, 0.2184,  ..., 0.0224, 0.0855, 0.0000],
        [0.1303, 0.1666, 0.3556,  ..., 0.0531, 0.1261, 0.0000],
        [0.1660, 0.1859, 0.4505,  ..., 0.0766, 0.1538, 0.0000],
        ...,
        [0.0007, 0.0953, 0.0122,  ..., 0.0000, 0.0230, 0.0000],
        [0.0007, 0.0953, 0.0122,  ..., 0.0000, 0.0230, 0.0000],
        [0.0007, 0.0953, 0.0122,  ..., 0.0000, 0.0230, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590380.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2828.7830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.3737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6789.8813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1131.9890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-702.0769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0294],
        [ 0.2477],
        [ 0.2762],
        ...,
        [-5.6752],
        [-5.6683],
        [-5.6687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335567., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0378],
        ...,
        [0.9962],
        [0.9951],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370306.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0379],
        ...,
        [0.9962],
        [0.9951],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370309.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106,  0.0089, -0.0007,  ...,  0.0182, -0.0003,  0.0060],
        [ 0.0197,  0.0161, -0.0007,  ...,  0.0329, -0.0005,  0.0118],
        [ 0.0050,  0.0045, -0.0006,  ...,  0.0090, -0.0001,  0.0023],
        ...,
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0006, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3530.9883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.6809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1162, device='cuda:0')



h[100].sum tensor(128.8630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.2440, device='cuda:0')



h[200].sum tensor(52.6702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0828, 0.0676, 0.0000,  ..., 0.1385, 0.0000, 0.0498],
        [0.0457, 0.0383, 0.0000,  ..., 0.0779, 0.0000, 0.0258],
        [0.0429, 0.0361, 0.0000,  ..., 0.0733, 0.0000, 0.0240],
        ...,
        [0.0007, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68873.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1392, 0.1680, 0.3756,  ..., 0.0587, 0.1325, 0.0000],
        [0.1170, 0.1577, 0.3168,  ..., 0.0421, 0.1160, 0.0000],
        [0.0983, 0.1481, 0.2674,  ..., 0.0293, 0.1020, 0.0000],
        ...,
        [0.0012, 0.0945, 0.0119,  ..., 0.0000, 0.0235, 0.0000],
        [0.0012, 0.0945, 0.0120,  ..., 0.0000, 0.0235, 0.0000],
        [0.0012, 0.0945, 0.0120,  ..., 0.0000, 0.0235, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582287.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2795.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.9966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7086.8892, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1091.2545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-670.8352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3298],
        [ 0.3337],
        [ 0.3118],
        ...,
        [-5.6533],
        [-5.6468],
        [-5.6482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328726.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0379],
        ...,
        [0.9962],
        [0.9951],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370309.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0379],
        ...,
        [0.9962],
        [0.9951],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370313.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0260,  0.0211, -0.0008,  ...,  0.0432, -0.0007,  0.0159],
        [ 0.0269,  0.0218, -0.0008,  ...,  0.0447, -0.0007,  0.0165],
        [ 0.0122,  0.0102, -0.0007,  ...,  0.0206, -0.0003,  0.0070],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3662.0679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.3212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0519, device='cuda:0')



h[100].sum tensor(128.3229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.0415, device='cuda:0')



h[200].sum tensor(55.2179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0823, 0.0672, 0.0000,  ..., 0.1373, 0.0000, 0.0494],
        [0.0831, 0.0679, 0.0000,  ..., 0.1387, 0.0000, 0.0499],
        [0.0794, 0.0650, 0.0000,  ..., 0.1327, 0.0000, 0.0475],
        ...,
        [0.0008, 0.0028, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0028, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0028, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71183.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1449, 0.1675, 0.3896,  ..., 0.0661, 0.1366, 0.0000],
        [0.1468, 0.1693, 0.3949,  ..., 0.0669, 0.1385, 0.0000],
        [0.1283, 0.1601, 0.3460,  ..., 0.0548, 0.1243, 0.0000],
        ...,
        [0.0016, 0.0938, 0.0117,  ..., 0.0000, 0.0239, 0.0000],
        [0.0016, 0.0938, 0.0117,  ..., 0.0000, 0.0239, 0.0000],
        [0.0016, 0.0938, 0.0117,  ..., 0.0000, 0.0239, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591399.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3086.6084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.1697, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6885.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1127.5057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-695.8523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3426],
        [ 0.3441],
        [ 0.2838],
        ...,
        [-5.6264],
        [-5.6040],
        [-5.5443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276704.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0357],
        [1.0379],
        ...,
        [0.9962],
        [0.9951],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370313.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0357],
        [1.0380],
        ...,
        [0.9962],
        [0.9950],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370316.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0053, -0.0007,  ...,  0.0107, -0.0002,  0.0030],
        [ 0.0057,  0.0050, -0.0006,  ...,  0.0101, -0.0001,  0.0028],
        [ 0.0116,  0.0097, -0.0007,  ...,  0.0197, -0.0003,  0.0066],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3187.7271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.4548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9502, device='cuda:0')



h[100].sum tensor(126.8393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.7684, device='cuda:0')



h[200].sum tensor(48.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8867, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0181, 0.0000,  ..., 0.0360, 0.0000, 0.0109],
        [0.0418, 0.0352, 0.0000,  ..., 0.0714, 0.0000, 0.0232],
        [0.0204, 0.0183, 0.0000,  ..., 0.0365, 0.0000, 0.0102],
        ...,
        [0.0007, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63654.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0503, 0.1210, 0.1422,  ..., 0.0076, 0.0632, 0.0000],
        [0.0708, 0.1334, 0.1962,  ..., 0.0150, 0.0796, 0.0000],
        [0.0554, 0.1249, 0.1553,  ..., 0.0083, 0.0675, 0.0000],
        ...,
        [0.0012, 0.0946, 0.0124,  ..., 0.0000, 0.0238, 0.0000],
        [0.0012, 0.0946, 0.0124,  ..., 0.0000, 0.0238, 0.0000],
        [0.0012, 0.0946, 0.0124,  ..., 0.0000, 0.0238, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561676.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2420.4485, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(275.3326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7237.6792, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1016.1303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.4951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3140],
        [-0.0598],
        [-0.3657],
        ...,
        [-5.6456],
        [-5.6388],
        [-5.6397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342307.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0357],
        [1.0380],
        ...,
        [0.9962],
        [0.9950],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370316.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0357],
        [1.0381],
        ...,
        [0.9961],
        [0.9950],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370319.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3911.0825, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.3987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4455, device='cuda:0')



h[100].sum tensor(130.1928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.2080, device='cuda:0')



h[200].sum tensor(58.4548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0026, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74745.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0948, 0.0202,  ..., 0.0000, 0.0253, 0.0000],
        [0.0043, 0.0960, 0.0226,  ..., 0.0000, 0.0262, 0.0000],
        [0.0050, 0.0968, 0.0245,  ..., 0.0000, 0.0269, 0.0000],
        ...,
        [0.0069, 0.0992, 0.0300,  ..., 0.0000, 0.0287, 0.0000],
        [0.0021, 0.0963, 0.0171,  ..., 0.0000, 0.0248, 0.0000],
        [0.0007, 0.0954, 0.0132,  ..., 0.0000, 0.0237, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603582.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2988.5305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(368.4855, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6523.7476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1179.5912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-736.5132, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0167],
        [-3.7508],
        [-3.5002],
        ...,
        [-4.7904],
        [-5.2650],
        [-5.5194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309273.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0357],
        [1.0381],
        ...,
        [0.9961],
        [0.9950],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370319.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0357],
        [1.0382],
        ...,
        [0.9961],
        [0.9950],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370321.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0120,  0.0100, -0.0007,  ...,  0.0205, -0.0003,  0.0069],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3365.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.3793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0705, device='cuda:0')



h[100].sum tensor(127.9990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1177, device='cuda:0')



h[200].sum tensor(50.9969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0196, 0.0177, 0.0000,  ..., 0.0356, 0.0000, 0.0107],
        [0.0129, 0.0124, 0.0000,  ..., 0.0246, 0.0000, 0.0072],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67281.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0764, 0.1364, 0.2181,  ..., 0.0283, 0.0836, 0.0000],
        [0.0547, 0.1243, 0.1597,  ..., 0.0177, 0.0663, 0.0000],
        [0.0361, 0.1139, 0.1099,  ..., 0.0096, 0.0517, 0.0000],
        ...,
        [0.0006, 0.0955, 0.0135,  ..., 0.0000, 0.0238, 0.0000],
        [0.0006, 0.0955, 0.0135,  ..., 0.0000, 0.0238, 0.0000],
        [0.0006, 0.0955, 0.0135,  ..., 0.0000, 0.0237, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576691., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2464.5017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(304.6843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6829.7256, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1070.1278, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-654.2719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0077],
        [-0.0525],
        [-0.1059],
        ...,
        [-5.6692],
        [-5.6626],
        [-5.6639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350218.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0357],
        [1.0382],
        ...,
        [0.9961],
        [0.9950],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370321.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0358],
        [1.0383],
        ...,
        [0.9961],
        [0.9949],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370324.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0070,  0.0061, -0.0007,  ...,  0.0123, -0.0002,  0.0037],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3691.6924, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.8050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1043, device='cuda:0')



h[100].sum tensor(128.9435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.1982, device='cuda:0')



h[200].sum tensor(56.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0117, 0.0000,  ..., 0.0230, 0.0000, 0.0057],
        [0.0077, 0.0083, 0.0000,  ..., 0.0161, 0.0000, 0.0038],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74275.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0271, 0.1095, 0.0848,  ..., 0.0000, 0.0455, 0.0000],
        [0.0179, 0.1044, 0.0602,  ..., 0.0000, 0.0379, 0.0000],
        [0.0091, 0.0994, 0.0366,  ..., 0.0000, 0.0309, 0.0000],
        ...,
        [0.0006, 0.0953, 0.0137,  ..., 0.0000, 0.0240, 0.0000],
        [0.0006, 0.0953, 0.0137,  ..., 0.0000, 0.0240, 0.0000],
        [0.0006, 0.0953, 0.0137,  ..., 0.0000, 0.0240, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615713.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3106.1167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(364.4359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6347.3076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1173.8873, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-731.8333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8498],
        [-3.0790],
        [-3.0574],
        ...,
        [-5.6625],
        [-5.6559],
        [-5.6571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312623.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0358],
        [1.0383],
        ...,
        [0.9961],
        [0.9949],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370324.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 230.0 event: 1150 loss: tensor(413.3329, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0358],
        [1.0384],
        ...,
        [0.9960],
        [0.9948],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370326.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115,  0.0096, -0.0007,  ...,  0.0195, -0.0003,  0.0065],
        [ 0.0198,  0.0162, -0.0007,  ...,  0.0332, -0.0005,  0.0119],
        [ 0.0065,  0.0057, -0.0007,  ...,  0.0114, -0.0002,  0.0033],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3682.1819, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0069, device='cuda:0')



h[100].sum tensor(128.3524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9070, device='cuda:0')



h[200].sum tensor(57.0031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0927, 0.0756, 0.0000,  ..., 0.1549, 0.0000, 0.0562],
        [0.0649, 0.0536, 0.0000,  ..., 0.1094, 0.0000, 0.0382],
        [0.0355, 0.0303, 0.0000,  ..., 0.0613, 0.0000, 0.0208],
        ...,
        [0.0007, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73152.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1682, 0.1851, 0.4620,  ..., 0.0811, 0.1574, 0.0000],
        [0.1412, 0.1714, 0.3893,  ..., 0.0626, 0.1365, 0.0000],
        [0.0942, 0.1458, 0.2634,  ..., 0.0352, 0.0993, 0.0000],
        ...,
        [0.0009, 0.0945, 0.0134,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0945, 0.0134,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0945, 0.0134,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603927.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3114.6587, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(354.7649, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6138.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1161.7855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-719.8002, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2241],
        [ 0.1590],
        [-0.0986],
        ...,
        [-5.6501],
        [-5.6436],
        [-5.6449]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272929.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0358],
        [1.0384],
        ...,
        [0.9960],
        [0.9948],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370326.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0358],
        [1.0385],
        ...,
        [0.9960],
        [0.9948],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370327.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0109,  0.0092, -0.0007,  ...,  0.0186, -0.0003,  0.0061],
        [ 0.0211,  0.0172, -0.0007,  ...,  0.0352, -0.0005,  0.0127],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3916.9966, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.1257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6566, device='cuda:0')



h[100].sum tensor(129.2376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.8389, device='cuda:0')



h[200].sum tensor(60.6802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4117, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0208, 0.0186, 0.0000,  ..., 0.0372, 0.0000, 0.0112],
        [0.0401, 0.0340, 0.0000,  ..., 0.0688, 0.0000, 0.0229],
        [0.0599, 0.0496, 0.0000,  ..., 0.1011, 0.0000, 0.0348],
        ...,
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76425.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0542, 0.1227, 0.1536,  ..., 0.0131, 0.0670, 0.0000],
        [0.0991, 0.1481, 0.2737,  ..., 0.0359, 0.1032, 0.0000],
        [0.1405, 0.1712, 0.3846,  ..., 0.0609, 0.1364, 0.0000],
        ...,
        [0.0064, 0.0973, 0.0270,  ..., 0.0000, 0.0291, 0.0000],
        [0.0011, 0.0941, 0.0131,  ..., 0.0000, 0.0247, 0.0000],
        [0.0011, 0.0941, 0.0131,  ..., 0.0000, 0.0247, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614940.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3310.7046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(384.1051, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6136.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1203.5760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-754.0354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0629],
        [ 0.1745],
        [ 0.2415],
        ...,
        [-4.5044],
        [-5.1489],
        [-5.4809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271536.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0358],
        [1.0385],
        ...,
        [0.9960],
        [0.9948],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370327.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0359],
        [1.0385],
        ...,
        [0.9959],
        [0.9947],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370329.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0055, -0.0007,  ...,  0.0111, -0.0002,  0.0031],
        [ 0.0159,  0.0131, -0.0007,  ...,  0.0268, -0.0004,  0.0093],
        [ 0.0194,  0.0159, -0.0007,  ...,  0.0325, -0.0005,  0.0116],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4050.6348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.3123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.5418, device='cuda:0')



h[100].sum tensor(130.3554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.4855, device='cuda:0')



h[200].sum tensor(62.2955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0336, 0.0287, 0.0000,  ..., 0.0583, 0.0000, 0.0187],
        [0.0619, 0.0511, 0.0000,  ..., 0.1046, 0.0000, 0.0361],
        [0.0901, 0.0734, 0.0000,  ..., 0.1507, 0.0000, 0.0544],
        ...,
        [0.0007, 0.0028, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77873.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1066, 0.1528, 0.2948,  ..., 0.0399, 0.1097, 0.0000],
        [0.1347, 0.1687, 0.3703,  ..., 0.0573, 0.1325, 0.0000],
        [0.1550, 0.1800, 0.4249,  ..., 0.0708, 0.1486, 0.0000],
        ...,
        [0.0009, 0.0943, 0.0130,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0943, 0.0130,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0942, 0.0130,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(616744.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3238.9324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(394.9174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6132.9854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1220.9694, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-770.6686, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0763],
        [ 0.0870],
        [ 0.0991],
        ...,
        [-5.7058],
        [-5.6991],
        [-5.7005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303433.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0359],
        [1.0385],
        ...,
        [0.9959],
        [0.9947],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370329.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0359],
        [1.0386],
        ...,
        [0.9959],
        [0.9947],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370331.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3291.2556, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.2432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8573, device='cuda:0')



h[100].sum tensor(127.9207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.4803, device='cuda:0')



h[200].sum tensor(51.2646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9879, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65670.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0921, 0.0124,  ..., 0.0000, 0.0235, 0.0000],
        [0.0006, 0.0928, 0.0125,  ..., 0.0000, 0.0237, 0.0000],
        [0.0007, 0.0932, 0.0126,  ..., 0.0000, 0.0238, 0.0000],
        ...,
        [0.0007, 0.0945, 0.0130,  ..., 0.0000, 0.0243, 0.0000],
        [0.0007, 0.0945, 0.0130,  ..., 0.0000, 0.0243, 0.0000],
        [0.0007, 0.0945, 0.0130,  ..., 0.0000, 0.0243, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570830., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2446.9536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(283.1812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6294.0942, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1051.1172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-644.0297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6865],
        [-5.7015],
        [-5.6779],
        ...,
        [-5.7204],
        [-5.7205],
        [-5.7224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321685.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0359],
        [1.0386],
        ...,
        [0.9959],
        [0.9947],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370331.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0360],
        [1.0387],
        ...,
        [0.9958],
        [0.9947],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370333.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3250.5305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.4656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4932, device='cuda:0')



h[100].sum tensor(128.0598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.3917, device='cuda:0')



h[200].sum tensor(50.5905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0006, 0.0026, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64716.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0922, 0.0123,  ..., 0.0000, 0.0233, 0.0000],
        [0.0006, 0.0929, 0.0124,  ..., 0.0000, 0.0235, 0.0000],
        [0.0006, 0.0933, 0.0125,  ..., 0.0000, 0.0237, 0.0000],
        ...,
        [0.0006, 0.0946, 0.0129,  ..., 0.0000, 0.0241, 0.0000],
        [0.0006, 0.0946, 0.0129,  ..., 0.0000, 0.0241, 0.0000],
        [0.0006, 0.0946, 0.0129,  ..., 0.0000, 0.0241, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565494.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2291.7622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.4709, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6406.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1034.1328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-632.1847, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2752],
        [-5.4733],
        [-5.6136],
        ...,
        [-5.7660],
        [-5.7590],
        [-5.7603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-350058.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0360],
        [1.0387],
        ...,
        [0.9958],
        [0.9947],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370333.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0360],
        [1.0388],
        ...,
        [0.9958],
        [0.9946],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370336.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0066,  0.0058, -0.0007,  ...,  0.0117, -0.0002,  0.0034],
        ...,
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0001,  0.0006, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3501.1545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0852, device='cuda:0')



h[100].sum tensor(128.8558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1513, device='cuda:0')



h[200].sum tensor(54.5027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1249, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0105, 0.0000,  ..., 0.0207, 0.0000, 0.0047],
        [0.0073, 0.0080, 0.0000,  ..., 0.0155, 0.0000, 0.0035],
        [0.0061, 0.0071, 0.0000,  ..., 0.0136, 0.0000, 0.0027],
        ...,
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0006, 0.0027, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68562.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0300, 0.1114, 0.0911,  ..., 0.0000, 0.0494, 0.0000],
        [0.0216, 0.1066, 0.0688,  ..., 0.0000, 0.0421, 0.0000],
        [0.0184, 0.1048, 0.0602,  ..., 0.0000, 0.0394, 0.0000],
        ...,
        [0.0007, 0.0945, 0.0128,  ..., 0.0000, 0.0243, 0.0000],
        [0.0007, 0.0945, 0.0128,  ..., 0.0000, 0.0242, 0.0000],
        [0.0007, 0.0944, 0.0128,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580243.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2565.4678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.8315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6339.0029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1088.4220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-673.5499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0738],
        [-1.7769],
        [-2.4963],
        ...,
        [-5.7652],
        [-5.7582],
        [-5.7595]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335382.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0360],
        [1.0388],
        ...,
        [0.9958],
        [0.9946],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370336.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0360],
        [1.0389],
        ...,
        [0.9958],
        [0.9946],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370339.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0046, -0.0006,  ...,  0.0092, -0.0001,  0.0024],
        [ 0.0051,  0.0046, -0.0006,  ...,  0.0092, -0.0001,  0.0024],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4000.2715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.7509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3903, device='cuda:0')



h[100].sum tensor(130.1479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0325, device='cuda:0')



h[200].sum tensor(62.2815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4935, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0175, 0.0000,  ..., 0.0347, 0.0000, 0.0094],
        [0.0147, 0.0139, 0.0000,  ..., 0.0273, 0.0000, 0.0065],
        [0.0101, 0.0102, 0.0000,  ..., 0.0197, 0.0000, 0.0043],
        ...,
        [0.0007, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0028, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78988.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0448, 0.1197, 0.1291,  ..., 0.0017, 0.0618, 0.0000],
        [0.0395, 0.1173, 0.1154,  ..., 0.0003, 0.0579, 0.0000],
        [0.0287, 0.1108, 0.0868,  ..., 0.0000, 0.0487, 0.0000],
        ...,
        [0.0009, 0.0941, 0.0127,  ..., 0.0000, 0.0246, 0.0000],
        [0.0009, 0.0941, 0.0127,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0941, 0.0127,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(634337.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3428.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(402.9529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6237.8638, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1230.0024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-781.2882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7188],
        [-0.8979],
        [-1.6078],
        ...,
        [-5.7443],
        [-5.7375],
        [-5.7389]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317640.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0360],
        [1.0389],
        ...,
        [0.9958],
        [0.9946],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370339.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0361],
        [1.0389],
        ...,
        [0.9957],
        [0.9946],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370342.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3238.9077, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.7460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2505, device='cuda:0')



h[100].sum tensor(126.4482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.6662, device='cuda:0')



h[200].sum tensor(52.2769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0029, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0029, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65863.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0921, 0.0141,  ..., 0.0000, 0.0249, 0.0000],
        [0.0014, 0.0924, 0.0129,  ..., 0.0000, 0.0245, 0.0000],
        [0.0011, 0.0925, 0.0123,  ..., 0.0000, 0.0243, 0.0000],
        ...,
        [0.0012, 0.0939, 0.0126,  ..., 0.0000, 0.0248, 0.0000],
        [0.0012, 0.0939, 0.0126,  ..., 0.0000, 0.0248, 0.0000],
        [0.0012, 0.0938, 0.0126,  ..., 0.0000, 0.0248, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571724.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2591.8228, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.8700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6376.0850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1050.5665, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-639.3205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2556],
        [-4.7659],
        [-5.1202],
        ...,
        [-5.7046],
        [-5.6974],
        [-5.6981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298593.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0361],
        [1.0389],
        ...,
        [0.9957],
        [0.9946],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370342.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0361],
        [1.0389],
        ...,
        [0.9957],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370345.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3466.4204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.2718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6403, device='cuda:0')



h[100].sum tensor(127.2120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.8212, device='cuda:0')



h[200].sum tensor(55.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0752, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0029, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0030, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0030, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68548.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0922, 0.0137,  ..., 0.0000, 0.0245, 0.0000],
        [0.0028, 0.0939, 0.0175,  ..., 0.0000, 0.0261, 0.0000],
        [0.0069, 0.0970, 0.0285,  ..., 0.0000, 0.0299, 0.0000],
        ...,
        [0.0010, 0.0942, 0.0127,  ..., 0.0000, 0.0246, 0.0000],
        [0.0010, 0.0942, 0.0127,  ..., 0.0000, 0.0246, 0.0000],
        [0.0010, 0.0941, 0.0127,  ..., 0.0000, 0.0246, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579860.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2696.4678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.8612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6136.4644, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1090.5381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.1500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3741],
        [-3.7303],
        [-2.8456],
        ...,
        [-5.7165],
        [-5.7100],
        [-5.7115]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282514.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0361],
        [1.0389],
        ...,
        [0.9957],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370345.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0362],
        [1.0389],
        ...,
        [0.9957],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370348.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5013.4233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(43.8219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.9856, device='cuda:0')



h[100].sum tensor(133.6881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(56.7607, device='cuda:0')



h[200].sum tensor(77.4227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.1176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0029, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0030, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96644.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0053, 0.0954, 0.0249,  ..., 0.0000, 0.0275, 0.0000],
        [0.0014, 0.0937, 0.0147,  ..., 0.0000, 0.0245, 0.0000],
        [0.0037, 0.0955, 0.0209,  ..., 0.0000, 0.0266, 0.0000],
        ...,
        [0.0006, 0.0948, 0.0128,  ..., 0.0000, 0.0244, 0.0000],
        [0.0006, 0.0948, 0.0128,  ..., 0.0000, 0.0244, 0.0000],
        [0.0007, 0.0948, 0.0128,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710726.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4480.2124, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(559.9136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5684.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1480.9200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-968.4731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7039],
        [-3.5947],
        [-3.9424],
        ...,
        [-5.7240],
        [-5.7175],
        [-5.7190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276025.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0362],
        [1.0389],
        ...,
        [0.9957],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370348.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 240.0 event: 1200 loss: tensor(490.5635, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0362],
        [1.0389],
        ...,
        [0.9957],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370349.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084,  0.0072, -0.0007,  ...,  0.0145, -0.0002,  0.0045],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4023.7878, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.0656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9194, device='cuda:0')



h[100].sum tensor(130.4383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.6247, device='cuda:0')



h[200].sum tensor(63.3419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0222, 0.0199, 0.0000,  ..., 0.0396, 0.0000, 0.0123],
        [0.0092, 0.0097, 0.0000,  ..., 0.0184, 0.0000, 0.0047],
        [0.0007, 0.0029, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0030, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78618.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0660, 0.1321, 0.1873,  ..., 0.0188, 0.0768, 0.0000],
        [0.0297, 0.1113, 0.0903,  ..., 0.0035, 0.0475, 0.0000],
        [0.0087, 0.0987, 0.0340,  ..., 0.0000, 0.0304, 0.0000],
        ...,
        [0.0006, 0.0950, 0.0126,  ..., 0.0000, 0.0242, 0.0000],
        [0.0006, 0.0950, 0.0126,  ..., 0.0000, 0.0242, 0.0000],
        [0.0006, 0.0950, 0.0126,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625477.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3182.4028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(401.6476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6072.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1223.1216, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-772.7909, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6731],
        [-1.7090],
        [-2.7618],
        ...,
        [-5.7450],
        [-5.7384],
        [-5.7399]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323971.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0362],
        [1.0389],
        ...,
        [0.9957],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370349.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0362],
        [1.0389],
        ...,
        [0.9956],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370351.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0075,  0.0065, -0.0007,  ...,  0.0131, -0.0002,  0.0040],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4453.4033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.2858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8189, device='cuda:0')



h[100].sum tensor(132.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.2932, device='cuda:0')



h[200].sum tensor(69.3368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0029, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0083, 0.0090, 0.0000,  ..., 0.0168, 0.0000, 0.0041],
        [0.0069, 0.0079, 0.0000,  ..., 0.0146, 0.0000, 0.0032],
        ...,
        [0.0007, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0007, 0.0030, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83869.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0953, 0.0234,  ..., 0.0000, 0.0270, 0.0000],
        [0.0147, 0.1025, 0.0499,  ..., 0.0000, 0.0357, 0.0000],
        [0.0218, 0.1077, 0.0692,  ..., 0.0000, 0.0424, 0.0000],
        ...,
        [0.0006, 0.0949, 0.0124,  ..., 0.0000, 0.0241, 0.0000],
        [0.0006, 0.0949, 0.0124,  ..., 0.0000, 0.0241, 0.0000],
        [0.0006, 0.0949, 0.0124,  ..., 0.0000, 0.0241, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645304.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3592.4722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(444.8665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5624.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1301.2147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-830.5381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2001],
        [-3.2217],
        [-2.1368],
        ...,
        [-5.7428],
        [-5.7360],
        [-5.7375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285473.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0362],
        [1.0389],
        ...,
        [0.9956],
        [0.9945],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370351.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0363],
        [1.0389],
        ...,
        [0.9956],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370353.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0089, -0.0007,  ...,  0.0180, -0.0002,  0.0059],
        [ 0.0185,  0.0153, -0.0007,  ...,  0.0311, -0.0004,  0.0111],
        [ 0.0333,  0.0270, -0.0008,  ...,  0.0553, -0.0008,  0.0207],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3665.5752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7959, device='cuda:0')



h[100].sum tensor(129.2876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.2761, device='cuda:0')



h[200].sum tensor(58.5525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0355, 0.0306, 0.0000,  ..., 0.0612, 0.0000, 0.0209],
        [0.0831, 0.0683, 0.0000,  ..., 0.1390, 0.0000, 0.0500],
        [0.1023, 0.0834, 0.0000,  ..., 0.1703, 0.0000, 0.0624],
        ...,
        [0.0007, 0.0031, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71313.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0969, 0.1478, 0.2661,  ..., 0.0343, 0.1022, 0.0000],
        [0.1659, 0.1859, 0.4481,  ..., 0.0764, 0.1574, 0.0000],
        [0.2169, 0.2129, 0.5826,  ..., 0.1099, 0.1979, 0.0000],
        ...,
        [0.0092, 0.0997, 0.0345,  ..., 0.0000, 0.0311, 0.0000],
        [0.0008, 0.0945, 0.0124,  ..., 0.0000, 0.0242, 0.0000],
        [0.0008, 0.0945, 0.0124,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588576.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2747.7466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.9177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6157.2017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1122.0430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-695.0616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0891],
        [ 0.2526],
        [ 0.2764],
        ...,
        [-3.4662],
        [-4.6552],
        [-5.3442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308868.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0363],
        [1.0389],
        ...,
        [0.9956],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370353.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0364],
        [1.0389],
        ...,
        [0.9956],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370355.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094,  0.0080, -0.0007,  ...,  0.0160, -0.0002,  0.0051],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0094,  0.0080, -0.0007,  ...,  0.0160, -0.0002,  0.0051],
        ...,
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0007, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3914.5801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.1762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4591, device='cuda:0')



h[100].sum tensor(130.0500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.2484, device='cuda:0')



h[200].sum tensor(62.2761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0085, 0.0092, 0.0000,  ..., 0.0168, 0.0000, 0.0042],
        [0.0354, 0.0305, 0.0000,  ..., 0.0608, 0.0000, 0.0191],
        [0.0086, 0.0093, 0.0000,  ..., 0.0170, 0.0000, 0.0042],
        ...,
        [0.0007, 0.0031, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76457.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0237, 0.1059, 0.0712,  ..., 0.0000, 0.0422, 0.0000],
        [0.0443, 0.1196, 0.1253,  ..., 0.0005, 0.0598, 0.0000],
        [0.0372, 0.1157, 0.1072,  ..., 0.0005, 0.0546, 0.0000],
        ...,
        [0.0011, 0.0942, 0.0124,  ..., 0.0000, 0.0242, 0.0000],
        [0.0011, 0.0942, 0.0124,  ..., 0.0000, 0.0242, 0.0000],
        [0.0011, 0.0942, 0.0124,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618138., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3282.6555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.2050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6053.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1192.8274, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-751.4625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0134],
        [-0.9515],
        [-0.2710],
        ...,
        [-5.7668],
        [-5.7602],
        [-5.7617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279423.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0364],
        [1.0389],
        ...,
        [0.9956],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370355.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0364],
        [1.0389],
        ...,
        [0.9956],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370357.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0058,  0.0052, -0.0006,  ...,  0.0102, -0.0001,  0.0028],
        [ 0.0056,  0.0050, -0.0006,  ...,  0.0097, -0.0001,  0.0027],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3446.6396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.7891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4015, device='cuda:0')



h[100].sum tensor(127.8202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1074, device='cuda:0')



h[200].sum tensor(55.9308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0077, 0.0000,  ..., 0.0135, 0.0000, 0.0029],
        [0.0200, 0.0183, 0.0000,  ..., 0.0354, 0.0000, 0.0107],
        [0.0408, 0.0348, 0.0000,  ..., 0.0693, 0.0000, 0.0225],
        ...,
        [0.0008, 0.0032, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0032, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0032, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68176.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6441e-02, 1.0726e-01, 7.7528e-02,  ..., 7.9520e-05, 4.5103e-02,
         0.0000e+00],
        [4.9772e-02, 1.2196e-01, 1.3803e-01,  ..., 5.4581e-03, 6.4283e-02,
         0.0000e+00],
        [6.9746e-02, 1.3429e-01, 1.8970e-01,  ..., 1.0813e-02, 8.0633e-02,
         0.0000e+00],
        ...,
        [1.3335e-03, 9.3873e-02, 1.2273e-02,  ..., 0.0000e+00, 2.4198e-02,
         0.0000e+00],
        [1.3337e-03, 9.3857e-02, 1.2272e-02,  ..., 0.0000e+00, 2.4195e-02,
         0.0000e+00],
        [1.3349e-03, 9.3832e-02, 1.2274e-02,  ..., 0.0000e+00, 2.4193e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577582.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2734.0073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(308.2665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6372.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1074.5928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-659.9108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3955],
        [-0.5023],
        [-0.0693],
        ...,
        [-5.7737],
        [-5.7671],
        [-5.7686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290205.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0364],
        [1.0389],
        ...,
        [0.9956],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370357.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0364],
        [1.0388],
        ...,
        [0.9955],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370359.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3978.2927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8151, device='cuda:0')



h[100].sum tensor(130.0302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.3129, device='cuda:0')



h[200].sum tensor(63.0112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0031, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0032, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0032, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0032, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74514.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0915, 0.0115,  ..., 0.0000, 0.0233, 0.0000],
        [0.0012, 0.0922, 0.0116,  ..., 0.0000, 0.0235, 0.0000],
        [0.0012, 0.0925, 0.0117,  ..., 0.0000, 0.0236, 0.0000],
        ...,
        [0.0013, 0.0938, 0.0120,  ..., 0.0000, 0.0241, 0.0000],
        [0.0013, 0.0938, 0.0120,  ..., 0.0000, 0.0241, 0.0000],
        [0.0013, 0.0938, 0.0120,  ..., 0.0000, 0.0241, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597898., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3028.6475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(363.0615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6381.2114, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1160.0750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-727.5946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8108],
        [-5.2523],
        [-5.5131],
        ...,
        [-5.7986],
        [-5.7917],
        [-5.7932]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285770.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0364],
        [1.0388],
        ...,
        [0.9955],
        [0.9944],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370359.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0365],
        [1.0388],
        ...,
        [0.9955],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370361.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4422.7529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(36.1200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4451, device='cuda:0')



h[100].sum tensor(132.1159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.1756, device='cuda:0')



h[200].sum tensor(68.6981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0031, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0031, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0032, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0032, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0032, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84691.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0123, 0.0985, 0.0404,  ..., 0.0000, 0.0325, 0.0000],
        [0.0102, 0.0978, 0.0351,  ..., 0.0000, 0.0309, 0.0000],
        [0.0328, 0.1100, 0.0936,  ..., 0.0073, 0.0491, 0.0000],
        ...,
        [0.0011, 0.0940, 0.0117,  ..., 0.0000, 0.0239, 0.0000],
        [0.0011, 0.0939, 0.0117,  ..., 0.0000, 0.0239, 0.0000],
        [0.0011, 0.0939, 0.0117,  ..., 0.0000, 0.0239, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658344.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3911.2961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.0065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6090.9092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1302.5677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-836.7006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0294],
        [-0.8643],
        [-0.4121],
        ...,
        [-5.8286],
        [-5.8213],
        [-5.8226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281414.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0365],
        [1.0388],
        ...,
        [0.9955],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370361.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0365],
        [1.0388],
        ...,
        [0.9955],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370363.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0138,  0.0116, -0.0007,  ...,  0.0232, -0.0003,  0.0081],
        [ 0.0201,  0.0166, -0.0007,  ...,  0.0336, -0.0004,  0.0122],
        [ 0.0138,  0.0116, -0.0007,  ...,  0.0232, -0.0003,  0.0081],
        ...,
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0001,  0.0007, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4327.8491, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.0835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8667, device='cuda:0')



h[100].sum tensor(132.0943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(44.4464, device='cuda:0')



h[200].sum tensor(66.9044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0760, 0.0628, 0.0000,  ..., 0.1269, 0.0000, 0.0456],
        [0.0819, 0.0675, 0.0000,  ..., 0.1365, 0.0000, 0.0494],
        [0.1398, 0.1133, 0.0000,  ..., 0.2309, 0.0000, 0.0867],
        ...,
        [0.0006, 0.0031, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0031, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0006, 0.0031, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81305.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1799, 0.1914, 0.4739,  ..., 0.0836, 0.1657, 0.0000],
        [0.2127, 0.2086, 0.5585,  ..., 0.1047, 0.1918, 0.0000],
        [0.2764, 0.2401, 0.7237,  ..., 0.1466, 0.2430, 0.0000],
        ...,
        [0.0008, 0.0945, 0.0117,  ..., 0.0000, 0.0236, 0.0000],
        [0.0008, 0.0945, 0.0117,  ..., 0.0000, 0.0236, 0.0000],
        [0.0034, 0.0961, 0.0184,  ..., 0.0000, 0.0258, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629761., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3377.9487, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(419.5775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6258.1206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1256.0482, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-799.6001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3108],
        [ 0.2985],
        [ 0.2807],
        ...,
        [-5.7234],
        [-5.4853],
        [-5.0949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305878.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0365],
        [1.0388],
        ...,
        [0.9955],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370363.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0366],
        [1.0388],
        ...,
        [0.9955],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370366.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3951.6326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.0668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2741, device='cuda:0')



h[100].sum tensor(130.4650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.6955, device='cuda:0')



h[200].sum tensor(61.6224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0031, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0031, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0031, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0032, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0032, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0006, 0.0032, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75821.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0951, 0.0224,  ..., 0.0000, 0.0264, 0.0000],
        [0.0014, 0.0937, 0.0133,  ..., 0.0000, 0.0237, 0.0000],
        [0.0007, 0.0935, 0.0115,  ..., 0.0000, 0.0232, 0.0000],
        ...,
        [0.0007, 0.0949, 0.0118,  ..., 0.0000, 0.0236, 0.0000],
        [0.0007, 0.0949, 0.0118,  ..., 0.0000, 0.0236, 0.0000],
        [0.0007, 0.0948, 0.0118,  ..., 0.0000, 0.0236, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609225.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3049.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(371.3380, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6252.6235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1184.8082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-739.8658, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3470],
        [-4.2487],
        [-4.8272],
        ...,
        [-5.8372],
        [-5.8293],
        [-5.8300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302296.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0366],
        [1.0388],
        ...,
        [0.9955],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370366.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0366],
        [1.0388],
        ...,
        [0.9954],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370369.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0300,  0.0244, -0.0008,  ...,  0.0496, -0.0007,  0.0185],
        [ 0.0330,  0.0268, -0.0008,  ...,  0.0544, -0.0007,  0.0205],
        [ 0.0211,  0.0173, -0.0007,  ...,  0.0350, -0.0005,  0.0128],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3324.7124, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.1423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4239, device='cuda:0')



h[100].sum tensor(126.6580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.1845, device='cuda:0')



h[200].sum tensor(53.6894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9396, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1434, 0.1163, 0.0000,  ..., 0.2365, 0.0000, 0.0892],
        [0.1208, 0.0984, 0.0000,  ..., 0.1996, 0.0000, 0.0745],
        [0.0921, 0.0757, 0.0000,  ..., 0.1528, 0.0000, 0.0559],
        ...,
        [0.0007, 0.0033, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0033, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0033, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65235.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2785, 0.2370, 0.7288,  ..., 0.1476, 0.2443, 0.0000],
        [0.2551, 0.2267, 0.6681,  ..., 0.1315, 0.2257, 0.0000],
        [0.2124, 0.2066, 0.5577,  ..., 0.1033, 0.1917, 0.0000],
        ...,
        [0.0009, 0.0946, 0.0122,  ..., 0.0000, 0.0240, 0.0000],
        [0.0009, 0.0946, 0.0122,  ..., 0.0000, 0.0240, 0.0000],
        [0.0009, 0.0946, 0.0122,  ..., 0.0000, 0.0240, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563587.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2385.2827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.1597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6851.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1038.7010, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.1272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3242],
        [ 0.3298],
        [ 0.3456],
        ...,
        [-5.8181],
        [-5.8105],
        [-5.8115]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305047., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0366],
        [1.0388],
        ...,
        [0.9954],
        [0.9943],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370369.2812, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 250.0 event: 1250 loss: tensor(477.9894, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0366],
        [1.0388],
        ...,
        [0.9954],
        [0.9942],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.6696e-03,  4.3626e-03, -6.4060e-04,  ...,  8.1859e-03,
         -9.8578e-05,  2.1477e-03],
        [ 4.6696e-03,  4.3626e-03, -6.4060e-04,  ...,  8.1859e-03,
         -9.8578e-05,  2.1477e-03],
        [ 1.9266e-04,  8.1526e-04, -6.1094e-04,  ...,  8.8787e-04,
          0.0000e+00, -7.4475e-04],
        ...,
        [ 1.9266e-04,  8.1526e-04, -6.1094e-04,  ...,  8.8787e-04,
          0.0000e+00, -7.4475e-04],
        [ 1.9266e-04,  8.1526e-04, -6.1094e-04,  ...,  8.8787e-04,
          0.0000e+00, -7.4475e-04],
        [ 1.9266e-04,  8.1526e-04, -6.1094e-04,  ...,  8.8787e-04,
          0.0000e+00, -7.4475e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3559.4170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9182, device='cuda:0')



h[100].sum tensor(126.5176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.6520, device='cuda:0')



h[200].sum tensor(57.6371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1062, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0092, 0.0100, 0.0000,  ..., 0.0173, 0.0000, 0.0039],
        [0.0093, 0.0101, 0.0000,  ..., 0.0175, 0.0000, 0.0039],
        [0.0093, 0.0101, 0.0000,  ..., 0.0175, 0.0000, 0.0039],
        ...,
        [0.0008, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68190.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.1047, 0.0653,  ..., 0.0000, 0.0421, 0.0000],
        [0.0232, 0.1067, 0.0707,  ..., 0.0000, 0.0441, 0.0000],
        [0.0215, 0.1060, 0.0663,  ..., 0.0000, 0.0427, 0.0000],
        ...,
        [0.0013, 0.0942, 0.0125,  ..., 0.0000, 0.0244, 0.0000],
        [0.0013, 0.0942, 0.0125,  ..., 0.0000, 0.0244, 0.0000],
        [0.0013, 0.0942, 0.0125,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573819.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2691.4387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.7325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6806.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1088.4688, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-645.9598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0981],
        [-1.8398],
        [-1.9222],
        ...,
        [-5.7892],
        [-5.7817],
        [-5.7826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253697.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0366],
        [1.0388],
        ...,
        [0.9954],
        [0.9942],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370372., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0367],
        [1.0388],
        ...,
        [0.9954],
        [0.9942],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370374.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0108,  0.0092, -0.0007,  ...,  0.0181, -0.0002,  0.0061],
        [ 0.0204,  0.0168, -0.0007,  ...,  0.0338, -0.0004,  0.0123],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3269.7705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0711, device='cuda:0')



h[100].sum tensor(125.5734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.1299, device='cuda:0')



h[200].sum tensor(53.4962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9002, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0120, 0.0000,  ..., 0.0214, 0.0000, 0.0063],
        [0.0391, 0.0338, 0.0000,  ..., 0.0661, 0.0000, 0.0232],
        [0.0953, 0.0783, 0.0000,  ..., 0.1578, 0.0000, 0.0580],
        ...,
        [0.0008, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64437.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0447, 0.1155, 0.1246,  ..., 0.0114, 0.0581, 0.0000],
        [0.1038, 0.1468, 0.2771,  ..., 0.0395, 0.1050, 0.0000],
        [0.1872, 0.1894, 0.4925,  ..., 0.0892, 0.1714, 0.0000],
        ...,
        [0.0012, 0.0945, 0.0129,  ..., 0.0000, 0.0244, 0.0000],
        [0.0012, 0.0945, 0.0129,  ..., 0.0000, 0.0244, 0.0000],
        [0.0012, 0.0944, 0.0129,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560448.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2397.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.8481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7221.1235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1034.1464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-600.1868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5602],
        [-0.5003],
        [ 0.1302],
        ...,
        [-5.7996],
        [-5.7919],
        [-5.7928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285890.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0367],
        [1.0388],
        ...,
        [0.9954],
        [0.9942],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370374.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0348],
        [1.0367],
        [1.0388],
        ...,
        [0.9953],
        [0.9942],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370376.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0073,  0.0065, -0.0007,  ...,  0.0126, -0.0002,  0.0039],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0119,  0.0101, -0.0007,  ...,  0.0200, -0.0003,  0.0069],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3797.4849, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.6374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3710, device='cuda:0')



h[100].sum tensor(128.3779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.9955, device='cuda:0')



h[200].sum tensor(60.2370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0389, 0.0337, 0.0000,  ..., 0.0661, 0.0000, 0.0225],
        [0.0364, 0.0317, 0.0000,  ..., 0.0619, 0.0000, 0.0201],
        [0.0256, 0.0231, 0.0000,  ..., 0.0443, 0.0000, 0.0146],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0166, 0.0160, 0.0000,  ..., 0.0297, 0.0000, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71452.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1136, 0.1552, 0.3047,  ..., 0.0396, 0.1126, 0.0000],
        [0.0894, 0.1438, 0.2424,  ..., 0.0234, 0.0941, 0.0000],
        [0.0817, 0.1393, 0.2226,  ..., 0.0205, 0.0881, 0.0000],
        ...,
        [0.0031, 0.0963, 0.0190,  ..., 0.0000, 0.0260, 0.0000],
        [0.0126, 0.1016, 0.0437,  ..., 0.0000, 0.0334, 0.0000],
        [0.0412, 0.1170, 0.1174,  ..., 0.0085, 0.0558, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587225.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2753.7808, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.0834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7024.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1137.3685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-673.7522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3372],
        [ 0.3538],
        [ 0.3579],
        ...,
        [-5.1293],
        [-4.2053],
        [-2.8978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292663.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0348],
        [1.0367],
        [1.0388],
        ...,
        [0.9953],
        [0.9942],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370376.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0367],
        [1.0388],
        ...,
        [0.9953],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370378.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0128,  0.0108, -0.0007,  ...,  0.0215, -0.0003,  0.0075],
        [ 0.0112,  0.0095, -0.0007,  ...,  0.0188, -0.0002,  0.0064],
        [ 0.0110,  0.0094, -0.0007,  ...,  0.0185, -0.0002,  0.0063],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3762.4136, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0178, device='cuda:0')



h[100].sum tensor(128.6474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9395, device='cuda:0')



h[200].sum tensor(59.3213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0478, 0.0407, 0.0000,  ..., 0.0805, 0.0000, 0.0276],
        [0.0632, 0.0530, 0.0000,  ..., 0.1057, 0.0000, 0.0375],
        [0.0605, 0.0508, 0.0000,  ..., 0.1012, 0.0000, 0.0357],
        ...,
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72118.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1214, 0.1600, 0.3252,  ..., 0.0427, 0.1186, 0.0000],
        [0.1533, 0.1775, 0.4074,  ..., 0.0620, 0.1434, 0.0000],
        [0.1577, 0.1803, 0.4188,  ..., 0.0648, 0.1468, 0.0000],
        ...,
        [0.0008, 0.0952, 0.0132,  ..., 0.0000, 0.0241, 0.0000],
        [0.0008, 0.0952, 0.0132,  ..., 0.0000, 0.0241, 0.0000],
        [0.0008, 0.0951, 0.0132,  ..., 0.0000, 0.0241, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593451.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2820.3481, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(342.7377, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7098.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1149.9200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-678.5078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1952],
        [ 0.3166],
        [ 0.3372],
        ...,
        [-5.6026],
        [-5.4140],
        [-5.2938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294686.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0367],
        [1.0388],
        ...,
        [0.9953],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370378.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370380.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0052, -0.0006,  ...,  0.0098, -0.0001,  0.0028],
        [ 0.0058,  0.0052, -0.0006,  ...,  0.0100, -0.0001,  0.0029],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3459.5569, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.2936, device='cuda:0')



h[100].sum tensor(127.7333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.7847, device='cuda:0')



h[200].sum tensor(54.7178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0401, 0.0346, 0.0000,  ..., 0.0680, 0.0000, 0.0226],
        [0.0199, 0.0186, 0.0000,  ..., 0.0350, 0.0000, 0.0110],
        [0.0064, 0.0080, 0.0000,  ..., 0.0131, 0.0000, 0.0030],
        ...,
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66603.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0691, 0.1320, 0.1901,  ..., 0.0110, 0.0779, 0.0000],
        [0.0505, 0.1222, 0.1420,  ..., 0.0056, 0.0635, 0.0000],
        [0.0293, 0.1106, 0.0872,  ..., 0.0000, 0.0471, 0.0000],
        ...,
        [0.0009, 0.0952, 0.0129,  ..., 0.0000, 0.0241, 0.0000],
        [0.0009, 0.0951, 0.0129,  ..., 0.0000, 0.0241, 0.0000],
        [0.0009, 0.0951, 0.0129,  ..., 0.0000, 0.0241, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(567512.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2396.3921, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.6280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7531.6909, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1070.8436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-614.7427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0507],
        [-0.0569],
        [-0.2713],
        ...,
        [-5.8884],
        [-5.8795],
        [-5.8782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324317.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370380.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370383.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3983.7341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.0687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5468, device='cuda:0')



h[100].sum tensor(129.9787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5108, device='cuda:0')



h[200].sum tensor(61.7439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3994, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0084, 0.0095, 0.0000,  ..., 0.0162, 0.0000, 0.0036],
        [0.0046, 0.0065, 0.0000,  ..., 0.0100, 0.0000, 0.0018],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75518.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0163, 0.1020, 0.0534,  ..., 0.0000, 0.0371, 0.0000],
        [0.0119, 0.0999, 0.0416,  ..., 0.0000, 0.0333, 0.0000],
        [0.0066, 0.0970, 0.0274,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0010, 0.0950, 0.0126,  ..., 0.0000, 0.0242, 0.0000],
        [0.0010, 0.0949, 0.0125,  ..., 0.0000, 0.0241, 0.0000],
        [0.0010, 0.0949, 0.0125,  ..., 0.0000, 0.0241, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610034.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3098.6528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(371.3763, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7372.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1198.4592, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-709.2874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8884],
        [-4.2196],
        [-4.6117],
        ...,
        [-5.9034],
        [-5.8949],
        [-5.8952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308764.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370383.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370386.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3482.1692, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.0721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4003, device='cuda:0')



h[100].sum tensor(128.3980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1036, device='cuda:0')



h[200].sum tensor(54.5010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69372.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0924, 0.0117,  ..., 0.0000, 0.0235, 0.0000],
        [0.0011, 0.0931, 0.0118,  ..., 0.0000, 0.0237, 0.0000],
        [0.0011, 0.0934, 0.0119,  ..., 0.0000, 0.0238, 0.0000],
        ...,
        [0.0012, 0.0948, 0.0123,  ..., 0.0000, 0.0243, 0.0000],
        [0.0012, 0.0948, 0.0122,  ..., 0.0000, 0.0243, 0.0000],
        [0.0012, 0.0947, 0.0122,  ..., 0.0000, 0.0243, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588419.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2791.7095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.0526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7547.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1116.0658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-643.1844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6221],
        [-5.7512],
        [-5.8322],
        ...,
        [-5.8873],
        [-5.8840],
        [-5.8870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307369.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370386.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370388.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3368.1509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.5407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.7754, device='cuda:0')



h[100].sum tensor(128.5617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.2354, device='cuda:0')



h[200].sum tensor(52.6328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0065, 0.0080, 0.0000,  ..., 0.0132, 0.0000, 0.0031],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66341.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0296, 0.1082, 0.0865,  ..., 0.0024, 0.0466, 0.0000],
        [0.0114, 0.0991, 0.0394,  ..., 0.0000, 0.0324, 0.0000],
        [0.0083, 0.0978, 0.0313,  ..., 0.0000, 0.0301, 0.0000],
        ...,
        [0.0011, 0.0949, 0.0123,  ..., 0.0000, 0.0243, 0.0000],
        [0.0011, 0.0949, 0.0123,  ..., 0.0000, 0.0243, 0.0000],
        [0.0011, 0.0948, 0.0123,  ..., 0.0000, 0.0243, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572764.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2490.8706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.6216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7866.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1069.5422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-608.9683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3587],
        [-2.2562],
        [-2.7093],
        ...,
        [-5.9306],
        [-5.9219],
        [-5.9221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-340582.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0368],
        [1.0388],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370388.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0349],
        [1.0369],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370391.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0001,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3746.7495, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2493, device='cuda:0')



h[100].sum tensor(130.8485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6317, device='cuda:0')



h[200].sum tensor(57.4469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0033, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0006, 0.0033, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0006, 0.0033, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0006, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70574.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0926, 0.0117,  ..., 0.0000, 0.0234, 0.0000],
        [0.0010, 0.0933, 0.0118,  ..., 0.0000, 0.0236, 0.0000],
        [0.0010, 0.0937, 0.0120,  ..., 0.0000, 0.0238, 0.0000],
        ...,
        [0.0011, 0.0950, 0.0123,  ..., 0.0000, 0.0242, 0.0000],
        [0.0011, 0.0950, 0.0123,  ..., 0.0000, 0.0242, 0.0000],
        [0.0011, 0.0949, 0.0123,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585098.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2714.6897, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(322.0977, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7622.5459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1133.6327, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-657.2972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.8180],
        [-5.8803],
        [-5.8984],
        ...,
        [-5.9465],
        [-5.9362],
        [-5.9355]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312540.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0349],
        [1.0369],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370391.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0369],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370394.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0179,  0.0149, -0.0007,  ...,  0.0298, -0.0004,  0.0108],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3590.4172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.3379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1727, device='cuda:0')



h[100].sum tensor(130.5738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.4129, device='cuda:0')



h[200].sum tensor(55.2038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0033, 0.0000,  ..., 0.0036, 0.0000, 0.0000],
        [0.0341, 0.0299, 0.0000,  ..., 0.0583, 0.0000, 0.0202],
        [0.0677, 0.0566, 0.0000,  ..., 0.1131, 0.0000, 0.0412],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69509.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0361, 0.1108, 0.1030,  ..., 0.0073, 0.0510, 0.0000],
        [0.0897, 0.1390, 0.2428,  ..., 0.0331, 0.0930, 0.0000],
        [0.1450, 0.1669, 0.3873,  ..., 0.0652, 0.1366, 0.0000],
        ...,
        [0.0012, 0.0948, 0.0122,  ..., 0.0000, 0.0243, 0.0000],
        [0.0012, 0.0948, 0.0122,  ..., 0.0000, 0.0243, 0.0000],
        [0.0012, 0.0948, 0.0122,  ..., 0.0000, 0.0243, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585422.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2691.2925, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.0031, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7826.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1116.5463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-645.3152, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5647],
        [-0.0882],
        [ 0.1811],
        ...,
        [-5.9596],
        [-5.9507],
        [-5.9507]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325969.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0369],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370394.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 260.0 event: 1300 loss: tensor(459.0543, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0350],
        [1.0370],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370397.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0048, -0.0006,  ...,  0.0091, -0.0001,  0.0025],
        [ 0.0063,  0.0057, -0.0007,  ...,  0.0109, -0.0001,  0.0033],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4096.2534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.9004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3830, device='cuda:0')



h[100].sum tensor(132.6571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0108, device='cuda:0')



h[200].sum tensor(62.1751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4927, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0422, 0.0363, 0.0000,  ..., 0.0713, 0.0000, 0.0240],
        [0.0199, 0.0187, 0.0000,  ..., 0.0351, 0.0000, 0.0110],
        [0.0133, 0.0134, 0.0000,  ..., 0.0242, 0.0000, 0.0067],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80287.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0735, 0.1316, 0.2002,  ..., 0.0150, 0.0809, 0.0000],
        [0.0552, 0.1224, 0.1528,  ..., 0.0072, 0.0669, 0.0000],
        [0.0418, 0.1155, 0.1181,  ..., 0.0011, 0.0565, 0.0000],
        ...,
        [0.0013, 0.0947, 0.0120,  ..., 0.0000, 0.0245, 0.0000],
        [0.0013, 0.0946, 0.0120,  ..., 0.0000, 0.0244, 0.0000],
        [0.0013, 0.0946, 0.0120,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647093.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3624.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.6924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7635.7451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1264.6251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-760.9629, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1278],
        [ 0.0200],
        [-0.1860],
        ...,
        [-5.9562],
        [-5.9473],
        [-5.9473]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-322728.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0350],
        [1.0370],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370397.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0370],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370400.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4240.1714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.9957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0277, device='cuda:0')



h[100].sum tensor(133.1487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.9380, device='cuda:0')



h[200].sum tensor(64.2476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0035, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0037, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81120.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0921, 0.0115,  ..., 0.0000, 0.0238, 0.0000],
        [0.0018, 0.0932, 0.0132,  ..., 0.0000, 0.0245, 0.0000],
        [0.0035, 0.0944, 0.0176,  ..., 0.0000, 0.0261, 0.0000],
        ...,
        [0.0013, 0.0945, 0.0120,  ..., 0.0000, 0.0246, 0.0000],
        [0.0033, 0.0955, 0.0172,  ..., 0.0000, 0.0262, 0.0000],
        [0.0100, 0.0990, 0.0345,  ..., 0.0000, 0.0313, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639186.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3485.9944, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(416.6848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7785.0693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1276.5009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-770.7754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8243],
        [-4.6231],
        [-4.3120],
        ...,
        [-5.7627],
        [-5.4000],
        [-4.6775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324247.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0370],
        [1.0389],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370400.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0370],
        [1.0390],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370403.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0066,  0.0059, -0.0007,  ...,  0.0114, -0.0001,  0.0035],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3531.9868, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.2854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7656, device='cuda:0')



h[100].sum tensor(130.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.1958, device='cuda:0')



h[200].sum tensor(54.2707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0034, 0.0000,  ..., 0.0037, 0.0000, 0.0000],
        [0.0074, 0.0087, 0.0000,  ..., 0.0146, 0.0000, 0.0036],
        [0.0062, 0.0078, 0.0000,  ..., 0.0127, 0.0000, 0.0028],
        ...,
        [0.0008, 0.0035, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70559.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0058, 0.0948, 0.0237,  ..., 0.0000, 0.0275, 0.0000],
        [0.0138, 0.0999, 0.0445,  ..., 0.0000, 0.0341, 0.0000],
        [0.0173, 0.1022, 0.0538,  ..., 0.0000, 0.0371, 0.0000],
        ...,
        [0.0013, 0.0947, 0.0120,  ..., 0.0000, 0.0246, 0.0000],
        [0.0013, 0.0946, 0.0120,  ..., 0.0000, 0.0246, 0.0000],
        [0.0013, 0.0946, 0.0120,  ..., 0.0000, 0.0246, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596532.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2888.7761, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.2161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7651.1963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1134.7029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-659.8110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.6151],
        [-3.9834],
        [-4.1131],
        ...,
        [-5.9570],
        [-5.9479],
        [-5.9479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302412.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0370],
        [1.0390],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370403.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0351],
        [1.0370],
        [1.0390],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370406.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4304.7959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.3313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.5937, device='cuda:0')



h[100].sum tensor(134.0700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.6302, device='cuda:0')



h[200].sum tensor(64.6020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6277, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81093.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0927, 0.0118,  ..., 0.0000, 0.0236, 0.0000],
        [0.0010, 0.0934, 0.0119,  ..., 0.0000, 0.0238, 0.0000],
        [0.0010, 0.0938, 0.0120,  ..., 0.0000, 0.0239, 0.0000],
        ...,
        [0.0185, 0.1047, 0.0579,  ..., 0.0000, 0.0380, 0.0000],
        [0.0176, 0.1042, 0.0556,  ..., 0.0000, 0.0373, 0.0000],
        [0.0134, 0.1019, 0.0447,  ..., 0.0000, 0.0340, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641100.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3513.2290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(415.7854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7360.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1281.8827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-774.6998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5976],
        [-5.7092],
        [-5.7415],
        ...,
        [-2.3830],
        [-2.4235],
        [-2.5945]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304511.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0351],
        [1.0370],
        [1.0390],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370406.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0371],
        [1.0390],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370409.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0082,  0.0072, -0.0007,  ...,  0.0141, -0.0002,  0.0045]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3706.5842, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.8552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8842, device='cuda:0')



h[100].sum tensor(132.0777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.5402, device='cuda:0')



h[200].sum tensor(56.0639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0033, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0092, 0.0101, 0.0000,  ..., 0.0177, 0.0000, 0.0048],
        [0.0076, 0.0089, 0.0000,  ..., 0.0152, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72260.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0073, 0.0966, 0.0287,  ..., 0.0000, 0.0285, 0.0000],
        [0.0040, 0.0955, 0.0202,  ..., 0.0000, 0.0261, 0.0000],
        [0.0071, 0.0976, 0.0285,  ..., 0.0000, 0.0288, 0.0000],
        ...,
        [0.0061, 0.0983, 0.0258,  ..., 0.0000, 0.0282, 0.0000],
        [0.0172, 0.1045, 0.0551,  ..., 0.0000, 0.0370, 0.0000],
        [0.0218, 0.1071, 0.0674,  ..., 0.0000, 0.0407, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597956.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2826.7605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(338.6448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7576.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1158.6028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-681.3431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2219],
        [-2.6502],
        [-2.7409],
        ...,
        [-5.3134],
        [-4.7867],
        [-4.3572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325706.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0371],
        [1.0390],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370409.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0371],
        [1.0391],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370412.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3525.7744, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7558, device='cuda:0')



h[100].sum tensor(131.4437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.1667, device='cuda:0')



h[200].sum tensor(53.5556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69405.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0100, 0.0981, 0.0358,  ..., 0.0000, 0.0308, 0.0000],
        [0.0049, 0.0958, 0.0222,  ..., 0.0000, 0.0269, 0.0000],
        [0.0051, 0.0963, 0.0231,  ..., 0.0000, 0.0272, 0.0000],
        ...,
        [0.0011, 0.0953, 0.0124,  ..., 0.0000, 0.0243, 0.0000],
        [0.0011, 0.0953, 0.0124,  ..., 0.0000, 0.0242, 0.0000],
        [0.0011, 0.0952, 0.0124,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587758.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2707.0991, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.6547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7431.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1122.7784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-653.3818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9092],
        [-2.5202],
        [-2.8648],
        ...,
        [-5.9971],
        [-5.9879],
        [-5.9878]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306297.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0371],
        [1.0391],
        ...,
        [0.9952],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370412.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0352],
        [1.0371],
        [1.0391],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370414.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3249.3159, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.9826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.0877, device='cuda:0')



h[100].sum tensor(130.5795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.1794, device='cuda:0')



h[200].sum tensor(49.8157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9021, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0034, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65436.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0926, 0.0118,  ..., 0.0000, 0.0234, 0.0000],
        [0.0011, 0.0934, 0.0119,  ..., 0.0000, 0.0236, 0.0000],
        [0.0011, 0.0937, 0.0120,  ..., 0.0000, 0.0237, 0.0000],
        ...,
        [0.0012, 0.0951, 0.0123,  ..., 0.0000, 0.0242, 0.0000],
        [0.0012, 0.0950, 0.0123,  ..., 0.0000, 0.0242, 0.0000],
        [0.0012, 0.0950, 0.0123,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574627.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2487.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(279.2943, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7614.2427, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1063.6093, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-612.5336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.4525],
        [-5.2101],
        [-4.7960],
        ...,
        [-6.0041],
        [-5.9949],
        [-5.9948]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323866.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0352],
        [1.0371],
        [1.0391],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370414.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0391],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370417.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0329,  0.0267, -0.0008,  ...,  0.0543, -0.0006,  0.0204],
        [ 0.0259,  0.0212, -0.0008,  ...,  0.0429, -0.0005,  0.0159],
        [ 0.0221,  0.0182, -0.0008,  ...,  0.0367, -0.0004,  0.0134],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0009,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3934.3110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.4128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5312, device='cuda:0')



h[100].sum tensor(133.7629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.4641, device='cuda:0')



h[200].sum tensor(59.1197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1001, 0.0822, 0.0000,  ..., 0.1661, 0.0000, 0.0614],
        [0.1261, 0.1028, 0.0000,  ..., 0.2085, 0.0000, 0.0781],
        [0.1331, 0.1083, 0.0000,  ..., 0.2199, 0.0000, 0.0826],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73410.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2495, 0.2157, 0.6640,  ..., 0.1354, 0.2186, 0.0000],
        [0.2992, 0.2392, 0.7947,  ..., 0.1696, 0.2580, 0.0000],
        [0.3193, 0.2486, 0.8476,  ..., 0.1837, 0.2740, 0.0000],
        ...,
        [0.0012, 0.0951, 0.0124,  ..., 0.0000, 0.0240, 0.0000],
        [0.0012, 0.0950, 0.0124,  ..., 0.0000, 0.0239, 0.0000],
        [0.0012, 0.0950, 0.0124,  ..., 0.0000, 0.0239, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596430.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2783.6221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.3738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7603.4033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1169.4084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-698.0795, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1217],
        [ 0.0712],
        [ 0.0277],
        ...,
        [-6.0217],
        [-6.0124],
        [-6.0122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-345934.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0391],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370417.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0391],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370419.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0266,  0.0218, -0.0008,  ...,  0.0441, -0.0005,  0.0164],
        [ 0.0180,  0.0149, -0.0007,  ...,  0.0300, -0.0003,  0.0108],
        [ 0.0362,  0.0294, -0.0008,  ...,  0.0597, -0.0007,  0.0226],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4242.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4679, device='cuda:0')



h[100].sum tensor(135.2382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.2543, device='cuda:0')



h[200].sum tensor(63.3589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0791, 0.0655, 0.0000,  ..., 0.1319, 0.0000, 0.0477],
        [0.1176, 0.0961, 0.0000,  ..., 0.1947, 0.0000, 0.0726],
        [0.0648, 0.0542, 0.0000,  ..., 0.1085, 0.0000, 0.0384],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79993.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1739, 0.1817, 0.4660,  ..., 0.0834, 0.1588, 0.0000],
        [0.2104, 0.1996, 0.5622,  ..., 0.1073, 0.1881, 0.0000],
        [0.1673, 0.1797, 0.4492,  ..., 0.0784, 0.1543, 0.0000],
        ...,
        [0.0011, 0.0951, 0.0126,  ..., 0.0000, 0.0238, 0.0000],
        [0.0011, 0.0950, 0.0126,  ..., 0.0000, 0.0237, 0.0000],
        [0.0011, 0.0950, 0.0126,  ..., 0.0000, 0.0237, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629926.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3364.2786, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(407.0178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7028.7827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1269.2106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-774.5958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2932],
        [ 0.3259],
        [ 0.2794],
        ...,
        [-6.0025],
        [-5.9921],
        [-5.9900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292285.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0391],
        ...,
        [0.9953],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370419.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0391],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370421.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0183,  0.0151, -0.0007,  ...,  0.0305, -0.0003,  0.0110],
        [ 0.0285,  0.0233, -0.0008,  ...,  0.0472, -0.0005,  0.0176],
        [ 0.0324,  0.0264, -0.0008,  ...,  0.0535, -0.0006,  0.0201],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3594.5037, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.5705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4725, device='cuda:0')



h[100].sum tensor(132.7406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.3092, device='cuda:0')



h[200].sum tensor(54.4478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0897, 0.0739, 0.0000,  ..., 0.1491, 0.0000, 0.0545],
        [0.0934, 0.0769, 0.0000,  ..., 0.1552, 0.0000, 0.0569],
        [0.1056, 0.0865, 0.0000,  ..., 0.1751, 0.0000, 0.0647],
        ...,
        [0.0008, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69122.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2225, 0.2039, 0.5915,  ..., 0.1161, 0.1966, 0.0000],
        [0.2183, 0.2028, 0.5807,  ..., 0.1129, 0.1935, 0.0000],
        [0.2078, 0.1973, 0.5531,  ..., 0.1071, 0.1854, 0.0000],
        ...,
        [0.0012, 0.0949, 0.0127,  ..., 0.0000, 0.0235, 0.0000],
        [0.0012, 0.0948, 0.0127,  ..., 0.0000, 0.0235, 0.0000],
        [0.0012, 0.0948, 0.0127,  ..., 0.0000, 0.0235, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583277.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2605.8872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.4445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7471.0103, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1109.6581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-658.1078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2752],
        [ 0.2907],
        [ 0.2731],
        ...,
        [-6.0272],
        [-6.0182],
        [-6.0195]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-326227.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0391],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370421.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 270.0 event: 1350 loss: tensor(486.9345, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0391],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370423.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3481.2690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.2700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6839, device='cuda:0')



h[100].sum tensor(132.1862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.9517, device='cuda:0')



h[200].sum tensor(52.8662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0801, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0033, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68658., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0930, 0.0144,  ..., 0.0000, 0.0233, 0.0000],
        [0.0105, 0.0986, 0.0376,  ..., 0.0000, 0.0305, 0.0000],
        [0.0231, 0.1059, 0.0706,  ..., 0.0000, 0.0405, 0.0000],
        ...,
        [0.0011, 0.0949, 0.0129,  ..., 0.0000, 0.0233, 0.0000],
        [0.0011, 0.0948, 0.0129,  ..., 0.0000, 0.0233, 0.0000],
        [0.0011, 0.0948, 0.0128,  ..., 0.0000, 0.0233, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582759.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2559.0349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.0870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7498.6421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1101.3058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-653.6023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7337],
        [-2.6818],
        [-1.5585],
        ...,
        [-6.0363],
        [-6.0260],
        [-6.0163]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337829.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0391],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370423.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370426.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3263.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.4106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.2898, device='cuda:0')



h[100].sum tensor(131.3136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(24.7837, device='cuda:0')



h[200].sum tensor(49.8327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0081, 0.0000,  ..., 0.0138, 0.0000, 0.0031],
        [0.0007, 0.0034, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0071, 0.0084, 0.0000,  ..., 0.0145, 0.0000, 0.0033],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65596.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0126, 0.0998, 0.0443,  ..., 0.0000, 0.0321, 0.0000],
        [0.0079, 0.0977, 0.0317,  ..., 0.0000, 0.0284, 0.0000],
        [0.0136, 0.1014, 0.0468,  ..., 0.0000, 0.0331, 0.0000],
        ...,
        [0.0009, 0.0951, 0.0130,  ..., 0.0000, 0.0230, 0.0000],
        [0.0009, 0.0951, 0.0130,  ..., 0.0000, 0.0230, 0.0000],
        [0.0009, 0.0950, 0.0130,  ..., 0.0000, 0.0230, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574200., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2364.7603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(283.0956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7510.7314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1056.7990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-622.4609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3413],
        [-4.6576],
        [-4.6599],
        ...,
        [-6.0618],
        [-6.0522],
        [-6.0519]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346436.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370426.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370428.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3905.2329, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.0861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4532, device='cuda:0')



h[100].sum tensor(133.6066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.2309, device='cuda:0')



h[200].sum tensor(58.7315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0007, 0.0033, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0007, 0.0033, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74354.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0934, 0.0142,  ..., 0.0000, 0.0226, 0.0000],
        [0.0017, 0.0945, 0.0160,  ..., 0.0000, 0.0234, 0.0000],
        [0.0045, 0.0966, 0.0237,  ..., 0.0000, 0.0260, 0.0000],
        ...,
        [0.0008, 0.0954, 0.0132,  ..., 0.0000, 0.0227, 0.0000],
        [0.0008, 0.0953, 0.0132,  ..., 0.0000, 0.0227, 0.0000],
        [0.0008, 0.0952, 0.0132,  ..., 0.0000, 0.0227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603863.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2847.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(359.6360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7068.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1183.9014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-719.9175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7777],
        [-4.3719],
        [-3.8041],
        ...,
        [-6.0642],
        [-6.0545],
        [-6.0541]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313668.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370428.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370428.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0059, -0.0007,  ...,  0.0115, -0.0001,  0.0034],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0066,  0.0059, -0.0007,  ...,  0.0115, -0.0001,  0.0034],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4525.6338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.6486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3061, device='cuda:0')



h[100].sum tensor(135.9968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.7497, device='cuda:0')



h[200].sum tensor(67.2585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0076, 0.0000,  ..., 0.0130, 0.0000, 0.0027],
        [0.0250, 0.0226, 0.0000,  ..., 0.0437, 0.0000, 0.0126],
        [0.0062, 0.0077, 0.0000,  ..., 0.0131, 0.0000, 0.0028],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85264.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0236, 0.1068, 0.0741,  ..., 0.0000, 0.0409, 0.0000],
        [0.0317, 0.1128, 0.0966,  ..., 0.0000, 0.0481, 0.0000],
        [0.0237, 0.1085, 0.0756,  ..., 0.0000, 0.0420, 0.0000],
        ...,
        [0.0008, 0.0954, 0.0132,  ..., 0.0000, 0.0227, 0.0000],
        [0.0008, 0.0953, 0.0132,  ..., 0.0000, 0.0227, 0.0000],
        [0.0008, 0.0952, 0.0132,  ..., 0.0000, 0.0227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654098.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3630.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(456.4455, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6689.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1338.5800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-836.3392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2898],
        [-1.3787],
        [-1.2566],
        ...,
        [-6.0713],
        [-6.0617],
        [-6.0614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300429.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370428.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0355],
        [1.0372],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370431.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0069,  0.0062, -0.0007,  ...,  0.0120, -0.0001,  0.0036],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3490.2974, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7087, device='cuda:0')



h[100].sum tensor(131.7551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0257, device='cuda:0')



h[200].sum tensor(53.2498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0829, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0198, 0.0000,  ..., 0.0380, 0.0000, 0.0111],
        [0.0191, 0.0180, 0.0000,  ..., 0.0342, 0.0000, 0.0103],
        [0.0301, 0.0267, 0.0000,  ..., 0.0521, 0.0000, 0.0158],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68886.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0451, 0.1206, 0.1329,  ..., 0.0013, 0.0588, 0.0000],
        [0.0477, 0.1227, 0.1396,  ..., 0.0027, 0.0610, 0.0000],
        [0.0486, 0.1237, 0.1422,  ..., 0.0014, 0.0620, 0.0000],
        ...,
        [0.0007, 0.0956, 0.0133,  ..., 0.0000, 0.0226, 0.0000],
        [0.0007, 0.0955, 0.0133,  ..., 0.0000, 0.0226, 0.0000],
        [0.0007, 0.0955, 0.0133,  ..., 0.0000, 0.0226, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583988.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2484.1306, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.6686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7095.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1105.5780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-661.8594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8993],
        [-0.5449],
        [-0.7215],
        ...,
        [-6.0659],
        [-6.0562],
        [-6.0559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325893.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0355],
        [1.0372],
        [1.0392],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370431.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0355],
        [1.0372],
        [1.0393],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370434.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0210,  0.0174, -0.0007,  ...,  0.0350, -0.0004,  0.0127],
        [ 0.0139,  0.0117, -0.0007,  ...,  0.0234, -0.0003,  0.0081],
        [ 0.0163,  0.0136, -0.0007,  ...,  0.0272, -0.0003,  0.0096],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3563.2759, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.7460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.1722, device='cuda:0')



h[100].sum tensor(131.5814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.4116, device='cuda:0')



h[200].sum tensor(54.5950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0606, 0.0509, 0.0000,  ..., 0.1018, 0.0000, 0.0356],
        [0.0699, 0.0583, 0.0000,  ..., 0.1170, 0.0000, 0.0415],
        [0.0635, 0.0532, 0.0000,  ..., 0.1066, 0.0000, 0.0374],
        ...,
        [0.0007, 0.0034, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0007, 0.0034, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71918.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1432, 0.1722, 0.3871,  ..., 0.0579, 0.1344, 0.0000],
        [0.1438, 0.1729, 0.3886,  ..., 0.0585, 0.1350, 0.0000],
        [0.1322, 0.1675, 0.3584,  ..., 0.0512, 0.1259, 0.0000],
        ...,
        [0.0006, 0.0957, 0.0135,  ..., 0.0000, 0.0226, 0.0000],
        [0.0006, 0.0957, 0.0134,  ..., 0.0000, 0.0226, 0.0000],
        [0.0006, 0.0956, 0.0134,  ..., 0.0000, 0.0226, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602571.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2753.1030, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.1330, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6955.4009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1147.4818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-694.4655, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4052],
        [ 0.3983],
        [ 0.3897],
        ...,
        [-6.0617],
        [-6.0521],
        [-6.0518]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315952., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0355],
        [1.0372],
        [1.0393],
        ...,
        [0.9952],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370434.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0372],
        [1.0392],
        ...,
        [0.9951],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370437.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3384.3035, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.8072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9627, device='cuda:0')



h[100].sum tensor(130.0786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7954, device='cuda:0')



h[200].sum tensor(52.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0055, 0.0072, 0.0000,  ..., 0.0120, 0.0000, 0.0023],
        [0.0008, 0.0034, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67318.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0269, 0.1097, 0.0843,  ..., 0.0000, 0.0439, 0.0000],
        [0.0144, 0.1026, 0.0504,  ..., 0.0000, 0.0336, 0.0000],
        [0.0054, 0.0973, 0.0263,  ..., 0.0000, 0.0263, 0.0000],
        ...,
        [0.0006, 0.0957, 0.0134,  ..., 0.0000, 0.0227, 0.0000],
        [0.0006, 0.0956, 0.0134,  ..., 0.0000, 0.0227, 0.0000],
        [0.0006, 0.0956, 0.0134,  ..., 0.0000, 0.0227, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576732.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2319.7881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.5804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7227.6694, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1083.3053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-644.7537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9639],
        [-1.9981],
        [-3.0111],
        ...,
        [-6.0509],
        [-6.0413],
        [-6.0410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321073.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0372],
        [1.0392],
        ...,
        [0.9951],
        [0.9941],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370437.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0372],
        [1.0392],
        ...,
        [0.9951],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370440.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0008, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4875.2959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(42.1428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.5109, device='cuda:0')



h[100].sum tensor(134.9733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(55.3413, device='cuda:0')



h[200].sum tensor(73.5125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.0646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.0275, 0.0000,  ..., 0.0535, 0.0000, 0.0179],
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0035, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(88935.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0973, 0.1440, 0.2660,  ..., 0.0385, 0.0985, 0.0000],
        [0.0302, 0.1099, 0.0909,  ..., 0.0062, 0.0458, 0.0000],
        [0.0082, 0.0987, 0.0338,  ..., 0.0000, 0.0286, 0.0000],
        ...,
        [0.0005, 0.0957, 0.0135,  ..., 0.0000, 0.0229, 0.0000],
        [0.0005, 0.0956, 0.0135,  ..., 0.0000, 0.0228, 0.0000],
        [0.0005, 0.0956, 0.0135,  ..., 0.0000, 0.0228, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(658803.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3586.0229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(493.4308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6816.1709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1386.5889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-874.3710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1031],
        [-0.6554],
        [-1.3421],
        ...,
        [-6.0354],
        [-6.0260],
        [-6.0257]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283944.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0372],
        [1.0392],
        ...,
        [0.9951],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370440.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0372],
        [1.0392],
        ...,
        [0.9951],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370443.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0068,  0.0061, -0.0007,  ...,  0.0117, -0.0001,  0.0035],
        [ 0.0154,  0.0129, -0.0007,  ...,  0.0258, -0.0003,  0.0090],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3716.9058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.3314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1124, device='cuda:0')



h[100].sum tensor(129.7527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2223, device='cuda:0')



h[200].sum tensor(57.8691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0748, 0.0622, 0.0000,  ..., 0.1246, 0.0000, 0.0446],
        [0.0394, 0.0341, 0.0000,  ..., 0.0670, 0.0000, 0.0225],
        [0.0410, 0.0354, 0.0000,  ..., 0.0697, 0.0000, 0.0236],
        ...,
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71069.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1539, 0.1774, 0.4156,  ..., 0.0629, 0.1436, 0.0000],
        [0.1119, 0.1570, 0.3066,  ..., 0.0368, 0.1109, 0.0000],
        [0.0878, 0.1440, 0.2435,  ..., 0.0240, 0.0921, 0.0000],
        ...,
        [0.0005, 0.0956, 0.0133,  ..., 0.0000, 0.0231, 0.0000],
        [0.0005, 0.0955, 0.0133,  ..., 0.0000, 0.0231, 0.0000],
        [0.0005, 0.0955, 0.0133,  ..., 0.0000, 0.0231, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585497.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2463.2607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(336.2480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7126.5732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1140.9116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-681.7841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4250],
        [ 0.3298],
        [-0.1085],
        ...,
        [-6.0286],
        [-6.0192],
        [-6.0190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286088.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0372],
        [1.0392],
        ...,
        [0.9951],
        [0.9941],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370443.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9950],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370446.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3311.4414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4905, device='cuda:0')



h[100].sum tensor(127.9945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.3836, device='cuda:0')



h[200].sum tensor(52.0938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0035, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0187, 0.0177, 0.0000,  ..., 0.0333, 0.0000, 0.0099],
        ...,
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67058.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0952, 0.0212,  ..., 0.0000, 0.0252, 0.0000],
        [0.0231, 0.1064, 0.0725,  ..., 0.0024, 0.0408, 0.0000],
        [0.0634, 0.1293, 0.1793,  ..., 0.0175, 0.0731, 0.0000],
        ...,
        [0.0004, 0.0956, 0.0127,  ..., 0.0000, 0.0232, 0.0000],
        [0.0004, 0.0955, 0.0127,  ..., 0.0000, 0.0232, 0.0000],
        [0.0004, 0.0955, 0.0127,  ..., 0.0000, 0.0232, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577212.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2261.9517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.1729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7264.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1086.8619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-635.9005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9023],
        [-1.1872],
        [-0.4203],
        ...,
        [-6.0451],
        [-6.0357],
        [-6.0356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297161.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0392],
        ...,
        [0.9950],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370446.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 280.0 event: 1400 loss: tensor(423.9568, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0392],
        ...,
        [0.9950],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370450.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3556.2637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9212, device='cuda:0')



h[100].sum tensor(128.8269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.6611, device='cuda:0')



h[200].sum tensor(55.0914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0035, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0185, 0.0176, 0.0000,  ..., 0.0330, 0.0000, 0.0098],
        ...,
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70614.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0083, 0.0983, 0.0340,  ..., 0.0000, 0.0291, 0.0000],
        [0.0298, 0.1111, 0.0911,  ..., 0.0030, 0.0463, 0.0000],
        [0.0636, 0.1312, 0.1817,  ..., 0.0147, 0.0735, 0.0000],
        ...,
        [0.0002, 0.0959, 0.0123,  ..., 0.0000, 0.0232, 0.0000],
        [0.0002, 0.0958, 0.0123,  ..., 0.0000, 0.0232, 0.0000],
        [0.0002, 0.0958, 0.0123,  ..., 0.0000, 0.0232, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589716.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2326.8257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.3278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7345.7529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1135.9769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-669.8216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6527],
        [-0.7137],
        [-0.0963],
        ...,
        [-5.9365],
        [-5.9752],
        [-6.0047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318832.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0392],
        ...,
        [0.9950],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370450.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0393],
        ...,
        [0.9950],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370453.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3679.8135, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.4412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6560, device='cuda:0')



h[100].sum tensor(129.1628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.8578, device='cuda:0')



h[200].sum tensor(56.5892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0035, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0036, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71964.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0212, 0.1060, 0.0692,  ..., 0.0000, 0.0398, 0.0000],
        [0.0245, 0.1087, 0.0781,  ..., 0.0000, 0.0426, 0.0000],
        [0.0244, 0.1090, 0.0779,  ..., 0.0000, 0.0426, 0.0000],
        ...,
        [0.0001, 0.0959, 0.0120,  ..., 0.0000, 0.0234, 0.0000],
        [0.0001, 0.0958, 0.0120,  ..., 0.0000, 0.0234, 0.0000],
        [0.0001, 0.0958, 0.0120,  ..., 0.0000, 0.0234, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592957.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2319.2502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(346.2648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7408.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1157.0419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-682.2675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0025],
        [-0.5670],
        [-0.2560],
        ...,
        [-6.0750],
        [-6.0657],
        [-6.0658]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320445.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0393],
        ...,
        [0.9950],
        [0.9940],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370453.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0393],
        ...,
        [0.9949],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370456.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0060, -0.0007,  ...,  0.0115, -0.0001,  0.0034],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0066,  0.0060, -0.0007,  ...,  0.0115, -0.0001,  0.0034],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3629.4395, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.1111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3713, device='cuda:0')



h[100].sum tensor(128.7466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.0065, device='cuda:0')



h[200].sum tensor(55.8400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1568, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0110, 0.0117, 0.0000,  ..., 0.0207, 0.0000, 0.0058],
        [0.0348, 0.0306, 0.0000,  ..., 0.0595, 0.0000, 0.0189],
        [0.0112, 0.0118, 0.0000,  ..., 0.0210, 0.0000, 0.0059],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71355.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0317, 0.1132, 0.0985,  ..., 0.0011, 0.0491, 0.0000],
        [0.0497, 0.1249, 0.1473,  ..., 0.0022, 0.0638, 0.0000],
        [0.0333, 0.1153, 0.1033,  ..., 0.0011, 0.0509, 0.0000],
        ...,
        [0.0002, 0.0956, 0.0116,  ..., 0.0000, 0.0238, 0.0000],
        [0.0002, 0.0956, 0.0116,  ..., 0.0000, 0.0237, 0.0000],
        [0.0002, 0.0955, 0.0116,  ..., 0.0000, 0.0237, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592758.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2351.5825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(339.9498, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7420.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1153.7817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-674.2828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4789],
        [-0.9003],
        [-1.0723],
        ...,
        [-6.0649],
        [-6.0550],
        [-6.0460]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292093.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0393],
        ...,
        [0.9949],
        [0.9940],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370456.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0394],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370458.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0146,  0.0123, -0.0007,  ...,  0.0244, -0.0003,  0.0085],
        [ 0.0068,  0.0061, -0.0007,  ...,  0.0117, -0.0001,  0.0035],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3359.1555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.7042, device='cuda:0')



h[100].sum tensor(128.0190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.0227, device='cuda:0')



h[200].sum tensor(51.7909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9708, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0186, 0.0000,  ..., 0.0349, 0.0000, 0.0107],
        [0.0213, 0.0199, 0.0000,  ..., 0.0375, 0.0000, 0.0117],
        [0.0346, 0.0304, 0.0000,  ..., 0.0591, 0.0000, 0.0187],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66751.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0634, 0.1297, 0.1820,  ..., 0.0122, 0.0740, 0.0000],
        [0.0589, 0.1289, 0.1712,  ..., 0.0078, 0.0709, 0.0000],
        [0.0665, 0.1349, 0.1925,  ..., 0.0088, 0.0772, 0.0000],
        ...,
        [0.0002, 0.0955, 0.0112,  ..., 0.0000, 0.0239, 0.0000],
        [0.0002, 0.0955, 0.0112,  ..., 0.0000, 0.0239, 0.0000],
        [0.0002, 0.0954, 0.0112,  ..., 0.0000, 0.0239, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574348.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2064.4194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.7530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7700.2373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1087.2772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-621.9021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4475],
        [ 0.4502],
        [ 0.4462],
        ...,
        [-6.0962],
        [-6.0869],
        [-6.0871]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312921.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0394],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370458.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0394],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370461.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0095,  0.0083, -0.0007,  ...,  0.0161, -0.0002,  0.0053],
        [ 0.0128,  0.0108, -0.0007,  ...,  0.0214, -0.0002,  0.0074],
        [ 0.0125,  0.0107, -0.0007,  ...,  0.0211, -0.0002,  0.0072],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4516.7339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.0681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9264, device='cuda:0')



h[100].sum tensor(132.7589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.6145, device='cuda:0')



h[200].sum tensor(67.4129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0216, 0.0201, 0.0000,  ..., 0.0378, 0.0000, 0.0119],
        [0.0340, 0.0299, 0.0000,  ..., 0.0580, 0.0000, 0.0191],
        [0.0734, 0.0612, 0.0000,  ..., 0.1222, 0.0000, 0.0438],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85484.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0719, 0.1346, 0.2041,  ..., 0.0154, 0.0804, 0.0000],
        [0.1022, 0.1505, 0.2843,  ..., 0.0342, 0.1042, 0.0000],
        [0.1491, 0.1730, 0.4077,  ..., 0.0668, 0.1408, 0.0000],
        ...,
        [0.0004, 0.0952, 0.0110,  ..., 0.0000, 0.0241, 0.0000],
        [0.0004, 0.0951, 0.0110,  ..., 0.0000, 0.0241, 0.0000],
        [0.0004, 0.0951, 0.0110,  ..., 0.0000, 0.0241, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653081.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3288.5146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(464.8391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7690.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1341.9115, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-818.4012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2698],
        [ 0.3530],
        [ 0.3391],
        ...,
        [-6.1133],
        [-6.1040],
        [-6.1041]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316625.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0353],
        [1.0371],
        [1.0394],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370461.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370463.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0112,  0.0096, -0.0007,  ...,  0.0188, -0.0002,  0.0063],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3382.4353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0298, device='cuda:0')



h[100].sum tensor(128.5279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.9960, device='cuda:0')



h[200].sum tensor(51.9796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0036, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0123, 0.0127, 0.0000,  ..., 0.0224, 0.0000, 0.0066],
        [0.0103, 0.0111, 0.0000,  ..., 0.0191, 0.0000, 0.0053],
        ...,
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67304.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0970, 0.0325,  ..., 0.0000, 0.0303, 0.0000],
        [0.0265, 0.1073, 0.0795,  ..., 0.0031, 0.0443, 0.0000],
        [0.0480, 0.1193, 0.1367,  ..., 0.0110, 0.0612, 0.0000],
        ...,
        [0.0007, 0.0946, 0.0107,  ..., 0.0000, 0.0245, 0.0000],
        [0.0007, 0.0946, 0.0107,  ..., 0.0000, 0.0245, 0.0000],
        [0.0007, 0.0945, 0.0107,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(579743.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2282.2056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(303.6307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8175.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1084.6427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-622.0342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0344],
        [-2.4941],
        [-1.4819],
        ...,
        [-6.1100],
        [-6.0999],
        [-6.0832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324803.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370463.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370466.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0167,  0.0140, -0.0007,  ...,  0.0279, -0.0003,  0.0099],
        [ 0.0385,  0.0312, -0.0009,  ...,  0.0633, -0.0007,  0.0240],
        [ 0.0158,  0.0133, -0.0007,  ...,  0.0264, -0.0003,  0.0093],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3740.4246, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.8908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1161, device='cuda:0')



h[100].sum tensor(130.9713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.2335, device='cuda:0')



h[200].sum tensor(56.4625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0840, 0.0696, 0.0000,  ..., 0.1393, 0.0000, 0.0506],
        [0.0668, 0.0560, 0.0000,  ..., 0.1113, 0.0000, 0.0394],
        [0.0680, 0.0570, 0.0000,  ..., 0.1133, 0.0000, 0.0410],
        ...,
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72625.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1527, 0.1718, 0.4155,  ..., 0.0704, 0.1428, 0.0000],
        [0.1507, 0.1723, 0.4098,  ..., 0.0687, 0.1410, 0.0000],
        [0.1349, 0.1633, 0.3675,  ..., 0.0606, 0.1289, 0.0000],
        ...,
        [0.0009, 0.0947, 0.0106,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0946, 0.0106,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0946, 0.0106,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598644.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2651.0366, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(347.1141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7891.3115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1160.6647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-679.3627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3664],
        [ 0.2889],
        [-0.0715],
        ...,
        [-6.1424],
        [-6.1329],
        [-6.1332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310392.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370466.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370466.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0123,  0.0104, -0.0007,  ...,  0.0206, -0.0002,  0.0070],
        [ 0.0072,  0.0064, -0.0007,  ...,  0.0123, -0.0001,  0.0037],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3486.1421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.8166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.6393, device='cuda:0')



h[100].sum tensor(129.9983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.8181, device='cuda:0')



h[200].sum tensor(52.9961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0424, 0.0366, 0.0000,  ..., 0.0716, 0.0000, 0.0237],
        [0.0236, 0.0217, 0.0000,  ..., 0.0409, 0.0000, 0.0131],
        [0.0081, 0.0094, 0.0000,  ..., 0.0156, 0.0000, 0.0039],
        ...,
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69040.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0735, 0.1348, 0.2068,  ..., 0.0145, 0.0817, 0.0000],
        [0.0526, 0.1233, 0.1504,  ..., 0.0082, 0.0652, 0.0000],
        [0.0271, 0.1089, 0.0818,  ..., 0.0007, 0.0452, 0.0000],
        ...,
        [0.0009, 0.0947, 0.0106,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0946, 0.0106,  ..., 0.0000, 0.0245, 0.0000],
        [0.0009, 0.0946, 0.0106,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586413.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2444.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.1810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8121.3223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1106.8605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-641.0086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0531],
        [-0.4636],
        [-1.3106],
        ...,
        [-6.1424],
        [-6.1329],
        [-6.1332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318144.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370466.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370468.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3369.0859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.8491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8105, device='cuda:0')



h[100].sum tensor(130.7731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.3404, device='cuda:0')



h[200].sum tensor(50.9366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0036, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66732.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0924, 0.0102,  ..., 0.0000, 0.0236, 0.0000],
        [0.0009, 0.0931, 0.0103,  ..., 0.0000, 0.0238, 0.0000],
        [0.0009, 0.0935, 0.0104,  ..., 0.0000, 0.0239, 0.0000],
        ...,
        [0.0010, 0.0948, 0.0106,  ..., 0.0000, 0.0244, 0.0000],
        [0.0010, 0.0947, 0.0106,  ..., 0.0000, 0.0243, 0.0000],
        [0.0010, 0.0947, 0.0106,  ..., 0.0000, 0.0243, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576151.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2306.5417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.2001, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8127.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1071.7977, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-616.6509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7654],
        [-4.6085],
        [-5.1359],
        ...,
        [-6.1687],
        [-6.1592],
        [-6.1594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-336380.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0395],
        ...,
        [0.9949],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370468.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370471.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3669.3606, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.3132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8002, device='cuda:0')



h[100].sum tensor(132.4699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.2888, device='cuda:0')



h[200].sum tensor(54.9105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0036, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72478.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0924, 0.0102,  ..., 0.0000, 0.0236, 0.0000],
        [0.0014, 0.0934, 0.0112,  ..., 0.0000, 0.0241, 0.0000],
        [0.0021, 0.0944, 0.0137,  ..., 0.0000, 0.0251, 0.0000],
        ...,
        [0.0029, 0.0960, 0.0155,  ..., 0.0000, 0.0258, 0.0000],
        [0.0063, 0.0982, 0.0253,  ..., 0.0000, 0.0287, 0.0000],
        [0.0080, 0.0993, 0.0301,  ..., 0.0000, 0.0302, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603122.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2770.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.1912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7892.3779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1153.8260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-679.0763, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5385],
        [-5.2157],
        [-4.7177],
        ...,
        [-5.6461],
        [-5.3036],
        [-5.0893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315598.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370471.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 290.0 event: 1450 loss: tensor(416.8470, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370474.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0088,  0.0077, -0.0007,  ...,  0.0150, -0.0001,  0.0048],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3122.6689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.2854, device='cuda:0')



h[100].sum tensor(129.9465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.7810, device='cuda:0')



h[200].sum tensor(47.9262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8126, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0037, 0.0000,  ..., 0.0038, 0.0000, 0.0000],
        [0.0098, 0.0108, 0.0000,  ..., 0.0184, 0.0000, 0.0050],
        [0.0308, 0.0275, 0.0000,  ..., 0.0526, 0.0000, 0.0177],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64475.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0176, 0.1020, 0.0553,  ..., 0.0000, 0.0373, 0.0000],
        [0.0374, 0.1132, 0.1084,  ..., 0.0067, 0.0528, 0.0000],
        [0.0855, 0.1383, 0.2371,  ..., 0.0312, 0.0903, 0.0000],
        ...,
        [0.0014, 0.0945, 0.0108,  ..., 0.0000, 0.0247, 0.0000],
        [0.0014, 0.0945, 0.0108,  ..., 0.0000, 0.0247, 0.0000],
        [0.0014, 0.0945, 0.0108,  ..., 0.0000, 0.0247, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573057.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2335.3706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.2949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8321.6465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1035.6617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-589.0334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1774],
        [-0.7715],
        [-0.2517],
        ...,
        [-6.1605],
        [-6.1511],
        [-6.1514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-342697.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370474.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370476.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0263,  0.0216, -0.0008,  ...,  0.0435, -0.0004,  0.0161],
        [ 0.0241,  0.0199, -0.0008,  ...,  0.0399, -0.0004,  0.0147],
        [ 0.0226,  0.0187, -0.0008,  ...,  0.0375, -0.0004,  0.0137],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3825.5117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.8628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5723, device='cuda:0')



h[100].sum tensor(132.4960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.5973, device='cuda:0')



h[200].sum tensor(57.9935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2907, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0571, 0.0484, 0.0000,  ..., 0.0956, 0.0000, 0.0340],
        [0.0810, 0.0674, 0.0000,  ..., 0.1345, 0.0000, 0.0486],
        [0.0878, 0.0728, 0.0000,  ..., 0.1456, 0.0000, 0.0530],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74060.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1657, 0.1784, 0.4518,  ..., 0.0793, 0.1523, 0.0000],
        [0.1866, 0.1902, 0.5080,  ..., 0.0921, 0.1687, 0.0000],
        [0.1931, 0.1945, 0.5252,  ..., 0.0956, 0.1737, 0.0000],
        ...,
        [0.0017, 0.0945, 0.0114,  ..., 0.0000, 0.0248, 0.0000],
        [0.0017, 0.0944, 0.0114,  ..., 0.0000, 0.0248, 0.0000],
        [0.0017, 0.0944, 0.0114,  ..., 0.0000, 0.0248, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607833.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2967.7974, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.2174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7937.0454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1172.6638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-691.3569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2147],
        [ 0.2153],
        [ 0.2186],
        ...,
        [-6.1422],
        [-6.1330],
        [-6.1334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308322.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0354],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370476.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0355],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370479.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.3337e-02,  1.1328e-02, -6.9781e-04,  ...,  2.2305e-02,
         -2.1795e-04,  7.6888e-03],
        [ 5.9393e-03,  5.4579e-03, -6.4881e-04,  ...,  1.0243e-02,
         -9.5012e-05,  2.9029e-03],
        [ 5.7683e-03,  5.3221e-03, -6.4768e-04,  ...,  9.9637e-03,
         -9.2170e-05,  2.7923e-03],
        ...,
        [ 2.2220e-04,  9.2063e-04, -6.1094e-04,  ...,  9.1968e-04,
          0.0000e+00, -7.9595e-04],
        [ 2.2220e-04,  9.2063e-04, -6.1094e-04,  ...,  9.1968e-04,
          0.0000e+00, -7.9595e-04],
        [ 2.2220e-04,  9.2063e-04, -6.1094e-04,  ...,  9.1968e-04,
          0.0000e+00, -7.9595e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3455.0383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.7150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.2516, device='cuda:0')



h[100].sum tensor(130.9048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.6593, device='cuda:0')



h[200].sum tensor(53.4544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0275, 0.0249, 0.0000,  ..., 0.0471, 0.0000, 0.0139],
        [0.0377, 0.0330, 0.0000,  ..., 0.0638, 0.0000, 0.0205],
        [0.0313, 0.0279, 0.0000,  ..., 0.0533, 0.0000, 0.0163],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68797.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0806, 0.1393, 0.2270,  ..., 0.0151, 0.0873, 0.0000],
        [0.0857, 0.1427, 0.2407,  ..., 0.0184, 0.0913, 0.0000],
        [0.0763, 0.1376, 0.2154,  ..., 0.0131, 0.0841, 0.0000],
        ...,
        [0.0020, 0.0943, 0.0118,  ..., 0.0000, 0.0249, 0.0000],
        [0.0020, 0.0942, 0.0118,  ..., 0.0000, 0.0249, 0.0000],
        [0.0020, 0.0942, 0.0118,  ..., 0.0000, 0.0249, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585096.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2684.2476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.0014, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8142.1597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1096.3324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-634.9697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4140],
        [ 0.4178],
        [ 0.4241],
        ...,
        [-6.1255],
        [-6.1164],
        [-6.1169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303662.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0355],
        [1.0371],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370479.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370481.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0188,  0.0156, -0.0007,  ...,  0.0312, -0.0003,  0.0112],
        [ 0.0086,  0.0076, -0.0007,  ...,  0.0146, -0.0001,  0.0046],
        [ 0.0094,  0.0082, -0.0007,  ...,  0.0160, -0.0002,  0.0052],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3551.3669, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.5726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7822, device='cuda:0')



h[100].sum tensor(131.7024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.2454, device='cuda:0')



h[200].sum tensor(54.9116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0479, 0.0412, 0.0000,  ..., 0.0806, 0.0000, 0.0272],
        [0.0525, 0.0448, 0.0000,  ..., 0.0881, 0.0000, 0.0301],
        [0.0314, 0.0281, 0.0000,  ..., 0.0536, 0.0000, 0.0173],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69869.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0866, 0.1406, 0.2413,  ..., 0.0251, 0.0905, 0.0000],
        [0.0878, 0.1416, 0.2442,  ..., 0.0262, 0.0915, 0.0000],
        [0.0728, 0.1340, 0.2040,  ..., 0.0172, 0.0799, 0.0000],
        ...,
        [0.0020, 0.0945, 0.0124,  ..., 0.0000, 0.0248, 0.0000],
        [0.0020, 0.0944, 0.0124,  ..., 0.0000, 0.0248, 0.0000],
        [0.0020, 0.0944, 0.0124,  ..., 0.0000, 0.0248, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587769.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2757.2136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.5525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7944.9458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1114.4069, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-648.4192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4111],
        [ 0.3679],
        [ 0.1440],
        ...,
        [-6.1252],
        [-6.1161],
        [-6.1165]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292668.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370481.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370483.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.9679e-03,  8.6657e-03, -6.7560e-04,  ...,  1.6875e-02,
         -1.5991e-04,  5.5195e-03],
        [ 6.5310e-03,  5.9378e-03, -6.5284e-04,  ...,  1.1270e-02,
         -1.0362e-04,  3.2961e-03],
        [ 5.1131e-03,  4.8125e-03, -6.4345e-04,  ...,  8.9571e-03,
         -8.0390e-05,  2.3789e-03],
        ...,
        [ 2.0557e-04,  9.1729e-04, -6.1094e-04,  ...,  9.5316e-04,
          0.0000e+00, -7.9591e-04],
        [ 2.0557e-04,  9.1729e-04, -6.1094e-04,  ...,  9.5316e-04,
          0.0000e+00, -7.9591e-04],
        [ 2.0557e-04,  9.1729e-04, -6.1094e-04,  ...,  9.5316e-04,
          0.0000e+00, -7.9591e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4118.7046, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.9839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.3969, device='cuda:0')



h[100].sum tensor(134.6288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.0522, device='cuda:0')



h[200].sum tensor(62.5324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0353, 0.0311, 0.0000,  ..., 0.0601, 0.0000, 0.0198],
        [0.0304, 0.0273, 0.0000,  ..., 0.0522, 0.0000, 0.0166],
        [0.0530, 0.0452, 0.0000,  ..., 0.0890, 0.0000, 0.0304],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78032.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1296, 0.1629, 0.3580,  ..., 0.0532, 0.1239, 0.0000],
        [0.1055, 0.1525, 0.2942,  ..., 0.0355, 0.1055, 0.0000],
        [0.1153, 0.1586, 0.3209,  ..., 0.0410, 0.1133, 0.0000],
        ...,
        [0.0021, 0.0947, 0.0129,  ..., 0.0000, 0.0245, 0.0000],
        [0.0021, 0.0947, 0.0129,  ..., 0.0000, 0.0245, 0.0000],
        [0.0021, 0.0946, 0.0129,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617054.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3190.5825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(398.0435, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7802.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1222.4253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-734.3049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1825],
        [ 0.2152],
        [ 0.2327],
        ...,
        [-6.1403],
        [-6.1314],
        [-6.1318]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315819.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370483.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370483.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3258.5544, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.6306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9421, device='cuda:0')



h[100].sum tensor(131.3467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.7442, device='cuda:0')



h[200].sum tensor(50.8536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8858, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0365, 0.0321, 0.0000,  ..., 0.0621, 0.0000, 0.0214],
        [0.0009, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65784.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0792, 0.1350, 0.2213,  ..., 0.0261, 0.0842, 0.0000],
        [0.0296, 0.1088, 0.0876,  ..., 0.0032, 0.0457, 0.0000],
        [0.0130, 0.0999, 0.0427,  ..., 0.0000, 0.0329, 0.0000],
        ...,
        [0.0021, 0.0947, 0.0129,  ..., 0.0000, 0.0245, 0.0000],
        [0.0021, 0.0947, 0.0129,  ..., 0.0000, 0.0245, 0.0000],
        [0.0021, 0.0946, 0.0129,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570429.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2486.8027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.9386, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8015.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1051.2234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-603.7464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3000],
        [-1.2029],
        [-2.3832],
        ...,
        [-6.1325],
        [-6.1208],
        [-6.1164]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325177.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9948],
        [0.9939],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370483.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370485.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4071.1431, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.4155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9574, device='cuda:0')



h[100].sum tensor(134.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.7384, device='cuda:0')



h[200].sum tensor(61.9996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0189, 0.0182, 0.0000,  ..., 0.0335, 0.0000, 0.0100],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78769.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0103, 0.0974, 0.0353,  ..., 0.0000, 0.0302, 0.0000],
        [0.0183, 0.1028, 0.0573,  ..., 0.0000, 0.0368, 0.0000],
        [0.0490, 0.1214, 0.1416,  ..., 0.0090, 0.0614, 0.0000],
        ...,
        [0.0022, 0.0948, 0.0132,  ..., 0.0000, 0.0244, 0.0000],
        [0.0021, 0.0947, 0.0132,  ..., 0.0000, 0.0244, 0.0000],
        [0.0021, 0.0947, 0.0132,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625959.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3410.2810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(404.7548, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7289.1050, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1240.2764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-745.0143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8303],
        [-2.3457],
        [-1.3157],
        ...,
        [-6.1469],
        [-6.1380],
        [-6.1385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273204.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0396],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370485.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370487.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.7924e-03,  5.3552e-03, -6.4791e-04,  ...,  1.0059e-02,
         -9.0121e-05,  2.7995e-03],
        [ 6.0538e-03,  5.5627e-03, -6.4964e-04,  ...,  1.0485e-02,
         -9.4342e-05,  2.9685e-03],
        [ 1.1636e-02,  9.9936e-03, -6.8661e-04,  ...,  1.9589e-02,
         -1.8446e-04,  6.5788e-03],
        ...,
        [ 2.1031e-04,  9.2429e-04, -6.1094e-04,  ...,  9.5524e-04,
          0.0000e+00, -8.1082e-04],
        [ 2.1031e-04,  9.2429e-04, -6.1094e-04,  ...,  9.5524e-04,
          0.0000e+00, -8.1082e-04],
        [ 2.1031e-04,  9.2429e-04, -6.1094e-04,  ...,  9.5524e-04,
          0.0000e+00, -8.1082e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4292.0879, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.2451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2984, device='cuda:0')



h[100].sum tensor(135.6638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.7474, device='cuda:0')



h[200].sum tensor(65.0780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0204, 0.0193, 0.0000,  ..., 0.0358, 0.0000, 0.0110],
        [0.0409, 0.0356, 0.0000,  ..., 0.0693, 0.0000, 0.0225],
        [0.0332, 0.0295, 0.0000,  ..., 0.0567, 0.0000, 0.0175],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82554.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0755, 0.1350, 0.2134,  ..., 0.0176, 0.0820, 0.0000],
        [0.0806, 0.1404, 0.2283,  ..., 0.0179, 0.0864, 0.0000],
        [0.0842, 0.1425, 0.2381,  ..., 0.0202, 0.0895, 0.0000],
        ...,
        [0.0023, 0.0945, 0.0132,  ..., 0.0000, 0.0244, 0.0000],
        [0.0023, 0.0944, 0.0132,  ..., 0.0000, 0.0244, 0.0000],
        [0.0023, 0.0944, 0.0132,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(645657.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3700.1313, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(440.3499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7473.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1285.9491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-784.5095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4170],
        [ 0.4462],
        [ 0.4556],
        ...,
        [-6.1462],
        [-6.1350],
        [-6.1333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295979.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0356],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370487.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370490.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0112,  0.0096, -0.0007,  ...,  0.0188, -0.0002,  0.0063],
        [ 0.0239,  0.0197, -0.0008,  ...,  0.0396, -0.0004,  0.0145],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3713.0215, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.8822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8287, device='cuda:0')



h[100].sum tensor(133.0106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.3741, device='cuda:0')



h[200].sum tensor(57.5996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0295, 0.0265, 0.0000,  ..., 0.0504, 0.0000, 0.0167],
        [0.0510, 0.0437, 0.0000,  ..., 0.0856, 0.0000, 0.0298],
        [0.0407, 0.0354, 0.0000,  ..., 0.0687, 0.0000, 0.0231],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72249.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0905, 0.1408, 0.2497,  ..., 0.0292, 0.0927, 0.0000],
        [0.1161, 0.1546, 0.3182,  ..., 0.0452, 0.1128, 0.0000],
        [0.1085, 0.1515, 0.2985,  ..., 0.0396, 0.1073, 0.0000],
        ...,
        [0.0026, 0.0939, 0.0131,  ..., 0.0000, 0.0246, 0.0000],
        [0.0026, 0.0939, 0.0131,  ..., 0.0000, 0.0246, 0.0000],
        [0.0026, 0.0939, 0.0131,  ..., 0.0000, 0.0246, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595965.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3047.0327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.3701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7653.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1140.8501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-675.5372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3499],
        [ 0.3592],
        [ 0.3395],
        ...,
        [-6.1437],
        [-6.1351],
        [-6.1355]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281574.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370490.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370492.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0110,  0.0095, -0.0007,  ...,  0.0185, -0.0002,  0.0061],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4214.8506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.9395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9565, device='cuda:0')



h[100].sum tensor(135.1957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.7251, device='cuda:0')



h[200].sum tensor(64.2307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5567, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0413, 0.0359, 0.0000,  ..., 0.0697, 0.0000, 0.0227],
        [0.0101, 0.0112, 0.0000,  ..., 0.0188, 0.0000, 0.0050],
        [0.0121, 0.0128, 0.0000,  ..., 0.0221, 0.0000, 0.0064],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79103.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0598, 0.1253, 0.1679,  ..., 0.0086, 0.0691, 0.0000],
        [0.0378, 0.1133, 0.1084,  ..., 0.0019, 0.0520, 0.0000],
        [0.0485, 0.1192, 0.1371,  ..., 0.0049, 0.0604, 0.0000],
        ...,
        [0.0027, 0.0938, 0.0130,  ..., 0.0000, 0.0246, 0.0000],
        [0.0027, 0.0937, 0.0130,  ..., 0.0000, 0.0245, 0.0000],
        [0.0027, 0.0937, 0.0130,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621949.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3447.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(412.2933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7603.7651, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1234.8324, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-749.0805, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2666],
        [ 0.2533],
        [ 0.2810],
        ...,
        [-6.1607],
        [-6.1521],
        [-6.1525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288290.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370492.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 300.0 event: 1500 loss: tensor(431.3278, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370494.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3817.6418, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.8639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5172, device='cuda:0')



h[100].sum tensor(134.0874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.4326, device='cuda:0')



h[200].sum tensor(58.4046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74454.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0915, 0.0123,  ..., 0.0000, 0.0237, 0.0000],
        [0.0025, 0.0922, 0.0124,  ..., 0.0000, 0.0239, 0.0000],
        [0.0026, 0.0925, 0.0125,  ..., 0.0000, 0.0240, 0.0000],
        ...,
        [0.0026, 0.0938, 0.0128,  ..., 0.0000, 0.0244, 0.0000],
        [0.0034, 0.0943, 0.0149,  ..., 0.0000, 0.0250, 0.0000],
        [0.0059, 0.0959, 0.0220,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607069.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3196.6013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(371.2017, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7631.9502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1170.1305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-699.6891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.3470],
        [-5.6287],
        [-5.8282],
        ...,
        [-6.1161],
        [-5.9668],
        [-5.7088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296225.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370494.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370496.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9565e-02,  1.6291e-02, -7.3923e-04,  ...,  3.2537e-02,
         -3.0379e-04,  1.1701e-02],
        [ 1.1784e-02,  1.0114e-02, -6.8770e-04,  ...,  1.9851e-02,
         -1.8177e-04,  6.6706e-03],
        [ 5.8872e-03,  5.4326e-03, -6.4864e-04,  ...,  1.0235e-02,
         -8.9286e-05,  2.8574e-03],
        ...,
        [ 1.9380e-04,  9.1292e-04, -6.1094e-04,  ...,  9.5239e-04,
          0.0000e+00, -8.2386e-04],
        [ 1.9380e-04,  9.1292e-04, -6.1094e-04,  ...,  9.5239e-04,
          0.0000e+00, -8.2386e-04],
        [ 1.9380e-04,  9.1292e-04, -6.1094e-04,  ...,  9.5239e-04,
          0.0000e+00, -8.2386e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3336.2446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.3747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5427, device='cuda:0')



h[100].sum tensor(132.9931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5397, device='cuda:0')



h[200].sum tensor(51.1762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0585, 0.0496, 0.0000,  ..., 0.0980, 0.0000, 0.0339],
        [0.0572, 0.0486, 0.0000,  ..., 0.0959, 0.0000, 0.0330],
        [0.0506, 0.0433, 0.0000,  ..., 0.0852, 0.0000, 0.0288],
        ...,
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66531.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1632, 0.1813, 0.4446,  ..., 0.0754, 0.1487, 0.0000],
        [0.1493, 0.1756, 0.4081,  ..., 0.0650, 0.1383, 0.0000],
        [0.1374, 0.1706, 0.3769,  ..., 0.0562, 0.1293, 0.0000],
        ...,
        [0.0025, 0.0941, 0.0126,  ..., 0.0000, 0.0243, 0.0000],
        [0.0025, 0.0941, 0.0126,  ..., 0.0000, 0.0242, 0.0000],
        [0.0025, 0.0940, 0.0126,  ..., 0.0000, 0.0242, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575280.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2636.3071, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.8700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7902.8901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1057.5417, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-615.9368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3045],
        [ 0.3349],
        [ 0.3557],
        ...,
        [-6.2302],
        [-6.2215],
        [-6.2218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-329060.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370496.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370499.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0158,  0.0133, -0.0007,  ...,  0.0264, -0.0002,  0.0093],
        [ 0.0166,  0.0139, -0.0007,  ...,  0.0277, -0.0003,  0.0098],
        [ 0.0225,  0.0187, -0.0008,  ...,  0.0374, -0.0003,  0.0136],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4121.5576, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.3558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2601, device='cuda:0')



h[100].sum tensor(136.2840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.6432, device='cuda:0')



h[200].sum tensor(61.3723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0897, 0.0744, 0.0000,  ..., 0.1490, 0.0000, 0.0542],
        [0.0674, 0.0567, 0.0000,  ..., 0.1126, 0.0000, 0.0405],
        [0.0369, 0.0325, 0.0000,  ..., 0.0629, 0.0000, 0.0217],
        ...,
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0081, 0.0097, 0.0000,  ..., 0.0160, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78596.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2226, 0.2063, 0.6018,  ..., 0.1223, 0.1951, 0.0000],
        [0.1707, 0.1818, 0.4635,  ..., 0.0866, 0.1548, 0.0000],
        [0.1144, 0.1541, 0.3132,  ..., 0.0487, 0.1111, 0.0000],
        ...,
        [0.0033, 0.0949, 0.0152,  ..., 0.0000, 0.0251, 0.0000],
        [0.0086, 0.0982, 0.0298,  ..., 0.0000, 0.0293, 0.0000],
        [0.0267, 0.1092, 0.0791,  ..., 0.0011, 0.0436, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624433.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3404.2017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(406.6500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7292.3628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1234.5599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-748.0893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2845],
        [ 0.3215],
        [ 0.3534],
        ...,
        [-5.5825],
        [-4.5860],
        [-3.0652]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293641.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0372],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370499.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0371],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370502.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.6683e-02,  1.4012e-02, -7.2020e-04,  ...,  2.7852e-02,
         -2.5500e-04,  9.8543e-03],
        [ 9.7639e-03,  8.5186e-03, -6.7438e-04,  ...,  1.6573e-02,
         -1.4807e-04,  5.3807e-03],
        [ 5.8808e-03,  5.4355e-03, -6.4867e-04,  ...,  1.0243e-02,
         -8.8051e-05,  2.8700e-03],
        ...,
        [ 1.8388e-04,  9.1209e-04, -6.1094e-04,  ...,  9.5582e-04,
          0.0000e+00, -8.1355e-04],
        [ 1.8388e-04,  9.1209e-04, -6.1094e-04,  ...,  9.5582e-04,
          0.0000e+00, -8.1355e-04],
        [ 1.8388e-04,  9.1209e-04, -6.1094e-04,  ...,  9.5582e-04,
          0.0000e+00, -8.1355e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3538.2979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.3626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7153, device='cuda:0')



h[100].sum tensor(133.9178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0456, device='cuda:0')



h[200].sum tensor(53.3350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0507, 0.0434, 0.0000,  ..., 0.0853, 0.0000, 0.0289],
        [0.0416, 0.0362, 0.0000,  ..., 0.0706, 0.0000, 0.0230],
        [0.0244, 0.0226, 0.0000,  ..., 0.0426, 0.0000, 0.0119],
        ...,
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69616.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0897, 0.1434, 0.2492,  ..., 0.0276, 0.0928, 0.0000],
        [0.0811, 0.1403, 0.2277,  ..., 0.0206, 0.0869, 0.0000],
        [0.0667, 0.1339, 0.1906,  ..., 0.0094, 0.0764, 0.0000],
        ...,
        [0.0022, 0.0943, 0.0123,  ..., 0.0000, 0.0244, 0.0000],
        [0.0022, 0.0942, 0.0123,  ..., 0.0000, 0.0244, 0.0000],
        [0.0022, 0.0942, 0.0123,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586707.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2770.9807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(330.7505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7658.7583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1103.1199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-649.1946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4752],
        [ 0.4966],
        [ 0.4374],
        ...,
        [-6.2504],
        [-6.2420],
        [-6.2427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-327586.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0371],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370502.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0371],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370502.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0158,  0.0133, -0.0007,  ...,  0.0264, -0.0002,  0.0093],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3690.2827, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.1835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7240, device='cuda:0')



h[100].sum tensor(134.4954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.0612, device='cuda:0')



h[200].sum tensor(55.3892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1961, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0233, 0.0217, 0.0000,  ..., 0.0408, 0.0000, 0.0129],
        [0.0449, 0.0388, 0.0000,  ..., 0.0759, 0.0000, 0.0260],
        ...,
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71239.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0197, 0.1021, 0.0593,  ..., 0.0006, 0.0374, 0.0000],
        [0.0614, 0.1259, 0.1718,  ..., 0.0190, 0.0703, 0.0000],
        [0.1098, 0.1519, 0.3014,  ..., 0.0451, 0.1081, 0.0000],
        ...,
        [0.0022, 0.0943, 0.0123,  ..., 0.0000, 0.0244, 0.0000],
        [0.0022, 0.0942, 0.0123,  ..., 0.0000, 0.0244, 0.0000],
        [0.0022, 0.0942, 0.0123,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593521., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2905.9353, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.0524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7479.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1129.2213, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-668.4543, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0582],
        [-1.7309],
        [-0.5914],
        ...,
        [-6.2500],
        [-6.2410],
        [-6.2410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314683.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0371],
        [1.0397],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370502.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0359],
        [1.0371],
        [1.0398],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370504.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0134,  0.0114, -0.0007,  ...,  0.0224, -0.0002,  0.0077],
        [ 0.0120,  0.0103, -0.0007,  ...,  0.0201, -0.0002,  0.0068],
        [ 0.0121,  0.0104, -0.0007,  ...,  0.0204, -0.0002,  0.0069],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3994.8130, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.5030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5628, device='cuda:0')



h[100].sum tensor(134.6482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5585, device='cuda:0')



h[200].sum tensor(59.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0543, 0.0463, 0.0000,  ..., 0.0909, 0.0000, 0.0312],
        [0.0369, 0.0325, 0.0000,  ..., 0.0626, 0.0000, 0.0199],
        [0.0382, 0.0335, 0.0000,  ..., 0.0648, 0.0000, 0.0207],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78133.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1204, 0.1560, 0.3271,  ..., 0.0504, 0.1166, 0.0000],
        [0.0895, 0.1426, 0.2462,  ..., 0.0277, 0.0931, 0.0000],
        [0.0768, 0.1366, 0.2126,  ..., 0.0194, 0.0833, 0.0000],
        ...,
        [0.0023, 0.0939, 0.0118,  ..., 0.0000, 0.0250, 0.0000],
        [0.0023, 0.0939, 0.0118,  ..., 0.0000, 0.0250, 0.0000],
        [0.0023, 0.0939, 0.0118,  ..., 0.0000, 0.0249, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628377.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3411.3374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(409.9191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7508.1514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1225.0665, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-740.7208, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2559],
        [ 0.1590],
        [-0.0967],
        ...,
        [-6.2384],
        [-6.2299],
        [-6.2303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298463.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0359],
        [1.0371],
        [1.0398],
        ...,
        [0.9947],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370504.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0359],
        [1.0370],
        [1.0398],
        ...,
        [0.9946],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370507., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.0857e-03,  6.3924e-03, -6.5651e-04,  ...,  1.2143e-02,
         -1.0481e-04,  3.6293e-03],
        [ 4.7539e-03,  4.5408e-03, -6.4107e-04,  ...,  8.3435e-03,
         -6.9301e-05,  2.1223e-03],
        [ 1.1636e-02,  1.0006e-02, -6.8663e-04,  ...,  1.9557e-02,
         -1.7411e-04,  6.5702e-03],
        ...,
        [ 2.0352e-04,  9.2730e-04, -6.1094e-04,  ...,  9.2932e-04,
          0.0000e+00, -8.1858e-04],
        [ 2.0352e-04,  9.2730e-04, -6.1094e-04,  ...,  9.2932e-04,
          0.0000e+00, -8.1858e-04],
        [ 2.0352e-04,  9.2730e-04, -6.1094e-04,  ...,  9.2932e-04,
          0.0000e+00, -8.1858e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3343.9934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.6868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5475, device='cuda:0')



h[100].sum tensor(131.7847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5540, device='cuda:0')



h[200].sum tensor(50.7859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9534, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0204, 0.0193, 0.0000,  ..., 0.0357, 0.0000, 0.0109],
        [0.0508, 0.0436, 0.0000,  ..., 0.0853, 0.0000, 0.0289],
        [0.0303, 0.0273, 0.0000,  ..., 0.0519, 0.0000, 0.0165],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66823.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0607, 0.1265, 0.1691,  ..., 0.0130, 0.0708, 0.0000],
        [0.0917, 0.1450, 0.2516,  ..., 0.0277, 0.0950, 0.0000],
        [0.0799, 0.1389, 0.2205,  ..., 0.0206, 0.0861, 0.0000],
        ...,
        [0.0021, 0.0940, 0.0115,  ..., 0.0000, 0.0251, 0.0000],
        [0.0021, 0.0940, 0.0115,  ..., 0.0000, 0.0251, 0.0000],
        [0.0021, 0.0939, 0.0115,  ..., 0.0000, 0.0251, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578591.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2584.9070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.9847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7811.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1066.9716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-617.8742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4360],
        [ 0.1649],
        [ 0.2015],
        ...,
        [-6.2458],
        [-6.2373],
        [-6.2377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331311.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0359],
        [1.0370],
        [1.0398],
        ...,
        [0.9946],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370507., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0359],
        [1.0370],
        [1.0398],
        ...,
        [0.9946],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370509.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0138,  0.0117, -0.0007,  ...,  0.0231, -0.0002,  0.0080],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4271.5361, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.1130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0003, device='cuda:0')



h[100].sum tensor(135.2408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.8561, device='cuda:0')



h[200].sum tensor(62.9759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0130, 0.0000,  ..., 0.0227, 0.0000, 0.0066],
        [0.0149, 0.0150, 0.0000,  ..., 0.0270, 0.0000, 0.0083],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80956.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0566, 0.1240, 0.1585,  ..., 0.0144, 0.0674, 0.0000],
        [0.0382, 0.1145, 0.1094,  ..., 0.0044, 0.0532, 0.0000],
        [0.0249, 0.1075, 0.0740,  ..., 0.0000, 0.0429, 0.0000],
        ...,
        [0.0017, 0.0946, 0.0116,  ..., 0.0000, 0.0250, 0.0000],
        [0.0017, 0.0946, 0.0116,  ..., 0.0000, 0.0250, 0.0000],
        [0.0017, 0.0945, 0.0116,  ..., 0.0000, 0.0250, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(638009.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3420.0769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(441.1025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7196.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1269.8788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-771.4794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1367],
        [-0.0414],
        [-0.2266],
        ...,
        [-6.2571],
        [-6.2485],
        [-6.2489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309927.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0359],
        [1.0370],
        [1.0398],
        ...,
        [0.9946],
        [0.9938],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370509.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0398],
        ...,
        [0.9946],
        [0.9937],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370512.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3758.4595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7673, device='cuda:0')



h[100].sum tensor(133.0428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.1907, device='cuda:0')



h[200].sum tensor(55.8472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0065, 0.0084, 0.0000,  ..., 0.0134, 0.0000, 0.0029],
        ...,
        [0.0008, 0.0039, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73125.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0933, 0.0131,  ..., 0.0000, 0.0249, 0.0000],
        [0.0066, 0.0970, 0.0260,  ..., 0.0000, 0.0288, 0.0000],
        [0.0183, 0.1051, 0.0584,  ..., 0.0000, 0.0384, 0.0000],
        ...,
        [0.0014, 0.0951, 0.0116,  ..., 0.0000, 0.0250, 0.0000],
        [0.0014, 0.0951, 0.0116,  ..., 0.0000, 0.0250, 0.0000],
        [0.0014, 0.0951, 0.0116,  ..., 0.0000, 0.0250, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600292.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2792.6638, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(373.4336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7078.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1167.8236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-688.3795, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2900],
        [-4.4114],
        [-3.1466],
        ...,
        [-6.2598],
        [-6.2518],
        [-6.2522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304885.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0398],
        ...,
        [0.9946],
        [0.9937],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370512.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0398],
        ...,
        [0.9946],
        [0.9937],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370515.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.8259e-04,  9.1289e-04, -6.1094e-04,  ...,  9.7832e-04,
          0.0000e+00, -7.6303e-04],
        [ 1.1654e-02,  1.0023e-02, -6.8690e-04,  ...,  1.9671e-02,
         -1.7093e-04,  6.6533e-03],
        [ 6.8906e-03,  6.2399e-03, -6.5536e-04,  ...,  1.1909e-02,
         -9.9954e-05,  3.5737e-03],
        ...,
        [ 1.8259e-04,  9.1289e-04, -6.1094e-04,  ...,  9.7832e-04,
          0.0000e+00, -7.6303e-04],
        [ 1.8259e-04,  9.1289e-04, -6.1094e-04,  ...,  9.7832e-04,
          0.0000e+00, -7.6303e-04],
        [ 1.8259e-04,  9.1289e-04, -6.1094e-04,  ...,  9.7832e-04,
          0.0000e+00, -7.6303e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4048.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.8055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6419, device='cuda:0')



h[100].sum tensor(133.6688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.7952, device='cuda:0')



h[200].sum tensor(59.7088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0286, 0.0259, 0.0000,  ..., 0.0494, 0.0000, 0.0148],
        [0.0175, 0.0171, 0.0000,  ..., 0.0314, 0.0000, 0.0084],
        [0.0447, 0.0387, 0.0000,  ..., 0.0756, 0.0000, 0.0252],
        ...,
        [0.0008, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78099.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0493, 0.1257, 0.1456,  ..., 0.0027, 0.0639, 0.0000],
        [0.0550, 0.1297, 0.1611,  ..., 0.0073, 0.0686, 0.0000],
        [0.0939, 0.1519, 0.2648,  ..., 0.0292, 0.0988, 0.0000],
        ...,
        [0.0011, 0.0955, 0.0115,  ..., 0.0000, 0.0252, 0.0000],
        [0.0011, 0.0955, 0.0115,  ..., 0.0000, 0.0252, 0.0000],
        [0.0011, 0.0955, 0.0115,  ..., 0.0000, 0.0252, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623416., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3089.7622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(418.7393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6885.8853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1242.8535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-742.4727, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3771],
        [ 0.3809],
        [ 0.4064],
        ...,
        [-6.2543],
        [-6.2461],
        [-6.2465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289221.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0398],
        ...,
        [0.9946],
        [0.9937],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370515.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 310.0 event: 1550 loss: tensor(440.4683, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9937],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370518.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3388.2610, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.4723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.4607, device='cuda:0')



h[100].sum tensor(130.2798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.2947, device='cuda:0')



h[200].sum tensor(51.2635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66768.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0941, 0.0146,  ..., 0.0000, 0.0261, 0.0000],
        [0.0012, 0.0939, 0.0114,  ..., 0.0000, 0.0253, 0.0000],
        [0.0012, 0.0943, 0.0115,  ..., 0.0000, 0.0254, 0.0000],
        ...,
        [0.0012, 0.0955, 0.0114,  ..., 0.0000, 0.0257, 0.0000],
        [0.0012, 0.0954, 0.0114,  ..., 0.0000, 0.0257, 0.0000],
        [0.0012, 0.0954, 0.0114,  ..., 0.0000, 0.0257, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574644.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2304.2590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(319.8893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7433.3501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1084.2443, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-619.8636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1407],
        [-4.0892],
        [-3.8095],
        ...,
        [-6.2320],
        [-6.2239],
        [-6.2242]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308855.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9937],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370518.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9936],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370521.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5095.3682, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(44.3679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.1501, device='cuda:0')



h[100].sum tensor(136.0871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(57.2523, device='cuda:0')



h[200].sum tensor(74.7005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.1359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91517.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0930, 0.0109,  ..., 0.0000, 0.0254, 0.0000],
        [0.0011, 0.0937, 0.0110,  ..., 0.0000, 0.0256, 0.0000],
        [0.0012, 0.0941, 0.0111,  ..., 0.0000, 0.0257, 0.0000],
        ...,
        [0.0012, 0.0954, 0.0114,  ..., 0.0000, 0.0262, 0.0000],
        [0.0012, 0.0953, 0.0114,  ..., 0.0000, 0.0262, 0.0000],
        [0.0012, 0.0953, 0.0114,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675140.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3791.2075, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(540.9570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7273.1670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1426.2662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-882.3627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5729],
        [-5.6523],
        [-5.6000],
        ...,
        [-6.2196],
        [-6.2116],
        [-6.2122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286619., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9936],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370521.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9936],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370521.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1726e-04,  9.2397e-04, -6.1094e-04,  ...,  9.4324e-04,
          0.0000e+00, -7.6425e-04],
        [ 2.1726e-04,  9.2397e-04, -6.1094e-04,  ...,  9.4324e-04,
          0.0000e+00, -7.6425e-04],
        [ 6.0566e-03,  5.5610e-03, -6.4960e-04,  ...,  1.0457e-02,
         -8.5741e-05,  3.0108e-03],
        ...,
        [ 2.1726e-04,  9.2397e-04, -6.1094e-04,  ...,  9.4324e-04,
          0.0000e+00, -7.6425e-04],
        [ 2.1726e-04,  9.2397e-04, -6.1094e-04,  ...,  9.4324e-04,
          0.0000e+00, -7.6425e-04],
        [ 2.1726e-04,  9.2397e-04, -6.1094e-04,  ...,  9.4324e-04,
          0.0000e+00, -7.6425e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3196.9282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.3675, device='cuda:0')



h[100].sum tensor(128.9081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.0264, device='cuda:0')



h[200].sum tensor(49.1458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0070, 0.0087, 0.0000,  ..., 0.0138, 0.0000, 0.0031],
        [0.0198, 0.0188, 0.0000,  ..., 0.0347, 0.0000, 0.0106],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64271.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0992, 0.0380,  ..., 0.0000, 0.0333, 0.0000],
        [0.0316, 0.1127, 0.0936,  ..., 0.0022, 0.0497, 0.0000],
        [0.0592, 0.1299, 0.1680,  ..., 0.0113, 0.0716, 0.0000],
        ...,
        [0.0012, 0.0954, 0.0114,  ..., 0.0000, 0.0262, 0.0000],
        [0.0012, 0.0953, 0.0114,  ..., 0.0000, 0.0262, 0.0000],
        [0.0012, 0.0953, 0.0114,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566017.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2163.7202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.4677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7746.8682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1048.7543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-591.9330, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6225],
        [-0.7459],
        [-0.0975],
        ...,
        [-6.2196],
        [-6.2116],
        [-6.2122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-304561.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9936],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370521.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9936],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370524.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0252,  0.0208, -0.0008,  ...,  0.0417, -0.0004,  0.0154],
        [ 0.0215,  0.0178, -0.0008,  ...,  0.0356, -0.0003,  0.0130],
        [ 0.0206,  0.0171, -0.0007,  ...,  0.0341, -0.0003,  0.0124],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3618.2253, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8219, device='cuda:0')



h[100].sum tensor(130.4691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3642, device='cuda:0')



h[200].sum tensor(54.8447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0691, 0.0580, 0.0000,  ..., 0.1151, 0.0000, 0.0410],
        [0.0961, 0.0794, 0.0000,  ..., 0.1590, 0.0000, 0.0584],
        [0.0673, 0.0566, 0.0000,  ..., 0.1121, 0.0000, 0.0398],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70361.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1351, 0.1684, 0.3697,  ..., 0.0578, 0.1310, 0.0000],
        [0.1593, 0.1810, 0.4336,  ..., 0.0747, 0.1499, 0.0000],
        [0.1356, 0.1699, 0.3707,  ..., 0.0587, 0.1311, 0.0000],
        ...,
        [0.0011, 0.0955, 0.0114,  ..., 0.0000, 0.0263, 0.0000],
        [0.0011, 0.0955, 0.0114,  ..., 0.0000, 0.0263, 0.0000],
        [0.0011, 0.0955, 0.0114,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587675.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2491.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.9520, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7439.0845, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1138.4788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-657.7742, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4455],
        [ 0.4306],
        [ 0.4177],
        ...,
        [-6.2258],
        [-6.2178],
        [-6.2184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291339., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0399],
        ...,
        [0.9945],
        [0.9936],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370524.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0400],
        ...,
        [0.9944],
        [0.9935],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370526.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4173.6899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.7934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0872, device='cuda:0')



h[100].sum tensor(132.9595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.1264, device='cuda:0')



h[200].sum tensor(62.0020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0153, 0.0152, 0.0000,  ..., 0.0276, 0.0000, 0.0078],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79967.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0990, 0.0327,  ..., 0.0000, 0.0317, 0.0000],
        [0.0216, 0.1078, 0.0681,  ..., 0.0000, 0.0421, 0.0000],
        [0.0463, 0.1237, 0.1351,  ..., 0.0062, 0.0616, 0.0000],
        ...,
        [0.0009, 0.0959, 0.0114,  ..., 0.0000, 0.0260, 0.0000],
        [0.0009, 0.0959, 0.0114,  ..., 0.0000, 0.0260, 0.0000],
        [0.0009, 0.0958, 0.0114,  ..., 0.0000, 0.0260, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(631312., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3166.5403, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(433.8850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6775.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1280.4349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-764.0366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4193],
        [-1.3296],
        [-0.4415],
        ...,
        [-6.2503],
        [-6.2418],
        [-6.2413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254779.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0400],
        ...,
        [0.9944],
        [0.9935],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370526.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0400],
        ...,
        [0.9944],
        [0.9935],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370528.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0186,  0.0155, -0.0007,  ...,  0.0310, -0.0003,  0.0112],
        [ 0.0107,  0.0092, -0.0007,  ...,  0.0181, -0.0002,  0.0061],
        [ 0.0152,  0.0128, -0.0007,  ...,  0.0254, -0.0002,  0.0090],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0093,  0.0081, -0.0007,  ...,  0.0158, -0.0001,  0.0052],
        [ 0.0093,  0.0081, -0.0007,  ...,  0.0158, -0.0001,  0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3658.5806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.3097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0431, device='cuda:0')



h[100].sum tensor(131.4535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.0255, device='cuda:0')



h[200].sum tensor(54.6110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0272, 0.0246, 0.0000,  ..., 0.0471, 0.0000, 0.0148],
        [0.0579, 0.0491, 0.0000,  ..., 0.0972, 0.0000, 0.0339],
        [0.0335, 0.0297, 0.0000,  ..., 0.0574, 0.0000, 0.0189],
        ...,
        [0.0182, 0.0176, 0.0000,  ..., 0.0325, 0.0000, 0.0097],
        [0.0182, 0.0176, 0.0000,  ..., 0.0325, 0.0000, 0.0097],
        [0.0182, 0.0176, 0.0000,  ..., 0.0325, 0.0000, 0.0097]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70136.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0749, 0.1392, 0.2117,  ..., 0.0193, 0.0833, 0.0000],
        [0.0919, 0.1489, 0.2568,  ..., 0.0306, 0.0965, 0.0000],
        [0.0752, 0.1401, 0.2123,  ..., 0.0198, 0.0837, 0.0000],
        ...,
        [0.0328, 0.1166, 0.0986,  ..., 0.0014, 0.0512, 0.0000],
        [0.0396, 0.1209, 0.1171,  ..., 0.0014, 0.0565, 0.0000],
        [0.0396, 0.1209, 0.1170,  ..., 0.0014, 0.0565, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586076.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2350.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.5876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7500.9160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1135.2101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-657.2544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2951],
        [ 0.3225],
        [ 0.3136],
        ...,
        [-3.5243],
        [-2.8360],
        [-2.8369]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-320825.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0400],
        ...,
        [0.9944],
        [0.9935],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370528.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0400],
        ...,
        [0.9944],
        [0.9935],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370530.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3390.8291, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.1349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5039, device='cuda:0')



h[100].sum tensor(130.7066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.4237, device='cuda:0')



h[200].sum tensor(50.8111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66675.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0936, 0.0106,  ..., 0.0000, 0.0251, 0.0000],
        [0.0009, 0.0945, 0.0113,  ..., 0.0000, 0.0256, 0.0000],
        [0.0043, 0.0973, 0.0207,  ..., 0.0000, 0.0287, 0.0000],
        ...,
        [0.0008, 0.0960, 0.0110,  ..., 0.0000, 0.0259, 0.0000],
        [0.0008, 0.0960, 0.0110,  ..., 0.0000, 0.0259, 0.0000],
        [0.0008, 0.0960, 0.0110,  ..., 0.0000, 0.0259, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575512.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2180.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(312.7989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7708.5425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1086.0035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-620.4930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.4527],
        [-4.9241],
        [-4.1008],
        ...,
        [-6.3219],
        [-6.3136],
        [-6.3141]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-337398.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0369],
        [1.0400],
        ...,
        [0.9944],
        [0.9935],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370530.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0357],
        [1.0368],
        [1.0400],
        ...,
        [0.9943],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370532.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9516e-04,  8.7818e-04, -6.1094e-04,  ...,  9.5635e-04,
          0.0000e+00, -7.4856e-04],
        [ 1.9516e-04,  8.7818e-04, -6.1094e-04,  ...,  9.5635e-04,
          0.0000e+00, -7.4856e-04],
        [ 1.9516e-04,  8.7818e-04, -6.1094e-04,  ...,  9.5635e-04,
          0.0000e+00, -7.4856e-04],
        ...,
        [ 4.8119e-03,  4.5441e-03, -6.4151e-04,  ...,  8.4790e-03,
         -6.5349e-05,  2.2373e-03],
        [ 7.8266e-03,  6.9379e-03, -6.6147e-04,  ...,  1.3391e-02,
         -1.0802e-04,  4.1871e-03],
        [ 1.9516e-04,  8.7818e-04, -6.1094e-04,  ...,  9.5635e-04,
          0.0000e+00, -7.4856e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3639.1096, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.4243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0877, device='cuda:0')



h[100].sum tensor(131.6844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.1587, device='cuda:0')



h[200].sum tensor(54.2198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0446, 0.0385, 0.0000,  ..., 0.0754, 0.0000, 0.0252],
        [0.0162, 0.0159, 0.0000,  ..., 0.0291, 0.0000, 0.0084],
        [0.0088, 0.0101, 0.0000,  ..., 0.0171, 0.0000, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72506.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0139, 0.1015, 0.0457,  ..., 0.0000, 0.0355, 0.0000],
        [0.0034, 0.0958, 0.0174,  ..., 0.0000, 0.0275, 0.0000],
        [0.0010, 0.0944, 0.0107,  ..., 0.0000, 0.0256, 0.0000],
        ...,
        [0.0821, 0.1451, 0.2289,  ..., 0.0221, 0.0893, 0.0000],
        [0.0531, 0.1277, 0.1514,  ..., 0.0098, 0.0669, 0.0000],
        [0.0274, 0.1119, 0.0821,  ..., 0.0015, 0.0467, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606404.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2722.4309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.5024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7557.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1168.9139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-684.5844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3116],
        [-3.6635],
        [-4.7202],
        ...,
        [-0.1550],
        [-1.1828],
        [-2.8229]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316946., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0357],
        [1.0368],
        [1.0400],
        ...,
        [0.9943],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370532.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0368],
        [1.0401],
        ...,
        [0.9943],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370534.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3223.2568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(21.7931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.6431, device='cuda:0')



h[100].sum tensor(130.3496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.8504, device='cuda:0')



h[200].sum tensor(48.7588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0036, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65049.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0931, 0.0104,  ..., 0.0000, 0.0254, 0.0000],
        [0.0012, 0.0938, 0.0105,  ..., 0.0000, 0.0256, 0.0000],
        [0.0012, 0.0942, 0.0106,  ..., 0.0000, 0.0258, 0.0000],
        ...,
        [0.0093, 0.1007, 0.0329,  ..., 0.0000, 0.0326, 0.0000],
        [0.0039, 0.0972, 0.0182,  ..., 0.0000, 0.0284, 0.0000],
        [0.0012, 0.0955, 0.0109,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571652.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2233.9307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.9529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7910.5239, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1062.2719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-605.7537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2936],
        [-4.9293],
        [-4.2979],
        ...,
        [-4.6592],
        [-5.3716],
        [-5.8958]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-324486.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0368],
        [1.0401],
        ...,
        [0.9943],
        [0.9934],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370534.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0368],
        [1.0401],
        ...,
        [0.9943],
        [0.9934],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370536.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0007],
        [ 0.0137,  0.0116, -0.0007,  ...,  0.0230, -0.0002,  0.0080]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4882.2095, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(41.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.7550, device='cuda:0')



h[100].sum tensor(137.3655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(53.0814, device='cuda:0')



h[200].sum tensor(70.8357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.9803, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0036, 0.0000,  ..., 0.0039, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0150, 0.0149, 0.0000,  ..., 0.0271, 0.0000, 0.0084],
        [0.0267, 0.0242, 0.0000,  ..., 0.0462, 0.0000, 0.0152]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91125.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0932, 0.0104,  ..., 0.0000, 0.0253, 0.0000],
        [0.0065, 0.0974, 0.0253,  ..., 0.0000, 0.0298, 0.0000],
        [0.0250, 0.1083, 0.0750,  ..., 0.0027, 0.0445, 0.0000],
        ...,
        [0.0115, 0.1017, 0.0386,  ..., 0.0000, 0.0342, 0.0000],
        [0.0386, 0.1167, 0.1108,  ..., 0.0085, 0.0551, 0.0000],
        [0.0717, 0.1349, 0.1989,  ..., 0.0236, 0.0808, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682920.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3944.2659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(524.3243, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7061.6528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1428.6644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-886.3311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6030],
        [-3.5221],
        [-2.0154],
        ...,
        [-4.6999],
        [-3.2721],
        [-1.7391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294867.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0368],
        [1.0401],
        ...,
        [0.9943],
        [0.9934],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370536.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 320.0 event: 1600 loss: tensor(431.7272, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0401],
        ...,
        [0.9943],
        [0.9934],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370538.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0009,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3153.0032, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.8537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.3307, device='cuda:0')



h[100].sum tensor(131.3001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.9164, device='cuda:0')



h[200].sum tensor(47.7151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0092, 0.0000,  ..., 0.0154, 0.0000, 0.0030],
        [0.0043, 0.0064, 0.0000,  ..., 0.0097, 0.0000, 0.0015],
        [0.0321, 0.0285, 0.0000,  ..., 0.0549, 0.0000, 0.0187],
        ...,
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63769.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0209, 0.1071, 0.0662,  ..., 0.0000, 0.0421, 0.0000],
        [0.0321, 0.1134, 0.0952,  ..., 0.0015, 0.0506, 0.0000],
        [0.0736, 0.1359, 0.2047,  ..., 0.0213, 0.0825, 0.0000],
        ...,
        [0.0012, 0.0954, 0.0110,  ..., 0.0000, 0.0262, 0.0000],
        [0.0012, 0.0954, 0.0110,  ..., 0.0000, 0.0262, 0.0000],
        [0.0012, 0.0954, 0.0110,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(566922.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2106.1799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.1723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8216.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1035.5236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-591.0675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5088],
        [-1.2461],
        [-0.2775],
        ...,
        [-6.3934],
        [-6.3846],
        [-6.3848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-368123.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0401],
        ...,
        [0.9943],
        [0.9934],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370538.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9943],
        [0.9934],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370540.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4635.6660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.4228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3795, device='cuda:0')



h[100].sum tensor(137.4873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(48.9693, device='cuda:0')



h[200].sum tensor(67.6367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0198, 0.0187, 0.0000,  ..., 0.0349, 0.0000, 0.0107],
        [0.0206, 0.0193, 0.0000,  ..., 0.0363, 0.0000, 0.0112],
        [0.0076, 0.0090, 0.0000,  ..., 0.0151, 0.0000, 0.0036],
        ...,
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(85334.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0447, 0.1205, 0.1287,  ..., 0.0030, 0.0596, 0.0000],
        [0.0453, 0.1215, 0.1302,  ..., 0.0033, 0.0602, 0.0000],
        [0.0291, 0.1119, 0.0867,  ..., 0.0003, 0.0477, 0.0000],
        ...,
        [0.0013, 0.0954, 0.0111,  ..., 0.0000, 0.0261, 0.0000],
        [0.0013, 0.0954, 0.0111,  ..., 0.0000, 0.0261, 0.0000],
        [0.0013, 0.0954, 0.0111,  ..., 0.0000, 0.0261, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657797.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3630.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(468.3659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7112.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1343.6959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-828.1286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5341],
        [-1.6511],
        [-2.4316],
        ...,
        [-6.4088],
        [-6.3999],
        [-6.3994]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286404.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9943],
        [0.9934],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370540.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370543.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.1052e-03,  5.5657e-03, -6.5011e-04,  ...,  1.0600e-02,
         -8.0692e-05,  3.0569e-03],
        [ 1.8961e-04,  8.6825e-04, -6.1094e-04,  ...,  9.6296e-04,
          0.0000e+00, -7.6860e-04],
        [ 1.8961e-04,  8.6825e-04, -6.1094e-04,  ...,  9.6296e-04,
          0.0000e+00, -7.6860e-04],
        ...,
        [ 1.8961e-04,  8.6825e-04, -6.1094e-04,  ...,  9.6296e-04,
          0.0000e+00, -7.6860e-04],
        [ 1.8961e-04,  8.6825e-04, -6.1094e-04,  ...,  9.6296e-04,
          0.0000e+00, -7.6860e-04],
        [ 1.8961e-04,  8.6825e-04, -6.1094e-04,  ...,  9.6296e-04,
          0.0000e+00, -7.6860e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4522.0767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.1877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5269, device='cuda:0')



h[100].sum tensor(137.2163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(46.4201, device='cuda:0')



h[200].sum tensor(66.4912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0209, 0.0196, 0.0000,  ..., 0.0367, 0.0000, 0.0114],
        [0.0069, 0.0085, 0.0000,  ..., 0.0140, 0.0000, 0.0032],
        [0.0008, 0.0036, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87888.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.1250, 0.1512,  ..., 0.0078, 0.0660, 0.0000],
        [0.0345, 0.1144, 0.1012,  ..., 0.0009, 0.0517, 0.0000],
        [0.0275, 0.1100, 0.0818,  ..., 0.0000, 0.0461, 0.0000],
        ...,
        [0.0013, 0.0953, 0.0115,  ..., 0.0000, 0.0262, 0.0000],
        [0.0013, 0.0953, 0.0115,  ..., 0.0000, 0.0262, 0.0000],
        [0.0013, 0.0953, 0.0115,  ..., 0.0000, 0.0261, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678785.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3843.9521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(492.9105, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7527.1255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1368.1738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-851.7642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0642],
        [-0.4169],
        [-0.7158],
        ...,
        [-6.3863],
        [-6.3770],
        [-6.3766]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-331979.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370543.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370546.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0111,  0.0095, -0.0007,  ...,  0.0187, -0.0001,  0.0063],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3641.7400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.3230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2501, device='cuda:0')



h[100].sum tensor(133.6638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.6445, device='cuda:0')



h[200].sum tensor(55.3331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0526, 0.0447, 0.0000,  ..., 0.0882, 0.0000, 0.0310],
        [0.0199, 0.0188, 0.0000,  ..., 0.0350, 0.0000, 0.0107],
        [0.0009, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72839.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1413, 0.1687, 0.3832,  ..., 0.0633, 0.1339, 0.0000],
        [0.0996, 0.1480, 0.2725,  ..., 0.0360, 0.1017, 0.0000],
        [0.0704, 0.1322, 0.1949,  ..., 0.0185, 0.0792, 0.0000],
        ...,
        [0.0016, 0.0949, 0.0120,  ..., 0.0000, 0.0263, 0.0000],
        [0.0016, 0.0949, 0.0120,  ..., 0.0000, 0.0263, 0.0000],
        [0.0016, 0.0948, 0.0120,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605238.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2895.2432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(357.8067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7498.5249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1165.3776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-694.9456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3141],
        [ 0.3169],
        [ 0.3188],
        ...,
        [-6.3864],
        [-6.3777],
        [-6.3777]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291300.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370546.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370548.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4559.6909, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.2089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.7868, device='cuda:0')



h[100].sum tensor(137.5069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(47.1972, device='cuda:0')



h[200].sum tensor(67.9245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.7608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0009, 0.0037, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89437.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0930, 0.0140,  ..., 0.0000, 0.0264, 0.0000],
        [0.0018, 0.0934, 0.0128,  ..., 0.0000, 0.0260, 0.0000],
        [0.0016, 0.0935, 0.0122,  ..., 0.0000, 0.0259, 0.0000],
        ...,
        [0.0017, 0.0949, 0.0125,  ..., 0.0000, 0.0263, 0.0000],
        [0.0017, 0.0948, 0.0125,  ..., 0.0000, 0.0263, 0.0000],
        [0.0017, 0.0948, 0.0125,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692704., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4137.8701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(506.4959, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7467.9990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1385.2360, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-870.0931, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9216],
        [-4.5904],
        [-5.1377],
        ...,
        [-6.3847],
        [-6.3759],
        [-6.3758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318471.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370548.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370551.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.0020e-03,  4.6979e-03, -6.4265e-04,  ...,  8.7813e-03,
         -6.3903e-05,  2.2988e-03],
        [ 1.3511e-02,  1.1457e-02, -6.9898e-04,  ...,  2.2639e-02,
         -1.7740e-04,  7.7997e-03],
        [ 2.1096e-04,  8.9249e-04, -6.1094e-04,  ...,  9.7899e-04,
          0.0000e+00, -7.9836e-04],
        ...,
        [ 2.1096e-04,  8.9249e-04, -6.1094e-04,  ...,  9.7899e-04,
          0.0000e+00, -7.9836e-04],
        [ 2.1096e-04,  8.9249e-04, -6.1094e-04,  ...,  9.7899e-04,
          0.0000e+00, -7.9836e-04],
        [ 2.1096e-04,  8.9249e-04, -6.1094e-04,  ...,  9.7899e-04,
          0.0000e+00, -7.9836e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3657.8789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.5887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.4641, device='cuda:0')



h[100].sum tensor(134.2510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.2841, device='cuda:0')



h[200].sum tensor(56.1916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1671, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0742, 0.0619, 0.0000,  ..., 0.1235, 0.0000, 0.0441],
        [0.0212, 0.0199, 0.0000,  ..., 0.0372, 0.0000, 0.0115],
        [0.0147, 0.0147, 0.0000,  ..., 0.0265, 0.0000, 0.0081],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71504.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1173, 0.1574, 0.3219,  ..., 0.0459, 0.1152, 0.0000],
        [0.0687, 0.1317, 0.1925,  ..., 0.0180, 0.0781, 0.0000],
        [0.0397, 0.1155, 0.1147,  ..., 0.0066, 0.0558, 0.0000],
        ...,
        [0.0017, 0.0948, 0.0129,  ..., 0.0000, 0.0264, 0.0000],
        [0.0017, 0.0948, 0.0129,  ..., 0.0000, 0.0264, 0.0000],
        [0.0017, 0.0947, 0.0129,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595726.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2751.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(346.0563, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7559.8374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1138.7172, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-680.2812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2743],
        [-0.3402],
        [-1.3892],
        ...,
        [-6.3485],
        [-6.3645],
        [-6.3693]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308134.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0402],
        ...,
        [0.9942],
        [0.9933],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370551.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370555.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0763e-04,  8.9188e-04, -6.1094e-04,  ...,  9.9627e-04,
          0.0000e+00, -7.9914e-04],
        [ 7.5748e-03,  6.7440e-03, -6.5971e-04,  ...,  1.2995e-02,
         -9.7539e-05,  3.9640e-03],
        [ 2.0763e-04,  8.9188e-04, -6.1094e-04,  ...,  9.9627e-04,
          0.0000e+00, -7.9914e-04],
        ...,
        [ 2.0763e-04,  8.9188e-04, -6.1094e-04,  ...,  9.9627e-04,
          0.0000e+00, -7.9914e-04],
        [ 2.0763e-04,  8.9188e-04, -6.1094e-04,  ...,  9.9627e-04,
          0.0000e+00, -7.9914e-04],
        [ 2.0763e-04,  8.9188e-04, -6.1094e-04,  ...,  9.9627e-04,
          0.0000e+00, -7.9914e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4138.7002, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.1015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.4472, device='cuda:0')



h[100].sum tensor(136.4469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(40.2027, device='cuda:0')



h[200].sum tensor(62.7597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4999, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0285, 0.0256, 0.0000,  ..., 0.0491, 0.0000, 0.0146],
        [0.0071, 0.0087, 0.0000,  ..., 0.0144, 0.0000, 0.0032],
        [0.0266, 0.0241, 0.0000,  ..., 0.0460, 0.0000, 0.0150],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76790.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0349, 0.1136, 0.1036,  ..., 0.0000, 0.0515, 0.0000],
        [0.0390, 0.1149, 0.1139,  ..., 0.0040, 0.0548, 0.0000],
        [0.0730, 0.1331, 0.2048,  ..., 0.0216, 0.0812, 0.0000],
        ...,
        [0.0018, 0.0950, 0.0133,  ..., 0.0000, 0.0263, 0.0000],
        [0.0018, 0.0949, 0.0133,  ..., 0.0000, 0.0263, 0.0000],
        [0.0018, 0.0949, 0.0133,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609197.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2967.7043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(393.1524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7468.9507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1208.7543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-735.5061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4218],
        [-1.4223],
        [-0.4433],
        ...,
        [-6.3800],
        [-6.3712],
        [-6.3710]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307036.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370555.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370558.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7634e-03,  6.0988e-03, -6.5424e-04,  ...,  1.1649e-02,
         -8.5963e-05,  3.4205e-03],
        [ 2.2182e-04,  9.0213e-04, -6.1094e-04,  ...,  9.9538e-04,
          0.0000e+00, -8.0896e-04],
        [ 2.2182e-04,  9.0213e-04, -6.1094e-04,  ...,  9.9538e-04,
          0.0000e+00, -8.0896e-04],
        ...,
        [ 2.2182e-04,  9.0213e-04, -6.1094e-04,  ...,  9.9538e-04,
          0.0000e+00, -8.0896e-04],
        [ 2.2182e-04,  9.0213e-04, -6.1094e-04,  ...,  9.9538e-04,
          0.0000e+00, -8.0896e-04],
        [ 2.2182e-04,  9.0213e-04, -6.1094e-04,  ...,  9.9538e-04,
          0.0000e+00, -8.0896e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3531.1609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.3595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5879, device='cuda:0')



h[100].sum tensor(133.8264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.6647, device='cuda:0')



h[200].sum tensor(55.0708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0218, 0.0203, 0.0000,  ..., 0.0381, 0.0000, 0.0118],
        [0.0077, 0.0091, 0.0000,  ..., 0.0152, 0.0000, 0.0035],
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71011.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0866, 0.1402, 0.2409,  ..., 0.0301, 0.0910, 0.0000],
        [0.0491, 0.1203, 0.1404,  ..., 0.0103, 0.0623, 0.0000],
        [0.0286, 0.1084, 0.0851,  ..., 0.0020, 0.0466, 0.0000],
        ...,
        [0.0019, 0.0946, 0.0136,  ..., 0.0000, 0.0265, 0.0000],
        [0.0019, 0.0946, 0.0135,  ..., 0.0000, 0.0265, 0.0000],
        [0.0019, 0.0945, 0.0135,  ..., 0.0000, 0.0264, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595023.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2776.9585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(342.0189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7644.2739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1128.3936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-673.6234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1742],
        [ 0.1002],
        [ 0.0407],
        ...,
        [-6.3428],
        [-6.3334],
        [-6.3327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302718., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370558.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370561.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3868.5471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.5825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5400, device='cuda:0')



h[100].sum tensor(134.8294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.5006, device='cuda:0')



h[200].sum tensor(59.7979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0128, 0.0131, 0.0000,  ..., 0.0234, 0.0000, 0.0060],
        [0.0045, 0.0066, 0.0000,  ..., 0.0100, 0.0000, 0.0015],
        [0.0074, 0.0089, 0.0000,  ..., 0.0147, 0.0000, 0.0033],
        ...,
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76703.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0349, 0.1136, 0.1046,  ..., 0.0005, 0.0523, 0.0000],
        [0.0301, 0.1112, 0.0913,  ..., 0.0000, 0.0486, 0.0000],
        [0.0332, 0.1132, 0.0993,  ..., 0.0000, 0.0509, 0.0000],
        ...,
        [0.0021, 0.0944, 0.0137,  ..., 0.0000, 0.0266, 0.0000],
        [0.0021, 0.0943, 0.0137,  ..., 0.0000, 0.0265, 0.0000],
        [0.0021, 0.0943, 0.0136,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621473.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3207.9771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(392.8240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7421.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1209.3304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-734.5043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7335],
        [-0.3794],
        [-0.1442],
        ...,
        [-6.3600],
        [-6.3512],
        [-6.3510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287762.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9933],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370561.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9932],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370564.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.2595e-04,  9.0240e-04, -6.1094e-04,  ...,  1.0133e-03,
          0.0000e+00, -8.1252e-04],
        [ 2.2595e-04,  9.0240e-04, -6.1094e-04,  ...,  1.0133e-03,
          0.0000e+00, -8.1252e-04],
        [ 5.2389e-03,  4.8850e-03, -6.4412e-04,  ...,  9.1779e-03,
         -6.4901e-05,  2.4291e-03],
        ...,
        [ 2.2595e-04,  9.0240e-04, -6.1094e-04,  ...,  1.0133e-03,
          0.0000e+00, -8.1252e-04],
        [ 2.2595e-04,  9.0240e-04, -6.1094e-04,  ...,  1.0133e-03,
          0.0000e+00, -8.1252e-04],
        [ 2.2595e-04,  9.0240e-04, -6.1094e-04,  ...,  1.0133e-03,
          0.0000e+00, -8.1252e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4045.5449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.5033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5439, device='cuda:0')



h[100].sum tensor(135.4369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.5019, device='cuda:0')



h[200].sum tensor(62.2076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3991, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0037, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0151, 0.0150, 0.0000,  ..., 0.0272, 0.0000, 0.0074],
        [0.0197, 0.0187, 0.0000,  ..., 0.0348, 0.0000, 0.0096],
        ...,
        [0.0010, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77824.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.1145, 0.1146,  ..., 0.0019, 0.0547, 0.0000],
        [0.0446, 0.1202, 0.1312,  ..., 0.0019, 0.0596, 0.0000],
        [0.0560, 0.1278, 0.1628,  ..., 0.0054, 0.0688, 0.0000],
        ...,
        [0.0020, 0.0945, 0.0138,  ..., 0.0000, 0.0264, 0.0000],
        [0.0020, 0.0944, 0.0138,  ..., 0.0000, 0.0264, 0.0000],
        [0.0020, 0.0944, 0.0138,  ..., 0.0000, 0.0264, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620455.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3176.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(403.3839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7372.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1223.6803, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-745.0293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3390],
        [ 0.3428],
        [ 0.3685],
        ...,
        [-6.3713],
        [-6.3625],
        [-6.3622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302632.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0358],
        [1.0369],
        [1.0403],
        ...,
        [0.9942],
        [0.9932],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370564.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 330.0 event: 1650 loss: tensor(479.1981, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0359],
        [1.0369],
        [1.0403],
        ...,
        [0.9941],
        [0.9932],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370567.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0259,  0.0213, -0.0008,  ...,  0.0428, -0.0003,  0.0158],
        [ 0.0175,  0.0146, -0.0007,  ...,  0.0291, -0.0002,  0.0103],
        [ 0.0094,  0.0082, -0.0007,  ...,  0.0159, -0.0001,  0.0051],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3808.2329, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.0087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.1452, device='cuda:0')



h[100].sum tensor(134.1612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.3204, device='cuda:0')



h[200].sum tensor(59.2654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2431, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0813, 0.0676, 0.0000,  ..., 0.1350, 0.0000, 0.0485],
        [0.0805, 0.0669, 0.0000,  ..., 0.1337, 0.0000, 0.0480],
        [0.0402, 0.0349, 0.0000,  ..., 0.0680, 0.0000, 0.0236],
        ...,
        [0.0010, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74827.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1701, 0.1824, 0.4667,  ..., 0.0852, 0.1551, 0.0000],
        [0.1605, 0.1783, 0.4412,  ..., 0.0782, 0.1480, 0.0000],
        [0.1152, 0.1551, 0.3192,  ..., 0.0483, 0.1133, 0.0000],
        ...,
        [0.0021, 0.0942, 0.0139,  ..., 0.0000, 0.0265, 0.0000],
        [0.0021, 0.0942, 0.0139,  ..., 0.0000, 0.0265, 0.0000],
        [0.0021, 0.0941, 0.0139,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610304.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3029.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(376.2934, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7547.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1183.3756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-713.7271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3286],
        [ 0.3121],
        [ 0.2502],
        ...,
        [-6.3663],
        [-6.3575],
        [-6.3572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295215.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0359],
        [1.0369],
        [1.0403],
        ...,
        [0.9941],
        [0.9932],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370567.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0359],
        [1.0369],
        [1.0404],
        ...,
        [0.9941],
        [0.9932],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370570.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4009.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.8064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.5035, device='cuda:0')



h[100].sum tensor(134.5320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.3813, device='cuda:0')



h[200].sum tensor(62.0300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77112.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0914, 0.0131,  ..., 0.0000, 0.0259, 0.0000],
        [0.0022, 0.0921, 0.0132,  ..., 0.0000, 0.0261, 0.0000],
        [0.0022, 0.0925, 0.0133,  ..., 0.0000, 0.0263, 0.0000],
        ...,
        [0.0023, 0.0938, 0.0136,  ..., 0.0000, 0.0268, 0.0000],
        [0.0023, 0.0938, 0.0136,  ..., 0.0000, 0.0268, 0.0000],
        [0.0023, 0.0937, 0.0136,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618271.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3172.6177, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(397.2053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7607.4893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1212.3397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-736.0103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0210],
        [-5.8244],
        [-5.4692],
        ...,
        [-6.3620],
        [-6.3532],
        [-6.3529]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287126.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0359],
        [1.0369],
        [1.0404],
        ...,
        [0.9941],
        [0.9932],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370570.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0359],
        [1.0369],
        [1.0404],
        ...,
        [0.9941],
        [0.9931],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370573.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3523.4995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3703, device='cuda:0')



h[100].sum tensor(132.7400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0140, device='cuda:0')



h[200].sum tensor(55.3579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71251.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0964, 0.0362,  ..., 0.0000, 0.0327, 0.0000],
        [0.0029, 0.0926, 0.0151,  ..., 0.0000, 0.0268, 0.0000],
        [0.0022, 0.0924, 0.0131,  ..., 0.0000, 0.0264, 0.0000],
        ...,
        [0.0023, 0.0937, 0.0134,  ..., 0.0000, 0.0268, 0.0000],
        [0.0023, 0.0937, 0.0134,  ..., 0.0000, 0.0268, 0.0000],
        [0.0023, 0.0936, 0.0134,  ..., 0.0000, 0.0268, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596748.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2817.0610, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.9112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7816.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1129.8915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-671.4566, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3441],
        [-4.3209],
        [-4.8382],
        ...,
        [-6.3711],
        [-6.3616],
        [-6.3609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297962.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0359],
        [1.0369],
        [1.0404],
        ...,
        [0.9941],
        [0.9931],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370573.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0369],
        [1.0404],
        ...,
        [0.9940],
        [0.9931],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370575.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0091,  0.0079, -0.0007,  ...,  0.0154, -0.0001,  0.0049],
        [ 0.0106,  0.0091, -0.0007,  ...,  0.0178, -0.0001,  0.0059],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4135.4229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.8874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0573, device='cuda:0')



h[100].sum tensor(135.4250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.0368, device='cuda:0')



h[200].sum tensor(63.0349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0926, 0.0766, 0.0000,  ..., 0.1535, 0.0000, 0.0559],
        [0.0331, 0.0293, 0.0000,  ..., 0.0565, 0.0000, 0.0191],
        [0.0117, 0.0123, 0.0000,  ..., 0.0216, 0.0000, 0.0061],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79165.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2374, 0.2094, 0.6508,  ..., 0.1407, 0.2079, 0.0000],
        [0.1389, 0.1625, 0.3840,  ..., 0.0707, 0.1319, 0.0000],
        [0.0713, 0.1296, 0.2009,  ..., 0.0273, 0.0798, 0.0000],
        ...,
        [0.0022, 0.0939, 0.0131,  ..., 0.0000, 0.0268, 0.0000],
        [0.0022, 0.0938, 0.0131,  ..., 0.0000, 0.0268, 0.0000],
        [0.0022, 0.0938, 0.0131,  ..., 0.0000, 0.0268, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627036.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3267.9194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(413.9440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7517.4043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1242.9388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-757.0046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0378],
        [ 0.0382],
        [-0.0319],
        ...,
        [-6.3677],
        [-6.3591],
        [-6.3604]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-291870.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0369],
        [1.0404],
        ...,
        [0.9940],
        [0.9931],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370575.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0360],
        [1.0369],
        [1.0404],
        ...,
        [0.9940],
        [0.9931],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370578.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3743.3711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.0422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7069, device='cuda:0')



h[100].sum tensor(134.1095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.0100, device='cuda:0')



h[200].sum tensor(57.3879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1942, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0103, 0.0112, 0.0000,  ..., 0.0194, 0.0000, 0.0043],
        ...,
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75278., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.1122, 0.1130,  ..., 0.0083, 0.0548, 0.0000],
        [0.0541, 0.1214, 0.1547,  ..., 0.0141, 0.0668, 0.0000],
        [0.0721, 0.1324, 0.2047,  ..., 0.0222, 0.0812, 0.0000],
        ...,
        [0.0021, 0.0939, 0.0125,  ..., 0.0000, 0.0270, 0.0000],
        [0.0021, 0.0938, 0.0125,  ..., 0.0000, 0.0270, 0.0000],
        [0.0021, 0.0938, 0.0125,  ..., 0.0000, 0.0269, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615731., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3077.8618, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.2924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7535.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1190.4750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-714.3887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1421],
        [ 0.0343],
        [ 0.1207],
        ...,
        [-6.4221],
        [-6.4127],
        [-6.4117]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289908.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0360],
        [1.0369],
        [1.0404],
        ...,
        [0.9940],
        [0.9931],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370578.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0405],
        ...,
        [0.9940],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370581.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0179,  0.0150, -0.0007,  ...,  0.0298, -0.0002,  0.0106],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3510.9116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.3381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4088, device='cuda:0')



h[100].sum tensor(133.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.1291, device='cuda:0')



h[200].sum tensor(54.0274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0494, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0038, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0193, 0.0184, 0.0000,  ..., 0.0340, 0.0000, 0.0110],
        [0.0161, 0.0158, 0.0000,  ..., 0.0288, 0.0000, 0.0089],
        ...,
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69178.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0127, 0.0970, 0.0406,  ..., 0.0000, 0.0346, 0.0000],
        [0.0387, 0.1115, 0.1114,  ..., 0.0089, 0.0548, 0.0000],
        [0.0538, 0.1198, 0.1524,  ..., 0.0153, 0.0665, 0.0000],
        ...,
        [0.0021, 0.0936, 0.0121,  ..., 0.0000, 0.0272, 0.0000],
        [0.0021, 0.0936, 0.0121,  ..., 0.0000, 0.0272, 0.0000],
        [0.0021, 0.0935, 0.0121,  ..., 0.0000, 0.0272, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585463., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2583.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.6953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7993.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1102.4235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-648.2438, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5309],
        [-3.2975],
        [-2.0512],
        ...,
        [-6.4384],
        [-6.4297],
        [-6.4297]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305896.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0405],
        ...,
        [0.9940],
        [0.9930],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370581.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0405],
        ...,
        [0.9939],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370584.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0149,  0.0126, -0.0007,  ...,  0.0249, -0.0002,  0.0087],
        [ 0.0118,  0.0101, -0.0007,  ...,  0.0198, -0.0001,  0.0067],
        [ 0.0321,  0.0263, -0.0008,  ...,  0.0530, -0.0004,  0.0198],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3936.5208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.2403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7957, device='cuda:0')



h[100].sum tensor(134.6140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2650, device='cuda:0')



h[200].sum tensor(59.5297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3157, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0350, 0.0308, 0.0000,  ..., 0.0595, 0.0000, 0.0203],
        [0.0935, 0.0773, 0.0000,  ..., 0.1549, 0.0000, 0.0565],
        [0.0879, 0.0729, 0.0000,  ..., 0.1459, 0.0000, 0.0529],
        ...,
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75986.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1025, 0.1433, 0.2850,  ..., 0.0466, 0.1039, 0.0000],
        [0.1754, 0.1811, 0.4835,  ..., 0.0960, 0.1604, 0.0000],
        [0.2031, 0.1963, 0.5592,  ..., 0.1136, 0.1820, 0.0000],
        ...,
        [0.0021, 0.0935, 0.0119,  ..., 0.0000, 0.0273, 0.0000],
        [0.0021, 0.0935, 0.0119,  ..., 0.0000, 0.0272, 0.0000],
        [0.0021, 0.0934, 0.0119,  ..., 0.0000, 0.0272, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615305.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2985.2349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(386.6184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7885.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1195.3831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-719.0334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4028],
        [ 0.1781],
        [ 0.2969],
        ...,
        [-6.4567],
        [-6.4476],
        [-6.4472]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-318642.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0405],
        ...,
        [0.9939],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370584.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0406],
        ...,
        [0.9939],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370586.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.9320e-03,  7.8312e-03, -6.6852e-04,  ...,  1.5167e-02,
         -1.0608e-04,  4.8184e-03],
        [ 2.3362e-04,  9.1670e-04, -6.1094e-04,  ...,  9.9473e-04,
          0.0000e+00, -8.1206e-04],
        [ 4.7891e-03,  4.5379e-03, -6.4110e-04,  ...,  8.4169e-03,
         -5.5555e-05,  2.1367e-03],
        ...,
        [ 1.5358e-02,  1.2939e-02, -7.1106e-04,  ...,  2.5637e-02,
         -1.8444e-04,  8.9779e-03],
        [ 2.3362e-04,  9.1670e-04, -6.1094e-04,  ...,  9.9473e-04,
          0.0000e+00, -8.1206e-04],
        [ 2.3362e-04,  9.1670e-04, -6.1094e-04,  ...,  9.9473e-04,
          0.0000e+00, -8.1206e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4518.0293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(38.1268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1575, device='cuda:0')



h[100].sum tensor(136.6720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.3159, device='cuda:0')



h[200].sum tensor(67.2973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0337, 0.0298, 0.0000,  ..., 0.0574, 0.0000, 0.0187],
        [0.0260, 0.0237, 0.0000,  ..., 0.0449, 0.0000, 0.0128],
        [0.0143, 0.0144, 0.0000,  ..., 0.0258, 0.0000, 0.0061],
        ...,
        [0.0140, 0.0142, 0.0000,  ..., 0.0254, 0.0000, 0.0076],
        [0.0169, 0.0165, 0.0000,  ..., 0.0302, 0.0000, 0.0095],
        [0.0010, 0.0039, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83381.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1028, 0.1477, 0.2876,  ..., 0.0380, 0.1050, 0.0000],
        [0.0782, 0.1368, 0.2221,  ..., 0.0184, 0.0869, 0.0000],
        [0.0564, 0.1258, 0.1633,  ..., 0.0041, 0.0705, 0.0000],
        ...,
        [0.0411, 0.1150, 0.1179,  ..., 0.0062, 0.0573, 0.0000],
        [0.0325, 0.1100, 0.0943,  ..., 0.0048, 0.0506, 0.0000],
        [0.0116, 0.0984, 0.0375,  ..., 0.0000, 0.0346, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643766.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3446.5015, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(452.1808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7735.2529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1299.7968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-800.0846, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3799],
        [ 0.4161],
        [ 0.4320],
        ...,
        [-3.1210],
        [-4.0140],
        [-5.0528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303583.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0361],
        [1.0369],
        [1.0406],
        ...,
        [0.9939],
        [0.9930],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370586.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0362],
        [1.0369],
        [1.0406],
        ...,
        [0.9939],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370589.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3981.0732, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.4500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.1332, device='cuda:0')



h[100].sum tensor(135.1526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.2741, device='cuda:0')



h[200].sum tensor(60.0962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78550.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0910, 0.0115,  ..., 0.0000, 0.0263, 0.0000],
        [0.0022, 0.0918, 0.0116,  ..., 0.0000, 0.0265, 0.0000],
        [0.0023, 0.0921, 0.0117,  ..., 0.0000, 0.0266, 0.0000],
        ...,
        [0.0023, 0.0934, 0.0120,  ..., 0.0000, 0.0271, 0.0000],
        [0.0023, 0.0934, 0.0120,  ..., 0.0000, 0.0271, 0.0000],
        [0.0023, 0.0933, 0.0120,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632528.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3322.0972, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(410.0328, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7465.4561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1233.4869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-749.0640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.2904],
        [-6.2967],
        [-6.2512],
        ...,
        [-6.4726],
        [-6.4636],
        [-6.4633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312447.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0362],
        [1.0369],
        [1.0406],
        ...,
        [0.9939],
        [0.9929],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370589.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0362],
        [1.0369],
        [1.0406],
        ...,
        [0.9938],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370590.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4526.3384, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.2823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2146, device='cuda:0')



h[100].sum tensor(137.8630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(45.4866, device='cuda:0')



h[200].sum tensor(67.2066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6970, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0037, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84958.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0912, 0.0116,  ..., 0.0000, 0.0258, 0.0000],
        [0.0025, 0.0920, 0.0121,  ..., 0.0000, 0.0262, 0.0000],
        [0.0030, 0.0927, 0.0135,  ..., 0.0000, 0.0268, 0.0000],
        ...,
        [0.0025, 0.0936, 0.0121,  ..., 0.0000, 0.0267, 0.0000],
        [0.0025, 0.0935, 0.0121,  ..., 0.0000, 0.0267, 0.0000],
        [0.0025, 0.0935, 0.0121,  ..., 0.0000, 0.0266, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(653331.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3689.5671, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(466.7048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7143.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1320.4095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-820.9622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9769],
        [-5.7067],
        [-5.2915],
        ...,
        [-6.4909],
        [-6.4817],
        [-6.4812]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-308885.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0362],
        [1.0369],
        [1.0406],
        ...,
        [0.9938],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370590.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 340.0 event: 1700 loss: tensor(481.6462, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0363],
        [1.0370],
        [1.0407],
        ...,
        [0.9938],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370592.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.1348e-04,  9.0833e-04, -6.1094e-04,  ...,  1.0646e-03,
          0.0000e+00, -8.1513e-04],
        [ 2.1348e-04,  9.0833e-04, -6.1094e-04,  ...,  1.0646e-03,
          0.0000e+00, -8.1513e-04],
        [ 2.1348e-04,  9.0833e-04, -6.1094e-04,  ...,  1.0646e-03,
          0.0000e+00, -8.1513e-04],
        ...,
        [ 8.0673e-03,  7.1518e-03, -6.6291e-04,  ...,  1.3855e-02,
         -9.3594e-05,  4.2606e-03],
        [ 2.1348e-04,  9.0833e-04, -6.1094e-04,  ...,  1.0646e-03,
          0.0000e+00, -8.1513e-04],
        [ 2.1348e-04,  9.0833e-04, -6.1094e-04,  ...,  1.0646e-03,
          0.0000e+00, -8.1513e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4256.6045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.2545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.7195, device='cuda:0')



h[100].sum tensor(136.6573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.0168, device='cuda:0')



h[200].sum tensor(63.8498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5302, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0261, 0.0238, 0.0000,  ..., 0.0455, 0.0000, 0.0145],
        [0.0092, 0.0104, 0.0000,  ..., 0.0180, 0.0000, 0.0045],
        [0.0009, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82026.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0070, 0.0935, 0.0236,  ..., 0.0000, 0.0291, 0.0000],
        [0.0186, 0.1012, 0.0555,  ..., 0.0000, 0.0386, 0.0000],
        [0.0331, 0.1104, 0.0957,  ..., 0.0012, 0.0503, 0.0000],
        ...,
        [0.0855, 0.1390, 0.2363,  ..., 0.0304, 0.0910, 0.0000],
        [0.0449, 0.1170, 0.1266,  ..., 0.0110, 0.0595, 0.0000],
        [0.0156, 0.1005, 0.0472,  ..., 0.0000, 0.0366, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649402.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3725.8845, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(442.0464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6885.1494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1280.4554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-791.6050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5700],
        [-1.4862],
        [-0.5392],
        ...,
        [-0.5375],
        [-2.0060],
        [-3.8550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-303358.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0363],
        [1.0370],
        [1.0407],
        ...,
        [0.9938],
        [0.9929],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370592.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0364],
        [1.0370],
        [1.0407],
        ...,
        [0.9938],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370594.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3510.2744, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.2915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3902, device='cuda:0')



h[100].sum tensor(132.9446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0735, device='cuda:0')



h[200].sum tensor(54.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0089, 0.0000,  ..., 0.0146, 0.0000, 0.0032],
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68939.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0235, 0.1031, 0.0678,  ..., 0.0000, 0.0426, 0.0000],
        [0.0091, 0.0948, 0.0282,  ..., 0.0000, 0.0310, 0.0000],
        [0.0050, 0.0926, 0.0170,  ..., 0.0000, 0.0277, 0.0000],
        ...,
        [0.0032, 0.0928, 0.0123,  ..., 0.0000, 0.0267, 0.0000],
        [0.0032, 0.0927, 0.0123,  ..., 0.0000, 0.0267, 0.0000],
        [0.0032, 0.0926, 0.0123,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583319.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2809.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(327.3662, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7458.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1091.5881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-649.5695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1921],
        [-3.3221],
        [-3.8904],
        ...,
        [-6.4815],
        [-6.4723],
        [-6.4718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313216.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0364],
        [1.0370],
        [1.0407],
        ...,
        [0.9938],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370594.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0408],
        ...,
        [0.9937],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370596.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 8.5022e-03,  7.5010e-03, -6.6553e-04,  ...,  1.4471e-02,
         -9.6848e-05,  4.4873e-03],
        [ 2.4812e-04,  9.3872e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -8.4045e-04],
        [ 2.3760e-02,  1.9631e-02, -7.6645e-04,  ...,  3.9302e-02,
         -2.7587e-04,  1.4335e-02],
        ...,
        [ 2.4812e-04,  9.3872e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -8.4045e-04],
        [ 2.4812e-04,  9.3872e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -8.4045e-04],
        [ 2.4812e-04,  9.3872e-04, -6.1094e-04,  ...,  1.0376e-03,
          0.0000e+00, -8.4045e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3980.8340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.1254, device='cuda:0')



h[100].sum tensor(134.1681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.2509, device='cuda:0')



h[200].sum tensor(61.0482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0080, 0.0094, 0.0000,  ..., 0.0156, 0.0000, 0.0036],
        [0.0481, 0.0413, 0.0000,  ..., 0.0809, 0.0000, 0.0269],
        [0.0246, 0.0226, 0.0000,  ..., 0.0427, 0.0000, 0.0134],
        ...,
        [0.0010, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77705.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0739, 0.1291, 0.2001,  ..., 0.0185, 0.0817, 0.0000],
        [0.0999, 0.1439, 0.2696,  ..., 0.0330, 0.1024, 0.0000],
        [0.0983, 0.1431, 0.2648,  ..., 0.0329, 0.1010, 0.0000],
        ...,
        [0.0035, 0.0924, 0.0122,  ..., 0.0000, 0.0269, 0.0000],
        [0.0035, 0.0924, 0.0122,  ..., 0.0000, 0.0269, 0.0000],
        [0.0035, 0.0923, 0.0121,  ..., 0.0000, 0.0269, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622550.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3525.6792, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(404.0619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7047.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1218.9161, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-748.2215, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4629],
        [ 0.4679],
        [ 0.4690],
        ...,
        [-6.4789],
        [-6.4696],
        [-6.4691]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267219.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0408],
        ...,
        [0.9937],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370596.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0408],
        ...,
        [0.9937],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370597.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1325e-02,  9.7523e-03, -6.8440e-04,  ...,  1.9165e-02,
         -1.2933e-04,  6.3531e-03],
        [ 7.2230e-03,  6.4908e-03, -6.5726e-04,  ...,  1.2487e-02,
         -8.1555e-05,  3.7048e-03],
        [ 5.6870e-03,  5.2697e-03, -6.4710e-04,  ...,  9.9862e-03,
         -6.3667e-05,  2.7133e-03],
        ...,
        [ 2.2001e-04,  9.2320e-04, -6.1094e-04,  ...,  1.0859e-03,
          0.0000e+00, -8.1599e-04],
        [ 2.2001e-04,  9.2320e-04, -6.1094e-04,  ...,  1.0859e-03,
          0.0000e+00, -8.1599e-04],
        [ 2.2001e-04,  9.2320e-04, -6.1094e-04,  ...,  1.0859e-03,
          0.0000e+00, -8.1599e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3447.9985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.9481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.8702, device='cuda:0')



h[100].sum tensor(132.8151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.5188, device='cuda:0')



h[200].sum tensor(53.5068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0366, 0.0322, 0.0000,  ..., 0.0626, 0.0000, 0.0197],
        [0.0334, 0.0297, 0.0000,  ..., 0.0574, 0.0000, 0.0176],
        [0.0129, 0.0133, 0.0000,  ..., 0.0240, 0.0000, 0.0060],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68574.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0685, 0.1314, 0.1902,  ..., 0.0106, 0.0788, 0.0000],
        [0.0574, 0.1254, 0.1602,  ..., 0.0065, 0.0702, 0.0000],
        [0.0340, 0.1117, 0.0972,  ..., 0.0005, 0.0518, 0.0000],
        ...,
        [0.0030, 0.0931, 0.0115,  ..., 0.0000, 0.0265, 0.0000],
        [0.0030, 0.0930, 0.0115,  ..., 0.0000, 0.0265, 0.0000],
        [0.0030, 0.0930, 0.0115,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583417., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2738.2722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(325.9639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7185.0820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1092.2651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-647.4465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2282],
        [-0.5248],
        [-1.8099],
        ...,
        [-6.5055],
        [-6.4974],
        [-6.4988]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-325716.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0408],
        ...,
        [0.9937],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370597.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0408],
        ...,
        [0.9937],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370597.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3923.0977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.5223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.6512, device='cuda:0')



h[100].sum tensor(134.5829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.8330, device='cuda:0')



h[200].sum tensor(59.7949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2995, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0258, 0.0236, 0.0000,  ..., 0.0450, 0.0000, 0.0143],
        ...,
        [0.0009, 0.0039, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76461.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0084, 0.0940, 0.0260,  ..., 0.0000, 0.0301, 0.0000],
        [0.0241, 0.1036, 0.0682,  ..., 0.0011, 0.0426, 0.0000],
        [0.0727, 0.1305, 0.1978,  ..., 0.0233, 0.0807, 0.0000],
        ...,
        [0.0030, 0.0931, 0.0115,  ..., 0.0000, 0.0265, 0.0000],
        [0.0030, 0.0930, 0.0115,  ..., 0.0000, 0.0265, 0.0000],
        [0.0030, 0.0930, 0.0115,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615077.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3239.9478, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(395.5758, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6940.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1203.5042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-733.3402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3279],
        [-1.8346],
        [-0.5895],
        ...,
        [-6.5142],
        [-6.5050],
        [-6.5044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-307183.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0365],
        [1.0371],
        [1.0408],
        ...,
        [0.9937],
        [0.9928],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370597.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0408],
        ...,
        [0.9937],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370600.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.0290e-03,  5.5488e-03, -6.4950e-04,  ...,  1.0615e-02,
         -6.7370e-05,  2.9749e-03],
        [ 2.0080e-04,  9.1528e-04, -6.1094e-04,  ...,  1.1223e-03,
          0.0000e+00, -7.8903e-04],
        [ 2.0080e-04,  9.1528e-04, -6.1094e-04,  ...,  1.1223e-03,
          0.0000e+00, -7.8903e-04],
        ...,
        [ 2.0080e-04,  9.1528e-04, -6.1094e-04,  ...,  1.1223e-03,
          0.0000e+00, -7.8903e-04],
        [ 2.0080e-04,  9.1528e-04, -6.1094e-04,  ...,  1.1223e-03,
          0.0000e+00, -7.8903e-04],
        [ 2.0080e-04,  9.1528e-04, -6.1094e-04,  ...,  1.1223e-03,
          0.0000e+00, -7.8903e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3487.7178, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.6626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0904, device='cuda:0')



h[100].sum tensor(133.1137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1771, device='cuda:0')



h[200].sum tensor(53.7429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0205, 0.0194, 0.0000,  ..., 0.0367, 0.0000, 0.0103],
        [0.0162, 0.0161, 0.0000,  ..., 0.0298, 0.0000, 0.0083],
        [0.0008, 0.0038, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71086.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0481, 0.1217, 0.1384,  ..., 0.0026, 0.0631, 0.0000],
        [0.0382, 0.1158, 0.1106,  ..., 0.0018, 0.0552, 0.0000],
        [0.0171, 0.1023, 0.0520,  ..., 0.0000, 0.0382, 0.0000],
        ...,
        [0.0026, 0.0935, 0.0110,  ..., 0.0000, 0.0263, 0.0000],
        [0.0026, 0.0935, 0.0110,  ..., 0.0000, 0.0263, 0.0000],
        [0.0026, 0.0934, 0.0110,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598513.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2872.0173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.6349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6930.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1130.6101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-674.1227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5082],
        [-1.0935],
        [-2.0728],
        ...,
        [-6.5374],
        [-6.5282],
        [-6.5276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344864.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0366],
        [1.0372],
        [1.0408],
        ...,
        [0.9937],
        [0.9927],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370600.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0372],
        [1.0408],
        ...,
        [0.9937],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370602.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3989.6821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.2555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0188, device='cuda:0')



h[100].sum tensor(134.7213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.9322, device='cuda:0')



h[200].sum tensor(60.4488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0096, 0.0000,  ..., 0.0167, 0.0000, 0.0032],
        [0.0126, 0.0132, 0.0000,  ..., 0.0240, 0.0000, 0.0060],
        [0.0171, 0.0168, 0.0000,  ..., 0.0313, 0.0000, 0.0089],
        ...,
        [0.0008, 0.0039, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0008, 0.0039, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77236.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0321, 0.1127, 0.0957,  ..., 0.0000, 0.0509, 0.0000],
        [0.0355, 0.1150, 0.1043,  ..., 0.0005, 0.0534, 0.0000],
        [0.0395, 0.1171, 0.1143,  ..., 0.0011, 0.0562, 0.0000],
        ...,
        [0.0025, 0.0938, 0.0105,  ..., 0.0000, 0.0263, 0.0000],
        [0.0025, 0.0938, 0.0105,  ..., 0.0000, 0.0263, 0.0000],
        [0.0025, 0.0937, 0.0105,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618310., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3118.1733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(402.2662, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6800.3096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1223.2227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-741.5258, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1917],
        [-0.4255],
        [-0.7696],
        ...,
        [-6.5391],
        [-6.5300],
        [-6.5294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-333813.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0372],
        [1.0408],
        ...,
        [0.9937],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370602.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0372],
        [1.0408],
        ...,
        [0.9936],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370605.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.0285e-03,  6.3586e-03, -6.5597e-04,  ...,  1.2225e-02,
         -7.7493e-05,  3.6323e-03],
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04],
        [ 2.1194e-02,  1.7621e-02, -7.4969e-04,  ...,  3.5312e-02,
         -2.3879e-04,  1.2787e-02],
        ...,
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04],
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04],
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3642.8997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.9960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8283, device='cuda:0')



h[100].sum tensor(132.1475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.3834, device='cuda:0')



h[200].sum tensor(56.6826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0085, 0.0000,  ..., 0.0141, 0.0000, 0.0029],
        [0.0298, 0.0269, 0.0000,  ..., 0.0517, 0.0000, 0.0170],
        [0.0189, 0.0182, 0.0000,  ..., 0.0340, 0.0000, 0.0108],
        ...,
        [0.0009, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71178.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0399, 0.1150, 0.1136,  ..., 0.0005, 0.0568, 0.0000],
        [0.0601, 0.1248, 0.1660,  ..., 0.0115, 0.0721, 0.0000],
        [0.0597, 0.1236, 0.1636,  ..., 0.0138, 0.0714, 0.0000],
        ...,
        [0.0027, 0.0936, 0.0100,  ..., 0.0000, 0.0267, 0.0000],
        [0.0026, 0.0935, 0.0100,  ..., 0.0000, 0.0267, 0.0000],
        [0.0026, 0.0935, 0.0100,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588429.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2717.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(346.3007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6975.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1146.0995, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-675.8163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1693],
        [ 0.2905],
        [ 0.0991],
        ...,
        [-6.4952],
        [-6.4866],
        [-6.4860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310202.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0372],
        [1.0408],
        ...,
        [0.9936],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370605.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0367],
        [1.0372],
        [1.0408],
        ...,
        [0.9936],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370605.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.7459e-03,  6.1338e-03, -6.5410e-04,  ...,  1.1764e-02,
         -7.4274e-05,  3.4496e-03],
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04],
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04],
        ...,
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04],
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04],
        [ 2.2301e-04,  9.4756e-04, -6.1094e-04,  ...,  1.1327e-03,
          0.0000e+00, -7.6567e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3644.1594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.0108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.8620, device='cuda:0')



h[100].sum tensor(132.1522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.4842, device='cuda:0')



h[200].sum tensor(56.6992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1000, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0153, 0.0000,  ..., 0.0280, 0.0000, 0.0077],
        [0.0077, 0.0093, 0.0000,  ..., 0.0157, 0.0000, 0.0036],
        [0.0009, 0.0040, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72251.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0573, 0.1239, 0.1598,  ..., 0.0142, 0.0700, 0.0000],
        [0.0271, 0.1074, 0.0778,  ..., 0.0017, 0.0463, 0.0000],
        [0.0098, 0.0974, 0.0307,  ..., 0.0000, 0.0328, 0.0000],
        ...,
        [0.0027, 0.0936, 0.0100,  ..., 0.0000, 0.0267, 0.0000],
        [0.0026, 0.0935, 0.0100,  ..., 0.0000, 0.0267, 0.0000],
        [0.0026, 0.0935, 0.0100,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595349.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2843.6013, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(355.4163, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6883.0063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1162.0566, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-688.2308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5777],
        [-1.6627],
        [-2.9229],
        ...,
        [-6.4952],
        [-6.4866],
        [-6.4860]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302439.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0367],
        [1.0372],
        [1.0408],
        ...,
        [0.9936],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370605.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0407],
        ...,
        [0.9936],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370608.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.2826e-03,  5.7681e-03, -6.5075e-04,  ...,  1.0898e-02,
         -6.8000e-05,  3.1119e-03],
        [ 2.6512e-04,  9.8328e-04, -6.1094e-04,  ...,  1.0902e-03,
          0.0000e+00, -7.7732e-04],
        [ 2.6512e-04,  9.8328e-04, -6.1094e-04,  ...,  1.0902e-03,
          0.0000e+00, -7.7732e-04],
        ...,
        [ 2.6512e-04,  9.8328e-04, -6.1094e-04,  ...,  1.0902e-03,
          0.0000e+00, -7.7732e-04],
        [ 2.6512e-04,  9.8328e-04, -6.1094e-04,  ...,  1.0902e-03,
          0.0000e+00, -7.7732e-04],
        [ 2.6512e-04,  9.8328e-04, -6.1094e-04,  ...,  1.0902e-03,
          0.0000e+00, -7.7732e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3491.5718, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.6733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9468, device='cuda:0')



h[100].sum tensor(129.8668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7479, device='cuda:0')



h[200].sum tensor(55.7753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0162, 0.0161, 0.0000,  ..., 0.0291, 0.0000, 0.0081],
        [0.0073, 0.0091, 0.0000,  ..., 0.0147, 0.0000, 0.0032],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70517.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0457, 0.1170, 0.1261,  ..., 0.0042, 0.0612, 0.0000],
        [0.0286, 0.1072, 0.0797,  ..., 0.0000, 0.0478, 0.0000],
        [0.0185, 0.1010, 0.0518,  ..., 0.0000, 0.0398, 0.0000],
        ...,
        [0.0031, 0.0929, 0.0096,  ..., 0.0000, 0.0275, 0.0000],
        [0.0031, 0.0929, 0.0096,  ..., 0.0000, 0.0275, 0.0000],
        [0.0031, 0.0928, 0.0096,  ..., 0.0000, 0.0275, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590510.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2872.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(337.7138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7058.1069, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1143.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-667.6458, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2270],
        [-0.6505],
        [-0.9349],
        ...,
        [-6.4410],
        [-6.4337],
        [-6.4341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259731.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0407],
        ...,
        [0.9936],
        [0.9927],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370608.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 350.0 event: 1750 loss: tensor(384.6173, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0407],
        ...,
        [0.9936],
        [0.9926],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370610.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0245,  0.0203, -0.0008,  ...,  0.0406, -0.0003,  0.0149],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0153,  0.0130, -0.0007,  ...,  0.0256, -0.0002,  0.0090],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4197.0400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.4599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.2587, device='cuda:0')



h[100].sum tensor(131.7617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.6390, device='cuda:0')



h[200].sum tensor(65.4430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0698, 0.0587, 0.0000,  ..., 0.1163, 0.0000, 0.0419],
        [0.0829, 0.0692, 0.0000,  ..., 0.1377, 0.0000, 0.0496],
        [0.0202, 0.0193, 0.0000,  ..., 0.0355, 0.0000, 0.0107],
        ...,
        [0.0012, 0.0042, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79847.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2124, 0.1935, 0.5645,  ..., 0.1161, 0.1904, 0.0000],
        [0.1708, 0.1760, 0.4550,  ..., 0.0850, 0.1584, 0.0000],
        [0.0978, 0.1428, 0.2629,  ..., 0.0329, 0.1021, 0.0000],
        ...,
        [0.0032, 0.0926, 0.0093,  ..., 0.0000, 0.0280, 0.0000],
        [0.0032, 0.0926, 0.0093,  ..., 0.0000, 0.0280, 0.0000],
        [0.0032, 0.0925, 0.0093,  ..., 0.0000, 0.0279, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(625186.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3387.6128, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(419.8400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7214.9385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1270.7079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-765.8168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2483],
        [ 0.2978],
        [ 0.3404],
        ...,
        [-6.4420],
        [-6.4339],
        [-6.4333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260709.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0407],
        ...,
        [0.9936],
        [0.9926],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370610.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0373],
        [1.0407],
        ...,
        [0.9936],
        [0.9926],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370612.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0142,  0.0121, -0.0007,  ...,  0.0237, -0.0002,  0.0082],
        [ 0.0142,  0.0120, -0.0007,  ...,  0.0237, -0.0002,  0.0082],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3788.9644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.7734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.9224, device='cuda:0')



h[100].sum tensor(130.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.6542, device='cuda:0')



h[200].sum tensor(59.9664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0155, 0.0000,  ..., 0.0277, 0.0000, 0.0084],
        [0.0275, 0.0251, 0.0000,  ..., 0.0473, 0.0000, 0.0154],
        [0.0822, 0.0687, 0.0000,  ..., 0.1366, 0.0000, 0.0492],
        ...,
        [0.0012, 0.0042, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73069.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0417, 0.1106, 0.1117,  ..., 0.0085, 0.0574, 0.0000],
        [0.0760, 0.1287, 0.2028,  ..., 0.0252, 0.0842, 0.0000],
        [0.1346, 0.1572, 0.3584,  ..., 0.0626, 0.1300, 0.0000],
        ...,
        [0.0031, 0.0926, 0.0090,  ..., 0.0000, 0.0282, 0.0000],
        [0.0031, 0.0925, 0.0090,  ..., 0.0000, 0.0282, 0.0000],
        [0.0031, 0.0924, 0.0090,  ..., 0.0000, 0.0281, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599767.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3000.9194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(357.5261, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7317.7075, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1179.5138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-694.7331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4215],
        [-0.5901],
        [ 0.1120],
        ...,
        [-6.4659],
        [-6.4577],
        [-6.4570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253122.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0373],
        [1.0407],
        ...,
        [0.9936],
        [0.9926],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370612.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0406],
        ...,
        [0.9936],
        [0.9926],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370613.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 9.1787e-03,  8.0992e-03, -6.7010e-04,  ...,  1.5668e-02,
         -9.8766e-05,  5.0132e-03],
        [ 2.9819e-02,  2.4516e-02, -8.0669e-04,  ...,  4.9332e-02,
         -3.2679e-04,  1.8364e-02],
        [ 1.4042e-02,  1.1967e-02, -7.0228e-04,  ...,  2.3600e-02,
         -1.5249e-04,  8.1590e-03],
        ...,
        [ 2.3886e-04,  9.8864e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -7.6970e-04],
        [ 2.3886e-04,  9.8864e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -7.6970e-04],
        [ 2.3886e-04,  9.8864e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -7.6970e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3376.1860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5519, device='cuda:0')



h[100].sum tensor(129.5081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.5673, device='cuda:0')



h[200].sum tensor(53.6320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0920, 0.0764, 0.0000,  ..., 0.1529, 0.0000, 0.0557],
        [0.0693, 0.0584, 0.0000,  ..., 0.1159, 0.0000, 0.0410],
        [0.0706, 0.0595, 0.0000,  ..., 0.1180, 0.0000, 0.0418],
        ...,
        [0.0010, 0.0042, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0010, 0.0042, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0010, 0.0042, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68935.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1454, 0.1652, 0.3920,  ..., 0.0695, 0.1379, 0.0000],
        [0.1521, 0.1707, 0.4103,  ..., 0.0723, 0.1433, 0.0000],
        [0.1507, 0.1705, 0.4066,  ..., 0.0712, 0.1423, 0.0000],
        ...,
        [0.0026, 0.0932, 0.0088,  ..., 0.0000, 0.0276, 0.0000],
        [0.0026, 0.0932, 0.0088,  ..., 0.0000, 0.0276, 0.0000],
        [0.0025, 0.0931, 0.0088,  ..., 0.0000, 0.0276, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587848.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2641.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.0851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7386.1655, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1120.5187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-653.8052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2241],
        [ 0.2592],
        [ 0.2463],
        ...,
        [-6.5441],
        [-6.5356],
        [-6.5349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309058.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0406],
        ...,
        [0.9936],
        [0.9926],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370613.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0406],
        ...,
        [0.9935],
        [0.9926],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370614.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4384.8145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.6990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6432, device='cuda:0')



h[100].sum tensor(134.1808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(43.7782, device='cuda:0')



h[200].sum tensor(66.1435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0092, 0.0000,  ..., 0.0152, 0.0000, 0.0034],
        [0.0009, 0.0041, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0262, 0.0242, 0.0000,  ..., 0.0459, 0.0000, 0.0148],
        ...,
        [0.0009, 0.0041, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0009, 0.0041, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0009, 0.0041, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83381.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0402, 0.1148, 0.1137,  ..., 0.0010, 0.0567, 0.0000],
        [0.0509, 0.1195, 0.1414,  ..., 0.0107, 0.0649, 0.0000],
        [0.1108, 0.1493, 0.3024,  ..., 0.0502, 0.1112, 0.0000],
        ...,
        [0.0021, 0.0937, 0.0087,  ..., 0.0000, 0.0272, 0.0000],
        [0.0021, 0.0937, 0.0087,  ..., 0.0000, 0.0272, 0.0000],
        [0.0021, 0.0936, 0.0087,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(650993.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3479.1235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(450.1238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6902.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1323.1681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-812.5336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2179],
        [ 0.1601],
        [ 0.1160],
        ...,
        [-6.6122],
        [-6.6035],
        [-6.6025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330175.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0368],
        [1.0372],
        [1.0406],
        ...,
        [0.9935],
        [0.9926],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370614.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0372],
        [1.0405],
        ...,
        [0.9935],
        [0.9925],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370615.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0724e-02,  9.3343e-03, -6.8059e-04,  ...,  1.8276e-02,
         -1.1451e-04,  6.0381e-03],
        [ 5.3122e-03,  5.0291e-03, -6.4477e-04,  ...,  9.4456e-03,
         -5.5618e-05,  2.5359e-03],
        [ 1.6849e-02,  1.4206e-02, -7.2113e-04,  ...,  2.8268e-02,
         -1.8116e-04,  1.0001e-02],
        ...,
        [ 2.0093e-04,  9.6338e-04, -6.1094e-04,  ...,  1.1067e-03,
          0.0000e+00, -7.7154e-04],
        [ 2.0093e-04,  9.6338e-04, -6.1094e-04,  ...,  1.1067e-03,
          0.0000e+00, -7.7154e-04],
        [ 2.0093e-04,  9.6338e-04, -6.1094e-04,  ...,  1.1067e-03,
          0.0000e+00, -7.7154e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3861.8079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.5939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.6679, device='cuda:0')



h[100].sum tensor(132.5700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(34.8830, device='cuda:0')



h[200].sum tensor(59.0680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3014, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0363, 0.0322, 0.0000,  ..., 0.0625, 0.0000, 0.0198],
        [0.0469, 0.0407, 0.0000,  ..., 0.0798, 0.0000, 0.0266],
        [0.0343, 0.0306, 0.0000,  ..., 0.0592, 0.0000, 0.0184],
        ...,
        [0.0008, 0.0041, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0008, 0.0041, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0008, 0.0041, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74611.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0945, 0.1472, 0.2635,  ..., 0.0279, 0.0990, 0.0000],
        [0.1006, 0.1509, 0.2797,  ..., 0.0325, 0.1037, 0.0000],
        [0.0961, 0.1492, 0.2680,  ..., 0.0288, 0.1004, 0.0000],
        ...,
        [0.0021, 0.0938, 0.0090,  ..., 0.0000, 0.0270, 0.0000],
        [0.0021, 0.0938, 0.0090,  ..., 0.0000, 0.0270, 0.0000],
        [0.0021, 0.0937, 0.0090,  ..., 0.0000, 0.0269, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605322.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2784.2861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(371.2127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7179.3076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1199.6544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-720.3576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3688],
        [ 0.3241],
        [ 0.2647],
        ...,
        [-6.6458],
        [-6.6384],
        [-6.6377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346448.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0372],
        [1.0405],
        ...,
        [0.9935],
        [0.9925],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370615.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0372],
        [1.0404],
        ...,
        [0.9935],
        [0.9925],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370616.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0151,  0.0128, -0.0007,  ...,  0.0253, -0.0002,  0.0088],
        [ 0.0214,  0.0178, -0.0008,  ...,  0.0355, -0.0002,  0.0129],
        [ 0.0188,  0.0157, -0.0007,  ...,  0.0313, -0.0002,  0.0112],
        ...,
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4003.6357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(32.3527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6230, device='cuda:0')



h[100].sum tensor(132.4724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(37.7385, device='cuda:0')



h[200].sum tensor(61.4862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4079, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0616, 0.0522, 0.0000,  ..., 0.1034, 0.0000, 0.0359],
        [0.0746, 0.0626, 0.0000,  ..., 0.1246, 0.0000, 0.0443],
        [0.0779, 0.0652, 0.0000,  ..., 0.1300, 0.0000, 0.0464],
        ...,
        [0.0010, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0010, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0010, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75640.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1246, 0.1581, 0.3403,  ..., 0.0528, 0.1208, 0.0000],
        [0.1504, 0.1716, 0.4095,  ..., 0.0707, 0.1408, 0.0000],
        [0.1582, 0.1755, 0.4301,  ..., 0.0767, 0.1467, 0.0000],
        ...,
        [0.0045, 0.0948, 0.0152,  ..., 0.0000, 0.0286, 0.0000],
        [0.0055, 0.0954, 0.0180,  ..., 0.0000, 0.0294, 0.0000],
        [0.0045, 0.0947, 0.0152,  ..., 0.0000, 0.0286, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607640.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2931.1353, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(379.1648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7403.4907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1210.5607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-729.9952, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2515],
        [ 0.3020],
        [ 0.2957],
        ...,
        [-5.9212],
        [-5.7924],
        [-5.7895]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334315.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0372],
        [1.0404],
        ...,
        [0.9935],
        [0.9925],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370616.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0371],
        [1.0404],
        ...,
        [0.9934],
        [0.9925],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370617.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0307,  0.0252, -0.0008,  ...,  0.0506, -0.0003,  0.0188],
        [ 0.0120,  0.0103, -0.0007,  ...,  0.0202, -0.0001,  0.0068],
        [ 0.0165,  0.0139, -0.0007,  ...,  0.0276, -0.0002,  0.0097],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3682.2312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.2634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.7279, device='cuda:0')



h[100].sum tensor(130.2395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.0727, device='cuda:0')



h[200].sum tensor(58.2118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1966, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0614, 0.0520, 0.0000,  ..., 0.1026, 0.0000, 0.0355],
        [0.0756, 0.0633, 0.0000,  ..., 0.1257, 0.0000, 0.0447],
        [0.0689, 0.0579, 0.0000,  ..., 0.1147, 0.0000, 0.0403],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72944.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1518, 0.1645, 0.4081,  ..., 0.0755, 0.1411, 0.0000],
        [0.1504, 0.1653, 0.4044,  ..., 0.0731, 0.1402, 0.0000],
        [0.1372, 0.1615, 0.3697,  ..., 0.0611, 0.1302, 0.0000],
        ...,
        [0.0032, 0.0928, 0.0104,  ..., 0.0000, 0.0274, 0.0000],
        [0.0032, 0.0927, 0.0104,  ..., 0.0000, 0.0274, 0.0000],
        [0.0032, 0.0927, 0.0104,  ..., 0.0000, 0.0273, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601725.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3002.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(353.0771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7809.3096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1168.1046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-699.5449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4303],
        [ 0.4420],
        [ 0.4439],
        ...,
        [-6.5956],
        [-6.5872],
        [-6.5868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-298775.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0371],
        [1.0404],
        ...,
        [0.9934],
        [0.9925],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370617.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0371],
        [1.0404],
        ...,
        [0.9934],
        [0.9924],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370617.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0169,  0.0142, -0.0007,  ...,  0.0281, -0.0002,  0.0099],
        [ 0.0140,  0.0119, -0.0007,  ...,  0.0234, -0.0001,  0.0080],
        [ 0.0167,  0.0140, -0.0007,  ...,  0.0278, -0.0002,  0.0097],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3419.3411, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.7063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3390, device='cuda:0')



h[100].sum tensor(129.2485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.9205, device='cuda:0')



h[200].sum tensor(54.9922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0522, 0.0446, 0.0000,  ..., 0.0874, 0.0000, 0.0294],
        [0.0679, 0.0571, 0.0000,  ..., 0.1131, 0.0000, 0.0396],
        [0.0762, 0.0637, 0.0000,  ..., 0.1266, 0.0000, 0.0449],
        ...,
        [0.0012, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0012, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0012, 0.0040, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69098.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1675, 0.1749, 0.4508,  ..., 0.0822, 0.1520, 0.0000],
        [0.1997, 0.1899, 0.5366,  ..., 0.1057, 0.1769, 0.0000],
        [0.2058, 0.1916, 0.5529,  ..., 0.1114, 0.1820, 0.0000],
        ...,
        [0.0034, 0.0927, 0.0113,  ..., 0.0000, 0.0271, 0.0000],
        [0.0034, 0.0927, 0.0113,  ..., 0.0000, 0.0271, 0.0000],
        [0.0034, 0.0926, 0.0113,  ..., 0.0000, 0.0270, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587098.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2849.8484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.8166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7918.6997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1113.0983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-660.5241, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1859],
        [ 0.1771],
        [ 0.1770],
        ...,
        [-6.6120],
        [-6.6037],
        [-6.6028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-292188.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0371],
        [1.0404],
        ...,
        [0.9934],
        [0.9924],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370617.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0370],
        [1.0403],
        ...,
        [0.9933],
        [0.9923],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370617.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.3041e-03,  5.7395e-03, -6.5108e-04,  ...,  1.0933e-02,
         -6.3999e-05,  3.0465e-03],
        [ 2.3814e-04,  9.1635e-04, -6.1094e-04,  ...,  1.0411e-03,
          0.0000e+00, -8.7637e-04],
        [ 2.3814e-04,  9.1635e-04, -6.1094e-04,  ...,  1.0411e-03,
          0.0000e+00, -8.7637e-04],
        ...,
        [ 2.3814e-04,  9.1635e-04, -6.1094e-04,  ...,  1.0411e-03,
          0.0000e+00, -8.7637e-04],
        [ 2.3814e-04,  9.1635e-04, -6.1094e-04,  ...,  1.0411e-03,
          0.0000e+00, -8.7637e-04],
        [ 2.3814e-04,  9.1635e-04, -6.1094e-04,  ...,  1.0411e-03,
          0.0000e+00, -8.7637e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4652.4033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(40.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9540, device='cuda:0')



h[100].sum tensor(135.2086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(50.6868, device='cuda:0')



h[200].sum tensor(70.6942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.8910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0149, 0.0148, 0.0000,  ..., 0.0270, 0.0000, 0.0072],
        [0.0073, 0.0088, 0.0000,  ..., 0.0146, 0.0000, 0.0032],
        [0.0010, 0.0038, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(91765.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.1152, 0.1205,  ..., 0.0040, 0.0557, 0.0000],
        [0.0237, 0.1051, 0.0706,  ..., 0.0000, 0.0419, 0.0000],
        [0.0109, 0.0975, 0.0350,  ..., 0.0000, 0.0320, 0.0000],
        ...,
        [0.0029, 0.0936, 0.0129,  ..., 0.0000, 0.0260, 0.0000],
        [0.0053, 0.0948, 0.0194,  ..., 0.0000, 0.0278, 0.0000],
        [0.0134, 0.0991, 0.0413,  ..., 0.0000, 0.0340, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698295.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4392.9399, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(517.6735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7260.2554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1424.5502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-909.3325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8053],
        [-2.0575],
        [-3.3608],
        ...,
        [-6.4246],
        [-5.9632],
        [-5.1247]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317858.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0370],
        [1.0403],
        ...,
        [0.9933],
        [0.9923],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370617.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0370],
        [1.0403],
        ...,
        [0.9933],
        [0.9923],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370617.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3404.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.3163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.4724, device='cuda:0')



h[100].sum tensor(131.8147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.3194, device='cuda:0')



h[200].sum tensor(53.8073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0036, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0037, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0008, 0.0037, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69287.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0921, 0.0139,  ..., 0.0000, 0.0242, 0.0000],
        [0.0023, 0.0928, 0.0140,  ..., 0.0000, 0.0244, 0.0000],
        [0.0023, 0.0931, 0.0141,  ..., 0.0000, 0.0246, 0.0000],
        ...,
        [0.0024, 0.0945, 0.0144,  ..., 0.0000, 0.0250, 0.0000],
        [0.0024, 0.0945, 0.0144,  ..., 0.0000, 0.0250, 0.0000],
        [0.0024, 0.0944, 0.0144,  ..., 0.0000, 0.0250, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589258., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2646.0649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(317.9740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7491.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1112.3829, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-673.4016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5799],
        [-6.6174],
        [-6.6107],
        ...,
        [-6.7458],
        [-6.7375],
        [-6.7369]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-372877.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0370],
        [1.0403],
        ...,
        [0.9933],
        [0.9923],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370617.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 360.0 event: 1800 loss: tensor(463.4474, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0370],
        [1.0403],
        ...,
        [0.9932],
        [0.9922],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370617.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.9009e-04,  8.6038e-04, -6.1094e-04,  ...,  1.0802e-03,
          0.0000e+00, -8.9213e-04],
        [ 4.9561e-03,  4.6492e-03, -6.4248e-04,  ...,  8.8544e-03,
         -4.9522e-05,  2.1903e-03],
        [ 4.9561e-03,  4.6492e-03, -6.4248e-04,  ...,  8.8544e-03,
         -4.9522e-05,  2.1903e-03],
        ...,
        [ 1.9009e-04,  8.6038e-04, -6.1094e-04,  ...,  1.0802e-03,
          0.0000e+00, -8.9213e-04],
        [ 1.9009e-04,  8.6038e-04, -6.1094e-04,  ...,  1.0802e-03,
          0.0000e+00, -8.9213e-04],
        [ 1.9009e-04,  8.6038e-04, -6.1094e-04,  ...,  1.0802e-03,
          0.0000e+00, -8.9213e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3371.7378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.7701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3792, device='cuda:0')



h[100].sum tensor(131.8213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.0406, device='cuda:0')



h[200].sum tensor(53.4332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0106, 0.0000,  ..., 0.0190, 0.0000, 0.0039],
        [0.0098, 0.0107, 0.0000,  ..., 0.0192, 0.0000, 0.0040],
        [0.0098, 0.0108, 0.0000,  ..., 0.0192, 0.0000, 0.0040],
        ...,
        [0.0008, 0.0036, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0008, 0.0036, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69699.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0259, 0.1089, 0.0830,  ..., 0.0000, 0.0425, 0.0000],
        [0.0221, 0.1079, 0.0733,  ..., 0.0000, 0.0400, 0.0000],
        [0.0211, 0.1075, 0.0703,  ..., 0.0000, 0.0393, 0.0000],
        ...,
        [0.0021, 0.0949, 0.0154,  ..., 0.0000, 0.0246, 0.0000],
        [0.0021, 0.0948, 0.0154,  ..., 0.0000, 0.0245, 0.0000],
        [0.0021, 0.0948, 0.0154,  ..., 0.0000, 0.0245, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594247.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2641.3389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(322.9898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7503.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1116.4830, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-678.6107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4132],
        [-3.3876],
        [-4.2145],
        ...,
        [-6.7699],
        [-6.7607],
        [-6.7593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-398909.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0370],
        [1.0403],
        ...,
        [0.9932],
        [0.9922],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370617.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0369],
        [1.0369],
        [1.0403],
        ...,
        [0.9931],
        [0.9922],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370618.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3844.6030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.6380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0565, device='cuda:0')



h[100].sum tensor(132.3942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.0449, device='cuda:0')



h[200].sum tensor(60.9239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0037, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0037, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0037, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0010, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77650.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0260, 0.1050, 0.0804,  ..., 0.0039, 0.0426, 0.0000],
        [0.0239, 0.1044, 0.0746,  ..., 0.0030, 0.0411, 0.0000],
        [0.0200, 0.1024, 0.0638,  ..., 0.0016, 0.0382, 0.0000],
        ...,
        [0.0023, 0.0942, 0.0156,  ..., 0.0000, 0.0252, 0.0000],
        [0.0023, 0.0942, 0.0156,  ..., 0.0000, 0.0252, 0.0000],
        [0.0023, 0.0941, 0.0156,  ..., 0.0000, 0.0251, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627759.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3222.8630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(397.6706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7281.4160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1228.3881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-760.4573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7095],
        [-0.8518],
        [-1.1336],
        ...,
        [-5.9824],
        [-5.9261],
        [-5.7199]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346962.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0369],
        [1.0369],
        [1.0403],
        ...,
        [0.9931],
        [0.9922],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370618.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0370],
        [1.0403],
        ...,
        [0.9930],
        [0.9921],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370619.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.8210e-03,  4.5526e-03, -6.4095e-04,  ...,  8.4431e-03,
         -4.6404e-05,  1.9911e-03],
        [ 7.0096e-03,  6.2930e-03, -6.5543e-04,  ...,  1.2011e-02,
         -6.8790e-05,  3.4059e-03],
        [ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04],
        ...,
        [ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04],
        [ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04],
        [ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4232.0190, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.8736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1761, device='cuda:0')



h[100].sum tensor(132.2779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.3819, device='cuda:0')



h[200].sum tensor(67.4310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5812, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0396, 0.0345, 0.0000,  ..., 0.0670, 0.0000, 0.0210],
        [0.0155, 0.0153, 0.0000,  ..., 0.0277, 0.0000, 0.0073],
        [0.0082, 0.0095, 0.0000,  ..., 0.0157, 0.0000, 0.0035],
        ...,
        [0.0012, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83544.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0764, 0.1334, 0.2168,  ..., 0.0197, 0.0818, 0.0000],
        [0.0503, 0.1193, 0.1458,  ..., 0.0088, 0.0621, 0.0000],
        [0.0262, 0.1057, 0.0799,  ..., 0.0009, 0.0438, 0.0000],
        ...,
        [0.0047, 0.0946, 0.0211,  ..., 0.0000, 0.0277, 0.0000],
        [0.0057, 0.0952, 0.0240,  ..., 0.0000, 0.0285, 0.0000],
        [0.0047, 0.0944, 0.0211,  ..., 0.0000, 0.0277, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649260.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3658.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(453.8526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7231.5908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1313.7648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-820.7756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1715],
        [-0.5619],
        [-1.7466],
        ...,
        [-5.8864],
        [-5.7752],
        [-5.8763]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293429.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0370],
        [1.0403],
        ...,
        [0.9930],
        [0.9921],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370619.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0370],
        [1.0403],
        ...,
        [0.9930],
        [0.9921],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370619.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04],
        [ 8.6076e-03,  7.5639e-03, -6.6600e-04,  ...,  1.4617e-02,
         -8.5135e-05,  4.4389e-03],
        [ 1.4963e-02,  1.2618e-02, -7.0804e-04,  ...,  2.4979e-02,
         -1.5014e-04,  8.5475e-03],
        ...,
        [ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04],
        [ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04],
        [ 2.8419e-04,  9.4471e-04, -6.1094e-04,  ...,  1.0462e-03,
          0.0000e+00, -9.4167e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3598.3992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.5096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6239, device='cuda:0')



h[100].sum tensor(129.9384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.7618, device='cuda:0')



h[200].sum tensor(59.1206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0168, 0.0163, 0.0000,  ..., 0.0297, 0.0000, 0.0081],
        [0.0288, 0.0259, 0.0000,  ..., 0.0495, 0.0000, 0.0149],
        [0.0491, 0.0421, 0.0000,  ..., 0.0825, 0.0000, 0.0271],
        ...,
        [0.0012, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72202.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0492, 0.1169, 0.1424,  ..., 0.0119, 0.0611, 0.0000],
        [0.0857, 0.1372, 0.2419,  ..., 0.0297, 0.0892, 0.0000],
        [0.1236, 0.1569, 0.3452,  ..., 0.0526, 0.1185, 0.0000],
        ...,
        [0.0026, 0.0932, 0.0154,  ..., 0.0000, 0.0261, 0.0000],
        [0.0026, 0.0932, 0.0154,  ..., 0.0000, 0.0261, 0.0000],
        [0.0026, 0.0931, 0.0154,  ..., 0.0000, 0.0261, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595409.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2773.9202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(354.1637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7800.3486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1149.5099, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-695.8385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3517],
        [-0.2658],
        [ 0.3030],
        ...,
        [-6.6292],
        [-6.6211],
        [-6.6203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328895.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0370],
        [1.0403],
        ...,
        [0.9930],
        [0.9921],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370619.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0370],
        [1.0370],
        [1.0403],
        ...,
        [0.9929],
        [0.9920],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370620.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2241e-02,  1.0460e-02, -6.8978e-04,  ...,  2.0465e-02,
         -1.2097e-04,  6.7442e-03],
        [ 3.2169e-04,  9.8009e-04, -6.1094e-04,  ...,  1.0344e-03,
          0.0000e+00, -9.5966e-04],
        [ 9.6184e-03,  8.3740e-03, -6.7243e-04,  ...,  1.6189e-02,
         -9.4349e-05,  5.0491e-03],
        ...,
        [ 3.2169e-04,  9.8009e-04, -6.1094e-04,  ...,  1.0344e-03,
          0.0000e+00, -9.5966e-04],
        [ 3.2169e-04,  9.8009e-04, -6.1094e-04,  ...,  1.0344e-03,
          0.0000e+00, -9.5966e-04],
        [ 3.2169e-04,  9.8009e-04, -6.1094e-04,  ...,  1.0344e-03,
          0.0000e+00, -9.5966e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3130.4561, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.2874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.7516, device='cuda:0')



h[100].sum tensor(126.7488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.1747, device='cuda:0')



h[200].sum tensor(54.1436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0208, 0.0195, 0.0000,  ..., 0.0359, 0.0000, 0.0096],
        [0.0437, 0.0378, 0.0000,  ..., 0.0734, 0.0000, 0.0234],
        [0.0611, 0.0516, 0.0000,  ..., 0.1016, 0.0000, 0.0356],
        ...,
        [0.0014, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0014, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0014, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66692.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0927, 0.1401, 0.2600,  ..., 0.0279, 0.0955, 0.0000],
        [0.1430, 0.1635, 0.3954,  ..., 0.0647, 0.1343, 0.0000],
        [0.2002, 0.1873, 0.5494,  ..., 0.1100, 0.1786, 0.0000],
        ...,
        [0.0028, 0.0926, 0.0155,  ..., 0.0000, 0.0268, 0.0000],
        [0.0028, 0.0926, 0.0155,  ..., 0.0000, 0.0268, 0.0000],
        [0.0028, 0.0925, 0.0154,  ..., 0.0000, 0.0268, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576365.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2535.6018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(309.9594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7832.2607, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1075.0111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-632.8315, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3722],
        [ 0.3432],
        [ 0.2849],
        ...,
        [-6.5694],
        [-6.5618],
        [-6.5612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296507.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0370],
        [1.0370],
        [1.0403],
        ...,
        [0.9929],
        [0.9920],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370620.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0371],
        [1.0370],
        [1.0403],
        ...,
        [0.9928],
        [0.9919],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370621.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.2475e-04,  9.8938e-04, -6.1094e-04,  ...,  1.0515e-03,
          0.0000e+00, -9.5764e-04],
        [ 8.8991e-03,  7.8088e-03, -6.6766e-04,  ...,  1.5031e-02,
         -8.6355e-05,  4.5853e-03],
        [ 6.1207e-03,  5.5991e-03, -6.4928e-04,  ...,  1.0501e-02,
         -5.8373e-05,  2.7892e-03],
        ...,
        [ 3.2475e-04,  9.8938e-04, -6.1094e-04,  ...,  1.0515e-03,
          0.0000e+00, -9.5764e-04],
        [ 3.2475e-04,  9.8938e-04, -6.1094e-04,  ...,  1.0515e-03,
          0.0000e+00, -9.5764e-04],
        [ 3.2475e-04,  9.8938e-04, -6.1094e-04,  ...,  1.0515e-03,
          0.0000e+00, -9.5764e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3341.2773, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.7609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0422, device='cuda:0')



h[100].sum tensor(126.5236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0330, device='cuda:0')



h[200].sum tensor(57.7205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0085, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0102, 0.0111, 0.0000,  ..., 0.0187, 0.0000, 0.0047],
        [0.0147, 0.0147, 0.0000,  ..., 0.0261, 0.0000, 0.0066],
        [0.0538, 0.0458, 0.0000,  ..., 0.0898, 0.0000, 0.0299],
        ...,
        [0.0014, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0014, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0014, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67749.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0336, 0.1083, 0.1010,  ..., 0.0031, 0.0502, 0.0000],
        [0.0594, 0.1233, 0.1715,  ..., 0.0134, 0.0702, 0.0000],
        [0.1044, 0.1470, 0.2935,  ..., 0.0380, 0.1048, 0.0000],
        ...,
        [0.0025, 0.0929, 0.0160,  ..., 0.0000, 0.0269, 0.0000],
        [0.0025, 0.0929, 0.0160,  ..., 0.0000, 0.0269, 0.0000],
        [0.0025, 0.0928, 0.0160,  ..., 0.0000, 0.0269, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574633.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2458.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.7260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7798.5679, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1097.9976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-644.8060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2691],
        [ 0.0598],
        [ 0.2840],
        ...,
        [-6.5400],
        [-6.5319],
        [-6.5219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277528.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0371],
        [1.0370],
        [1.0403],
        ...,
        [0.9928],
        [0.9919],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370621.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0372],
        [1.0370],
        [1.0403],
        ...,
        [0.9928],
        [0.9918],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370621.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3310.9146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.4651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.7303, device='cuda:0')



h[100].sum tensor(126.1122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.1006, device='cuda:0')



h[200].sum tensor(57.5897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0040, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0012, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0012, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0013, 0.0041, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0013, 0.0041, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0013, 0.0041, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71006.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0916, 0.0163,  ..., 0.0000, 0.0255, 0.0000],
        [0.0028, 0.0930, 0.0196,  ..., 0.0000, 0.0266, 0.0000],
        [0.0111, 0.0979, 0.0428,  ..., 0.0000, 0.0332, 0.0000],
        ...,
        [0.0017, 0.0940, 0.0169,  ..., 0.0000, 0.0264, 0.0000],
        [0.0017, 0.0940, 0.0169,  ..., 0.0000, 0.0264, 0.0000],
        [0.0017, 0.0939, 0.0169,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596969., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2671.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(350.8515, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7109.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1159.5807, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-683.1473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4137],
        [-3.7516],
        [-2.6300],
        ...,
        [-6.5522],
        [-6.5460],
        [-6.5461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271483.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0372],
        [1.0370],
        [1.0403],
        ...,
        [0.9928],
        [0.9918],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370621.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0373],
        [1.0370],
        [1.0403],
        ...,
        [0.9927],
        [0.9917],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370622.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3578.2439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(29.5180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.5835, device='cuda:0')



h[100].sum tensor(127.0884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.6411, device='cuda:0')



h[200].sum tensor(60.9299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1805, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0040, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0011, 0.0040, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0011, 0.0040, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0040, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0011, 0.0040, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0011, 0.0040, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73427.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0926, 0.0169,  ..., 0.0000, 0.0250, 0.0000],
        [0.0007, 0.0933, 0.0170,  ..., 0.0000, 0.0252, 0.0000],
        [0.0008, 0.0937, 0.0172,  ..., 0.0000, 0.0254, 0.0000],
        ...,
        [0.0008, 0.0950, 0.0176,  ..., 0.0000, 0.0258, 0.0000],
        [0.0008, 0.0950, 0.0176,  ..., 0.0000, 0.0258, 0.0000],
        [0.0008, 0.0949, 0.0175,  ..., 0.0000, 0.0258, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600884.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2524.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(372.7832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6953.2402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1200.2184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-709.5426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0841],
        [-5.9976],
        [-5.8259],
        ...,
        [-6.5948],
        [-6.5870],
        [-6.5861]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-290389.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0373],
        [1.0370],
        [1.0403],
        ...,
        [0.9927],
        [0.9917],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370622.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0373],
        [1.0370],
        [1.0403],
        ...,
        [0.9926],
        [0.9917],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370622.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3342.9653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(25.9323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0685, device='cuda:0')



h[100].sum tensor(126.2271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.1119, device='cuda:0')



h[200].sum tensor(57.5940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0115, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69975.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0003, 0.0934, 0.0173,  ..., 0.0000, 0.0246, 0.0000],
        [0.0003, 0.0941, 0.0174,  ..., 0.0000, 0.0248, 0.0000],
        [0.0003, 0.0944, 0.0176,  ..., 0.0000, 0.0249, 0.0000],
        ...,
        [0.0003, 0.0958, 0.0180,  ..., 0.0000, 0.0254, 0.0000],
        [0.0003, 0.0958, 0.0180,  ..., 0.0000, 0.0254, 0.0000],
        [0.0003, 0.0957, 0.0179,  ..., 0.0000, 0.0253, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586598.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2190.2002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(341.1736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6707.2466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1161.4435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-675.3212, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.7869],
        [-6.0157],
        [-6.1960],
        ...,
        [-6.6266],
        [-6.6179],
        [-6.6163]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-296523.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0373],
        [1.0370],
        [1.0403],
        ...,
        [0.9926],
        [0.9917],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370622.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0374],
        [1.0369],
        [1.0403],
        ...,
        [0.9926],
        [0.9916],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370621.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3431.0496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.7876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.7121, device='cuda:0')



h[100].sum tensor(126.5078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.0358, device='cuda:0')



h[200].sum tensor(58.3934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0038, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72122.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0934, 0.0170,  ..., 0.0000, 0.0245, 0.0000],
        [0.0001, 0.0941, 0.0171,  ..., 0.0000, 0.0246, 0.0000],
        [0.0002, 0.0944, 0.0172,  ..., 0.0000, 0.0248, 0.0000],
        ...,
        [0.0002, 0.0958, 0.0176,  ..., 0.0000, 0.0252, 0.0000],
        [0.0002, 0.0958, 0.0176,  ..., 0.0000, 0.0252, 0.0000],
        [0.0002, 0.0957, 0.0176,  ..., 0.0000, 0.0252, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600792., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2293.8650, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.5633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6877.8809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1189.8030, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-697.8846, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9237],
        [-6.2066],
        [-6.3514],
        ...,
        [-6.6764],
        [-6.6685],
        [-6.6678]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-317787.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0374],
        [1.0369],
        [1.0403],
        ...,
        [0.9926],
        [0.9916],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370621.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 370.0 event: 1850 loss: tensor(394.9612, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0369],
        [1.0402],
        ...,
        [0.9925],
        [0.9916],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370620.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3515e-04,  9.3427e-04, -6.1094e-04,  ...,  1.1124e-03,
          0.0000e+00, -9.4206e-04],
        [ 6.6104e-03,  6.0051e-03, -6.5313e-04,  ...,  1.1512e-02,
         -6.1794e-05,  3.1832e-03],
        [ 4.9834e-03,  4.7110e-03, -6.4236e-04,  ...,  8.8584e-03,
         -4.6024e-05,  2.1305e-03],
        ...,
        [ 2.3515e-04,  9.3427e-04, -6.1094e-04,  ...,  1.1124e-03,
          0.0000e+00, -9.4206e-04],
        [ 2.3515e-04,  9.3427e-04, -6.1094e-04,  ...,  1.1124e-03,
          0.0000e+00, -9.4206e-04],
        [ 2.3515e-04,  9.3427e-04, -6.1094e-04,  ...,  1.1124e-03,
          0.0000e+00, -9.4206e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3120.9065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(23.7900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9256, device='cuda:0')



h[100].sum tensor(125.1724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6948, device='cuda:0')



h[200].sum tensor(53.9177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0075, 0.0091, 0.0000,  ..., 0.0153, 0.0000, 0.0033],
        [0.0154, 0.0154, 0.0000,  ..., 0.0282, 0.0000, 0.0074],
        [0.0505, 0.0433, 0.0000,  ..., 0.0855, 0.0000, 0.0281],
        ...,
        [0.0010, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68311.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5796e-02, 1.0887e-01, 8.8572e-02,  ..., 1.8067e-03, 4.4835e-02,
         0.0000e+00],
        [5.3719e-02, 1.2593e-01, 1.6631e-01,  ..., 1.3030e-02, 6.6365e-02,
         0.0000e+00],
        [1.0099e-01, 1.5220e-01, 2.9723e-01,  ..., 3.9362e-02, 1.0254e-01,
         0.0000e+00],
        ...,
        [2.3375e-04, 9.5096e-02, 1.6497e-02,  ..., 0.0000e+00, 2.5447e-02,
         0.0000e+00],
        [2.3354e-04, 9.5070e-02, 1.6492e-02,  ..., 0.0000e+00, 2.5440e-02,
         0.0000e+00],
        [2.3318e-04, 9.4982e-02, 1.6475e-02,  ..., 0.0000e+00, 2.5415e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585875.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2053.6790, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(328.6216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7229.0522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1131.7926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-652.7032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8590],
        [-0.2456],
        [ 0.1725],
        ...,
        [-6.7120],
        [-6.7037],
        [-6.7028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-344032.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0369],
        [1.0402],
        ...,
        [0.9925],
        [0.9916],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370620.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0402],
        ...,
        [0.9925],
        [0.9915],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370619.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.4322e-03,  5.8591e-03, -6.5187e-04,  ...,  1.1175e-02,
         -5.9486e-05,  3.0496e-03],
        [ 2.4672e-04,  9.3902e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -9.5232e-04],
        [ 2.4672e-04,  9.3902e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -9.5232e-04],
        ...,
        [ 2.4672e-04,  9.3902e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -9.5232e-04],
        [ 2.4672e-04,  9.3902e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -9.5232e-04],
        [ 2.4672e-04,  9.3902e-04, -6.1094e-04,  ...,  1.0869e-03,
          0.0000e+00, -9.5232e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3900.7354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.4446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8358, device='cuda:0')



h[100].sum tensor(127.8517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.3746, device='cuda:0')



h[200].sum tensor(63.6930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4317, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0192, 0.0000,  ..., 0.0358, 0.0000, 0.0105],
        [0.0357, 0.0315, 0.0000,  ..., 0.0611, 0.0000, 0.0195],
        [0.0298, 0.0268, 0.0000,  ..., 0.0514, 0.0000, 0.0166],
        ...,
        [0.0010, 0.0040, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77441.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0749, 0.1357, 0.2220,  ..., 0.0207, 0.0825, 0.0000],
        [0.0962, 0.1468, 0.2800,  ..., 0.0370, 0.0988, 0.0000],
        [0.1000, 0.1483, 0.2903,  ..., 0.0414, 0.1017, 0.0000],
        ...,
        [0.0003, 0.0943, 0.0154,  ..., 0.0000, 0.0257, 0.0000],
        [0.0003, 0.0943, 0.0154,  ..., 0.0000, 0.0257, 0.0000],
        [0.0003, 0.0942, 0.0154,  ..., 0.0000, 0.0257, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623642.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2704.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(408.8708, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7061.7856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1260.6987, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-751.5629, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3721],
        [ 0.3441],
        [ 0.3164],
        ...,
        [-6.7422],
        [-6.7339],
        [-6.7332]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310424.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0402],
        ...,
        [0.9925],
        [0.9915],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370619.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0402],
        ...,
        [0.9924],
        [0.9915],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370619.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0182,  0.0153, -0.0007,  ...,  0.0304, -0.0002,  0.0107],
        [ 0.0259,  0.0213, -0.0008,  ...,  0.0429, -0.0002,  0.0156],
        [ 0.0187,  0.0156, -0.0007,  ...,  0.0311, -0.0002,  0.0110],
        ...,
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010],
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010],
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3933.2905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(34.5767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9626, device='cuda:0')



h[100].sum tensor(127.7101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(38.7538, device='cuda:0')



h[200].sum tensor(63.6546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0995, 0.0822, 0.0000,  ..., 0.1649, 0.0000, 0.0597],
        [0.0874, 0.0726, 0.0000,  ..., 0.1452, 0.0000, 0.0518],
        [0.0727, 0.0609, 0.0000,  ..., 0.1212, 0.0000, 0.0423],
        ...,
        [0.0011, 0.0040, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0040, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0040, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79487.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2719e-01, 2.0481e-01, 6.3382e-01,  ..., 1.3386e-01, 2.0050e-01,
         0.0000e+00],
        [2.1387e-01, 2.0003e-01, 5.9748e-01,  ..., 1.2282e-01, 1.9025e-01,
         0.0000e+00],
        [1.9187e-01, 1.9101e-01, 5.3742e-01,  ..., 1.0506e-01, 1.7317e-01,
         0.0000e+00],
        ...,
        [5.2981e-04, 9.3388e-02, 1.4067e-02,  ..., 0.0000e+00, 2.6081e-02,
         0.0000e+00],
        [5.2956e-04, 9.3362e-02, 1.4062e-02,  ..., 0.0000e+00, 2.6073e-02,
         0.0000e+00],
        [5.2872e-04, 9.3274e-02, 1.4048e-02,  ..., 0.0000e+00, 2.6047e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630747.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2894.5491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(427.3595, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7184.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1288.4459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-771.6668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1061],
        [ 0.1043],
        [ 0.1089],
        ...,
        [-6.2010],
        [-6.0397],
        [-6.0378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-300555.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0402],
        ...,
        [0.9924],
        [0.9915],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370619.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0401],
        ...,
        [0.9923],
        [0.9914],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370618.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010],
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010],
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010],
        ...,
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010],
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010],
        [ 0.0003,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3812.7739, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(33.0810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.3142, device='cuda:0')



h[100].sum tensor(127.4200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(36.8154, device='cuda:0')



h[200].sum tensor(61.3702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76824.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0301, 0.1077, 0.0942,  ..., 0.0028, 0.0479, 0.0000],
        [0.0321, 0.1093, 0.0999,  ..., 0.0039, 0.0497, 0.0000],
        [0.0357, 0.1116, 0.1097,  ..., 0.0050, 0.0525, 0.0000],
        ...,
        [0.0005, 0.0931, 0.0133,  ..., 0.0000, 0.0259, 0.0000],
        [0.0005, 0.0930, 0.0132,  ..., 0.0000, 0.0259, 0.0000],
        [0.0005, 0.0929, 0.0132,  ..., 0.0000, 0.0258, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619195.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2753.7583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(401.8235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7062.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1256.5021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-745.4987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0530],
        [-0.0294],
        [ 0.0217],
        ...,
        [-6.8165],
        [-6.8077],
        [-6.8067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282941.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0401],
        ...,
        [0.9923],
        [0.9914],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370618.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0401],
        ...,
        [0.9923],
        [0.9914],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370617.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2987.0286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(22.2233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.6052, device='cuda:0')



h[100].sum tensor(125.3614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.7368, device='cuda:0')



h[200].sum tensor(49.4112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0037, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0084, 0.0098, 0.0000,  ..., 0.0168, 0.0000, 0.0039],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64208.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0918, 0.0151,  ..., 0.0000, 0.0249, 0.0000],
        [0.0069, 0.0960, 0.0317,  ..., 0.0000, 0.0297, 0.0000],
        [0.0240, 0.1068, 0.0799,  ..., 0.0010, 0.0432, 0.0000],
        ...,
        [0.0001, 0.0935, 0.0129,  ..., 0.0000, 0.0249, 0.0000],
        [0.0001, 0.0935, 0.0128,  ..., 0.0000, 0.0249, 0.0000],
        [0.0001, 0.0934, 0.0128,  ..., 0.0000, 0.0248, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568823.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1674.2903, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.1847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7754.3618, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1071.8461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-606.8042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2799],
        [-4.1544],
        [-2.6890],
        ...,
        [-6.9013],
        [-6.8921],
        [-6.8910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-409425.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0401],
        ...,
        [0.9923],
        [0.9914],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370617.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0400],
        ...,
        [0.9923],
        [0.9913],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370617.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0175,  0.0147, -0.0007,  ...,  0.0293, -0.0002,  0.0103],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3420.1096, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.5828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2113, device='cuda:0')



h[100].sum tensor(127.5122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.5283, device='cuda:0')



h[200].sum tensor(54.5002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0037, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0188, 0.0180, 0.0000,  ..., 0.0339, 0.0000, 0.0107],
        [0.0156, 0.0155, 0.0000,  ..., 0.0288, 0.0000, 0.0086],
        ...,
        [0.0008, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0008, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70555.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0970, 0.0408,  ..., 0.0000, 0.0316, 0.0000],
        [0.0330, 0.1104, 0.1051,  ..., 0.0066, 0.0497, 0.0000],
        [0.0429, 0.1166, 0.1328,  ..., 0.0086, 0.0576, 0.0000],
        ...,
        [0.0000, 0.0937, 0.0127,  ..., 0.0000, 0.0245, 0.0000],
        [0.0000, 0.0937, 0.0127,  ..., 0.0000, 0.0245, 0.0000],
        [0.0000, 0.0936, 0.0126,  ..., 0.0000, 0.0244, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595039.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2055.6333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.3569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7266.1621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1169.3173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-679.6724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7387],
        [-3.2583],
        [-1.8665],
        ...,
        [-6.9224],
        [-6.9133],
        [-6.9134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-393504.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0400],
        ...,
        [0.9923],
        [0.9913],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370617.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0399],
        ...,
        [0.9922],
        [0.9913],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370619.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.0481e-04,  9.0593e-04, -6.1094e-04,  ...,  1.0992e-03,
          0.0000e+00, -9.1589e-04],
        [ 6.1338e-03,  5.6231e-03, -6.5018e-04,  ...,  1.0768e-02,
         -5.4851e-05,  2.9230e-03],
        [ 5.5414e-03,  5.1518e-03, -6.4626e-04,  ...,  9.8021e-03,
         -4.9371e-05,  2.5395e-03],
        ...,
        [ 2.0481e-04,  9.0593e-04, -6.1094e-04,  ...,  1.0992e-03,
          0.0000e+00, -9.1589e-04],
        [ 2.0481e-04,  9.0593e-04, -6.1094e-04,  ...,  1.0992e-03,
          0.0000e+00, -9.1589e-04],
        [ 2.0481e-04,  9.0593e-04, -6.1094e-04,  ...,  1.0992e-03,
          0.0000e+00, -9.1589e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3239.3503, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.5883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0181, device='cuda:0')



h[100].sum tensor(126.7697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.9612, device='cuda:0')



h[200].sum tensor(52.3808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0059, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0086, 0.0000,  ..., 0.0145, 0.0000, 0.0030],
        [0.0115, 0.0122, 0.0000,  ..., 0.0219, 0.0000, 0.0050],
        [0.0279, 0.0253, 0.0000,  ..., 0.0487, 0.0000, 0.0137],
        ...,
        [0.0009, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0009, 0.0038, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69019.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.1117e-02, 1.1108e-01, 1.0108e-01,  ..., 1.1977e-03, 4.8595e-02,
         0.0000e+00],
        [4.6440e-02, 1.2174e-01, 1.4479e-01,  ..., 5.2516e-03, 6.0811e-02,
         0.0000e+00],
        [6.2061e-02, 1.3209e-01, 1.8910e-01,  ..., 9.8145e-03, 7.3177e-02,
         0.0000e+00],
        ...,
        [6.1405e-05, 9.3520e-02, 1.2664e-02,  ..., 0.0000e+00, 2.4722e-02,
         0.0000e+00],
        [6.1200e-05, 9.3493e-02, 1.2659e-02,  ..., 0.0000e+00, 2.4714e-02,
         0.0000e+00],
        [6.1091e-05, 9.3403e-02, 1.2646e-02,  ..., 0.0000e+00, 2.4689e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589995.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2012.1005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(332.3114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7375.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1145.8668, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-662.9286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7001],
        [-0.1050],
        [ 0.2172],
        ...,
        [-6.9011],
        [-6.8920],
        [-6.8909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-385942.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0375],
        [1.0370],
        [1.0399],
        ...,
        [0.9922],
        [0.9913],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370619.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0398],
        ...,
        [0.9921],
        [0.9912],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370621.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0244,  0.0201, -0.0008,  ...,  0.0404, -0.0002,  0.0147],
        [ 0.0328,  0.0268, -0.0008,  ...,  0.0541, -0.0003,  0.0202],
        [ 0.0253,  0.0209, -0.0008,  ...,  0.0419, -0.0002,  0.0153],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3749.7668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.3184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.7880, device='cuda:0')



h[100].sum tensor(128.0568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(35.2421, device='cuda:0')



h[200].sum tensor(59.8368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.3148, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1138, 0.0937, 0.0000,  ..., 0.1885, 0.0000, 0.0693],
        [0.1288, 0.1056, 0.0000,  ..., 0.2129, 0.0000, 0.0790],
        [0.1266, 0.1039, 0.0000,  ..., 0.2093, 0.0000, 0.0776],
        ...,
        [0.0010, 0.0040, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0010, 0.0039, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76604.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4592e-01, 2.1275e-01, 6.9313e-01,  ..., 1.5474e-01, 2.1569e-01,
         0.0000e+00],
        [2.7506e-01, 2.2570e-01, 7.7379e-01,  ..., 1.7842e-01, 2.3875e-01,
         0.0000e+00],
        [2.7052e-01, 2.2412e-01, 7.6136e-01,  ..., 1.7457e-01, 2.3531e-01,
         0.0000e+00],
        ...,
        [3.7305e-04, 9.2960e-02, 1.2794e-02,  ..., 0.0000e+00, 2.5579e-02,
         0.0000e+00],
        [3.7283e-04, 9.2936e-02, 1.2789e-02,  ..., 0.0000e+00, 2.5572e-02,
         0.0000e+00],
        [3.7217e-04, 9.2842e-02, 1.2775e-02,  ..., 0.0000e+00, 2.5545e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621538.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2636.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(402.3946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7298.0752, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1249.5249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-744.1771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0114],
        [-0.0097],
        [-0.0328],
        ...,
        [-6.8257],
        [-6.8168],
        [-6.8155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332137.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0398],
        ...,
        [0.9921],
        [0.9912],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370621.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0398],
        ...,
        [0.9921],
        [0.9911],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370623.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3370.5815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.8838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5297, device='cuda:0')



h[100].sum tensor(126.0434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.4905, device='cuda:0')



h[200].sum tensor(55.7623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0629, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0108, 0.0000,  ..., 0.0183, 0.0000, 0.0037],
        [0.0055, 0.0075, 0.0000,  ..., 0.0114, 0.0000, 0.0019],
        [0.0074, 0.0090, 0.0000,  ..., 0.0146, 0.0000, 0.0031],
        ...,
        [0.0012, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70235.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0223, 0.1046, 0.0744,  ..., 0.0000, 0.0435, 0.0000],
        [0.0192, 0.1031, 0.0654,  ..., 0.0000, 0.0412, 0.0000],
        [0.0228, 0.1057, 0.0757,  ..., 0.0000, 0.0443, 0.0000],
        ...,
        [0.0009, 0.0923, 0.0129,  ..., 0.0000, 0.0265, 0.0000],
        [0.0009, 0.0923, 0.0129,  ..., 0.0000, 0.0265, 0.0000],
        [0.0009, 0.0922, 0.0129,  ..., 0.0000, 0.0264, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590271.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2338.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(349.1504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7603.4458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1158.2588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-675.4203, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3251],
        [-2.0663],
        [-1.7092],
        ...,
        [-6.7464],
        [-6.7385],
        [-6.7380]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-301746.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0398],
        ...,
        [0.9921],
        [0.9911],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370623.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0397],
        ...,
        [0.9920],
        [0.9911],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370625.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0747e-02,  9.3031e-03, -6.8006e-04,  ...,  1.8054e-02,
         -9.4390e-05,  5.8634e-03],
        [ 7.6270e-03,  6.8202e-03, -6.5942e-04,  ...,  1.2969e-02,
         -6.6201e-05,  3.8430e-03],
        [ 2.9997e-04,  9.8949e-04, -6.1094e-04,  ...,  1.0265e-03,
          0.0000e+00, -9.0162e-04],
        ...,
        [ 2.9997e-04,  9.8949e-04, -6.1094e-04,  ...,  1.0265e-03,
          0.0000e+00, -9.0162e-04],
        [ 2.9997e-04,  9.8949e-04, -6.1094e-04,  ...,  1.0265e-03,
          0.0000e+00, -9.0162e-04],
        [ 2.9997e-04,  9.8949e-04, -6.1094e-04,  ...,  1.0265e-03,
          0.0000e+00, -9.0162e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3555.5015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.6486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.3836, device='cuda:0')



h[100].sum tensor(126.3399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.0436, device='cuda:0')



h[200].sum tensor(58.7771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0438, 0.0380, 0.0000,  ..., 0.0736, 0.0000, 0.0238],
        [0.0183, 0.0177, 0.0000,  ..., 0.0321, 0.0000, 0.0092],
        [0.0089, 0.0102, 0.0000,  ..., 0.0167, 0.0000, 0.0040],
        ...,
        [0.0013, 0.0042, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0013, 0.0042, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0013, 0.0042, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71906.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0805, 0.1335, 0.2315,  ..., 0.0235, 0.0879, 0.0000],
        [0.0554, 0.1212, 0.1630,  ..., 0.0093, 0.0690, 0.0000],
        [0.0357, 0.1109, 0.1089,  ..., 0.0016, 0.0540, 0.0000],
        ...,
        [0.0014, 0.0919, 0.0129,  ..., 0.0000, 0.0272, 0.0000],
        [0.0014, 0.0919, 0.0129,  ..., 0.0000, 0.0272, 0.0000],
        [0.0014, 0.0918, 0.0129,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592921.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2493.1147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(367.8588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7622.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1178.7319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-692.5932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.7104e-01],
        [-1.3221e-03],
        [-5.5775e-01],
        ...,
        [-6.6945e+00],
        [-6.6868e+00],
        [-6.6859e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276561.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0397],
        ...,
        [0.9920],
        [0.9911],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370625.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 380.0 event: 1900 loss: tensor(462.7905, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0397],
        ...,
        [0.9920],
        [0.9911],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370627.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3357.0176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.1974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.0611, device='cuda:0')



h[100].sum tensor(125.8353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.0897, device='cuda:0')



h[200].sum tensor(56.3902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0107, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0013, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0013, 0.0042, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0013, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0013, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0013, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69364.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0897, 0.0126,  ..., 0.0000, 0.0263, 0.0000],
        [0.0014, 0.0903, 0.0127,  ..., 0.0000, 0.0265, 0.0000],
        [0.0025, 0.0913, 0.0159,  ..., 0.0000, 0.0275, 0.0000],
        ...,
        [0.0014, 0.0920, 0.0131,  ..., 0.0000, 0.0271, 0.0000],
        [0.0014, 0.0919, 0.0131,  ..., 0.0000, 0.0271, 0.0000],
        [0.0014, 0.0918, 0.0131,  ..., 0.0000, 0.0271, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582607.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2320.9963, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(346.6734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7758.0303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1140.2764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-665.6487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0352],
        [-5.7123],
        [-5.1469],
        ...,
        [-6.0148],
        [-6.4562],
        [-6.6030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288991.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0397],
        ...,
        [0.9920],
        [0.9911],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370627.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0397],
        ...,
        [0.9919],
        [0.9910],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370628.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4033.9456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.4487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1007, device='cuda:0')



h[100].sum tensor(128.9101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(39.1665, device='cuda:0')



h[200].sum tensor(65.1167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.4612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0012, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79281.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.1009, 0.0630,  ..., 0.0000, 0.0405, 0.0000],
        [0.0102, 0.0970, 0.0397,  ..., 0.0000, 0.0341, 0.0000],
        [0.0073, 0.0954, 0.0313,  ..., 0.0000, 0.0319, 0.0000],
        ...,
        [0.0013, 0.0922, 0.0134,  ..., 0.0000, 0.0269, 0.0000],
        [0.0013, 0.0922, 0.0134,  ..., 0.0000, 0.0269, 0.0000],
        [0.0013, 0.0921, 0.0134,  ..., 0.0000, 0.0268, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623486., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2961.4124, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(434.8638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7102.7295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1285.1715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-776.2822, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3513],
        [-2.3118],
        [-3.0864],
        ...,
        [-6.6781],
        [-6.6481],
        [-6.5416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266624.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0376],
        [1.0371],
        [1.0397],
        ...,
        [0.9919],
        [0.9910],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370628.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0377],
        [1.0371],
        [1.0396],
        ...,
        [0.9919],
        [0.9910],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370627.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.2321e-02,  1.0573e-02, -6.9066e-04,  ...,  2.0714e-02,
         -1.0633e-04,  6.9329e-03],
        [ 7.4837e-03,  6.7234e-03, -6.5866e-04,  ...,  1.2830e-02,
         -6.3644e-05,  3.8013e-03],
        [ 1.6328e-02,  1.3762e-02, -7.1717e-04,  ...,  2.7245e-02,
         -1.4169e-04,  9.5272e-03],
        ...,
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04],
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04],
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4248.6904, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(37.3390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1196, device='cuda:0')



h[100].sum tensor(130.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(42.2129, device='cuda:0')



h[200].sum tensor(67.6859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0312, 0.0280, 0.0000,  ..., 0.0534, 0.0000, 0.0159],
        [0.0417, 0.0364, 0.0000,  ..., 0.0705, 0.0000, 0.0235],
        [0.0224, 0.0210, 0.0000,  ..., 0.0391, 0.0000, 0.0119],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81272.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0737, 0.1342, 0.2176,  ..., 0.0152, 0.0833, 0.0000],
        [0.0827, 0.1382, 0.2410,  ..., 0.0240, 0.0898, 0.0000],
        [0.0699, 0.1309, 0.2052,  ..., 0.0168, 0.0799, 0.0000],
        ...,
        [0.0012, 0.0923, 0.0134,  ..., 0.0000, 0.0265, 0.0000],
        [0.0012, 0.0923, 0.0134,  ..., 0.0000, 0.0265, 0.0000],
        [0.0012, 0.0922, 0.0134,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(626336.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2968.4390, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(453.9216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7056.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1310.8778, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-799.9467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4188],
        [ 0.4971],
        [ 0.4979],
        ...,
        [-6.6456],
        [-6.6084],
        [-6.5944]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295517.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0377],
        [1.0371],
        [1.0396],
        ...,
        [0.9919],
        [0.9910],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370627.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0377],
        [1.0371],
        [1.0396],
        ...,
        [0.9919],
        [0.9910],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370627.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9736e-03,  5.5215e-03, -6.4867e-04,  ...,  1.0369e-02,
         -5.0318e-05,  2.8236e-03],
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04],
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04],
        ...,
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04],
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04],
        [ 2.7111e-04,  9.8337e-04, -6.1094e-04,  ...,  1.0751e-03,
          0.0000e+00, -8.6819e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3270.9634, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.5634, device='cuda:0')



h[100].sum tensor(126.9872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(25.6017, device='cuda:0')



h[200].sum tensor(55.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0211, 0.0200, 0.0000,  ..., 0.0370, 0.0000, 0.0111],
        [0.0070, 0.0088, 0.0000,  ..., 0.0141, 0.0000, 0.0029],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68797.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0658, 0.1275, 0.1934,  ..., 0.0180, 0.0762, 0.0000],
        [0.0356, 0.1112, 0.1097,  ..., 0.0043, 0.0530, 0.0000],
        [0.0145, 0.0994, 0.0511,  ..., 0.0000, 0.0367, 0.0000],
        ...,
        [0.0012, 0.0923, 0.0134,  ..., 0.0000, 0.0265, 0.0000],
        [0.0012, 0.0923, 0.0134,  ..., 0.0000, 0.0265, 0.0000],
        [0.0012, 0.0922, 0.0134,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580716.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2304.2827, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(342.0525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7207.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1138.1671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-667.9606, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0532],
        [-0.6643],
        [-1.6504],
        ...,
        [-6.7136],
        [-6.7056],
        [-6.7042]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-282954.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0377],
        [1.0371],
        [1.0396],
        ...,
        [0.9919],
        [0.9910],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370627.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0377],
        [1.0371],
        [1.0396],
        ...,
        [0.9918],
        [0.9909],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370626.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6104e-04,  9.7869e-04, -6.1094e-04,  ...,  1.0765e-03,
          0.0000e+00, -8.7321e-04],
        [ 2.6104e-04,  9.7869e-04, -6.1094e-04,  ...,  1.0765e-03,
          0.0000e+00, -8.7321e-04],
        [ 1.1370e-02,  9.8194e-03, -6.8442e-04,  ...,  1.9177e-02,
         -9.7241e-05,  6.3146e-03],
        ...,
        [ 2.6104e-04,  9.7869e-04, -6.1094e-04,  ...,  1.0765e-03,
          0.0000e+00, -8.7321e-04],
        [ 2.6104e-04,  9.7869e-04, -6.1094e-04,  ...,  1.0765e-03,
          0.0000e+00, -8.7321e-04],
        [ 2.6104e-04,  9.7869e-04, -6.1094e-04,  ...,  1.0765e-03,
          0.0000e+00, -8.7321e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3141.1147, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.4245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.9053, device='cuda:0')



h[100].sum tensor(127.1670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.6341, device='cuda:0')



h[200].sum tensor(53.0912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.8817, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0040, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0174, 0.0170, 0.0000,  ..., 0.0310, 0.0000, 0.0087],
        [0.0164, 0.0163, 0.0000,  ..., 0.0294, 0.0000, 0.0072],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66743.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0222, 0.1020, 0.0712,  ..., 0.0000, 0.0422, 0.0000],
        [0.0358, 0.1117, 0.1100,  ..., 0.0016, 0.0532, 0.0000],
        [0.0464, 0.1192, 0.1405,  ..., 0.0033, 0.0620, 0.0000],
        ...,
        [0.0014, 0.0920, 0.0131,  ..., 0.0000, 0.0263, 0.0000],
        [0.0013, 0.0920, 0.0131,  ..., 0.0000, 0.0263, 0.0000],
        [0.0013, 0.0919, 0.0131,  ..., 0.0000, 0.0262, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571686., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2151.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.8140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7435.3413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1098.8800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-645.6339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6895],
        [-0.4900],
        [-0.1159],
        ...,
        [-6.7002],
        [-6.7252],
        [-6.7281]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335219., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0377],
        [1.0371],
        [1.0396],
        ...,
        [0.9918],
        [0.9909],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370626.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0378],
        [1.0372],
        [1.0396],
        ...,
        [0.9918],
        [0.9909],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370625.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0011,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3463.1294, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.2962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.9522, device='cuda:0')



h[100].sum tensor(128.7570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.7536, device='cuda:0')



h[200].sum tensor(57.1951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70445.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0908, 0.0193,  ..., 0.0000, 0.0276, 0.0000],
        [0.0040, 0.0917, 0.0198,  ..., 0.0000, 0.0279, 0.0000],
        [0.0055, 0.0930, 0.0240,  ..., 0.0000, 0.0292, 0.0000],
        ...,
        [0.0033, 0.0924, 0.0175,  ..., 0.0000, 0.0277, 0.0000],
        [0.0016, 0.0915, 0.0129,  ..., 0.0000, 0.0264, 0.0000],
        [0.0016, 0.0914, 0.0129,  ..., 0.0000, 0.0263, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585351.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2426.8289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.9579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7379.4600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1145.1852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-686.6828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0102],
        [-4.0140],
        [-3.5628],
        ...,
        [-6.2086],
        [-6.5672],
        [-6.7186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-335323.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0378],
        [1.0372],
        [1.0396],
        ...,
        [0.9918],
        [0.9909],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370625.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0379],
        [1.0372],
        [1.0396],
        ...,
        [0.9918],
        [0.9908],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370624.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3643.8276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.9051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.0205, device='cuda:0')



h[100].sum tensor(129.4832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.9475, device='cuda:0')



h[200].sum tensor(59.6318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0011, 0.0042, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73711.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0895, 0.0161,  ..., 0.0000, 0.0270, 0.0000],
        [0.0025, 0.0894, 0.0135,  ..., 0.0000, 0.0264, 0.0000],
        [0.0021, 0.0895, 0.0123,  ..., 0.0000, 0.0261, 0.0000],
        ...,
        [0.0022, 0.0908, 0.0126,  ..., 0.0000, 0.0266, 0.0000],
        [0.0022, 0.0907, 0.0126,  ..., 0.0000, 0.0266, 0.0000],
        [0.0022, 0.0906, 0.0126,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597116.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2778.3521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(391.5492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7168.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1187.7433, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-724.3161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2968],
        [-5.0721],
        [-5.5204],
        ...,
        [-6.7924],
        [-6.7843],
        [-6.7832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-314599.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0379],
        [1.0372],
        [1.0396],
        ...,
        [0.9918],
        [0.9908],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370624.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0380],
        [1.0373],
        [1.0396],
        ...,
        [0.9917],
        [0.9908],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370623., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3339.6855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(27.7089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.3636, device='cuda:0')



h[100].sum tensor(128.5170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(27.9939, device='cuda:0')



h[200].sum tensor(55.5652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0041, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0012, 0.0042, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69062.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0880, 0.0118,  ..., 0.0000, 0.0259, 0.0000],
        [0.0025, 0.0886, 0.0119,  ..., 0.0000, 0.0261, 0.0000],
        [0.0025, 0.0889, 0.0120,  ..., 0.0000, 0.0262, 0.0000],
        ...,
        [0.0026, 0.0902, 0.0123,  ..., 0.0000, 0.0267, 0.0000],
        [0.0026, 0.0902, 0.0123,  ..., 0.0000, 0.0267, 0.0000],
        [0.0025, 0.0901, 0.0123,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580905.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2645.3618, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(351.1964, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7192.6787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1119.7899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-675.8244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.4488],
        [-6.2638],
        [-5.9283],
        ...,
        [-6.8129],
        [-6.8046],
        [-6.8034]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-315566.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0380],
        [1.0373],
        [1.0396],
        ...,
        [0.9917],
        [0.9908],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370623., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0380],
        [1.0373],
        [1.0396],
        ...,
        [0.9917],
        [0.9907],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370622.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.6799e-04,  9.8172e-04, -6.1094e-04,  ...,  1.0076e-03,
          0.0000e+00, -9.1395e-04],
        [ 1.1968e-02,  1.0292e-02, -6.8826e-04,  ...,  2.0030e-02,
         -9.9148e-05,  6.6351e-03],
        [ 1.2697e-02,  1.0873e-02, -6.9308e-04,  ...,  2.1216e-02,
         -1.0533e-04,  7.1056e-03],
        ...,
        [ 2.6799e-04,  9.8172e-04, -6.1094e-04,  ...,  1.0076e-03,
          0.0000e+00, -9.1395e-04],
        [ 2.6799e-04,  9.8172e-04, -6.1094e-04,  ...,  1.0076e-03,
          0.0000e+00, -9.1395e-04],
        [ 2.6799e-04,  9.8172e-04, -6.1094e-04,  ...,  1.0076e-03,
          0.0000e+00, -9.1395e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3651.1841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(31.1430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.2418, device='cuda:0')



h[100].sum tensor(130.2151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(33.6091, device='cuda:0')



h[200].sum tensor(59.1647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0132, 0.0136, 0.0000,  ..., 0.0238, 0.0000, 0.0068],
        [0.0240, 0.0223, 0.0000,  ..., 0.0414, 0.0000, 0.0129],
        [0.0693, 0.0584, 0.0000,  ..., 0.1151, 0.0000, 0.0402],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73777.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0350, 0.1049, 0.0966,  ..., 0.0051, 0.0513, 0.0000],
        [0.0648, 0.1208, 0.1750,  ..., 0.0173, 0.0748, 0.0000],
        [0.1160, 0.1457, 0.3093,  ..., 0.0476, 0.1152, 0.0000],
        ...,
        [0.0028, 0.0900, 0.0118,  ..., 0.0000, 0.0267, 0.0000],
        [0.0028, 0.0900, 0.0118,  ..., 0.0000, 0.0267, 0.0000],
        [0.0028, 0.0899, 0.0118,  ..., 0.0000, 0.0267, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(603989.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3073.8022, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(394.3341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6996.6787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1183.2681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-727.2633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1275],
        [-1.0921],
        [-0.1345],
        ...,
        [-6.8431],
        [-6.8347],
        [-6.8334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310642.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0380],
        [1.0373],
        [1.0396],
        ...,
        [0.9917],
        [0.9907],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370622.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0381],
        [1.0374],
        [1.0396],
        ...,
        [0.9916],
        [0.9907],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370622.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4116.3179, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(35.8455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9504, device='cuda:0')



h[100].sum tensor(132.7265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(41.7070, device='cuda:0')



h[200].sum tensor(64.5595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.5560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0010, 0.0040, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(82155.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0880, 0.0109,  ..., 0.0000, 0.0257, 0.0000],
        [0.0036, 0.0893, 0.0135,  ..., 0.0000, 0.0267, 0.0000],
        [0.0076, 0.0923, 0.0246,  ..., 0.0000, 0.0303, 0.0000],
        ...,
        [0.0028, 0.0903, 0.0113,  ..., 0.0000, 0.0266, 0.0000],
        [0.0028, 0.0903, 0.0113,  ..., 0.0000, 0.0266, 0.0000],
        [0.0028, 0.0901, 0.0113,  ..., 0.0000, 0.0265, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(638966.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3631.3186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(467.7917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6690.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1305.2726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-821.2578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6880],
        [-4.3096],
        [-3.4334],
        ...,
        [-6.8490],
        [-6.8548],
        [-6.8571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297946.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0381],
        [1.0374],
        [1.0396],
        ...,
        [0.9916],
        [0.9907],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370622.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 390.0 event: 1950 loss: tensor(430.9025, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0382],
        [1.0374],
        [1.0396],
        ...,
        [0.9916],
        [0.9907],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370622.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0009],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0010,  0.0000, -0.0009]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3560.8682, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.4506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6543, device='cuda:0')



h[100].sum tensor(131.7432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(31.8527, device='cuda:0')



h[200].sum tensor(56.4395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0039, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74948.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0886, 0.0101,  ..., 0.0000, 0.0257, 0.0000],
        [0.0024, 0.0892, 0.0102,  ..., 0.0000, 0.0259, 0.0000],
        [0.0025, 0.0896, 0.0103,  ..., 0.0000, 0.0260, 0.0000],
        ...,
        [0.0025, 0.0909, 0.0106,  ..., 0.0000, 0.0265, 0.0000],
        [0.0025, 0.0908, 0.0106,  ..., 0.0000, 0.0265, 0.0000],
        [0.0025, 0.0907, 0.0106,  ..., 0.0000, 0.0264, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615521.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3115.3081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(405.8096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7154.1064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1197.9930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-739.3729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5407],
        [-5.7821],
        [-5.9135],
        ...,
        [-6.9131],
        [-6.9044],
        [-6.9031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-373337.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0382],
        [1.0374],
        [1.0396],
        ...,
        [0.9916],
        [0.9907],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370622.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0382],
        [1.0375],
        [1.0395],
        ...,
        [0.9916],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370623.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3626.6150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(28.6263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8403, device='cuda:0')



h[100].sum tensor(132.4767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(32.4089, device='cuda:0')



h[200].sum tensor(56.8812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.2091, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0039, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73089.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0895, 0.0112,  ..., 0.0000, 0.0265, 0.0000],
        [0.0058, 0.0923, 0.0197,  ..., 0.0000, 0.0294, 0.0000],
        [0.0139, 0.0977, 0.0418,  ..., 0.0000, 0.0363, 0.0000],
        ...,
        [0.0024, 0.0913, 0.0101,  ..., 0.0000, 0.0267, 0.0000],
        [0.0024, 0.0913, 0.0100,  ..., 0.0000, 0.0267, 0.0000],
        [0.0024, 0.0912, 0.0100,  ..., 0.0000, 0.0266, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596238.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2875.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(386.5009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6712.3057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1187.5167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-722.2457, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0002],
        [-4.1598],
        [-3.0670],
        ...,
        [-6.9227],
        [-6.9140],
        [-6.9127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-332959.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0382],
        [1.0375],
        [1.0395],
        ...,
        [0.9916],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370623.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0382],
        [1.0375],
        [1.0395],
        ...,
        [0.9916],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370623.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0127,  0.0109, -0.0007,  ...,  0.0214, -0.0001,  0.0072],
        [ 0.0232,  0.0192, -0.0008,  ...,  0.0385, -0.0002,  0.0140],
        [ 0.0280,  0.0231, -0.0008,  ...,  0.0463, -0.0002,  0.0171],
        ...,
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008],
        [ 0.0002,  0.0009, -0.0006,  ...,  0.0011,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3400.2119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.0309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-9.5960, device='cuda:0')



h[100].sum tensor(131.6510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(28.6889, device='cuda:0')



h[200].sum tensor(53.9542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.0703, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0841, 0.0701, 0.0000,  ..., 0.1397, 0.0000, 0.0503],
        [0.1119, 0.0923, 0.0000,  ..., 0.1850, 0.0000, 0.0682],
        [0.1116, 0.0920, 0.0000,  ..., 0.1844, 0.0000, 0.0680],
        ...,
        [0.0009, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0009, 0.0039, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70898.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2125, 0.1954, 0.5629,  ..., 0.1166, 0.1917, 0.0000],
        [0.2460, 0.2099, 0.6498,  ..., 0.1422, 0.2182, 0.0000],
        [0.2570, 0.2150, 0.6785,  ..., 0.1505, 0.2270, 0.0000],
        ...,
        [0.0024, 0.0913, 0.0101,  ..., 0.0000, 0.0267, 0.0000],
        [0.0024, 0.0913, 0.0100,  ..., 0.0000, 0.0267, 0.0000],
        [0.0024, 0.0912, 0.0100,  ..., 0.0000, 0.0266, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591982.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2788.0474, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(368.0527, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6843.3496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1153.3075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-697.2338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3845],
        [ 0.3707],
        [ 0.3665],
        ...,
        [-6.9185],
        [-6.9089],
        [-6.9066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-346851.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0382],
        [1.0375],
        [1.0395],
        ...,
        [0.9916],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370623.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0383],
        [1.0375],
        [1.0395],
        ...,
        [0.9915],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370625.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.1091e-02,  9.5971e-03, -6.8276e-04,  ...,  1.8708e-02,
         -8.9223e-05,  6.1929e-03],
        [ 9.0458e-03,  7.9694e-03, -6.6924e-04,  ...,  1.5382e-02,
         -7.2424e-05,  4.8716e-03],
        [ 2.0269e-02,  1.6900e-02, -7.4343e-04,  ...,  3.3634e-02,
         -1.6460e-04,  1.2121e-02],
        ...,
        [ 2.2738e-04,  9.5228e-04, -6.1094e-04,  ...,  1.0396e-03,
          0.0000e+00, -8.2503e-04],
        [ 2.2738e-04,  9.5228e-04, -6.1094e-04,  ...,  1.0396e-03,
          0.0000e+00, -8.2503e-04],
        [ 2.2738e-04,  9.5228e-04, -6.1094e-04,  ...,  1.0396e-03,
          0.0000e+00, -8.2503e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2837.7827, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(20.0557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-6.3073, device='cuda:0')



h[100].sum tensor(128.7838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.8566, device='cuda:0')



h[200].sum tensor(47.1947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7035, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0185, 0.0000,  ..., 0.0340, 0.0000, 0.0101],
        [0.0591, 0.0502, 0.0000,  ..., 0.0989, 0.0000, 0.0341],
        [0.0614, 0.0521, 0.0000,  ..., 0.1026, 0.0000, 0.0356],
        ...,
        [0.0010, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000],
        [0.0010, 0.0040, 0.0000,  ..., 0.0044, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62107.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.1171, 0.1435,  ..., 0.0122, 0.0668, 0.0000],
        [0.0966, 0.1400, 0.2574,  ..., 0.0359, 0.1012, 0.0000],
        [0.1133, 0.1488, 0.3009,  ..., 0.0468, 0.1144, 0.0000],
        ...,
        [0.0025, 0.0913, 0.0097,  ..., 0.0000, 0.0274, 0.0000],
        [0.0025, 0.0913, 0.0097,  ..., 0.0000, 0.0274, 0.0000],
        [0.0025, 0.0912, 0.0097,  ..., 0.0000, 0.0274, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553295.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2128.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.6561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7621.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1026.6820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-596.7688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7786],
        [-0.4578],
        [ 0.2769],
        ...,
        [-6.8923],
        [-6.8839],
        [-6.8827]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-382372., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0383],
        [1.0375],
        [1.0395],
        ...,
        [0.9915],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370625.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0384],
        [1.0375],
        [1.0394],
        ...,
        [0.9915],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370626.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.9350e-03,  5.4944e-03, -6.4845e-04,  ...,  1.0244e-02,
         -4.6232e-05,  2.8420e-03],
        [ 2.6070e-04,  9.7903e-04, -6.1094e-04,  ...,  1.0168e-03,
          0.0000e+00, -8.2388e-04],
        [ 5.9350e-03,  5.4944e-03, -6.4845e-04,  ...,  1.0244e-02,
         -4.6232e-05,  2.8420e-03],
        ...,
        [ 2.6070e-04,  9.7903e-04, -6.1094e-04,  ...,  1.0168e-03,
          0.0000e+00, -8.2388e-04],
        [ 2.6070e-04,  9.7903e-04, -6.1094e-04,  ...,  1.0168e-03,
          0.0000e+00, -8.2388e-04],
        [ 2.6070e-04,  9.7903e-04, -6.1094e-04,  ...,  1.0168e-03,
          0.0000e+00, -8.2388e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3306.3821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(26.4250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-8.9536, device='cuda:0')



h[100].sum tensor(129.1536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(26.7683, device='cuda:0')



h[200].sum tensor(54.2662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.9987, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0079, 0.0000,  ..., 0.0120, 0.0000, 0.0022],
        [0.0225, 0.0212, 0.0000,  ..., 0.0391, 0.0000, 0.0104],
        [0.0059, 0.0079, 0.0000,  ..., 0.0121, 0.0000, 0.0023],
        ...,
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0041, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68197.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0162, 0.0976, 0.0463,  ..., 0.0000, 0.0390, 0.0000],
        [0.0262, 0.1046, 0.0736,  ..., 0.0000, 0.0476, 0.0000],
        [0.0164, 0.0987, 0.0469,  ..., 0.0000, 0.0395, 0.0000],
        ...,
        [0.0028, 0.0911, 0.0096,  ..., 0.0000, 0.0284, 0.0000],
        [0.0028, 0.0911, 0.0096,  ..., 0.0000, 0.0284, 0.0000],
        [0.0028, 0.0910, 0.0096,  ..., 0.0000, 0.0284, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(573726.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2537.5928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(344.1639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7586.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1117.7997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-658.1169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3155],
        [-4.3686],
        [-4.7705],
        ...,
        [-6.8341],
        [-6.8261],
        [-6.8249]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-330283.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0384],
        [1.0375],
        [1.0394],
        ...,
        [0.9915],
        [0.9906],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370626.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0384],
        [1.0375],
        [1.0394],
        ...,
        [0.9915],
        [0.9906],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370628.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9515e-04,  1.0075e-03, -6.1094e-04,  ...,  9.8937e-04,
          0.0000e+00, -8.2409e-04],
        [ 8.1000e-03,  7.2184e-03, -6.6253e-04,  ...,  1.3680e-02,
         -6.3086e-05,  4.2191e-03],
        [ 2.9371e-02,  2.4145e-02, -8.0314e-04,  ...,  4.8267e-02,
         -2.3501e-04,  1.7963e-02],
        ...,
        [ 2.9515e-04,  1.0075e-03, -6.1094e-04,  ...,  9.8937e-04,
          0.0000e+00, -8.2409e-04],
        [ 2.9515e-04,  1.0075e-03, -6.1094e-04,  ...,  9.8937e-04,
          0.0000e+00, -8.2409e-04],
        [ 2.9515e-04,  1.0075e-03, -6.1094e-04,  ...,  9.8937e-04,
          0.0000e+00, -8.2409e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(5555.1201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(53.1832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1944, device='cuda:0')



h[100].sum tensor(136.0984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(66.3537, device='cuda:0')



h[200].sum tensor(84.3338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.4755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0225, 0.0211, 0.0000,  ..., 0.0387, 0.0000, 0.0121],
        [0.0643, 0.0544, 0.0000,  ..., 0.1067, 0.0000, 0.0382],
        [0.0858, 0.0715, 0.0000,  ..., 0.1416, 0.0000, 0.0512],
        ...,
        [0.0012, 0.0043, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0012, 0.0043, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0012, 0.0042, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(102829.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1060, 0.1385, 0.2766,  ..., 0.0458, 0.1101, 0.0000],
        [0.1937, 0.1779, 0.5031,  ..., 0.1042, 0.1797, 0.0000],
        [0.2643, 0.2089, 0.6858,  ..., 0.1541, 0.2359, 0.0000],
        ...,
        [0.0031, 0.0909, 0.0096,  ..., 0.0000, 0.0294, 0.0000],
        [0.0031, 0.0909, 0.0096,  ..., 0.0000, 0.0294, 0.0000],
        [0.0031, 0.0907, 0.0096,  ..., 0.0000, 0.0294, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(726475.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4926.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(652.3676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7093.6025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1602.6034, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1024.0525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0293],
        [ 0.1589],
        [ 0.1701],
        ...,
        [-6.7715],
        [-6.7638],
        [-6.7628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275307.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0384],
        [1.0375],
        [1.0394],
        ...,
        [0.9915],
        [0.9906],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370628.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0385],
        [1.0375],
        [1.0393],
        ...,
        [0.9915],
        [0.9906],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370629.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3017.8591, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(24.8913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.1613, device='cuda:0')



h[100].sum tensor(125.9457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.4098, device='cuda:0')



h[200].sum tensor(52.2125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.7987, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0013, 0.0043, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0013, 0.0043, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0013, 0.0043, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0013, 0.0043, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0013, 0.0043, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0013, 0.0043, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64802.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0885, 0.0091,  ..., 0.0000, 0.0293, 0.0000],
        [0.0033, 0.0891, 0.0092,  ..., 0.0000, 0.0295, 0.0000],
        [0.0050, 0.0903, 0.0137,  ..., 0.0000, 0.0310, 0.0000],
        ...,
        [0.0034, 0.0907, 0.0095,  ..., 0.0000, 0.0302, 0.0000],
        [0.0034, 0.0907, 0.0095,  ..., 0.0000, 0.0302, 0.0000],
        [0.0034, 0.0906, 0.0095,  ..., 0.0000, 0.0301, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561687., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2412.5503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.7719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8326.3662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1068.0515, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-610.1819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9857],
        [-5.5879],
        [-4.8618],
        ...,
        [-6.7336],
        [-6.7263],
        [-6.7254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312004.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0385],
        [1.0375],
        [1.0393],
        ...,
        [0.9915],
        [0.9906],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370629.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0386],
        [1.0376],
        [1.0392],
        ...,
        [0.9915],
        [0.9905],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370630.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3504.3210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(30.5530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.0191, device='cuda:0')



h[100].sum tensor(127.3630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(29.9537, device='cuda:0')



h[200].sum tensor(58.5723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0013, 0.0043, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0014, 0.0043, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        [0.0014, 0.0043, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0014, 0.0044, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0014, 0.0044, 0.0000,  ..., 0.0041, 0.0000, 0.0000],
        [0.0014, 0.0044, 0.0000,  ..., 0.0041, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71021.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0886, 0.0090,  ..., 0.0000, 0.0295, 0.0000],
        [0.0032, 0.0893, 0.0091,  ..., 0.0000, 0.0298, 0.0000],
        [0.0033, 0.0896, 0.0092,  ..., 0.0000, 0.0299, 0.0000],
        ...,
        [0.0033, 0.0909, 0.0094,  ..., 0.0000, 0.0305, 0.0000],
        [0.0033, 0.0909, 0.0094,  ..., 0.0000, 0.0305, 0.0000],
        [0.0033, 0.0908, 0.0094,  ..., 0.0000, 0.0304, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585011.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2844.4045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(370.3037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7875.6689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1165.0436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-677.8217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9943],
        [-6.0642],
        [-6.0242],
        ...,
        [-6.7271],
        [-6.7199],
        [-6.7190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259667.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0386],
        [1.0376],
        [1.0392],
        ...,
        [0.9915],
        [0.9905],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370630.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0386],
        [1.0376],
        [1.0391],
        ...,
        [0.9914],
        [0.9905],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(370630.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 3.0658e-02,  2.5199e-02, -8.1163e-04,  ...,  5.0350e-02,
         -2.3960e-04,  1.8823e-02],
        [ 2.0351e-02,  1.6994e-02, -7.4349e-04,  ...,  3.3594e-02,
         -1.5825e-04,  1.2161e-02],
        [ 1.0739e-02,  9.3434e-03, -6.7996e-04,  ...,  1.7968e-02,
         -8.2397e-05,  5.9493e-03],
        ...,
        [ 2.9924e-04,  1.0328e-03, -6.1094e-04,  ...,  9.9508e-04,
          0.0000e+00, -7.9845e-04],
        [ 2.9924e-04,  1.0328e-03, -6.1094e-04,  ...,  9.9508e-04,
          0.0000e+00, -7.9845e-04],
        [ 2.9924e-04,  1.0328e-03, -6.1094e-04,  ...,  9.9508e-04,
          0.0000e+00, -7.9845e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4384.4980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(39.7551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8849, device='cuda:0')



h[100].sum tensor(131.1323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(44.5010, device='cuda:0')



h[200].sum tensor(69.2116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.6602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0944, 0.0784, 0.0000,  ..., 0.1555, 0.0000, 0.0569],
        [0.0696, 0.0587, 0.0000,  ..., 0.1153, 0.0000, 0.0408],
        [0.0395, 0.0347, 0.0000,  ..., 0.0663, 0.0000, 0.0222],
        ...,
        [0.0013, 0.0044, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0013, 0.0044, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0013, 0.0044, 0.0000,  ..., 0.0042, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(84735.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2032, 0.1840, 0.5270,  ..., 0.1076, 0.1881, 0.0000],
        [0.1747, 0.1726, 0.4539,  ..., 0.0862, 0.1658, 0.0000],
        [0.1369, 0.1569, 0.3567,  ..., 0.0586, 0.1361, 0.0000],
        ...,
        [0.0030, 0.0916, 0.0092,  ..., 0.0000, 0.0301, 0.0000],
        [0.0030, 0.0916, 0.0092,  ..., 0.0000, 0.0301, 0.0000],
        [0.0030, 0.0914, 0.0092,  ..., 0.0000, 0.0300, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646526.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3764.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(492.6583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7084.6240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1362.9325, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-826.9751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3515],
        [ 0.3707],
        [ 0.3926],
        ...,
        [-6.7688],
        [-6.7614],
        [-6.7604]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257486.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0386],
        [1.0376],
        [1.0391],
        ...,
        [0.9914],
        [0.9905],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(370630.4375, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:01:00.572847
evaluation loss: 460.8457946777344
epoch: 1 mean loss: 450.74505615234375
=> saveing checkpoint at epoch 1
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



training loss:
 [492.17965698 450.74505615] 

\evaluation loss:
 [500.98223877 460.84579468]



eval_efficiency:
 [0.50821835 0.49632619 0.48437847 0.47410275 0.46134155 0.44934484
 0.43632761 0.42325197 0.41240976 0.39934914 0.38703854 0.37313186
 0.35999195 0.34680254 0.33400263 0.3186659  0.30436474 0.29182803
 0.28076335 0.26758105 0.25562634 0.24428896 0.23193093 0.22230083
 0.21155135 0.20271777 0.18981638 0.18004491 0.17000803 0.16653169
 0.15832953 0.14906229 0.14124469 0.140234   0.13811637 0.13651408
 0.12903137 0.12098723 0.1136195  0.11214291 0.10743497 0.09967288
 0.09369589 0.0837406  0.07877659 0.07416926 0.0677758  0.06603221
 0.05932653 0.05546283 0.05285932 0.04814834 0.04728925 0.04497159
 0.04430614 0.03957569 0.03653308 0.03582804 0.03346333 0.03294848
 0.02893587 0.02726772 0.02497728 0.02451853 0.02152167 0.01947747
 0.01737242 0.01611372 0.0163256  0.01493059 0.01316888 0.01236243
 0.01280423 0.01328798 0.01328798 0.0115118  0.01110727 0.01038305
 0.01038305 0.00931604 0.00825377 0.00825377 0.00825377 0.00788064
 0.00750751 0.00750751 0.00750751 0.00750751 0.00776829 0.00738427
 0.0091779  0.00858089 0.00858089 0.00858089 0.00800946 0.00800946
 0.00747183 0.00747183 0.00747183 0.00747183] 


eval_purity:
 [0.67142847 0.66971843 0.66918741 0.66793779 0.66687168 0.66672786
 0.66342118 0.66313367 0.66183312 0.65890659 0.65781969 0.65328786
 0.65116548 0.64732314 0.64572158 0.64083233 0.63791664 0.63787222
 0.64079351 0.63532754 0.63687826 0.63488645 0.63913901 0.6422102
 0.64309561 0.64958269 0.64673646 0.66525441 0.66333328        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan        nan        nan
        nan        nan        nan        nan]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.0386],
        [1.0375],
        [1.0391],
        ...,
        [0.9914],
        [0.9905],
        [0.9890]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(74126.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(610.1503, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.7110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-1.4692, device='cuda:0')



h[100].sum tensor(25.4131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.3923, device='cuda:0')



h[200].sum tensor(10.2310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.1639, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0042, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(12699.2529, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0900, 0.0087,  ..., 0.0000, 0.0287, 0.0000],
        [0.0034, 0.0911, 0.0112,  ..., 0.0000, 0.0296, 0.0000],
        [0.0072, 0.0938, 0.0214,  ..., 0.0000, 0.0330, 0.0000],
        ...,
        [0.0026, 0.0922, 0.0091,  ..., 0.0000, 0.0296, 0.0000],
        [0.0026, 0.0922, 0.0091,  ..., 0.0000, 0.0296, 0.0000],
        [0.0026, 0.0921, 0.0091,  ..., 0.0000, 0.0296, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(110420.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(423.5468, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(61.0353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1620.3025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(211.3624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.8290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6939],
        [-4.4298],
        [-3.5294],
        ...,
        [-6.1498],
        [-6.1412],
        [-6.2823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-69905.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0386],
        [1.0375],
        [1.0391],
        ...,
        [0.9914],
        [0.9905],
        [0.9890]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(74126.0469, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network after training 
result1: tensor([[-4.6939],
        [-4.4298],
        [-3.5294],
        ...,
        [-6.1498],
        [-6.1412],
        [-6.2823]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.0386],
        [1.0375],
        [1.0391],
        ...,
        [0.9914],
        [0.9905],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(148252.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        ...,
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008],
        [ 0.0003,  0.0010, -0.0006,  ...,  0.0010,  0.0000, -0.0008]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(1757.9722, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(15.5622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-5.8648, device='cuda:0')



h[100].sum tensor(52.7800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.5337, device='cuda:0')



h[200].sum tensor(27.3870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.6541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0042, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000],
        [0.0011, 0.0043, 0.0000,  ..., 0.0043, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(34829.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0156, 0.0973, 0.0431,  ..., 0.0000, 0.0392, 0.0000],
        [0.0052, 0.0922, 0.0160,  ..., 0.0000, 0.0311, 0.0000],
        [0.0026, 0.0909, 0.0089,  ..., 0.0000, 0.0291, 0.0000],
        ...,
        [0.0026, 0.0922, 0.0091,  ..., 0.0000, 0.0296, 0.0000],
        [0.0026, 0.0922, 0.0091,  ..., 0.0000, 0.0296, 0.0000],
        [0.0026, 0.0921, 0.0091,  ..., 0.0000, 0.0296, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(265045.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1511.4656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.9909, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3072.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(554.0342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-340.6789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6847],
        [-4.0835],
        [-5.0555],
        ...,
        [-6.8249],
        [-6.8172],
        [-6.8162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-129613.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0386],
        [1.0375],
        [1.0391],
        ...,
        [0.9914],
        [0.9905],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(148252.1094, device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network after training 
result1: tensor([[-4.6939],
        [-4.4298],
        [-3.5294],
        ...,
        [-6.1498],
        [-6.1412],
        [-6.2823]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



total time: 0:01:04.240685 hpmesh elements: 44 to 45

real	1m39.042s
user	1m6.482s
sys	0m24.205s
