0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        465.19.01
supported:      external
license:        NVIDIA
firmware:       nvidia/465.19.01/gsp.bin
retpoline:      Y
rhelversion:    7.8
srcversion:     976AD09EB9C3B8943CBA8C4
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           nv_cap_enable_devfs:Enable (1) or disable (0) nv-caps devfs support. Default: 1 (int)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jul 18 16:17:30 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0    33W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.505273344

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b49581b4fa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.484s
user	0m2.467s
sys	0m0.970s
[16:17:36] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
Matplotlib created a temporary config/cache directory at /tmp/matplotlib-5l61ehj1 because the default path (/hpcfs/bes/mlgpu/hoseinkk/home/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.3444],
        [ 1.8655],
        [ 1.3617],
        ...,
        [-0.7100],
        [-1.5285],
        [-0.5707]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-75.9204, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0704,  0.1326, -0.1015,  0.0298,  0.0263,  0.0251,  0.1516,  0.0064,
         -0.0130, -0.0346,  0.0600, -0.0145,  0.0779, -0.1151,  0.0846,  0.0265,
         -0.0011, -0.0156, -0.0670,  0.0136,  0.1527,  0.0096, -0.0947, -0.1402,
          0.0784, -0.0171, -0.1262,  0.0299, -0.0500, -0.1465,  0.0116, -0.0198,
         -0.0496, -0.0985, -0.0219, -0.1511,  0.0167,  0.0099, -0.0189,  0.1254,
         -0.0930,  0.1490,  0.1386, -0.0842, -0.0700, -0.1249,  0.0235,  0.0015,
         -0.0353, -0.1219,  0.0530, -0.0647, -0.0660,  0.0398,  0.0620,  0.0174,
          0.0853,  0.0097, -0.0525,  0.1516, -0.0471,  0.1210,  0.1099,  0.0784,
         -0.1056, -0.0123,  0.0369,  0.1165, -0.1170, -0.0151,  0.0408,  0.0328,
         -0.0934,  0.0041, -0.0622,  0.1139, -0.1334, -0.0260,  0.1012, -0.1225,
         -0.0463,  0.0203, -0.0114, -0.0624, -0.0641, -0.0783, -0.0226, -0.0754,
         -0.1502, -0.0064,  0.0043,  0.1308,  0.0042, -0.0388, -0.0582,  0.0971,
          0.0790, -0.1074,  0.1236, -0.1313, -0.1266,  0.0298,  0.0473, -0.0634,
          0.1446, -0.0915, -0.0329,  0.1008, -0.1483,  0.1137,  0.0940, -0.0357,
         -0.0182,  0.0235,  0.0004,  0.0068, -0.0016, -0.1043,  0.0170,  0.0937,
          0.0854, -0.0480,  0.0411,  0.0316,  0.0287, -0.1196,  0.0338,  0.0284,
          0.1519,  0.1212, -0.1234,  0.0276, -0.0025,  0.0184, -0.0557,  0.0409,
          0.1072, -0.0305, -0.0169,  0.1337,  0.0332,  0.0292, -0.1077,  0.0278,
          0.0009, -0.1487, -0.1118, -0.1358, -0.0854,  0.0140, -0.0719, -0.0904,
          0.1047, -0.0174,  0.1302,  0.1432, -0.1425, -0.0969,  0.0818,  0.0776,
          0.0785,  0.0918, -0.0030, -0.0475, -0.0919, -0.1191,  0.0666,  0.0903,
          0.0553,  0.0747, -0.1466, -0.1106, -0.0497, -0.1379, -0.0392,  0.1127,
         -0.0493, -0.0074, -0.1166, -0.1125,  0.0350, -0.0473, -0.0624,  0.1213,
          0.1152, -0.1335,  0.1428,  0.0087,  0.0913, -0.1209,  0.1433,  0.1101,
         -0.1002,  0.0079, -0.0489,  0.0288,  0.1514,  0.0926, -0.1434, -0.0803,
         -0.0880, -0.1295, -0.0847,  0.0115,  0.1217, -0.0773,  0.0958, -0.0650,
          0.0843, -0.0029, -0.1064,  0.0199,  0.1422, -0.0448, -0.1327, -0.0828,
          0.0804, -0.0352,  0.0675, -0.1006, -0.1016, -0.1079, -0.0801, -0.0221,
         -0.0649, -0.1267,  0.1238, -0.0063,  0.1304,  0.1445, -0.1455, -0.0160,
         -0.0505,  0.0323,  0.0946,  0.0467, -0.1521, -0.0959, -0.0476,  0.0108,
          0.1072, -0.0838, -0.0701, -0.1180, -0.1523, -0.0925,  0.1075, -0.0163,
          0.1278,  0.0512, -0.0717,  0.0324, -0.0090, -0.0118, -0.1049, -0.1012]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0704,  0.1326, -0.1015,  0.0298,  0.0263,  0.0251,  0.1516,  0.0064,
         -0.0130, -0.0346,  0.0600, -0.0145,  0.0779, -0.1151,  0.0846,  0.0265,
         -0.0011, -0.0156, -0.0670,  0.0136,  0.1527,  0.0096, -0.0947, -0.1402,
          0.0784, -0.0171, -0.1262,  0.0299, -0.0500, -0.1465,  0.0116, -0.0198,
         -0.0496, -0.0985, -0.0219, -0.1511,  0.0167,  0.0099, -0.0189,  0.1254,
         -0.0930,  0.1490,  0.1386, -0.0842, -0.0700, -0.1249,  0.0235,  0.0015,
         -0.0353, -0.1219,  0.0530, -0.0647, -0.0660,  0.0398,  0.0620,  0.0174,
          0.0853,  0.0097, -0.0525,  0.1516, -0.0471,  0.1210,  0.1099,  0.0784,
         -0.1056, -0.0123,  0.0369,  0.1165, -0.1170, -0.0151,  0.0408,  0.0328,
         -0.0934,  0.0041, -0.0622,  0.1139, -0.1334, -0.0260,  0.1012, -0.1225,
         -0.0463,  0.0203, -0.0114, -0.0624, -0.0641, -0.0783, -0.0226, -0.0754,
         -0.1502, -0.0064,  0.0043,  0.1308,  0.0042, -0.0388, -0.0582,  0.0971,
          0.0790, -0.1074,  0.1236, -0.1313, -0.1266,  0.0298,  0.0473, -0.0634,
          0.1446, -0.0915, -0.0329,  0.1008, -0.1483,  0.1137,  0.0940, -0.0357,
         -0.0182,  0.0235,  0.0004,  0.0068, -0.0016, -0.1043,  0.0170,  0.0937,
          0.0854, -0.0480,  0.0411,  0.0316,  0.0287, -0.1196,  0.0338,  0.0284,
          0.1519,  0.1212, -0.1234,  0.0276, -0.0025,  0.0184, -0.0557,  0.0409,
          0.1072, -0.0305, -0.0169,  0.1337,  0.0332,  0.0292, -0.1077,  0.0278,
          0.0009, -0.1487, -0.1118, -0.1358, -0.0854,  0.0140, -0.0719, -0.0904,
          0.1047, -0.0174,  0.1302,  0.1432, -0.1425, -0.0969,  0.0818,  0.0776,
          0.0785,  0.0918, -0.0030, -0.0475, -0.0919, -0.1191,  0.0666,  0.0903,
          0.0553,  0.0747, -0.1466, -0.1106, -0.0497, -0.1379, -0.0392,  0.1127,
         -0.0493, -0.0074, -0.1166, -0.1125,  0.0350, -0.0473, -0.0624,  0.1213,
          0.1152, -0.1335,  0.1428,  0.0087,  0.0913, -0.1209,  0.1433,  0.1101,
         -0.1002,  0.0079, -0.0489,  0.0288,  0.1514,  0.0926, -0.1434, -0.0803,
         -0.0880, -0.1295, -0.0847,  0.0115,  0.1217, -0.0773,  0.0958, -0.0650,
          0.0843, -0.0029, -0.1064,  0.0199,  0.1422, -0.0448, -0.1327, -0.0828,
          0.0804, -0.0352,  0.0675, -0.1006, -0.1016, -0.1079, -0.0801, -0.0221,
         -0.0649, -0.1267,  0.1238, -0.0063,  0.1304,  0.1445, -0.1455, -0.0160,
         -0.0505,  0.0323,  0.0946,  0.0467, -0.1521, -0.0959, -0.0476,  0.0108,
          0.1072, -0.0838, -0.0701, -0.1180, -0.1523, -0.0925,  0.1075, -0.0163,
          0.1278,  0.0512, -0.0717,  0.0324, -0.0090, -0.0118, -0.1049, -0.1012]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0681,  0.0224, -0.0023,  ..., -0.1166,  0.1233, -0.0565],
        [ 0.0063,  0.0014, -0.0219,  ...,  0.0456, -0.0147,  0.0565],
        [-0.0793, -0.0094, -0.0509,  ...,  0.0944, -0.0631,  0.0629],
        ...,
        [-0.0523, -0.0187,  0.1079,  ..., -0.0076, -0.0756,  0.1013],
        [-0.0533, -0.0791,  0.0817,  ..., -0.0429,  0.0195,  0.0742],
        [-0.0781,  0.0728, -0.0515,  ..., -0.0952,  0.0308, -0.0900]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0681,  0.0224, -0.0023,  ..., -0.1166,  0.1233, -0.0565],
        [ 0.0063,  0.0014, -0.0219,  ...,  0.0456, -0.0147,  0.0565],
        [-0.0793, -0.0094, -0.0509,  ...,  0.0944, -0.0631,  0.0629],
        ...,
        [-0.0523, -0.0187,  0.1079,  ..., -0.0076, -0.0756,  0.1013],
        [-0.0533, -0.0791,  0.0817,  ..., -0.0429,  0.0195,  0.0742],
        [-0.0781,  0.0728, -0.0515,  ..., -0.0952,  0.0308, -0.0900]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0408, -0.0646, -0.1568,  ...,  0.1759,  0.0419,  0.1374],
        [-0.1360,  0.0328, -0.0561,  ...,  0.0667, -0.0904, -0.0598],
        [ 0.0781,  0.0406, -0.0508,  ...,  0.0923,  0.1388, -0.1216],
        ...,
        [ 0.0886,  0.0181, -0.1647,  ..., -0.0486,  0.0673, -0.0165],
        [ 0.0586, -0.0780,  0.0906,  ..., -0.0961,  0.1470,  0.1163],
        [ 0.1395, -0.0032, -0.1528,  ...,  0.0573, -0.0413, -0.0662]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0408, -0.0646, -0.1568,  ...,  0.1759,  0.0419,  0.1374],
        [-0.1360,  0.0328, -0.0561,  ...,  0.0667, -0.0904, -0.0598],
        [ 0.0781,  0.0406, -0.0508,  ...,  0.0923,  0.1388, -0.1216],
        ...,
        [ 0.0886,  0.0181, -0.1647,  ..., -0.0486,  0.0673, -0.0165],
        [ 0.0586, -0.0780,  0.0906,  ..., -0.0961,  0.1470,  0.1163],
        [ 0.1395, -0.0032, -0.1528,  ...,  0.0573, -0.0413, -0.0662]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.2374,  0.1459,  0.0951,  ..., -0.1344,  0.1187, -0.0733],
        [-0.0100,  0.1050, -0.1627,  ...,  0.2180, -0.1769, -0.1085],
        [-0.1832, -0.2283, -0.2214,  ..., -0.0713,  0.1892, -0.2414],
        ...,
        [-0.1827,  0.1522, -0.1349,  ...,  0.1572, -0.1100,  0.2485],
        [ 0.0446,  0.0305,  0.2423,  ..., -0.2108, -0.2380,  0.0072],
        [-0.0737,  0.0419, -0.0106,  ...,  0.1774, -0.1416,  0.2288]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.2374,  0.1459,  0.0951,  ..., -0.1344,  0.1187, -0.0733],
        [-0.0100,  0.1050, -0.1627,  ...,  0.2180, -0.1769, -0.1085],
        [-0.1832, -0.2283, -0.2214,  ..., -0.0713,  0.1892, -0.2414],
        ...,
        [-0.1827,  0.1522, -0.1349,  ...,  0.1572, -0.1100,  0.2485],
        [ 0.0446,  0.0305,  0.2423,  ..., -0.2108, -0.2380,  0.0072],
        [-0.0737,  0.0419, -0.0106,  ...,  0.1774, -0.1416,  0.2288]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2731],
        [ 0.3326],
        [ 0.3924],
        [-0.0353],
        [ 0.3534],
        [-0.3979],
        [-0.0862],
        [ 0.2158],
        [-0.0931],
        [-0.1916],
        [ 0.0011],
        [-0.0394],
        [ 0.0092],
        [-0.4063],
        [ 0.1925],
        [-0.3027],
        [-0.3030],
        [ 0.3457],
        [ 0.2829],
        [-0.3799],
        [-0.1382],
        [-0.1784],
        [-0.2070],
        [-0.3617],
        [ 0.0383],
        [ 0.1032],
        [ 0.0810],
        [ 0.2582],
        [-0.2166],
        [ 0.1563],
        [ 0.2168],
        [ 0.3193]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2731],
        [ 0.3326],
        [ 0.3924],
        [-0.0353],
        [ 0.3534],
        [-0.3979],
        [-0.0862],
        [ 0.2158],
        [-0.0931],
        [-0.1916],
        [ 0.0011],
        [-0.0394],
        [ 0.0092],
        [-0.4063],
        [ 0.1925],
        [-0.3027],
        [-0.3030],
        [ 0.3457],
        [ 0.2829],
        [-0.3799],
        [-0.1382],
        [-0.1784],
        [-0.2070],
        [-0.3617],
        [ 0.0383],
        [ 0.1032],
        [ 0.0810],
        [ 0.2582],
        [-0.2166],
        [ 0.1563],
        [ 0.2168],
        [ 0.3193]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 6.7567e-02, -4.8436e-02, -3.8971e-02,  7.1829e-02, -1.1472e-01,
         -6.1995e-03, -1.3172e-02,  1.2926e-01,  2.4187e-02,  7.6834e-02,
         -2.2311e-02, -1.2532e-01,  1.4890e-01, -7.2075e-02,  1.1439e-01,
         -1.2801e-01, -6.9131e-02,  1.0666e-01,  5.9431e-02,  5.3099e-02,
          6.2963e-02, -2.6888e-02, -1.2032e-01, -1.3108e-01, -3.4674e-02,
         -1.1171e-01, -3.0020e-02, -1.1034e-01,  1.0207e-01, -1.1316e-01,
          7.6651e-03, -1.5036e-01, -1.3138e-02,  7.6861e-02,  8.0866e-02,
         -1.5072e-01,  1.2578e-01, -6.4997e-02,  1.2218e-01,  5.8756e-02,
          7.3049e-02,  1.0060e-01, -8.3918e-02,  9.9646e-03, -2.9810e-02,
          4.6607e-02,  2.5055e-02,  1.3910e-01,  1.4366e-01, -6.6005e-02,
          8.7086e-02, -1.4471e-01, -1.2762e-01, -1.3226e-02,  4.9238e-02,
          1.2835e-01, -1.0999e-01,  4.6799e-02,  2.7001e-02, -6.0056e-02,
          1.0928e-01, -4.4240e-02,  1.1803e-02, -6.1762e-02,  1.3781e-01,
         -1.1312e-01,  6.5688e-02, -1.4199e-01, -1.2468e-01, -6.5272e-02,
          5.0071e-02, -9.8961e-02, -3.1378e-02,  1.4780e-01,  1.3377e-01,
         -1.1245e-01, -4.8605e-02,  1.7610e-02,  5.2965e-02,  7.5062e-02,
          2.1256e-05,  1.2692e-01,  7.9236e-02, -3.2130e-02, -1.1297e-01,
          1.5122e-02, -4.9360e-02,  1.3223e-01, -1.2095e-01, -7.9685e-02,
          5.9600e-02, -5.2950e-02, -4.0611e-02, -1.4580e-02, -5.4390e-02,
         -8.5769e-02, -4.3635e-02,  1.5010e-01, -3.9734e-02, -4.8796e-02,
          3.4414e-02, -7.2582e-02, -1.0572e-01, -8.7413e-02,  1.1916e-01,
          1.2517e-01,  1.9563e-02,  8.3724e-02,  4.3417e-02, -1.2034e-01,
         -2.8817e-02,  1.1008e-01,  6.7295e-02, -5.9445e-02, -8.0153e-02,
         -1.3487e-01, -1.2720e-01,  6.4760e-03, -7.7686e-02,  1.3463e-01,
          1.5187e-01,  1.3929e-01, -1.0485e-01, -9.2512e-02,  6.4410e-02,
         -4.8659e-02, -1.6319e-02,  1.4789e-01, -1.2644e-01,  8.5874e-04,
          1.4846e-01,  1.3102e-01,  9.7666e-02, -6.2188e-04, -1.2859e-01,
          4.1992e-02, -6.8456e-03, -1.3488e-01, -7.8894e-02,  5.5246e-02,
         -7.3772e-02, -8.3526e-02, -1.1960e-01, -3.8435e-02, -1.3167e-01,
         -1.2701e-01, -2.1251e-04, -9.5365e-02, -1.2162e-01, -8.9897e-02,
          8.0289e-02,  3.7422e-02,  1.2534e-01, -1.3695e-01, -1.2183e-01,
          1.1746e-01, -2.2082e-02, -1.4677e-01,  7.7154e-02,  4.4093e-02,
          1.8371e-02,  4.3504e-04,  2.3786e-03, -5.7415e-03, -1.4712e-01,
          2.2551e-02,  1.0340e-01, -2.8952e-02,  1.1043e-01, -1.4330e-01,
          9.6403e-02, -1.5092e-01, -1.3421e-01,  3.5388e-02, -1.1379e-01,
          1.0981e-01, -1.2259e-01, -4.5711e-02,  6.7718e-02,  9.0442e-02,
          8.0894e-02, -1.0267e-01,  2.7762e-02,  1.8089e-03, -6.5993e-02,
         -1.4666e-01, -9.5789e-03,  9.3407e-02, -1.4740e-01, -1.2977e-01,
         -1.2582e-01,  1.1682e-01,  8.9473e-02, -1.2195e-01,  1.4919e-01,
          6.3352e-02,  1.2594e-02, -9.3627e-02,  9.2673e-02, -8.7442e-02,
         -1.2348e-01, -7.8682e-02, -1.2748e-01, -1.1205e-01,  9.3669e-02,
          2.2267e-02,  5.5766e-02,  1.4771e-02,  9.9148e-02,  4.9713e-02,
         -2.9446e-02,  1.1620e-01, -1.3539e-03,  1.2229e-01,  6.0243e-02,
          5.8886e-02, -1.2738e-01,  9.7446e-02, -1.0343e-02,  1.0098e-01,
          7.8790e-02, -1.2576e-01, -1.0745e-01, -1.3709e-01, -7.2457e-02,
         -1.2925e-01,  2.2174e-02,  5.1589e-02,  3.8186e-02,  1.6484e-02,
         -1.3253e-01,  7.4715e-02, -1.4473e-01,  7.8459e-03,  1.1019e-01,
         -1.1864e-01, -8.1028e-02, -1.1784e-01,  1.2056e-02, -1.3531e-01,
         -1.0094e-01, -9.8761e-02,  8.0812e-02,  1.2271e-01, -1.6972e-02,
         -3.3882e-02,  1.2171e-02,  1.1105e-01,  1.2040e-01,  8.7867e-02,
         -5.4003e-02, -1.2306e-01, -1.3189e-01,  2.9440e-02, -5.8128e-02,
         -6.8697e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 6.7567e-02, -4.8436e-02, -3.8971e-02,  7.1829e-02, -1.1472e-01,
         -6.1995e-03, -1.3172e-02,  1.2926e-01,  2.4187e-02,  7.6834e-02,
         -2.2311e-02, -1.2532e-01,  1.4890e-01, -7.2075e-02,  1.1439e-01,
         -1.2801e-01, -6.9131e-02,  1.0666e-01,  5.9431e-02,  5.3099e-02,
          6.2963e-02, -2.6888e-02, -1.2032e-01, -1.3108e-01, -3.4674e-02,
         -1.1171e-01, -3.0020e-02, -1.1034e-01,  1.0207e-01, -1.1316e-01,
          7.6651e-03, -1.5036e-01, -1.3138e-02,  7.6861e-02,  8.0866e-02,
         -1.5072e-01,  1.2578e-01, -6.4997e-02,  1.2218e-01,  5.8756e-02,
          7.3049e-02,  1.0060e-01, -8.3918e-02,  9.9646e-03, -2.9810e-02,
          4.6607e-02,  2.5055e-02,  1.3910e-01,  1.4366e-01, -6.6005e-02,
          8.7086e-02, -1.4471e-01, -1.2762e-01, -1.3226e-02,  4.9238e-02,
          1.2835e-01, -1.0999e-01,  4.6799e-02,  2.7001e-02, -6.0056e-02,
          1.0928e-01, -4.4240e-02,  1.1803e-02, -6.1762e-02,  1.3781e-01,
         -1.1312e-01,  6.5688e-02, -1.4199e-01, -1.2468e-01, -6.5272e-02,
          5.0071e-02, -9.8961e-02, -3.1378e-02,  1.4780e-01,  1.3377e-01,
         -1.1245e-01, -4.8605e-02,  1.7610e-02,  5.2965e-02,  7.5062e-02,
          2.1256e-05,  1.2692e-01,  7.9236e-02, -3.2130e-02, -1.1297e-01,
          1.5122e-02, -4.9360e-02,  1.3223e-01, -1.2095e-01, -7.9685e-02,
          5.9600e-02, -5.2950e-02, -4.0611e-02, -1.4580e-02, -5.4390e-02,
         -8.5769e-02, -4.3635e-02,  1.5010e-01, -3.9734e-02, -4.8796e-02,
          3.4414e-02, -7.2582e-02, -1.0572e-01, -8.7413e-02,  1.1916e-01,
          1.2517e-01,  1.9563e-02,  8.3724e-02,  4.3417e-02, -1.2034e-01,
         -2.8817e-02,  1.1008e-01,  6.7295e-02, -5.9445e-02, -8.0153e-02,
         -1.3487e-01, -1.2720e-01,  6.4760e-03, -7.7686e-02,  1.3463e-01,
          1.5187e-01,  1.3929e-01, -1.0485e-01, -9.2512e-02,  6.4410e-02,
         -4.8659e-02, -1.6319e-02,  1.4789e-01, -1.2644e-01,  8.5874e-04,
          1.4846e-01,  1.3102e-01,  9.7666e-02, -6.2188e-04, -1.2859e-01,
          4.1992e-02, -6.8456e-03, -1.3488e-01, -7.8894e-02,  5.5246e-02,
         -7.3772e-02, -8.3526e-02, -1.1960e-01, -3.8435e-02, -1.3167e-01,
         -1.2701e-01, -2.1251e-04, -9.5365e-02, -1.2162e-01, -8.9897e-02,
          8.0289e-02,  3.7422e-02,  1.2534e-01, -1.3695e-01, -1.2183e-01,
          1.1746e-01, -2.2082e-02, -1.4677e-01,  7.7154e-02,  4.4093e-02,
          1.8371e-02,  4.3504e-04,  2.3786e-03, -5.7415e-03, -1.4712e-01,
          2.2551e-02,  1.0340e-01, -2.8952e-02,  1.1043e-01, -1.4330e-01,
          9.6403e-02, -1.5092e-01, -1.3421e-01,  3.5388e-02, -1.1379e-01,
          1.0981e-01, -1.2259e-01, -4.5711e-02,  6.7718e-02,  9.0442e-02,
          8.0894e-02, -1.0267e-01,  2.7762e-02,  1.8089e-03, -6.5993e-02,
         -1.4666e-01, -9.5789e-03,  9.3407e-02, -1.4740e-01, -1.2977e-01,
         -1.2582e-01,  1.1682e-01,  8.9473e-02, -1.2195e-01,  1.4919e-01,
          6.3352e-02,  1.2594e-02, -9.3627e-02,  9.2673e-02, -8.7442e-02,
         -1.2348e-01, -7.8682e-02, -1.2748e-01, -1.1205e-01,  9.3669e-02,
          2.2267e-02,  5.5766e-02,  1.4771e-02,  9.9148e-02,  4.9713e-02,
         -2.9446e-02,  1.1620e-01, -1.3539e-03,  1.2229e-01,  6.0243e-02,
          5.8886e-02, -1.2738e-01,  9.7446e-02, -1.0343e-02,  1.0098e-01,
          7.8790e-02, -1.2576e-01, -1.0745e-01, -1.3709e-01, -7.2457e-02,
         -1.2925e-01,  2.2174e-02,  5.1589e-02,  3.8186e-02,  1.6484e-02,
         -1.3253e-01,  7.4715e-02, -1.4473e-01,  7.8459e-03,  1.1019e-01,
         -1.1864e-01, -8.1028e-02, -1.1784e-01,  1.2056e-02, -1.3531e-01,
         -1.0094e-01, -9.8761e-02,  8.0812e-02,  1.2271e-01, -1.6972e-02,
         -3.3882e-02,  1.2171e-02,  1.1105e-01,  1.2040e-01,  8.7867e-02,
         -5.4003e-02, -1.2306e-01, -1.3189e-01,  2.9440e-02, -5.8128e-02,
         -6.8697e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-1.8552e-02, -4.8496e-02,  2.2110e-02,  ...,  1.1884e-01,
          4.9045e-02, -8.0688e-02],
        [ 2.0689e-02,  5.3447e-02, -9.3281e-06,  ...,  8.5668e-02,
         -7.7725e-02, -6.1097e-02],
        [ 9.8983e-02, -3.5947e-02,  2.3453e-02,  ..., -6.2902e-02,
         -1.1414e-01,  1.0979e-01],
        ...,
        [ 3.6087e-02,  5.7633e-03, -3.3488e-03,  ..., -1.2031e-01,
          6.8430e-03, -3.4247e-02],
        [-8.9015e-02,  7.4167e-02, -9.8134e-02,  ...,  8.6215e-02,
          7.7245e-02, -8.4575e-02],
        [-5.5662e-02,  7.2281e-02,  8.3330e-02,  ...,  7.7131e-02,
          3.7578e-02, -5.9213e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-1.8552e-02, -4.8496e-02,  2.2110e-02,  ...,  1.1884e-01,
          4.9045e-02, -8.0688e-02],
        [ 2.0689e-02,  5.3447e-02, -9.3281e-06,  ...,  8.5668e-02,
         -7.7725e-02, -6.1097e-02],
        [ 9.8983e-02, -3.5947e-02,  2.3453e-02,  ..., -6.2902e-02,
         -1.1414e-01,  1.0979e-01],
        ...,
        [ 3.6087e-02,  5.7633e-03, -3.3488e-03,  ..., -1.2031e-01,
          6.8430e-03, -3.4247e-02],
        [-8.9015e-02,  7.4167e-02, -9.8134e-02,  ...,  8.6215e-02,
          7.7245e-02, -8.4575e-02],
        [-5.5662e-02,  7.2281e-02,  8.3330e-02,  ...,  7.7131e-02,
          3.7578e-02, -5.9213e-02]], device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0662,  0.1334, -0.0136,  ..., -0.1469,  0.0449,  0.1535],
        [ 0.0281, -0.1424, -0.0340,  ...,  0.0342,  0.0593, -0.0212],
        [ 0.0298,  0.0631, -0.1152,  ..., -0.1733, -0.0693, -0.0895],
        ...,
        [-0.1535, -0.0976,  0.0231,  ..., -0.0252,  0.0399,  0.0621],
        [ 0.0178, -0.0115,  0.0767,  ...,  0.0141, -0.0706,  0.1164],
        [-0.0758, -0.0119,  0.0418,  ...,  0.0598,  0.1695,  0.1121]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0662,  0.1334, -0.0136,  ..., -0.1469,  0.0449,  0.1535],
        [ 0.0281, -0.1424, -0.0340,  ...,  0.0342,  0.0593, -0.0212],
        [ 0.0298,  0.0631, -0.1152,  ..., -0.1733, -0.0693, -0.0895],
        ...,
        [-0.1535, -0.0976,  0.0231,  ..., -0.0252,  0.0399,  0.0621],
        [ 0.0178, -0.0115,  0.0767,  ...,  0.0141, -0.0706,  0.1164],
        [-0.0758, -0.0119,  0.0418,  ...,  0.0598,  0.1695,  0.1121]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.2286, -0.0672, -0.1927,  ...,  0.0634,  0.2368,  0.2478],
        [ 0.2030, -0.1248,  0.0116,  ...,  0.1716, -0.0131,  0.1695],
        [ 0.1173,  0.2294,  0.2114,  ..., -0.0722,  0.0923, -0.1876],
        ...,
        [-0.1483,  0.1977, -0.1958,  ...,  0.0085, -0.2449, -0.2266],
        [-0.1528,  0.1751, -0.0569,  ...,  0.2117,  0.1222, -0.0982],
        [-0.1091, -0.1743,  0.0170,  ..., -0.1643, -0.0836,  0.0478]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.2286, -0.0672, -0.1927,  ...,  0.0634,  0.2368,  0.2478],
        [ 0.2030, -0.1248,  0.0116,  ...,  0.1716, -0.0131,  0.1695],
        [ 0.1173,  0.2294,  0.2114,  ..., -0.0722,  0.0923, -0.1876],
        ...,
        [-0.1483,  0.1977, -0.1958,  ...,  0.0085, -0.2449, -0.2266],
        [-0.1528,  0.1751, -0.0569,  ...,  0.2117,  0.1222, -0.0982],
        [-0.1091, -0.1743,  0.0170,  ..., -0.1643, -0.0836,  0.0478]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.2133],
        [ 0.0722],
        [ 0.1681],
        [ 0.1522],
        [-0.0931],
        [ 0.2736],
        [-0.2572],
        [-0.2565],
        [-0.4248],
        [ 0.2401],
        [ 0.2436],
        [ 0.2333],
        [-0.1952],
        [ 0.1124],
        [ 0.2037],
        [-0.0019],
        [ 0.1472],
        [-0.1225],
        [ 0.1381],
        [-0.3512],
        [ 0.2306],
        [-0.2297],
        [-0.3954],
        [-0.0547],
        [-0.3355],
        [-0.0435],
        [ 0.2542],
        [-0.2248],
        [ 0.0258],
        [-0.3929],
        [-0.3960],
        [-0.1530]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.2133],
        [ 0.0722],
        [ 0.1681],
        [ 0.1522],
        [-0.0931],
        [ 0.2736],
        [-0.2572],
        [-0.2565],
        [-0.4248],
        [ 0.2401],
        [ 0.2436],
        [ 0.2333],
        [-0.1952],
        [ 0.1124],
        [ 0.2037],
        [-0.0019],
        [ 0.1472],
        [-0.1225],
        [ 0.1381],
        [-0.3512],
        [ 0.2306],
        [-0.2297],
        [-0.3954],
        [-0.0547],
        [-0.3355],
        [-0.0435],
        [ 0.2542],
        [-0.2248],
        [ 0.0258],
        [-0.3929],
        [-0.3960],
        [-0.1530]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(62.5433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.2117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(0.2173, device='cuda:0')



h[100].sum tensor(-2.2608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-2.3203, device='cuda:0')



h[200].sum tensor(4.5308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(4.6500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3114.0571, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0016, 0.0013,  ..., 0.0000, 0.0017, 0.0000],
        [0.0056, 0.0083, 0.0066,  ..., 0.0000, 0.0088, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(13878.1689, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(220.0281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.6104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-0.8561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-24.0817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1524],
        [-0.1865],
        [-0.2689],
        ...,
        [-0.0430],
        [-0.0431],
        [-0.0341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-3843.9858, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.1524],
        [-0.1865],
        [-0.2689],
        ...,
        [-0.0430],
        [-0.0431],
        [-0.0341]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-23.8571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-7.8280, device='cuda:0')



h[100].sum tensor(1.8296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.8148, device='cuda:0')



h[200].sum tensor(-2.9700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-2.9459, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(14589.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0004, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(79666.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-16.3119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-15.4009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-29.9464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.2837],
        [0.1740],
        [0.1065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(16667.1484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.1524],
        [-0.1865],
        [-0.2689],
        ...,
        [-0.0430],
        [-0.0431],
        [-0.0341]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./Training.py", line 5, in <module>
    from Model import *
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/Model.py", line 209, in <module>
    plt.savefig(f'/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/{modelname}/results/{t}\
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/pyplot.py", line 966, in savefig
    res = fig.savefig(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/figure.py", line 3015, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 2255, in print_figure
    result = print_method(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/backend_bases.py", line 1669, in wrapper
    return func(*args, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py", line 509, in print_png
    mpl.image.imsave(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/matplotlib/image.py", line 1616, in imsave
    image.save(fname, **pil_kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/PIL/Image.py", line 2237, in save
    fp = builtins.open(filename, "w+b")
OSError: [Errno 122] Disk quota exceeded: '/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/results/2022-07-18 16:17:53.085422    passing three random events (20, 30, 31) from network before training.png'

real	0m20.601s
user	0m15.290s
sys	0m4.534s
