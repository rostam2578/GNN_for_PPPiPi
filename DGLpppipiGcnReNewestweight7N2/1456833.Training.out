0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        465.19.01
supported:      external
license:        NVIDIA
firmware:       nvidia/465.19.01/gsp.bin
retpoline:      Y
rhelversion:    7.8
srcversion:     976AD09EB9C3B8943CBA8C4
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           nv_cap_enable_devfs:Enable (1) or disable (0) nv-caps devfs support. Default: 1 (int)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           rm_firmware_active:charp

nvidia-smi:
Wed Jul  6 19:14:27 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   22C    P0    31W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.505273344

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2aeb01315fa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	1m50.036s
user	0m3.658s
sys	0m3.299s
[19:16:19] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.3022],
        [ 0.5080],
        [-2.0738],
        ...,
        [ 0.3317],
        [ 0.4516],
        [-1.1000]], device='cuda:0', requires_grad=True) 
node features sum: tensor(61.2566, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (2000, 6796) (2000, 6796)
sum 189931 265535
shape torch.Size([2000, 6796]) torch.Size([2000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.1168, -0.0226, -0.1059, -0.0013, -0.0770,  0.0090, -0.0374,  0.1158,
          0.0937, -0.0530,  0.0872,  0.0174, -0.1500, -0.0488,  0.0116,  0.0475,
          0.1127, -0.1336,  0.1343,  0.0608,  0.1450,  0.1464, -0.0550,  0.0023,
         -0.0930,  0.0725,  0.0903,  0.0836,  0.0215, -0.1248,  0.0459,  0.0125,
         -0.0693, -0.0074,  0.0204, -0.0457,  0.0085,  0.0005, -0.0349, -0.0011,
          0.0208, -0.0900,  0.0640, -0.1335, -0.1268,  0.1133, -0.1268, -0.1008,
          0.0034, -0.0016,  0.0889,  0.0924, -0.1282,  0.1367, -0.1336,  0.0269,
          0.0563, -0.0535, -0.0083, -0.0243, -0.0529,  0.1426, -0.1399, -0.1384,
         -0.1258, -0.0957, -0.0620, -0.0149,  0.0915,  0.0561,  0.0951, -0.1464,
         -0.1093,  0.0543,  0.1389,  0.0631,  0.1147,  0.1272,  0.0128, -0.0437,
          0.0649, -0.1194, -0.1379, -0.0250, -0.1385,  0.0440,  0.0175, -0.1305,
         -0.0289, -0.0074, -0.0263,  0.0316, -0.0368,  0.0443,  0.0277, -0.1152,
         -0.1481, -0.0451, -0.0610, -0.0558, -0.0901, -0.1172, -0.0870, -0.1194,
         -0.1432,  0.1087,  0.0781,  0.1289, -0.0155, -0.0647,  0.1167,  0.0646,
          0.0618, -0.0270, -0.1277, -0.0310,  0.0576, -0.0529, -0.0075,  0.0770,
         -0.0211, -0.0024, -0.1151, -0.0208,  0.0493, -0.1498, -0.0607,  0.0580,
          0.1345,  0.1193,  0.0664,  0.0352, -0.1071,  0.1465,  0.1460, -0.0996,
          0.0972,  0.1106, -0.0019, -0.0589,  0.0711,  0.0543,  0.1136, -0.0772,
          0.0341,  0.0244,  0.0719,  0.0486,  0.1441, -0.1438,  0.1140, -0.1389,
          0.1004,  0.1251, -0.1379, -0.1080,  0.0747, -0.0641,  0.0615, -0.0222,
         -0.1231, -0.0188,  0.0820, -0.0915,  0.0480, -0.1111,  0.0036, -0.1311,
         -0.0231,  0.0248, -0.0719,  0.0773, -0.0745, -0.0121,  0.0764, -0.0602,
          0.1366, -0.0240, -0.0646,  0.0093, -0.0413, -0.0773, -0.0909, -0.0247,
          0.1121, -0.0065,  0.1183,  0.0996,  0.1237,  0.0391, -0.1211,  0.0411,
          0.0761, -0.1266, -0.1409,  0.0012, -0.0779, -0.0643, -0.0686, -0.0635,
         -0.0811, -0.0332, -0.0030,  0.0869,  0.1301, -0.0959,  0.0756,  0.0809,
         -0.0020, -0.0067, -0.0839, -0.0372,  0.1362,  0.0056, -0.0038, -0.1206,
         -0.1513,  0.1357,  0.0026, -0.0850,  0.0654, -0.1002, -0.0262,  0.0363,
          0.0021, -0.1332, -0.1126,  0.1364,  0.0565, -0.0063, -0.0070,  0.0508,
          0.1327,  0.0667, -0.0994, -0.1490, -0.0813,  0.0262,  0.0798,  0.1387,
         -0.0228, -0.0133, -0.1294,  0.1202, -0.1130,  0.0959, -0.0049, -0.0628,
         -0.0213, -0.0135, -0.0936,  0.0416,  0.0801, -0.0677, -0.0553,  0.1215]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1168, -0.0226, -0.1059, -0.0013, -0.0770,  0.0090, -0.0374,  0.1158,
          0.0937, -0.0530,  0.0872,  0.0174, -0.1500, -0.0488,  0.0116,  0.0475,
          0.1127, -0.1336,  0.1343,  0.0608,  0.1450,  0.1464, -0.0550,  0.0023,
         -0.0930,  0.0725,  0.0903,  0.0836,  0.0215, -0.1248,  0.0459,  0.0125,
         -0.0693, -0.0074,  0.0204, -0.0457,  0.0085,  0.0005, -0.0349, -0.0011,
          0.0208, -0.0900,  0.0640, -0.1335, -0.1268,  0.1133, -0.1268, -0.1008,
          0.0034, -0.0016,  0.0889,  0.0924, -0.1282,  0.1367, -0.1336,  0.0269,
          0.0563, -0.0535, -0.0083, -0.0243, -0.0529,  0.1426, -0.1399, -0.1384,
         -0.1258, -0.0957, -0.0620, -0.0149,  0.0915,  0.0561,  0.0951, -0.1464,
         -0.1093,  0.0543,  0.1389,  0.0631,  0.1147,  0.1272,  0.0128, -0.0437,
          0.0649, -0.1194, -0.1379, -0.0250, -0.1385,  0.0440,  0.0175, -0.1305,
         -0.0289, -0.0074, -0.0263,  0.0316, -0.0368,  0.0443,  0.0277, -0.1152,
         -0.1481, -0.0451, -0.0610, -0.0558, -0.0901, -0.1172, -0.0870, -0.1194,
         -0.1432,  0.1087,  0.0781,  0.1289, -0.0155, -0.0647,  0.1167,  0.0646,
          0.0618, -0.0270, -0.1277, -0.0310,  0.0576, -0.0529, -0.0075,  0.0770,
         -0.0211, -0.0024, -0.1151, -0.0208,  0.0493, -0.1498, -0.0607,  0.0580,
          0.1345,  0.1193,  0.0664,  0.0352, -0.1071,  0.1465,  0.1460, -0.0996,
          0.0972,  0.1106, -0.0019, -0.0589,  0.0711,  0.0543,  0.1136, -0.0772,
          0.0341,  0.0244,  0.0719,  0.0486,  0.1441, -0.1438,  0.1140, -0.1389,
          0.1004,  0.1251, -0.1379, -0.1080,  0.0747, -0.0641,  0.0615, -0.0222,
         -0.1231, -0.0188,  0.0820, -0.0915,  0.0480, -0.1111,  0.0036, -0.1311,
         -0.0231,  0.0248, -0.0719,  0.0773, -0.0745, -0.0121,  0.0764, -0.0602,
          0.1366, -0.0240, -0.0646,  0.0093, -0.0413, -0.0773, -0.0909, -0.0247,
          0.1121, -0.0065,  0.1183,  0.0996,  0.1237,  0.0391, -0.1211,  0.0411,
          0.0761, -0.1266, -0.1409,  0.0012, -0.0779, -0.0643, -0.0686, -0.0635,
         -0.0811, -0.0332, -0.0030,  0.0869,  0.1301, -0.0959,  0.0756,  0.0809,
         -0.0020, -0.0067, -0.0839, -0.0372,  0.1362,  0.0056, -0.0038, -0.1206,
         -0.1513,  0.1357,  0.0026, -0.0850,  0.0654, -0.1002, -0.0262,  0.0363,
          0.0021, -0.1332, -0.1126,  0.1364,  0.0565, -0.0063, -0.0070,  0.0508,
          0.1327,  0.0667, -0.0994, -0.1490, -0.0813,  0.0262,  0.0798,  0.1387,
         -0.0228, -0.0133, -0.1294,  0.1202, -0.1130,  0.0959, -0.0049, -0.0628,
         -0.0213, -0.0135, -0.0936,  0.0416,  0.0801, -0.0677, -0.0553,  0.1215]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0760,  0.1103, -0.0789,  ...,  0.1161,  0.0715,  0.1035],
        [-0.1174, -0.0904,  0.0289,  ...,  0.0745, -0.0674,  0.0570],
        [ 0.0799,  0.0764, -0.1116,  ...,  0.0791,  0.0431, -0.0093],
        ...,
        [-0.0697,  0.0956, -0.0670,  ..., -0.0104, -0.0057,  0.0435],
        [ 0.0803,  0.0436,  0.1089,  ...,  0.0559,  0.0097, -0.0224],
        [-0.0369,  0.1070,  0.0109,  ...,  0.1014,  0.1169,  0.0007]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0760,  0.1103, -0.0789,  ...,  0.1161,  0.0715,  0.1035],
        [-0.1174, -0.0904,  0.0289,  ...,  0.0745, -0.0674,  0.0570],
        [ 0.0799,  0.0764, -0.1116,  ...,  0.0791,  0.0431, -0.0093],
        ...,
        [-0.0697,  0.0956, -0.0670,  ..., -0.0104, -0.0057,  0.0435],
        [ 0.0803,  0.0436,  0.1089,  ...,  0.0559,  0.0097, -0.0224],
        [-0.0369,  0.1070,  0.0109,  ...,  0.1014,  0.1169,  0.0007]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1110,  0.1013, -0.1001,  ..., -0.0079,  0.0566, -0.0639],
        [-0.0906,  0.0109, -0.1652,  ..., -0.0166,  0.0487,  0.0462],
        [-0.1028, -0.1608,  0.0511,  ...,  0.0307,  0.0906, -0.1202],
        ...,
        [ 0.1740,  0.0619, -0.1698,  ...,  0.0668,  0.0554,  0.0012],
        [-0.0956, -0.0966, -0.1714,  ...,  0.1432, -0.1064, -0.1084],
        [ 0.0655, -0.1058, -0.0175,  ..., -0.0010,  0.0304, -0.1361]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1110,  0.1013, -0.1001,  ..., -0.0079,  0.0566, -0.0639],
        [-0.0906,  0.0109, -0.1652,  ..., -0.0166,  0.0487,  0.0462],
        [-0.1028, -0.1608,  0.0511,  ...,  0.0307,  0.0906, -0.1202],
        ...,
        [ 0.1740,  0.0619, -0.1698,  ...,  0.0668,  0.0554,  0.0012],
        [-0.0956, -0.0966, -0.1714,  ...,  0.1432, -0.1064, -0.1084],
        [ 0.0655, -0.1058, -0.0175,  ..., -0.0010,  0.0304, -0.1361]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1400,  0.0849,  0.0572,  ..., -0.2004,  0.0319, -0.1812],
        [-0.0871,  0.1588, -0.2414,  ..., -0.1529, -0.0475,  0.1441],
        [ 0.1829, -0.0805,  0.1725,  ...,  0.2074,  0.0709,  0.0664],
        ...,
        [ 0.0175, -0.2350,  0.1792,  ...,  0.0051, -0.1628,  0.0416],
        [-0.0639, -0.1022, -0.2040,  ..., -0.0024, -0.2073, -0.1882],
        [-0.0525,  0.1482, -0.1760,  ..., -0.0443, -0.0672,  0.1365]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1400,  0.0849,  0.0572,  ..., -0.2004,  0.0319, -0.1812],
        [-0.0871,  0.1588, -0.2414,  ..., -0.1529, -0.0475,  0.1441],
        [ 0.1829, -0.0805,  0.1725,  ...,  0.2074,  0.0709,  0.0664],
        ...,
        [ 0.0175, -0.2350,  0.1792,  ...,  0.0051, -0.1628,  0.0416],
        [-0.0639, -0.1022, -0.2040,  ..., -0.0024, -0.2073, -0.1882],
        [-0.0525,  0.1482, -0.1760,  ..., -0.0443, -0.0672,  0.1365]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.1439],
        [-0.2366],
        [-0.3135],
        [-0.1853],
        [-0.1048],
        [-0.3190],
        [-0.2968],
        [-0.1749],
        [-0.1898],
        [-0.3442],
        [ 0.2603],
        [-0.3902],
        [ 0.0270],
        [-0.0777],
        [-0.3141],
        [-0.3085],
        [ 0.1884],
        [-0.2088],
        [-0.1815],
        [-0.2247],
        [ 0.0641],
        [-0.3394],
        [ 0.2610],
        [-0.4037],
        [ 0.3462],
        [-0.1692],
        [-0.0056],
        [-0.1651],
        [ 0.3147],
        [-0.2607],
        [-0.1316],
        [ 0.1355]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.1439],
        [-0.2366],
        [-0.3135],
        [-0.1853],
        [-0.1048],
        [-0.3190],
        [-0.2968],
        [-0.1749],
        [-0.1898],
        [-0.3442],
        [ 0.2603],
        [-0.3902],
        [ 0.0270],
        [-0.0777],
        [-0.3141],
        [-0.3085],
        [ 0.1884],
        [-0.2088],
        [-0.1815],
        [-0.2247],
        [ 0.0641],
        [-0.3394],
        [ 0.2610],
        [-0.4037],
        [ 0.3462],
        [-0.1692],
        [-0.0056],
        [-0.1651],
        [ 0.3147],
        [-0.2607],
        [-0.1316],
        [ 0.1355]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.1272, -0.1334,  0.0030,  0.1261, -0.1044, -0.0181,  0.0674,  0.1322,
         -0.1507,  0.0925,  0.0753, -0.0152,  0.0007, -0.1412, -0.1410, -0.1491,
         -0.1394, -0.0457, -0.1464,  0.0402, -0.0260,  0.0977,  0.0062, -0.0232,
          0.0908,  0.1324,  0.0018, -0.0592,  0.0081,  0.0828,  0.0180, -0.1125,
          0.0891,  0.1245, -0.0543,  0.0997,  0.0342, -0.0087, -0.0346, -0.0258,
         -0.1252, -0.1265,  0.0768, -0.0863, -0.0738,  0.0054, -0.1520, -0.1276,
         -0.0072,  0.0460, -0.0312, -0.0880,  0.0709, -0.1388, -0.1189, -0.0448,
          0.0149,  0.0006,  0.0315, -0.0439, -0.0486,  0.1312,  0.1088, -0.1190,
          0.0129,  0.1392,  0.1242,  0.0895,  0.1354, -0.0726, -0.0569,  0.0339,
          0.1199, -0.0935,  0.1021,  0.0929, -0.0942, -0.0625,  0.1268,  0.1081,
          0.0415,  0.0459,  0.1503,  0.0329,  0.0258, -0.0503,  0.1331,  0.0133,
          0.1408, -0.1426,  0.1149, -0.0415, -0.0183,  0.0780,  0.1439, -0.0770,
         -0.1387, -0.1315, -0.0726,  0.0562,  0.0312,  0.0049, -0.0269, -0.0953,
         -0.1411, -0.1127,  0.0553, -0.1053, -0.0214, -0.0946, -0.0041, -0.0860,
         -0.0393,  0.0090,  0.1311,  0.0695, -0.0248, -0.1386, -0.1424, -0.1277,
          0.0059,  0.1060, -0.0672,  0.1290,  0.0026, -0.0056, -0.1348, -0.0919,
          0.0057,  0.1066,  0.1258,  0.1073, -0.1354, -0.0980, -0.0171,  0.1100,
          0.0812, -0.0045,  0.1030, -0.0785, -0.0738,  0.0507, -0.0900, -0.0952,
          0.0051, -0.1246,  0.1411, -0.1234,  0.0865,  0.0289, -0.0538,  0.0071,
          0.1340,  0.0161, -0.1047,  0.1144,  0.0419, -0.0950, -0.0729, -0.1364,
          0.1427, -0.0107, -0.1089, -0.0805, -0.1285,  0.0834,  0.0779,  0.0661,
         -0.1002, -0.0090,  0.0490, -0.0537,  0.0711,  0.0893, -0.1484,  0.0656,
         -0.0549, -0.0786, -0.0770, -0.0706, -0.1499,  0.1279, -0.0102,  0.0121,
          0.0892,  0.1380, -0.1294,  0.1215,  0.0033,  0.1449, -0.1007,  0.1073,
          0.0108, -0.0156, -0.1320,  0.0584, -0.0892,  0.1267,  0.0424, -0.0824,
          0.0167, -0.0219, -0.1334, -0.0962, -0.0885,  0.1425, -0.1056,  0.1002,
          0.0487, -0.0567, -0.1035, -0.0728, -0.0688,  0.0868,  0.0409, -0.1229,
          0.0834,  0.0156, -0.0886, -0.1483,  0.1394,  0.0830,  0.1011, -0.0866,
         -0.1177,  0.0579, -0.0549, -0.0960, -0.0851, -0.0951, -0.0870, -0.0663,
         -0.0729,  0.0003,  0.0301,  0.1080,  0.0094, -0.0464,  0.0803,  0.1312,
         -0.1038,  0.1496, -0.1326,  0.1111, -0.1285,  0.1277, -0.0026, -0.0998,
          0.0934,  0.1300,  0.0018,  0.0375,  0.0141, -0.0494, -0.1443,  0.1430]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1272, -0.1334,  0.0030,  0.1261, -0.1044, -0.0181,  0.0674,  0.1322,
         -0.1507,  0.0925,  0.0753, -0.0152,  0.0007, -0.1412, -0.1410, -0.1491,
         -0.1394, -0.0457, -0.1464,  0.0402, -0.0260,  0.0977,  0.0062, -0.0232,
          0.0908,  0.1324,  0.0018, -0.0592,  0.0081,  0.0828,  0.0180, -0.1125,
          0.0891,  0.1245, -0.0543,  0.0997,  0.0342, -0.0087, -0.0346, -0.0258,
         -0.1252, -0.1265,  0.0768, -0.0863, -0.0738,  0.0054, -0.1520, -0.1276,
         -0.0072,  0.0460, -0.0312, -0.0880,  0.0709, -0.1388, -0.1189, -0.0448,
          0.0149,  0.0006,  0.0315, -0.0439, -0.0486,  0.1312,  0.1088, -0.1190,
          0.0129,  0.1392,  0.1242,  0.0895,  0.1354, -0.0726, -0.0569,  0.0339,
          0.1199, -0.0935,  0.1021,  0.0929, -0.0942, -0.0625,  0.1268,  0.1081,
          0.0415,  0.0459,  0.1503,  0.0329,  0.0258, -0.0503,  0.1331,  0.0133,
          0.1408, -0.1426,  0.1149, -0.0415, -0.0183,  0.0780,  0.1439, -0.0770,
         -0.1387, -0.1315, -0.0726,  0.0562,  0.0312,  0.0049, -0.0269, -0.0953,
         -0.1411, -0.1127,  0.0553, -0.1053, -0.0214, -0.0946, -0.0041, -0.0860,
         -0.0393,  0.0090,  0.1311,  0.0695, -0.0248, -0.1386, -0.1424, -0.1277,
          0.0059,  0.1060, -0.0672,  0.1290,  0.0026, -0.0056, -0.1348, -0.0919,
          0.0057,  0.1066,  0.1258,  0.1073, -0.1354, -0.0980, -0.0171,  0.1100,
          0.0812, -0.0045,  0.1030, -0.0785, -0.0738,  0.0507, -0.0900, -0.0952,
          0.0051, -0.1246,  0.1411, -0.1234,  0.0865,  0.0289, -0.0538,  0.0071,
          0.1340,  0.0161, -0.1047,  0.1144,  0.0419, -0.0950, -0.0729, -0.1364,
          0.1427, -0.0107, -0.1089, -0.0805, -0.1285,  0.0834,  0.0779,  0.0661,
         -0.1002, -0.0090,  0.0490, -0.0537,  0.0711,  0.0893, -0.1484,  0.0656,
         -0.0549, -0.0786, -0.0770, -0.0706, -0.1499,  0.1279, -0.0102,  0.0121,
          0.0892,  0.1380, -0.1294,  0.1215,  0.0033,  0.1449, -0.1007,  0.1073,
          0.0108, -0.0156, -0.1320,  0.0584, -0.0892,  0.1267,  0.0424, -0.0824,
          0.0167, -0.0219, -0.1334, -0.0962, -0.0885,  0.1425, -0.1056,  0.1002,
          0.0487, -0.0567, -0.1035, -0.0728, -0.0688,  0.0868,  0.0409, -0.1229,
          0.0834,  0.0156, -0.0886, -0.1483,  0.1394,  0.0830,  0.1011, -0.0866,
         -0.1177,  0.0579, -0.0549, -0.0960, -0.0851, -0.0951, -0.0870, -0.0663,
         -0.0729,  0.0003,  0.0301,  0.1080,  0.0094, -0.0464,  0.0803,  0.1312,
         -0.1038,  0.1496, -0.1326,  0.1111, -0.1285,  0.1277, -0.0026, -0.0998,
          0.0934,  0.1300,  0.0018,  0.0375,  0.0141, -0.0494, -0.1443,  0.1430]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0696,  0.0017,  0.0943,  ...,  0.0289,  0.0010, -0.0065],
        [ 0.0913,  0.0699, -0.0380,  ..., -0.1037,  0.0303,  0.1152],
        [ 0.0309, -0.0746, -0.0954,  ...,  0.0716, -0.1157,  0.0100],
        ...,
        [ 0.0982,  0.0326,  0.0034,  ...,  0.1021,  0.0457, -0.0947],
        [ 0.1063, -0.0693,  0.0262,  ...,  0.1006, -0.0164,  0.0040],
        [ 0.1118,  0.1027,  0.0454,  ...,  0.0438,  0.1198,  0.0378]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0696,  0.0017,  0.0943,  ...,  0.0289,  0.0010, -0.0065],
        [ 0.0913,  0.0699, -0.0380,  ..., -0.1037,  0.0303,  0.1152],
        [ 0.0309, -0.0746, -0.0954,  ...,  0.0716, -0.1157,  0.0100],
        ...,
        [ 0.0982,  0.0326,  0.0034,  ...,  0.1021,  0.0457, -0.0947],
        [ 0.1063, -0.0693,  0.0262,  ...,  0.1006, -0.0164,  0.0040],
        [ 0.1118,  0.1027,  0.0454,  ...,  0.0438,  0.1198,  0.0378]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0748, -0.1557, -0.1150,  ..., -0.0353, -0.0908,  0.0647],
        [ 0.0586, -0.1338,  0.0875,  ..., -0.0452,  0.0767, -0.0250],
        [-0.1612, -0.1698, -0.1541,  ..., -0.0141,  0.0695, -0.0060],
        ...,
        [-0.0315, -0.1113,  0.0718,  ..., -0.0572,  0.1235,  0.0983],
        [-0.0210,  0.0601, -0.0555,  ...,  0.0505,  0.1699, -0.0030],
        [ 0.1160, -0.0601, -0.0499,  ...,  0.1623, -0.0983,  0.1653]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0748, -0.1557, -0.1150,  ..., -0.0353, -0.0908,  0.0647],
        [ 0.0586, -0.1338,  0.0875,  ..., -0.0452,  0.0767, -0.0250],
        [-0.1612, -0.1698, -0.1541,  ..., -0.0141,  0.0695, -0.0060],
        ...,
        [-0.0315, -0.1113,  0.0718,  ..., -0.0572,  0.1235,  0.0983],
        [-0.0210,  0.0601, -0.0555,  ...,  0.0505,  0.1699, -0.0030],
        [ 0.1160, -0.0601, -0.0499,  ...,  0.1623, -0.0983,  0.1653]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1150, -0.0775, -0.2430,  ..., -0.0217,  0.2469, -0.1767],
        [ 0.0195,  0.1698, -0.0047,  ..., -0.0866,  0.0631, -0.0331],
        [-0.2006,  0.1723,  0.1280,  ...,  0.1517,  0.1287,  0.1773],
        ...,
        [ 0.2471, -0.0761, -0.2460,  ..., -0.0625,  0.1220,  0.1823],
        [-0.0963,  0.2338, -0.1511,  ...,  0.1310, -0.0111, -0.2113],
        [ 0.0360,  0.1937,  0.1583,  ..., -0.1727, -0.0389, -0.1882]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1150, -0.0775, -0.2430,  ..., -0.0217,  0.2469, -0.1767],
        [ 0.0195,  0.1698, -0.0047,  ..., -0.0866,  0.0631, -0.0331],
        [-0.2006,  0.1723,  0.1280,  ...,  0.1517,  0.1287,  0.1773],
        ...,
        [ 0.2471, -0.0761, -0.2460,  ..., -0.0625,  0.1220,  0.1823],
        [-0.0963,  0.2338, -0.1511,  ...,  0.1310, -0.0111, -0.2113],
        [ 0.0360,  0.1937,  0.1583,  ..., -0.1727, -0.0389, -0.1882]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 2.9547e-01],
        [ 1.0725e-04],
        [-1.6311e-02],
        [ 9.8886e-02],
        [ 1.6371e-01],
        [ 2.5653e-01],
        [ 5.0918e-02],
        [-3.9758e-01],
        [-6.3195e-02],
        [ 3.3088e-01],
        [-8.4882e-02],
        [-4.0748e-01],
        [-8.5199e-02],
        [ 2.4564e-01],
        [-2.0577e-01],
        [ 6.0777e-02],
        [ 1.7539e-01],
        [ 4.1458e-01],
        [-3.3668e-01],
        [-3.5509e-01],
        [-3.1830e-02],
        [-3.5808e-01],
        [-1.4816e-01],
        [-1.8731e-01],
        [ 2.7532e-01],
        [-1.3703e-01],
        [-5.8863e-02],
        [ 6.5258e-02],
        [ 2.5281e-01],
        [-4.1279e-01],
        [-1.7331e-01],
        [ 2.9975e-01]], device='cuda:0') 
 Parameter containing:
tensor([[ 2.9547e-01],
        [ 1.0725e-04],
        [-1.6311e-02],
        [ 9.8886e-02],
        [ 1.6371e-01],
        [ 2.5653e-01],
        [ 5.0918e-02],
        [-3.9758e-01],
        [-6.3195e-02],
        [ 3.3088e-01],
        [-8.4882e-02],
        [-4.0748e-01],
        [-8.5199e-02],
        [ 2.4564e-01],
        [-2.0577e-01],
        [ 6.0777e-02],
        [ 1.7539e-01],
        [ 4.1458e-01],
        [-3.3668e-01],
        [-3.5509e-01],
        [-3.1830e-02],
        [-3.5808e-01],
        [-1.4816e-01],
        [-1.8731e-01],
        [ 2.7532e-01],
        [-1.3703e-01],
        [-5.8863e-02],
        [ 6.5258e-02],
        [ 2.5281e-01],
        [-4.1279e-01],
        [-1.7331e-01],
        [ 2.9975e-01]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0060, -0.0052,  0.0014,  ...,  0.0086, -0.0053,  0.0010],
        [ 0.0200, -0.0173,  0.0045,  ...,  0.0284, -0.0176,  0.0033],
        [ 0.0133, -0.0115,  0.0030,  ...,  0.0189, -0.0117,  0.0022],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(691.9768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(16.7735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4264, device='cuda:0')



h[100].sum tensor(-9.5285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8994, device='cuda:0')



h[200].sum tensor(12.6580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0459, 0.0000, 0.0103,  ..., 0.0653, 0.0000, 0.0075],
        [0.0404, 0.0000, 0.0090,  ..., 0.0574, 0.0000, 0.0066],
        [0.0556, 0.0000, 0.0124,  ..., 0.0790, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(22497.3301, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2358, 0.2357, 0.0000,  ..., 0.0000, 0.0000, 0.0862],
        [0.2312, 0.2311, 0.0000,  ..., 0.0000, 0.0000, 0.0845],
        [0.2323, 0.2323, 0.0000,  ..., 0.0000, 0.0000, 0.0849],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(137734.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3789.2427, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.7644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-144.5308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2267.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(144.6897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.4481],
        [0.4315],
        [0.3873],
        ...,
        [0.0033],
        [0.0034],
        [0.0035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(23041.3203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.4481],
        [0.4315],
        [0.3873],
        ...,
        [0.0033],
        [0.0034],
        [0.0035]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0067,  0.0082,  0.0064,  ..., -0.0077, -0.0023, -0.0080],
        [-0.0223,  0.0270,  0.0213,  ..., -0.0254, -0.0077, -0.0266],
        [-0.0148,  0.0179,  0.0141,  ..., -0.0169, -0.0051, -0.0176],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-29.9672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.6868, device='cuda:0')



h[100].sum tensor(29.2679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(30.3635, device='cuda:0')



h[200].sum tensor(-21.1400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-21.9313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0621, 0.0488,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0546, 0.0430,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0751, 0.0591,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(26713.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0713, 0.0000,  ..., 0.0000, 0.0021, 0.0274],
        [0.0000, 0.0699, 0.0000,  ..., 0.0000, 0.0021, 0.0268],
        [0.0000, 0.0703, 0.0000,  ..., 0.0000, 0.0021, 0.0270],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(167204.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-348.8183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-359.1010, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1878.3181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(125.9428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9725e-01],
        [-4.7863e-01],
        [-4.2985e-01],
        ...,
        [-1.6689e-05],
        [-2.7538e-04],
        [-1.8464e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-34869.0352, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.4481],
        [0.4315],
        [0.3873],
        ...,
        [0.0033],
        [0.0034],
        [0.0035]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



load_model False 
TraEvN 1999 
BatchSize 15 
EpochNum 10 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0089, -0.0155,  0.1205,  0.1498,  0.1423,  0.0703, -0.1114, -0.0714,
         -0.0039,  0.0120,  0.1365,  0.0515,  0.0745,  0.0036, -0.0614,  0.1267,
         -0.1093, -0.0965,  0.0374, -0.0094,  0.1320, -0.0165,  0.1211,  0.1008,
         -0.0695, -0.1359, -0.0312,  0.0094, -0.0769, -0.1252,  0.0359, -0.1139,
         -0.0326, -0.1509, -0.1362,  0.1234, -0.0529, -0.0039,  0.0946,  0.0149,
         -0.0608,  0.1171, -0.1216,  0.0138, -0.0573, -0.0796, -0.1432,  0.1240,
         -0.0061, -0.1090, -0.1220,  0.0205,  0.0338, -0.1194, -0.0023, -0.1239,
          0.0608,  0.1259, -0.0488, -0.0568,  0.0458, -0.0460, -0.0368, -0.0217,
          0.0855, -0.1205, -0.1178, -0.1393,  0.1016, -0.0585,  0.0793, -0.1248,
          0.1514, -0.1126,  0.1205, -0.1450,  0.1320, -0.0362,  0.0408, -0.0772,
         -0.0949,  0.0143,  0.0098,  0.1224, -0.1143, -0.1031, -0.1042,  0.0822,
          0.1023, -0.1211,  0.1382, -0.0419,  0.0090, -0.0705, -0.0231, -0.0676,
          0.0673, -0.1029,  0.1100,  0.0592, -0.0805,  0.0636,  0.1031,  0.0119,
         -0.0461, -0.1040, -0.0616,  0.1467, -0.0726,  0.1008,  0.0854,  0.0920,
         -0.0601, -0.0676, -0.0112, -0.0852, -0.0251,  0.1330, -0.0959, -0.1234,
         -0.1502,  0.1185,  0.1452, -0.0379,  0.0690,  0.0806,  0.0628,  0.1450,
          0.0996, -0.0301, -0.1056, -0.0277, -0.1009, -0.0181, -0.1417,  0.1371,
         -0.1465, -0.0720,  0.0142,  0.1479, -0.0177, -0.1051,  0.0652, -0.0191,
          0.0464, -0.1061, -0.0766,  0.0352,  0.0242, -0.1353,  0.0358, -0.1051,
          0.1527,  0.1066,  0.1012, -0.0059, -0.1362, -0.0315,  0.0442, -0.0335,
         -0.1319,  0.0531,  0.1033, -0.1022,  0.0175,  0.1038,  0.0194,  0.1124,
          0.0116,  0.0169, -0.1357, -0.1319,  0.0446,  0.1151, -0.0975,  0.1365,
         -0.0983, -0.1185, -0.0926, -0.0096,  0.1470, -0.0817, -0.0307,  0.0133,
          0.1049,  0.0244, -0.0170,  0.0973, -0.1062, -0.0867, -0.0300,  0.0550,
          0.0473,  0.0937,  0.0668,  0.1255, -0.1144, -0.1472,  0.1429,  0.1401,
         -0.0706,  0.0343,  0.0826, -0.1468, -0.0902, -0.1033, -0.0649, -0.0651,
         -0.0635,  0.0759,  0.0578,  0.0682, -0.0156,  0.0418, -0.0040, -0.1252,
         -0.0959,  0.0222,  0.0192,  0.1447, -0.0090, -0.1510,  0.0710, -0.1356,
         -0.0876,  0.0630,  0.0437,  0.0411,  0.1133, -0.1357, -0.0319, -0.1475,
         -0.0553,  0.0065,  0.1446,  0.1216,  0.1424, -0.1424, -0.0358, -0.0019,
          0.1332, -0.1527, -0.0473,  0.0875,  0.0055,  0.1378,  0.0020, -0.0089,
         -0.1210,  0.1078, -0.0664, -0.0791, -0.0814, -0.1483, -0.0171,  0.1279]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1005, -0.0101, -0.1117,  ..., -0.0406, -0.0638,  0.0522],
        [ 0.0217, -0.0819, -0.0457,  ...,  0.0955, -0.0845, -0.0478],
        [ 0.0927,  0.0729,  0.1061,  ...,  0.0866,  0.0861, -0.1186],
        ...,
        [-0.0009,  0.1121, -0.0785,  ...,  0.0010, -0.0526, -0.0722],
        [ 0.0856, -0.0025, -0.1240,  ...,  0.1023,  0.0255, -0.0158],
        [ 0.0308,  0.0696, -0.0324,  ...,  0.0575, -0.0034, -0.0219]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0553, -0.1084,  0.0523,  ..., -0.1148,  0.0126,  0.0424],
        [-0.1567,  0.0472, -0.0264,  ..., -0.0776, -0.0257,  0.0871],
        [-0.0224, -0.1746, -0.0176,  ..., -0.1576,  0.1721,  0.1002],
        ...,
        [-0.0967, -0.1470, -0.0116,  ..., -0.1381,  0.0102, -0.0078],
        [ 0.0869,  0.1194,  0.1403,  ..., -0.0585, -0.0691,  0.0319],
        [-0.1393, -0.1204, -0.0574,  ...,  0.0065,  0.1688, -0.1639]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1197, -0.1371, -0.1018,  ...,  0.0462, -0.2205, -0.2187],
        [-0.1216, -0.0029,  0.0954,  ...,  0.0276,  0.0476, -0.1857],
        [-0.0338, -0.0459, -0.0074,  ...,  0.0139,  0.0838,  0.0858],
        ...,
        [-0.0804,  0.0067, -0.0235,  ...,  0.2102,  0.1635, -0.1113],
        [ 0.0463, -0.1588, -0.0020,  ..., -0.0127,  0.1030, -0.2315],
        [ 0.1856,  0.0203, -0.1178,  ..., -0.2216, -0.1676, -0.0276]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2452],
        [ 0.0806],
        [-0.1725],
        [-0.3192],
        [ 0.2039],
        [-0.2611],
        [ 0.1670],
        [ 0.1860],
        [ 0.2749],
        [-0.1766],
        [-0.1055],
        [ 0.2111],
        [-0.0858],
        [-0.0921],
        [-0.0317],
        [ 0.3605],
        [ 0.2675],
        [ 0.1905],
        [ 0.3316],
        [ 0.3060],
        [-0.4208],
        [ 0.2351],
        [-0.3235],
        [-0.0916],
        [ 0.1211],
        [ 0.4150],
        [ 0.2907],
        [ 0.3470],
        [-0.3586],
        [-0.3067],
        [ 0.0670],
        [-0.1697]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0089, -0.0155,  0.1205,  0.1498,  0.1423,  0.0703, -0.1114, -0.0714,
         -0.0039,  0.0120,  0.1365,  0.0515,  0.0745,  0.0036, -0.0614,  0.1267,
         -0.1093, -0.0965,  0.0374, -0.0094,  0.1320, -0.0165,  0.1211,  0.1008,
         -0.0695, -0.1359, -0.0312,  0.0094, -0.0769, -0.1252,  0.0359, -0.1139,
         -0.0326, -0.1509, -0.1362,  0.1234, -0.0529, -0.0039,  0.0946,  0.0149,
         -0.0608,  0.1171, -0.1216,  0.0138, -0.0573, -0.0796, -0.1432,  0.1240,
         -0.0061, -0.1090, -0.1220,  0.0205,  0.0338, -0.1194, -0.0023, -0.1239,
          0.0608,  0.1259, -0.0488, -0.0568,  0.0458, -0.0460, -0.0368, -0.0217,
          0.0855, -0.1205, -0.1178, -0.1393,  0.1016, -0.0585,  0.0793, -0.1248,
          0.1514, -0.1126,  0.1205, -0.1450,  0.1320, -0.0362,  0.0408, -0.0772,
         -0.0949,  0.0143,  0.0098,  0.1224, -0.1143, -0.1031, -0.1042,  0.0822,
          0.1023, -0.1211,  0.1382, -0.0419,  0.0090, -0.0705, -0.0231, -0.0676,
          0.0673, -0.1029,  0.1100,  0.0592, -0.0805,  0.0636,  0.1031,  0.0119,
         -0.0461, -0.1040, -0.0616,  0.1467, -0.0726,  0.1008,  0.0854,  0.0920,
         -0.0601, -0.0676, -0.0112, -0.0852, -0.0251,  0.1330, -0.0959, -0.1234,
         -0.1502,  0.1185,  0.1452, -0.0379,  0.0690,  0.0806,  0.0628,  0.1450,
          0.0996, -0.0301, -0.1056, -0.0277, -0.1009, -0.0181, -0.1417,  0.1371,
         -0.1465, -0.0720,  0.0142,  0.1479, -0.0177, -0.1051,  0.0652, -0.0191,
          0.0464, -0.1061, -0.0766,  0.0352,  0.0242, -0.1353,  0.0358, -0.1051,
          0.1527,  0.1066,  0.1012, -0.0059, -0.1362, -0.0315,  0.0442, -0.0335,
         -0.1319,  0.0531,  0.1033, -0.1022,  0.0175,  0.1038,  0.0194,  0.1124,
          0.0116,  0.0169, -0.1357, -0.1319,  0.0446,  0.1151, -0.0975,  0.1365,
         -0.0983, -0.1185, -0.0926, -0.0096,  0.1470, -0.0817, -0.0307,  0.0133,
          0.1049,  0.0244, -0.0170,  0.0973, -0.1062, -0.0867, -0.0300,  0.0550,
          0.0473,  0.0937,  0.0668,  0.1255, -0.1144, -0.1472,  0.1429,  0.1401,
         -0.0706,  0.0343,  0.0826, -0.1468, -0.0902, -0.1033, -0.0649, -0.0651,
         -0.0635,  0.0759,  0.0578,  0.0682, -0.0156,  0.0418, -0.0040, -0.1252,
         -0.0959,  0.0222,  0.0192,  0.1447, -0.0090, -0.1510,  0.0710, -0.1356,
         -0.0876,  0.0630,  0.0437,  0.0411,  0.1133, -0.1357, -0.0319, -0.1475,
         -0.0553,  0.0065,  0.1446,  0.1216,  0.1424, -0.1424, -0.0358, -0.0019,
          0.1332, -0.1527, -0.0473,  0.0875,  0.0055,  0.1378,  0.0020, -0.0089,
         -0.1210,  0.1078, -0.0664, -0.0791, -0.0814, -0.1483, -0.0171,  0.1279]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1005, -0.0101, -0.1117,  ..., -0.0406, -0.0638,  0.0522],
        [ 0.0217, -0.0819, -0.0457,  ...,  0.0955, -0.0845, -0.0478],
        [ 0.0927,  0.0729,  0.1061,  ...,  0.0866,  0.0861, -0.1186],
        ...,
        [-0.0009,  0.1121, -0.0785,  ...,  0.0010, -0.0526, -0.0722],
        [ 0.0856, -0.0025, -0.1240,  ...,  0.1023,  0.0255, -0.0158],
        [ 0.0308,  0.0696, -0.0324,  ...,  0.0575, -0.0034, -0.0219]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0553, -0.1084,  0.0523,  ..., -0.1148,  0.0126,  0.0424],
        [-0.1567,  0.0472, -0.0264,  ..., -0.0776, -0.0257,  0.0871],
        [-0.0224, -0.1746, -0.0176,  ..., -0.1576,  0.1721,  0.1002],
        ...,
        [-0.0967, -0.1470, -0.0116,  ..., -0.1381,  0.0102, -0.0078],
        [ 0.0869,  0.1194,  0.1403,  ..., -0.0585, -0.0691,  0.0319],
        [-0.1393, -0.1204, -0.0574,  ...,  0.0065,  0.1688, -0.1639]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1197, -0.1371, -0.1018,  ...,  0.0462, -0.2205, -0.2187],
        [-0.1216, -0.0029,  0.0954,  ...,  0.0276,  0.0476, -0.1857],
        [-0.0338, -0.0459, -0.0074,  ...,  0.0139,  0.0838,  0.0858],
        ...,
        [-0.0804,  0.0067, -0.0235,  ...,  0.2102,  0.1635, -0.1113],
        [ 0.0463, -0.1588, -0.0020,  ..., -0.0127,  0.1030, -0.2315],
        [ 0.1856,  0.0203, -0.1178,  ..., -0.2216, -0.1676, -0.0276]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.2452],
        [ 0.0806],
        [-0.1725],
        [-0.3192],
        [ 0.2039],
        [-0.2611],
        [ 0.1670],
        [ 0.1860],
        [ 0.2749],
        [-0.1766],
        [-0.1055],
        [ 0.2111],
        [-0.0858],
        [-0.0921],
        [-0.0317],
        [ 0.3605],
        [ 0.2675],
        [ 0.1905],
        [ 0.3316],
        [ 0.3060],
        [-0.4208],
        [ 0.2351],
        [-0.3235],
        [-0.0916],
        [ 0.1211],
        [ 0.4150],
        [ 0.2907],
        [ 0.3470],
        [-0.3586],
        [-0.3067],
        [ 0.0670],
        [-0.1697]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1891.0745, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-173.8550, device='cuda:0')



h[100].sum tensor(-121.4871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(172.0233, device='cuda:0')



h[200].sum tensor(-106.6376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-124.2514, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0210,  ..., 0.0000, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158126.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0523, 0.1200,  ..., 0.0919, 0.0000, 0.0000],
        [0.0000, 0.0222, 0.0510,  ..., 0.0391, 0.0000, 0.0000],
        [0.0000, 0.0151, 0.0347,  ..., 0.0266, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(792873.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1207.9707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2492.7075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26406.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-958.3113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[6.0082e-01],
        [5.2750e-01],
        [5.1735e-01],
        ...,
        [2.5279e-06],
        [3.3208e-07],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(222066., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1981.4137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-181.9124, device='cuda:0')



h[100].sum tensor(-127.3137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(179.9958, device='cuda:0')



h[200].sum tensor(-111.7327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-130.0099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0000, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(162376.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0283, 0.0650,  ..., 0.0498, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0177,  ..., 0.0135, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0071,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0123, 0.0281,  ..., 0.0215, 0.0000, 0.0000],
        [0.0000, 0.0207, 0.0475,  ..., 0.0364, 0.0000, 0.0000],
        [0.0000, 0.0304, 0.0699,  ..., 0.0535, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(806143.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1240.6946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2559.3647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26849.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-984.5813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.2800],
        [0.2248],
        [0.2372],
        ...,
        [0.1985],
        [0.2577],
        [0.3164]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(226316.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0016,  0.0127,  ..., -0.0156, -0.0018,  0.0135],
        [-0.0020, -0.0036,  0.0282,  ..., -0.0348, -0.0040,  0.0300],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2058.9817, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-189.6510, device='cuda:0')



h[100].sum tensor(-132.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(187.6528, device='cuda:0')



h[200].sum tensor(-116.1076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-135.5406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1128,  ..., 0.0000, 0.0000, 0.1198],
        [0.0000, 0.0000, 0.0568,  ..., 0.0000, 0.0000, 0.0603],
        [0.0000, 0.0000, 0.0638,  ..., 0.0000, 0.0000, 0.0677],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(171446.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1939, 0.4452,  ..., 0.3410, 0.0000, 0.0000],
        [0.0000, 0.1481, 0.3399,  ..., 0.2604, 0.0000, 0.0000],
        [0.0000, 0.1364, 0.3131,  ..., 0.2398, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(849528., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1310.2754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2701.9775, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(28283.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1040.1271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.7189],
        [1.4867],
        [1.2970],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(235496.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097462.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ...,  0.0000e+00,
          0.0000e+00, -6.3881e-05],
        [-1.4743e-03, -2.6057e-03,  2.0477e-02,  ..., -2.5296e-02,
         -2.8637e-03,  2.1742e-02],
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ...,  0.0000e+00,
          0.0000e+00, -6.3881e-05],
        ...,
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ...,  0.0000e+00,
          0.0000e+00, -6.3881e-05],
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ...,  0.0000e+00,
          0.0000e+00, -6.3881e-05],
        [ 0.0000e+00,  0.0000e+00, -6.3881e-05,  ...,  0.0000e+00,
          0.0000e+00, -6.3881e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1824.5597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-166.2080, device='cuda:0')



h[100].sum tensor(-116.2570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(164.4568, device='cuda:0')



h[200].sum tensor(-101.9939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-118.7862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0837,  ..., 0.0000, 0.0000, 0.0889],
        [0.0000, 0.0000, 0.0167,  ..., 0.0000, 0.0000, 0.0177],
        [0.0000, 0.0000, 0.0205,  ..., 0.0000, 0.0000, 0.0217],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0000, 0.0081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(154731.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.5026e-02, 2.2719e-01,  ..., 1.7105e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.7625e-02, 1.1537e-01,  ..., 8.6314e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1248e-02, 7.6641e-02,  ..., 5.6953e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.3117e-04, 6.8172e-04, 3.5072e-03,  ..., 1.5499e-03, 0.0000e+00,
         3.5010e-04],
        [2.7662e-04, 9.2844e-03, 2.4237e-02,  ..., 1.7267e-02, 0.0000e+00,
         2.2461e-04],
        [0.0000e+00, 2.7874e-02, 6.8994e-02,  ..., 5.1208e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(770491.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(162.2349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1111.4620, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2416.1909, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(27127.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-941.6502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.4927],
        [0.3671],
        [0.2481],
        ...,
        [0.0243],
        [0.0779],
        [0.1573]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(129754.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097462.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097355.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0000, -0.0001],
        [-0.0006, -0.0010,  0.0078,  ..., -0.0098, -0.0011,  0.0083],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0000, -0.0001],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0000, -0.0001],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0000, -0.0001],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0000, -0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2109.0024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-192.2107, device='cuda:0')



h[100].sum tensor(-134.2637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.1856, device='cuda:0')



h[200].sum tensor(-117.7709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-137.3699, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0397,  ..., 0.0000, 0.0000, 0.0421],
        [0.0000, 0.0000, 0.0120,  ..., 0.0000, 0.0000, 0.0128],
        [0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0000, 0.0083],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(179730.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0428, 0.1104,  ..., 0.0810, 0.0000, 0.0000],
        [0.0000, 0.0339, 0.0884,  ..., 0.0645, 0.0000, 0.0000],
        [0.0000, 0.0422, 0.1082,  ..., 0.0793, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0000, 0.0029,  ..., 0.0002, 0.0000, 0.0008],
        [0.0010, 0.0000, 0.0029,  ..., 0.0002, 0.0000, 0.0008],
        [0.0010, 0.0000, 0.0029,  ..., 0.0002, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(886791.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(310.1086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1242.1112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2790.7095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32299.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1096.7336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1535],
        [ 0.2358],
        [ 0.3711],
        ...,
        [-0.0148],
        [-0.0147],
        [-0.0146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(87363.6016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097355.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097247.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2107.1516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-192.6567, device='cuda:0')



h[100].sum tensor(-133.9661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(190.6269, device='cuda:0')



h[200].sum tensor(-117.4894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-137.6887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(182779.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0014, 0.0000, 0.0040,  ..., 0.0002, 0.0000, 0.0011],
        [0.0014, 0.0000, 0.0047,  ..., 0.0007, 0.0000, 0.0010],
        [0.0012, 0.0002, 0.0063,  ..., 0.0019, 0.0000, 0.0008],
        ...,
        [0.0014, 0.0000, 0.0040,  ..., 0.0002, 0.0000, 0.0011],
        [0.0014, 0.0000, 0.0040,  ..., 0.0002, 0.0000, 0.0011],
        [0.0014, 0.0000, 0.0040,  ..., 0.0002, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(896464.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(492.9240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1216.4810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2823.6709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33547.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1117.5620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0511],
        [-0.0536],
        [-0.0508],
        ...,
        [-0.0182],
        [-0.0181],
        [-0.0181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(37415.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097247.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097247.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2188.8320, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-199.4222, device='cuda:0')



h[100].sum tensor(-139.3688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(197.3211, device='cuda:0')



h[200].sum tensor(-122.2276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-142.5239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(190924.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0059, 0.0216,  ..., 0.0133, 0.0000, 0.0004],
        [0.0012, 0.0005, 0.0070,  ..., 0.0024, 0.0000, 0.0007],
        [0.0011, 0.0011, 0.0075,  ..., 0.0028, 0.0000, 0.0009],
        ...,
        [0.0014, 0.0000, 0.0040,  ..., 0.0002, 0.0000, 0.0011],
        [0.0014, 0.0000, 0.0040,  ..., 0.0002, 0.0000, 0.0011],
        [0.0014, 0.0000, 0.0040,  ..., 0.0002, 0.0000, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(948118.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(446.1709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1276.7649, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2950.9277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35419.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1167.7719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0138],
        [-0.0384],
        [-0.0486],
        ...,
        [-0.0182],
        [-0.0181],
        [-0.0181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(44919.8828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097247.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097247.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [-0.0021, -0.0038,  0.0301,  ..., -0.0372, -0.0042,  0.0320],
        [-0.0009, -0.0017,  0.0132,  ..., -0.0165, -0.0018,  0.0141],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1700.2162, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-153.6751, device='cuda:0')



h[100].sum tensor(-107.0499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(152.0560, device='cuda:0')



h[200].sum tensor(-93.8836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-109.8292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0742,  ..., 0.0000, 0.0000, 0.0788],
        [0.0000, 0.0000, 0.0723,  ..., 0.0000, 0.0000, 0.0768],
        [0.0000, 0.0000, 0.1063,  ..., 0.0000, 0.0000, 0.1129],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(147505.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.3790e-01, 3.4531e-01,  ..., 2.5440e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5504e-01, 3.8745e-01,  ..., 2.8580e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6727e-01, 4.1798e-01,  ..., 3.0860e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.3890e-03, 0.0000e+00, 3.9945e-03,  ..., 2.0729e-04, 0.0000e+00,
         1.1278e-03],
        [1.3890e-03, 0.0000e+00, 3.9945e-03,  ..., 2.0729e-04, 0.0000e+00,
         1.1278e-03],
        [1.3890e-03, 0.0000e+00, 3.9945e-03,  ..., 2.0729e-04, 0.0000e+00,
         1.1278e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(727224.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(505.3124, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-949.9969, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2269.4478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(27723.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-902.5289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9244],
        [ 0.9444],
        [ 0.9110],
        ...,
        [-0.0189],
        [-0.0186],
        [-0.0184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(5638.2104, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097247.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097140.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [-0.0005, -0.0010,  0.0076,  ..., -0.0096, -0.0011,  0.0081],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2033.5612, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-184.8423, device='cuda:0')



h[100].sum tensor(-129.0845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(182.8948, device='cuda:0')



h[200].sum tensor(-113.1884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-132.1039, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0275,  ..., 0.0000, 0.0000, 0.0292],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0000, 0.0065],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(177581.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.5003e-02, 7.5569e-02,  ..., 5.2884e-02, 0.0000e+00,
         0.0000e+00],
        [2.2420e-05, 1.4578e-02, 4.7901e-02,  ..., 3.2246e-02, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6521e-02, 5.2523e-02,  ..., 3.5634e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.7180e-03, 0.0000e+00, 4.9518e-03,  ..., 2.4846e-04, 0.0000e+00,
         1.3925e-03],
        [1.7180e-03, 0.0000e+00, 4.9518e-03,  ..., 2.4846e-04, 0.0000e+00,
         1.3925e-03],
        [1.7180e-03, 0.0000e+00, 4.9518e-03,  ..., 2.4846e-04, 0.0000e+00,
         1.3925e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(869653.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(608.4500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1132.9325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2728.0522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33549.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1088.9270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0851],
        [ 0.0971],
        [ 0.1196],
        ...,
        [-0.0230],
        [-0.0230],
        [-0.0231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-16598.0098, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097140.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097033.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011, -0.0021,  0.0164,  ..., -0.0204, -0.0023,  0.0174],
        [-0.0024, -0.0044,  0.0353,  ..., -0.0437, -0.0048,  0.0375],
        [-0.0023, -0.0042,  0.0334,  ..., -0.0413, -0.0046,  0.0354],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0000, -0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1728.5527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-156.7760, device='cuda:0')



h[100].sum tensor(-108.6608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(155.1242, device='cuda:0')



h[200].sum tensor(-95.2632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-112.0453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1108,  ..., 0.0000, 0.0000, 0.1177],
        [0.0000, 0.0000, 0.1242,  ..., 0.0000, 0.0000, 0.1319],
        [0.0000, 0.0000, 0.1428,  ..., 0.0000, 0.0000, 0.1516],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(155037.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.8377e-01, 4.7482e-01,  ..., 3.4523e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1628e-01, 5.5497e-01,  ..., 4.0400e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2784e-01, 5.8342e-01,  ..., 4.2485e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.0045e-03, 0.0000e+00, 5.7890e-03,  ..., 2.7970e-04, 0.0000e+00,
         1.6247e-03],
        [2.0045e-03, 0.0000e+00, 5.7890e-03,  ..., 2.7970e-04, 0.0000e+00,
         1.6247e-03],
        [2.0045e-03, 0.0000e+00, 5.7890e-03,  ..., 2.7970e-04, 0.0000e+00,
         1.6247e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(775272.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(847.0041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-930.0316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2365.6948, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(30733.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-951.7180, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5051],
        [ 0.5725],
        [ 0.5712],
        ...,
        [-0.0342],
        [-0.0295],
        [-0.0273]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-54740.9883, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097033.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096927., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0021, -0.0037,  0.0301,  ..., -0.0373, -0.0041,  0.0319],
        [-0.0021, -0.0039,  0.0311,  ..., -0.0386, -0.0043,  0.0330],
        [-0.0020, -0.0037,  0.0299,  ..., -0.0370, -0.0041,  0.0317],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1658.5624, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-149.6748, device='cuda:0')



h[100].sum tensor(-103.9812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(148.0979, device='cuda:0')



h[200].sum tensor(-91.1446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-106.9702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1288,  ..., 0.0000, 0.0000, 0.1368],
        [0.0000, 0.0000, 0.1086,  ..., 0.0000, 0.0000, 0.1154],
        [0.0000, 0.0000, 0.1114,  ..., 0.0000, 0.0000, 0.1183],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(147342.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0644e-01, 5.3742e-01,  ..., 3.8860e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.9533e-01, 5.0980e-01,  ..., 3.6847e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.8268e-01, 4.7818e-01,  ..., 3.4539e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.2577e-03, 0.0000e+00, 6.5309e-03,  ..., 3.0777e-04, 0.0000e+00,
         1.8290e-03],
        [2.2577e-03, 0.0000e+00, 6.5309e-03,  ..., 3.0777e-04, 0.0000e+00,
         1.8290e-03],
        [2.2577e-03, 0.0000e+00, 6.5309e-03,  ..., 3.0777e-04, 0.0000e+00,
         1.8290e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(724071.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1021.4984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-840.3920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2235.9360, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(29472.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-906.0854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3393],
        [ 0.3311],
        [ 0.2819],
        ...,
        [-0.0302],
        [-0.0301],
        [-0.0300]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-90494.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096927., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 150 loss: tensor(0.1530, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096820.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        ...,
        [-0.0013, -0.0024,  0.0191,  ..., -0.0238, -0.0026,  0.0203],
        [-0.0012, -0.0021,  0.0170,  ..., -0.0213, -0.0023,  0.0181],
        [-0.0012, -0.0021,  0.0169,  ..., -0.0211, -0.0023,  0.0179]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1788.0303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-163.2289, device='cuda:0')



h[100].sum tensor(-113.0609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(161.5092, device='cuda:0')



h[200].sum tensor(-99.0859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-116.6572, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0284,  ..., 0.0000, 0.0000, 0.0301],
        ...,
        [0.0000, 0.0000, 0.0578,  ..., 0.0000, 0.0000, 0.0615],
        [0.0000, 0.0000, 0.0632,  ..., 0.0000, 0.0000, 0.0672],
        [0.0000, 0.0000, 0.0605,  ..., 0.0000, 0.0000, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160030.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0023, 0.0145,  ..., 0.0057, 0.0000, 0.0016],
        [0.0013, 0.0133, 0.0446,  ..., 0.0276, 0.0000, 0.0010],
        [0.0000, 0.0434, 0.1258,  ..., 0.0867, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0745, 0.2098,  ..., 0.1483, 0.0000, 0.0000],
        [0.0000, 0.0860, 0.2392,  ..., 0.1697, 0.0000, 0.0000],
        [0.0000, 0.0888, 0.2466,  ..., 0.1752, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(782868.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1117.1674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-905.5208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2426.2432, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32128.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-985.8503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1023],
        [-0.0956],
        [-0.0756],
        ...,
        [ 0.0351],
        [ 0.0607],
        [ 0.0752]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-112935.9766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096820.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096713.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010, -0.0019,  0.0155,  ..., -0.0194, -0.0021,  0.0165],
        [-0.0004, -0.0008,  0.0060,  ..., -0.0077, -0.0008,  0.0064],
        [-0.0006, -0.0012,  0.0092,  ..., -0.0117, -0.0013,  0.0098],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1937.4519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-178.5701, device='cuda:0')



h[100].sum tensor(-123.8225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(176.6887, device='cuda:0')



h[200].sum tensor(-108.4982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.6212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0252,  ..., 0.0000, 0.0000, 0.0268],
        [0.0000, 0.0000, 0.0586,  ..., 0.0000, 0.0000, 0.0623],
        [0.0000, 0.0000, 0.0255,  ..., 0.0000, 0.0000, 0.0271],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(174808.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0479, 0.1461,  ..., 0.1013, 0.0000, 0.0000],
        [0.0000, 0.0649, 0.1900,  ..., 0.1332, 0.0000, 0.0000],
        [0.0000, 0.0437, 0.1333,  ..., 0.0919, 0.0000, 0.0000],
        ...,
        [0.0027, 0.0000, 0.0078,  ..., 0.0003, 0.0000, 0.0022],
        [0.0027, 0.0000, 0.0078,  ..., 0.0003, 0.0000, 0.0022],
        [0.0027, 0.0000, 0.0078,  ..., 0.0003, 0.0000, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(857735.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1187.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-989.2427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2650.9819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35385.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1078.5094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0548],
        [-0.0744],
        [-0.0996],
        ...,
        [-0.0362],
        [-0.0361],
        [-0.0360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-131551.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096713.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096607., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0000, -0.0003],
        ...,
        [-0.0006, -0.0012,  0.0094,  ..., -0.0120, -0.0013,  0.0101],
        [-0.0006, -0.0011,  0.0089,  ..., -0.0113, -0.0012,  0.0094],
        [-0.0012, -0.0022,  0.0179,  ..., -0.0224, -0.0024,  0.0190]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1780.3625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-163.2258, device='cuda:0')



h[100].sum tensor(-113.3443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(161.5061, device='cuda:0')



h[200].sum tensor(-99.2992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-116.6549, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0083,  ..., 0.0000, 0.0000, 0.0088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0533,  ..., 0.0000, 0.0000, 0.0567],
        [0.0000, 0.0000, 0.0419,  ..., 0.0000, 0.0000, 0.0446],
        [0.0000, 0.0000, 0.0374,  ..., 0.0000, 0.0000, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164669.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.4924e-04, 1.5387e-02, 5.9516e-02,  ..., 3.7689e-02, 0.0000e+00,
         9.1482e-05],
        [1.8221e-03, 3.8594e-03, 2.3238e-02,  ..., 1.1233e-02, 0.0000e+00,
         1.1647e-03],
        [2.7300e-03, 1.1359e-04, 1.0578e-02,  ..., 1.9825e-03, 0.0000e+00,
         1.9201e-03],
        ...,
        [0.0000e+00, 8.1496e-02, 2.3687e-01,  ..., 1.6631e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.8894e-02, 2.3056e-01,  ..., 1.6180e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.6983e-02, 2.2585e-01,  ..., 1.5841e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(808476., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1341.2310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-890.9482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2486.0349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33989.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1017.3398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1568],
        [-0.1540],
        [-0.1435],
        ...,
        [ 0.0526],
        [ 0.0466],
        [ 0.0396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-156445.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096607., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096500.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012, -0.0022,  0.0183,  ..., -0.0229, -0.0025,  0.0195],
        [-0.0014, -0.0026,  0.0216,  ..., -0.0269, -0.0029,  0.0230],
        [-0.0006, -0.0012,  0.0094,  ..., -0.0120, -0.0013,  0.0100],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1862.7258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-172.9948, device='cuda:0')



h[100].sum tensor(-119.7432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(171.1722, device='cuda:0')



h[200].sum tensor(-104.8867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-123.6367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0691,  ..., 0.0000, 0.0000, 0.0735],
        [0.0000, 0.0000, 0.0608,  ..., 0.0000, 0.0000, 0.0647],
        [0.0000, 0.0000, 0.0644,  ..., 0.0000, 0.0000, 0.0685],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(171325.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1011, 0.2895,  ..., 0.2031, 0.0000, 0.0000],
        [0.0000, 0.0848, 0.2473,  ..., 0.1728, 0.0000, 0.0000],
        [0.0000, 0.0776, 0.2292,  ..., 0.1598, 0.0000, 0.0000],
        ...,
        [0.0030, 0.0000, 0.0088,  ..., 0.0004, 0.0000, 0.0024],
        [0.0030, 0.0000, 0.0088,  ..., 0.0004, 0.0000, 0.0024],
        [0.0030, 0.0000, 0.0088,  ..., 0.0004, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(841866.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1519.4666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-923.8312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2586.9436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35332.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1057.4548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0451],
        [-0.0811],
        [-0.1149],
        ...,
        [-0.0412],
        [-0.0410],
        [-0.0410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-158175.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096500.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096393.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0023, -0.0043,  0.0359,  ..., -0.0445, -0.0048,  0.0382],
        [-0.0004, -0.0008,  0.0062,  ..., -0.0081, -0.0009,  0.0066],
        [-0.0006, -0.0010,  0.0085,  ..., -0.0108, -0.0012,  0.0090],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [-0.0010, -0.0018,  0.0150,  ..., -0.0189, -0.0020,  0.0160],
        [-0.0004, -0.0008,  0.0065,  ..., -0.0084, -0.0009,  0.0069]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2223.2666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-211.8526, device='cuda:0')



h[100].sum tensor(-146.3451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(209.6206, device='cuda:0')



h[200].sum tensor(-128.1654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-151.4077, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0686,  ..., 0.0000, 0.0000, 0.0729],
        [0.0000, 0.0000, 0.0897,  ..., 0.0000, 0.0000, 0.0954],
        [0.0000, 0.0000, 0.0248,  ..., 0.0000, 0.0000, 0.0264],
        ...,
        [0.0000, 0.0000, 0.0256,  ..., 0.0000, 0.0000, 0.0272],
        [0.0000, 0.0000, 0.0291,  ..., 0.0000, 0.0000, 0.0310],
        [0.0000, 0.0000, 0.0474,  ..., 0.0000, 0.0000, 0.0504]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(209413.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1417, 0.3978,  ..., 0.2798, 0.0000, 0.0000],
        [0.0000, 0.1259, 0.3564,  ..., 0.2501, 0.0000, 0.0000],
        [0.0000, 0.0696, 0.2075,  ..., 0.1432, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0430, 0.1376,  ..., 0.0932, 0.0000, 0.0000],
        [0.0000, 0.0527, 0.1657,  ..., 0.1135, 0.0000, 0.0000],
        [0.0000, 0.0558, 0.1737,  ..., 0.1193, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1028944.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1395.6047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1180.9717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3173.8792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(42809.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1295.4597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1264],
        [ 0.0807],
        [ 0.0137],
        ...,
        [-0.0676],
        [-0.0627],
        [-0.0676]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-182460.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096393.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096287.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [-0.0004, -0.0008,  0.0062,  ..., -0.0081, -0.0009,  0.0066],
        [-0.0004, -0.0008,  0.0062,  ..., -0.0081, -0.0009,  0.0066],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1778.7664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-166.8894, device='cuda:0')



h[100].sum tensor(-115.0998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(165.1311, device='cuda:0')



h[200].sum tensor(-100.7837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.2732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0254,  ..., 0.0000, 0.0000, 0.0271],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0000, 0.0119],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0000, 0.0119],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168319.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0312, 0.1139,  ..., 0.0763, 0.0000, 0.0000],
        [0.0000, 0.0248, 0.0943,  ..., 0.0621, 0.0000, 0.0000],
        [0.0000, 0.0309, 0.1100,  ..., 0.0732, 0.0000, 0.0000],
        ...,
        [0.0033, 0.0000, 0.0096,  ..., 0.0004, 0.0000, 0.0027],
        [0.0033, 0.0000, 0.0096,  ..., 0.0004, 0.0000, 0.0027],
        [0.0033, 0.0000, 0.0096,  ..., 0.0004, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(820743.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1736.3267, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-866.1576, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2531.0378, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35155.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1040.7491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2240],
        [-0.2105],
        [-0.1752],
        ...,
        [-0.0457],
        [-0.0456],
        [-0.0458]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-185548.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096287.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096180.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0025, -0.0046,  0.0391,  ..., -0.0484, -0.0051,  0.0416],
        [-0.0008, -0.0015,  0.0128,  ..., -0.0162, -0.0017,  0.0136],
        [-0.0031, -0.0058,  0.0490,  ..., -0.0605, -0.0064,  0.0521],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1757.3735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-166.5132, device='cuda:0')



h[100].sum tensor(-114.3016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(164.7589, device='cuda:0')



h[200].sum tensor(-100.0669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.0044, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0762,  ..., 0.0000, 0.0000, 0.0810],
        [0.0000, 0.0000, 0.1659,  ..., 0.0000, 0.0000, 0.1762],
        [0.0000, 0.0000, 0.1540,  ..., 0.0000, 0.0000, 0.1636],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165767.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.7402e-01, 4.8982e-01,  ..., 3.4347e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5686e-01, 7.0502e-01,  ..., 4.9648e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.8559e-01, 7.7957e-01,  ..., 5.4951e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.4095e-03, 0.0000e+00, 9.9482e-03,  ..., 4.0398e-04, 0.0000e+00,
         2.7503e-03],
        [3.4095e-03, 0.0000e+00, 9.9482e-03,  ..., 4.0398e-04, 0.0000e+00,
         2.7503e-03],
        [3.4095e-03, 0.0000e+00, 9.9482e-03,  ..., 4.0398e-04, 0.0000e+00,
         2.7503e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(813618.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1753.0049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-828.6329, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2485.4192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35296.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1027.3838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1056],
        [ 0.2063],
        [ 0.2701],
        ...,
        [-0.0471],
        [-0.0469],
        [-0.0468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-207175.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096180.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096074.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [-0.0004, -0.0007,  0.0058,  ..., -0.0076, -0.0008,  0.0061],
        [-0.0013, -0.0024,  0.0206,  ..., -0.0257, -0.0027,  0.0219],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1596.3916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-150.4762, device='cuda:0')



h[100].sum tensor(-103.1631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(148.8908, device='cuda:0')



h[200].sum tensor(-90.2994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-107.5430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0104,  ..., 0.0000, 0.0000, 0.0111],
        [0.0000, 0.0000, 0.0359,  ..., 0.0000, 0.0000, 0.0382],
        [0.0000, 0.0000, 0.0546,  ..., 0.0000, 0.0000, 0.0581],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(155898.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0286, 0.1040,  ..., 0.0680, 0.0000, 0.0000],
        [0.0000, 0.0603, 0.1915,  ..., 0.1305, 0.0000, 0.0000],
        [0.0000, 0.0849, 0.2592,  ..., 0.1788, 0.0000, 0.0000],
        ...,
        [0.0035, 0.0000, 0.0103,  ..., 0.0004, 0.0000, 0.0028],
        [0.0035, 0.0000, 0.0103,  ..., 0.0004, 0.0000, 0.0028],
        [0.0035, 0.0000, 0.0103,  ..., 0.0004, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(774628.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1910.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-746.1887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2329.2583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33906.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-965.3442, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2143],
        [-0.2006],
        [-0.1894],
        ...,
        [-0.0487],
        [-0.0485],
        [-0.0484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-209888.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096074.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095967.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [-0.0004, -0.0008,  0.0068,  ..., -0.0088, -0.0009,  0.0072],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1634.7189, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-155.4959, device='cuda:0')



h[100].sum tensor(-106.7972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(153.8576, device='cuda:0')



h[200].sum tensor(-93.4636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-111.1305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0122,  ..., 0.0000, 0.0000, 0.0130],
        [0.0000, 0.0000, 0.0169,  ..., 0.0000, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160723.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.0379e-03, 5.5647e-03, 3.5573e-02,  ..., 1.8548e-02, 0.0000e+00,
         1.1947e-03],
        [7.6776e-04, 1.8657e-02, 7.8880e-02,  ..., 4.9800e-02, 0.0000e+00,
         0.0000e+00],
        [6.2639e-05, 2.8585e-02, 1.0903e-01,  ..., 7.1547e-02, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.6132e-03, 0.0000e+00, 1.0556e-02,  ..., 4.1418e-04, 0.0000e+00,
         2.9109e-03],
        [3.4777e-03, 0.0000e+00, 1.2556e-02,  ..., 1.8578e-03, 0.0000e+00,
         2.5489e-03],
        [2.1887e-03, 6.4268e-03, 3.4246e-02,  ..., 1.7398e-02, 0.0000e+00,
         1.4554e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(795643.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1974.5442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-768.2236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2401.3870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34964.6289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-996.3953, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2376],
        [-0.2576],
        [-0.2698],
        ...,
        [-0.0659],
        [-0.0823],
        [-0.0957]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-221814.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095967.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095861., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005, -0.0009,  0.0076,  ..., -0.0098, -0.0010,  0.0081],
        [-0.0020, -0.0039,  0.0334,  ..., -0.0414, -0.0043,  0.0355],
        [-0.0008, -0.0016,  0.0132,  ..., -0.0166, -0.0017,  0.0140],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1554.5522, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-148.1653, device='cuda:0')



h[100].sum tensor(-101.5470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(146.6043, device='cuda:0')



h[200].sum tensor(-88.8530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-105.8914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1136,  ..., 0.0000, 0.0000, 0.1207],
        [0.0000, 0.0000, 0.0769,  ..., 0.0000, 0.0000, 0.0818],
        [0.0000, 0.0000, 0.0909,  ..., 0.0000, 0.0000, 0.0966],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156267.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.5614e-01, 4.4879e-01,  ..., 3.1182e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6536e-01, 4.7522e-01,  ..., 3.3070e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.7731e-01, 5.0772e-01,  ..., 3.5375e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.6981e-03, 0.0000e+00, 1.0816e-02,  ..., 4.2052e-04, 0.0000e+00,
         2.9793e-03],
        [3.6981e-03, 0.0000e+00, 1.0816e-02,  ..., 4.2052e-04, 0.0000e+00,
         2.9793e-03],
        [3.6981e-03, 0.0000e+00, 1.0816e-02,  ..., 4.2052e-04, 0.0000e+00,
         2.9793e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(783065.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2013.8550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-722.3692, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2328.3254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34765.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-970.3169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0902],
        [ 0.1504],
        [ 0.1828],
        ...,
        [-0.0538],
        [-0.0534],
        [-0.0532]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-234801.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095861., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 300 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095754.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0026, -0.0051,  0.0438,  ..., -0.0541, -0.0056,  0.0465],
        [-0.0024, -0.0047,  0.0403,  ..., -0.0498, -0.0052,  0.0428],
        [-0.0019, -0.0037,  0.0322,  ..., -0.0398, -0.0041,  0.0342],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1675.5444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-162.8935, device='cuda:0')



h[100].sum tensor(-111.6901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(161.1773, device='cuda:0')



h[200].sum tensor(-97.7107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-116.4174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1732,  ..., 0.0000, 0.0000, 0.1840],
        [0.0000, 0.0000, 0.1612,  ..., 0.0000, 0.0000, 0.1712],
        [0.0000, 0.0000, 0.1361,  ..., 0.0000, 0.0000, 0.1446],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166328.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.0874e-01, 8.5513e-01,  ..., 5.9793e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.9297e-01, 8.1379e-01,  ..., 5.6879e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.6539e-01, 7.4163e-01,  ..., 5.1788e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.7764e-03, 0.0000e+00, 1.1056e-02,  ..., 4.2467e-04, 0.0000e+00,
         3.0401e-03],
        [3.7764e-03, 0.0000e+00, 1.1056e-02,  ..., 4.2467e-04, 0.0000e+00,
         3.0401e-03],
        [3.7764e-03, 0.0000e+00, 1.1056e-02,  ..., 4.2467e-04, 0.0000e+00,
         3.0401e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(813583.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2081.4546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-787.0219, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2482.9431, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35973.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1032.4844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7400],
        [ 0.7218],
        [ 0.6768],
        ...,
        [-0.0527],
        [-0.0525],
        [-0.0524]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-236611.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095754.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095648., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [-0.0010, -0.0019,  0.0164,  ..., -0.0206, -0.0021,  0.0174],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1622.3665, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-158.3624, device='cuda:0')



h[100].sum tensor(-108.5490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(156.6940, device='cuda:0')



h[200].sum tensor(-94.9457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.1791, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0224,  ..., 0.0000, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0203,  ..., 0.0000, 0.0000, 0.0216],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167551.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0074, 0.0420,  ..., 0.0225, 0.0000, 0.0013],
        [0.0005, 0.0260, 0.1012,  ..., 0.0649, 0.0000, 0.0000],
        [0.0000, 0.0336, 0.1255,  ..., 0.0824, 0.0000, 0.0000],
        ...,
        [0.0038, 0.0000, 0.0113,  ..., 0.0004, 0.0000, 0.0031],
        [0.0038, 0.0000, 0.0113,  ..., 0.0004, 0.0000, 0.0031],
        [0.0038, 0.0000, 0.0113,  ..., 0.0004, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(843058.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2066.0649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-784.6252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2497.6824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37321.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1041.3695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2585],
        [-0.2799],
        [-0.2946],
        ...,
        [-0.0539],
        [-0.0537],
        [-0.0536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-249743.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095648., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095541.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1510.4017, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-147.1540, device='cuda:0')



h[100].sum tensor(-100.7310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(145.6036, device='cuda:0')



h[200].sum tensor(-88.0915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-105.1686, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156315.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0000, 0.0115,  ..., 0.0004, 0.0000, 0.0032],
        [0.0038, 0.0000, 0.0144,  ..., 0.0025, 0.0000, 0.0027],
        [0.0035, 0.0003, 0.0189,  ..., 0.0058, 0.0000, 0.0021],
        ...,
        [0.0039, 0.0000, 0.0115,  ..., 0.0004, 0.0000, 0.0031],
        [0.0039, 0.0000, 0.0115,  ..., 0.0004, 0.0000, 0.0031],
        [0.0039, 0.0000, 0.0115,  ..., 0.0004, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(777537.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2221.2954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-693.6152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2322.3083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35085.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-972.7639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1725],
        [-0.2093],
        [-0.2510],
        ...,
        [-0.0549],
        [-0.0547],
        [-0.0546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-255063.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095541.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095435., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1784.2715, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-181.5063, device='cuda:0')



h[100].sum tensor(-123.5345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(179.5939, device='cuda:0')



h[200].sum tensor(-108.0142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-129.7197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(183647.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.9253e-03, 3.3277e-03, 2.8326e-02,  ..., 1.2342e-02, 0.0000e+00,
         1.5756e-03],
        [3.7569e-03, 9.6994e-05, 1.5816e-02,  ..., 3.4126e-03, 0.0000e+00,
         2.5453e-03],
        [2.3824e-03, 1.1271e-02, 5.0141e-02,  ..., 2.7699e-02, 0.0000e+00,
         1.5459e-03],
        ...,
        [3.9698e-03, 0.0000e+00, 1.1646e-02,  ..., 4.3279e-04, 0.0000e+00,
         3.1901e-03],
        [3.9698e-03, 0.0000e+00, 1.1646e-02,  ..., 4.3279e-04, 0.0000e+00,
         3.1901e-03],
        [3.9698e-03, 0.0000e+00, 1.1646e-02,  ..., 4.3279e-04, 0.0000e+00,
         3.1901e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(902780.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2146.5444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-886.1150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2745.3618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39885.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1141.9883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2673],
        [-0.2685],
        [-0.2696],
        ...,
        [-0.0604],
        [-0.0608],
        [-0.0613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-262102.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095435., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095328.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012, -0.0024,  0.0210,  ..., -0.0262, -0.0027,  0.0224],
        [-0.0024, -0.0048,  0.0430,  ..., -0.0529, -0.0054,  0.0456],
        [-0.0031, -0.0061,  0.0546,  ..., -0.0671, -0.0068,  0.0580],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1945.5403, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-201.7895, device='cuda:0')



h[100].sum tensor(-137.8381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(199.6635, device='cuda:0')



h[200].sum tensor(-120.4990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-144.2158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1030,  ..., 0.0000, 0.0000, 0.1095],
        [0.0000, 0.0000, 0.1446,  ..., 0.0000, 0.0000, 0.1537],
        [0.0000, 0.0000, 0.1831,  ..., 0.0000, 0.0000, 0.1945],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(204350.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0237e-01, 5.8421e-01,  ..., 4.0421e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.6141e-01, 7.4067e-01,  ..., 5.1392e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1098e-01, 8.7215e-01,  ..., 6.0607e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.0230e-03, 0.0000e+00, 1.1813e-02,  ..., 4.3394e-04, 0.0000e+00,
         3.2327e-03],
        [4.0230e-03, 0.0000e+00, 1.1813e-02,  ..., 4.3394e-04, 0.0000e+00,
         3.2327e-03],
        [4.0230e-03, 0.0000e+00, 1.1813e-02,  ..., 4.3394e-04, 0.0000e+00,
         3.2327e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1007326.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2059.1089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1027.3596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3064.4521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44118.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1272.0559, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3124],
        [ 0.4194],
        [ 0.5016],
        ...,
        [-0.0568],
        [-0.0565],
        [-0.0564]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-279423.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095328.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095221.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0005, -0.0009,  0.0079,  ..., -0.0103, -0.0010,  0.0085],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1547.3801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-156.6529, device='cuda:0')



h[100].sum tensor(-106.7663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(155.0025, device='cuda:0')



h[200].sum tensor(-93.3189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-111.9574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0348,  ..., 0.0000, 0.0000, 0.0371],
        [0.0000, 0.0000, 0.0064,  ..., 0.0000, 0.0000, 0.0068],
        [0.0000, 0.0000, 0.0197,  ..., 0.0000, 0.0000, 0.0211],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165922.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0340, 0.1301,  ..., 0.0849, 0.0000, 0.0000],
        [0.0000, 0.0253, 0.1043,  ..., 0.0665, 0.0000, 0.0000],
        [0.0000, 0.0330, 0.1281,  ..., 0.0835, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0000, 0.0120,  ..., 0.0004, 0.0000, 0.0033],
        [0.0041, 0.0000, 0.0120,  ..., 0.0004, 0.0000, 0.0033],
        [0.0041, 0.0000, 0.0120,  ..., 0.0004, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(831227., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2273.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-741.1981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2464.7288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37462.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1034.1855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3236],
        [-0.3313],
        [-0.3407],
        ...,
        [-0.0575],
        [-0.0573],
        [-0.0572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-273160.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095221.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095115.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0004, -0.0007,  0.0062,  ..., -0.0082, -0.0008,  0.0066],
        [-0.0004, -0.0007,  0.0062,  ..., -0.0082, -0.0008,  0.0066],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1301.5559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-128.8808, device='cuda:0')



h[100].sum tensor(-87.5429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(127.5229, device='cuda:0')



h[200].sum tensor(-76.5029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-92.1091, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0170,  ..., 0.0000, 0.0000, 0.0181],
        [0.0000, 0.0000, 0.0224,  ..., 0.0000, 0.0000, 0.0238],
        [0.0000, 0.0000, 0.0165,  ..., 0.0000, 0.0000, 0.0177],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(139397., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0224, 0.0960,  ..., 0.0605, 0.0000, 0.0000],
        [0.0000, 0.0301, 0.1203,  ..., 0.0779, 0.0000, 0.0000],
        [0.0002, 0.0264, 0.1098,  ..., 0.0705, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0000, 0.0121,  ..., 0.0004, 0.0000, 0.0033],
        [0.0041, 0.0000, 0.0121,  ..., 0.0004, 0.0000, 0.0033],
        [0.0041, 0.0000, 0.0121,  ..., 0.0004, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(695093.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2538.0537, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-550.4720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2053.8601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31973.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-866.0488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3296],
        [-0.3467],
        [-0.3564],
        ...,
        [-0.0583],
        [-0.0580],
        [-0.0579]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-251125.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095115.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095115.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0007, -0.0014,  0.0127,  ..., -0.0160, -0.0016,  0.0135],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1620.1636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-167.5285, device='cuda:0')



h[100].sum tensor(-113.9399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(165.7634, device='cuda:0')



h[200].sum tensor(-99.5709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.7299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0206,  ..., 0.0000, 0.0000, 0.0219],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0457,  ..., 0.0000, 0.0000, 0.0487],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(174211.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0238, 0.0985,  ..., 0.0621, 0.0000, 0.0000],
        [0.0002, 0.0196, 0.0851,  ..., 0.0525, 0.0000, 0.0000],
        [0.0000, 0.0348, 0.1299,  ..., 0.0842, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0000, 0.0121,  ..., 0.0004, 0.0000, 0.0033],
        [0.0041, 0.0000, 0.0121,  ..., 0.0004, 0.0000, 0.0033],
        [0.0041, 0.0000, 0.0121,  ..., 0.0004, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(860513.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2362.7156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-802.5634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2595.2925, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38366.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1082.6500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3578],
        [-0.3600],
        [-0.3642],
        ...,
        [-0.0583],
        [-0.0581],
        [-0.0580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-264637.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095115.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095008.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004, -0.0007,  0.0062,  ..., -0.0081, -0.0008,  0.0066],
        [-0.0004, -0.0007,  0.0062,  ..., -0.0081, -0.0008,  0.0066],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1510.7405, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-156.4983, device='cuda:0')



h[100].sum tensor(-106.0291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(154.8495, device='cuda:0')



h[200].sum tensor(-92.6409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-111.8469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0000, 0.0119],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0000, 0.0119],
        [0.0000, 0.0000, 0.0272,  ..., 0.0000, 0.0000, 0.0290],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(161127.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0122, 0.0670,  ..., 0.0399, 0.0000, 0.0000],
        [0.0005, 0.0171, 0.0812,  ..., 0.0499, 0.0000, 0.0000],
        [0.0000, 0.0306, 0.1186,  ..., 0.0762, 0.0000, 0.0000],
        ...,
        [0.0042, 0.0000, 0.0122,  ..., 0.0004, 0.0000, 0.0033],
        [0.0042, 0.0000, 0.0122,  ..., 0.0004, 0.0000, 0.0033],
        [0.0042, 0.0000, 0.0122,  ..., 0.0004, 0.0000, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(788492.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2415.3447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-695.4900, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2388.5852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36056.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1005.1365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2658],
        [-0.2754],
        [-0.2829],
        ...,
        [-0.0589],
        [-0.0587],
        [-0.0586]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-279147.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095008.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094902., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008, -0.0016,  0.0143,  ..., -0.0180, -0.0018,  0.0152],
        [-0.0015, -0.0031,  0.0283,  ..., -0.0351, -0.0035,  0.0301],
        [-0.0020, -0.0042,  0.0380,  ..., -0.0468, -0.0046,  0.0404],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1537.0511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-161.7497, device='cuda:0')



h[100].sum tensor(-109.4718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(160.0455, device='cuda:0')



h[200].sum tensor(-95.6314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-115.6000, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1068,  ..., 0.0000, 0.0000, 0.1136],
        [0.0000, 0.0000, 0.1018,  ..., 0.0000, 0.0000, 0.1083],
        [0.0000, 0.0000, 0.1186,  ..., 0.0000, 0.0000, 0.1261],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167900.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6799e-01, 4.9801e-01,  ..., 3.4232e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6625e-01, 4.9363e-01,  ..., 3.3928e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6315e-01, 4.8509e-01,  ..., 3.3333e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.6327e-03, 2.5892e-04, 2.2765e-02,  ..., 8.0175e-03, 0.0000e+00,
         1.7817e-03],
        [4.0024e-03, 0.0000e+00, 1.5808e-02,  ..., 2.9669e-03, 0.0000e+00,
         2.8322e-03],
        [4.1872e-03, 0.0000e+00, 1.2329e-02,  ..., 4.4215e-04, 0.0000e+00,
         3.3577e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(827199.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2487.0317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-748.3732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2495.5581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37164.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1043.2043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1865],
        [-0.1905],
        [-0.2092],
        ...,
        [-0.1250],
        [-0.1153],
        [-0.1207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-264757., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094902., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 450 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094795.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008, -0.0016,  0.0144,  ..., -0.0181, -0.0018,  0.0154],
        [-0.0013, -0.0028,  0.0252,  ..., -0.0312, -0.0031,  0.0268],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1434.5746, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-149.7798, device='cuda:0')



h[100].sum tensor(-101.9271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(148.2017, device='cuda:0')



h[200].sum tensor(-89.0244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-107.0452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0989,  ..., 0.0000, 0.0000, 0.1052],
        [0.0000, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0558],
        [0.0000, 0.0000, 0.0360,  ..., 0.0000, 0.0000, 0.0383],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158138.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.0704e-01, 3.3198e-01,  ..., 2.2575e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4571e-02, 2.6974e-01,  ..., 1.8206e-01, 0.0000e+00,
         0.0000e+00],
        [8.2834e-05, 5.6249e-02, 1.8866e-01,  ..., 1.2487e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.2208e-03, 0.0000e+00, 1.2431e-02,  ..., 4.4359e-04, 0.0000e+00,
         3.3821e-03],
        [4.2208e-03, 0.0000e+00, 1.2431e-02,  ..., 4.4359e-04, 0.0000e+00,
         3.3821e-03],
        [4.2208e-03, 0.0000e+00, 1.2431e-02,  ..., 4.4359e-04, 0.0000e+00,
         3.3821e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(779333., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2536.5396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-673.4905, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2341.8689, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35416.1914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-982.5726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3021],
        [-0.3131],
        [-0.3307],
        ...,
        [-0.0601],
        [-0.0599],
        [-0.0598]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-266054.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094795.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094689., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010, -0.0021,  0.0191,  ..., -0.0239, -0.0023,  0.0204],
        [-0.0017, -0.0036,  0.0334,  ..., -0.0412, -0.0040,  0.0355],
        [-0.0019, -0.0039,  0.0366,  ..., -0.0450, -0.0044,  0.0389],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1558.3557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-168.4924, device='cuda:0')



h[100].sum tensor(-113.9558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(166.7172, device='cuda:0')



h[200].sum tensor(-99.5123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-120.4189, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0770,  ..., 0.0000, 0.0000, 0.0819],
        [0.0000, 0.0000, 0.1063,  ..., 0.0000, 0.0000, 0.1130],
        [0.0000, 0.0000, 0.1216,  ..., 0.0000, 0.0000, 0.1292],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(179631.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.0623e-01, 3.3051e-01,  ..., 2.2450e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.3898e-01, 4.1933e-01,  ..., 2.8658e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5520e-01, 4.6389e-01,  ..., 3.1772e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.2474e-03, 0.0000e+00, 1.2519e-02,  ..., 4.4399e-04, 0.0000e+00,
         3.4046e-03],
        [4.2474e-03, 0.0000e+00, 1.2519e-02,  ..., 4.4399e-04, 0.0000e+00,
         3.4046e-03],
        [4.2474e-03, 0.0000e+00, 1.2519e-02,  ..., 4.4399e-04, 0.0000e+00,
         3.4046e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(909228.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2470.7896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-822.0782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2675.1440, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40552.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1118.0961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2843],
        [-0.2610],
        [-0.2518],
        ...,
        [-0.0606],
        [-0.0603],
        [-0.0603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-277419.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094689., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094582.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024, -0.0049,  0.0460,  ..., -0.0564, -0.0055,  0.0489],
        [-0.0027, -0.0056,  0.0522,  ..., -0.0639, -0.0062,  0.0554],
        [-0.0019, -0.0039,  0.0369,  ..., -0.0454, -0.0044,  0.0392],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0008, -0.0016,  0.0144,  ..., -0.0181, -0.0018,  0.0154]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1372.3815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-145.8599, device='cuda:0')



h[100].sum tensor(-98.9058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(144.3231, device='cuda:0')



h[200].sum tensor(-86.3540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-104.2438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1309,  ..., 0.0000, 0.0000, 0.1391],
        [0.0000, 0.0000, 0.1513,  ..., 0.0000, 0.0000, 0.1607],
        [0.0000, 0.0000, 0.1582,  ..., 0.0000, 0.0000, 0.1681],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0268,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0000, 0.0464,  ..., 0.0000, 0.0000, 0.0493]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159253.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.7399e-01, 5.1194e-01,  ..., 3.5062e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.0368e-01, 5.9402e-01,  ..., 4.0812e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.0897e-01, 6.0916e-01,  ..., 4.1872e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [1.8525e-03, 1.9912e-02, 8.0429e-02,  ..., 4.8287e-02, 0.0000e+00,
         8.7200e-04],
        [2.2323e-04, 5.6085e-02, 1.8772e-01,  ..., 1.2384e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4572e-02, 2.6952e-01,  ..., 1.8143e-01, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(801457.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2579.7505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-673.5393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2356.9929, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36395.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-990.1598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1933],
        [-0.1884],
        [-0.2055],
        ...,
        [-0.1406],
        [-0.1411],
        [-0.1364]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-271379.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094582.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094475.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0008, -0.0017,  0.0161,  ..., -0.0202, -0.0019,  0.0171],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1371.8899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-148.3872, device='cuda:0')



h[100].sum tensor(-100.1018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(146.8238, device='cuda:0')



h[200].sum tensor(-87.3822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-106.0500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0220,  ..., 0.0000, 0.0000, 0.0234],
        [0.0000, 0.0000, 0.0251,  ..., 0.0000, 0.0000, 0.0268],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160997.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0089, 0.0483,  ..., 0.0257, 0.0000, 0.0015],
        [0.0007, 0.0300, 0.1174,  ..., 0.0747, 0.0000, 0.0000],
        [0.0000, 0.0446, 0.1623,  ..., 0.1065, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0000, 0.0127,  ..., 0.0004, 0.0000, 0.0034],
        [0.0043, 0.0000, 0.0127,  ..., 0.0004, 0.0000, 0.0034],
        [0.0043, 0.0000, 0.0127,  ..., 0.0004, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(808935.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2612.9856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-685.0293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2384.5923, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36638.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1000.2213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3073],
        [-0.3248],
        [-0.3337],
        ...,
        [-0.0616],
        [-0.0614],
        [-0.0613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-271166.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094475.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094369.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0005, -0.0010,  0.0094,  ..., -0.0120, -0.0012,  0.0100],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1267.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-135.9936, device='cuda:0')



h[100].sum tensor(-91.8965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(134.5608, device='cuda:0')



h[200].sum tensor(-80.2048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-97.1925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0384,  ..., 0.0000, 0.0000, 0.0408],
        [0.0000, 0.0000, 0.0334,  ..., 0.0000, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0320,  ..., 0.0000, 0.0000, 0.0340],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0000, 0.0119]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(147599.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.3679e-02, 2.9536e-01,  ..., 1.9908e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.5023e-02, 2.7310e-01,  ..., 1.8369e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.5341e-02, 2.4488e-01,  ..., 1.6380e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.1276e-03, 0.0000e+00, 1.5767e-02,  ..., 2.5806e-03, 0.0000e+00,
         2.8928e-03],
        [2.9372e-03, 5.1269e-03, 3.4395e-02,  ..., 1.5710e-02, 0.0000e+00,
         1.7273e-03],
        [1.5190e-03, 2.3322e-02, 9.3903e-02,  ..., 5.7613e-02, 0.0000e+00,
         2.0045e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(733436.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2698.2183, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-585.4390, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2175.6875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33749.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-916.8336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2400],
        [-0.2494],
        [-0.2695],
        ...,
        [-0.0993],
        [-0.1261],
        [-0.1506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-265811.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094369.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094262.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0007, -0.0016,  0.0145,  ..., -0.0182, -0.0017,  0.0154],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1699.9254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-197.8961, device='cuda:0')



h[100].sum tensor(-133.0223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(195.8111, device='cuda:0')



h[200].sum tensor(-116.0770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-141.4332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0145,  ..., 0.0000, 0.0000, 0.0154],
        [0.0000, 0.0000, 0.0117,  ..., 0.0000, 0.0000, 0.0125],
        [0.0000, 0.0000, 0.0524,  ..., 0.0000, 0.0000, 0.0557],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(200233.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0168, 0.0784,  ..., 0.0470, 0.0000, 0.0000],
        [0.0007, 0.0196, 0.0862,  ..., 0.0524, 0.0000, 0.0000],
        [0.0000, 0.0352, 0.1316,  ..., 0.0843, 0.0000, 0.0000],
        ...,
        [0.0043, 0.0000, 0.0128,  ..., 0.0004, 0.0000, 0.0035],
        [0.0043, 0.0000, 0.0128,  ..., 0.0004, 0.0000, 0.0035],
        [0.0043, 0.0000, 0.0128,  ..., 0.0004, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(980558.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2375.6873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-956.0205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2989.6060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(43774.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1248.2161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3480],
        [-0.3388],
        [-0.3326],
        ...,
        [-0.0624],
        [-0.0621],
        [-0.0620]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-307853.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094262.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094155.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0019,  0.0180,  ..., -0.0224, -0.0021,  0.0192],
        [-0.0013, -0.0028,  0.0264,  ..., -0.0326, -0.0031,  0.0281],
        [-0.0032, -0.0069,  0.0671,  ..., -0.0818, -0.0078,  0.0713],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1248.8630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-137.6304, device='cuda:0')



h[100].sum tensor(-92.5881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(136.1803, device='cuda:0')



h[200].sum tensor(-80.7787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-98.3623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0610,  ..., 0.0000, 0.0000, 0.0649],
        [0.0000, 0.0000, 0.1484,  ..., 0.0000, 0.0000, 0.1576],
        [0.0000, 0.0000, 0.1715,  ..., 0.0000, 0.0000, 0.1822],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(147196.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.4805e-01, 4.4700e-01,  ..., 3.0486e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1756e-01, 6.3547e-01,  ..., 4.3617e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.6613e-01, 7.6648e-01,  ..., 5.2730e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.3508e-03, 0.0000e+00, 1.2869e-02,  ..., 4.4794e-04, 0.0000e+00,
         3.4811e-03],
        [4.3508e-03, 0.0000e+00, 1.2869e-02,  ..., 4.4794e-04, 0.0000e+00,
         3.4811e-03],
        [4.3508e-03, 0.0000e+00, 1.2869e-02,  ..., 4.4794e-04, 0.0000e+00,
         3.4811e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(729977.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2722.2603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-571.9625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2166.3528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33910.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-917.6491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1012],
        [-0.0510],
        [-0.0142],
        ...,
        [-0.0626],
        [-0.0623],
        [-0.0622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-277983.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094155.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094049.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005, -0.0010,  0.0096,  ..., -0.0123, -0.0012,  0.0103],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1377.2815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-158.2631, device='cuda:0')



h[100].sum tensor(-106.0683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(156.5956, device='cuda:0')



h[200].sum tensor(-92.5225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.1081, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0077,  ..., 0.0000, 0.0000, 0.0083],
        [0.0000, 0.0000, 0.0096,  ..., 0.0000, 0.0000, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167642.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0102, 0.0594,  ..., 0.0335, 0.0000, 0.0000],
        [0.0024, 0.0076, 0.0510,  ..., 0.0275, 0.0000, 0.0003],
        [0.0036, 0.0017, 0.0284,  ..., 0.0115, 0.0000, 0.0015],
        ...,
        [0.0044, 0.0000, 0.0129,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0129,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0129,  ..., 0.0004, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(838021.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2635.1206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-714.6445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2483.3345, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38338.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1046.6671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3379],
        [-0.3424],
        [-0.3485],
        ...,
        [-0.0639],
        [-0.0635],
        [-0.0632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-294997.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094049.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093942.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004, -0.0008,  0.0070,  ..., -0.0091, -0.0009,  0.0075],
        [-0.0009, -0.0019,  0.0184,  ..., -0.0229, -0.0021,  0.0196],
        [-0.0012, -0.0027,  0.0260,  ..., -0.0320, -0.0030,  0.0276],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1406.9716, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-165.3276, device='cuda:0')



h[100].sum tensor(-110.4351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(163.5857, device='cuda:0')



h[200].sum tensor(-96.3138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-118.1570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0486,  ..., 0.0000, 0.0000, 0.0517],
        [0.0000, 0.0000, 0.0590,  ..., 0.0000, 0.0000, 0.0628],
        [0.0000, 0.0000, 0.1039,  ..., 0.0000, 0.0000, 0.1105],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(173214.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 8.7533e-02, 2.8772e-01,  ..., 1.9419e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.1866e-01, 3.7176e-01,  ..., 2.5261e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6512e-01, 4.9700e-01,  ..., 3.3960e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.3808e-03, 0.0000e+00, 1.2967e-02,  ..., 4.4824e-04, 0.0000e+00,
         3.4963e-03],
        [4.3808e-03, 0.0000e+00, 1.2967e-02,  ..., 4.4824e-04, 0.0000e+00,
         3.4963e-03],
        [4.3808e-03, 0.0000e+00, 1.2967e-02,  ..., 4.4824e-04, 0.0000e+00,
         3.4963e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(858347.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2616.2126, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-754.4862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2569.4717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39111.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1080.5100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2871],
        [-0.2379],
        [-0.1863],
        ...,
        [-0.0641],
        [-0.0638],
        [-0.0635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-297334.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093942.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093836., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0020,  0.0191,  ..., -0.0237, -0.0022,  0.0203],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0007, -0.0016,  0.0150,  ..., -0.0188, -0.0017,  0.0160],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1172.0691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-133.4654, device='cuda:0')



h[100].sum tensor(-89.0167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.0593, device='cuda:0')



h[200].sum tensor(-77.6198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-95.3856, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0154,  ..., 0.0000, 0.0000, 0.0164],
        [0.0000, 0.0000, 0.0460,  ..., 0.0000, 0.0000, 0.0489],
        [0.0000, 0.0000, 0.0451,  ..., 0.0000, 0.0000, 0.0480],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(143770.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0534, 0.1840,  ..., 0.1207, 0.0000, 0.0000],
        [0.0000, 0.0849, 0.2739,  ..., 0.1837, 0.0000, 0.0000],
        [0.0000, 0.1039, 0.3279,  ..., 0.2215, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0000, 0.0130,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0130,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0130,  ..., 0.0004, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(715881.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2883.2505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-553.3824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2115.7703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32859.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-890.4357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2354],
        [-0.2418],
        [-0.2519],
        ...,
        [-0.0635],
        [-0.0632],
        [-0.0631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-253211.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093836., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 600 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093729.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003, -0.0006,  0.0060,  ..., -0.0079, -0.0007,  0.0064],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1388.9502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-167.7383, device='cuda:0')



h[100].sum tensor(-111.9004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(165.9711, device='cuda:0')



h[200].sum tensor(-97.5556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-119.8799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0291,  ..., 0.0000, 0.0000, 0.0310],
        [0.0000, 0.0000, 0.0169,  ..., 0.0000, 0.0000, 0.0180],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(180284.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0435, 0.1614,  ..., 0.1054, 0.0000, 0.0000],
        [0.0007, 0.0290, 0.1178,  ..., 0.0747, 0.0000, 0.0000],
        [0.0022, 0.0103, 0.0579,  ..., 0.0323, 0.0000, 0.0009],
        ...,
        [0.0044, 0.0000, 0.0131,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0131,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0131,  ..., 0.0004, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(910831., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2716.3787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-809.9238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2681.5916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40718.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1121.1108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3753],
        [-0.3739],
        [-0.3682],
        ...,
        [-0.0637],
        [-0.0634],
        [-0.0633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-278761.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093729.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093622.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0020, -0.0044,  0.0442,  ..., -0.0540, -0.0050,  0.0470],
        [-0.0022, -0.0049,  0.0488,  ..., -0.0595, -0.0055,  0.0519],
        [-0.0024, -0.0053,  0.0537,  ..., -0.0654, -0.0060,  0.0571],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1355.1675, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-165.2930, device='cuda:0')



h[100].sum tensor(-110.1765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(163.5515, device='cuda:0')



h[200].sum tensor(-96.0348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-118.1323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1248,  ..., 0.0000, 0.0000, 0.1326],
        [0.0000, 0.0000, 0.1724,  ..., 0.0000, 0.0000, 0.1831],
        [0.0000, 0.0000, 0.1515,  ..., 0.0000, 0.0000, 0.1610],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168608.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0994e-01, 6.1882e-01,  ..., 4.2375e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3338e-01, 6.8083e-01,  ..., 4.6672e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1498e-01, 6.2919e-01,  ..., 4.3064e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.4121e-03, 0.0000e+00, 1.3085e-02,  ..., 4.4682e-04, 0.0000e+00,
         3.5199e-03],
        [4.4121e-03, 0.0000e+00, 1.3085e-02,  ..., 4.4682e-04, 0.0000e+00,
         3.5199e-03],
        [4.4121e-03, 0.0000e+00, 1.3085e-02,  ..., 4.4682e-04, 0.0000e+00,
         3.5199e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(824153.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2673.4739, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-713.1085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2495.5623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38001.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1053.9354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1855],
        [-0.1787],
        [-0.1928],
        ...,
        [-0.0639],
        [-0.0636],
        [-0.0635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-302804.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093622.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093515.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1158.2432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2688, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-137.2323, device='cuda:0')



h[100].sum tensor(-91.6496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(135.7865, device='cuda:0')



h[200].sum tensor(-79.8711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-98.0778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0000, 0.0169],
        [0.0000, 0.0000, 0.0493,  ..., 0.0000, 0.0000, 0.0524],
        ...,
        [0.0000, 0.0000, 0.0229,  ..., 0.0000, 0.0000, 0.0243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(149642.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7214e-03, 1.9706e-02, 8.0434e-02,  ..., 4.7599e-02, 0.0000e+00,
         7.7781e-04],
        [2.9227e-04, 5.1468e-02, 1.7444e-01,  ..., 1.1342e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.0050e-01, 3.1223e-01,  ..., 2.0962e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.2874e-04, 4.5471e-02, 1.6236e-01,  ..., 1.0554e-01, 0.0000e+00,
         0.0000e+00],
        [1.8888e-03, 1.6732e-02, 7.4068e-02,  ..., 4.3427e-02, 0.0000e+00,
         7.7781e-04],
        [3.5523e-03, 2.5650e-03, 2.8317e-02,  ..., 1.1223e-02, 0.0000e+00,
         1.7624e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(746357.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2789.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-581.0992, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2201.8809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34652.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-932.5205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2720],
        [-0.2657],
        [-0.2537],
        ...,
        [-0.1819],
        [-0.1683],
        [-0.1414]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-282903.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093515.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093409.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0012, -0.0027,  0.0275,  ..., -0.0337, -0.0031,  0.0292],
        [-0.0006, -0.0014,  0.0140,  ..., -0.0174, -0.0016,  0.0148],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1224.0094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-150.0580, device='cuda:0')



h[100].sum tensor(-99.9450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(148.4771, device='cuda:0')



h[200].sum tensor(-87.0842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-107.2441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1170,  ..., 0.0000, 0.0000, 0.1244],
        [0.0000, 0.0000, 0.0488,  ..., 0.0000, 0.0000, 0.0519],
        [0.0000, 0.0000, 0.0444,  ..., 0.0000, 0.0000, 0.0473],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158945.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6335e-01, 4.9018e-01,  ..., 3.3389e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.0855e-01, 3.3856e-01,  ..., 2.2832e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.0124e-02, 2.9000e-01,  ..., 1.9475e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.4276e-03, 0.0000e+00, 1.3144e-02,  ..., 4.4665e-04, 0.0000e+00,
         3.5239e-03],
        [4.4276e-03, 0.0000e+00, 1.3144e-02,  ..., 4.4665e-04, 0.0000e+00,
         3.5239e-03],
        [4.4276e-03, 0.0000e+00, 1.3144e-02,  ..., 4.4665e-04, 0.0000e+00,
         3.5239e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(786365.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2796.6099, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-647.4707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2346.6772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36223.0820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-990.4227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1906],
        [-0.2529],
        [-0.3054],
        ...,
        [-0.0643],
        [-0.0640],
        [-0.0639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-284710.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093409.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093302.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0014, -0.0032,  0.0323,  ..., -0.0395, -0.0036,  0.0343],
        [-0.0013, -0.0030,  0.0301,  ..., -0.0369, -0.0033,  0.0320],
        [-0.0019, -0.0044,  0.0446,  ..., -0.0544, -0.0049,  0.0474],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1325.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-169.0615, device='cuda:0')



h[100].sum tensor(-112.3474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(167.2803, device='cuda:0')



h[200].sum tensor(-97.8725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-120.8256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0796,  ..., 0.0000, 0.0000, 0.0846],
        [0.0000, 0.0000, 0.1618,  ..., 0.0000, 0.0000, 0.1720],
        [0.0000, 0.0000, 0.1545,  ..., 0.0000, 0.0000, 0.1642],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(175623.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.5679e-01, 4.7559e-01,  ..., 3.2405e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1997e-01, 6.4631e-01,  ..., 4.4254e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2996e-01, 6.7384e-01,  ..., 4.6163e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [4.4342e-03, 0.0000e+00, 1.3171e-02,  ..., 4.4707e-04, 0.0000e+00,
         3.5310e-03],
        [4.4342e-03, 0.0000e+00, 1.3171e-02,  ..., 4.4707e-04, 0.0000e+00,
         3.5310e-03],
        [4.4342e-03, 0.0000e+00, 1.3171e-02,  ..., 4.4707e-04, 0.0000e+00,
         3.5310e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(861061.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2769.0483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-770.2722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2607.0591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(38983.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1092.7310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1328],
        [-0.0976],
        [-0.0808],
        ...,
        [-0.0699],
        [-0.0820],
        [-0.1054]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-284066.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093302.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093195.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1111.3667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-137.0248, device='cuda:0')



h[100].sum tensor(-91.0484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(135.5812, device='cuda:0')



h[200].sum tensor(-79.3029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-97.9295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150659.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0000, 0.0133,  ..., 0.0005, 0.0000, 0.0036],
        [0.0045, 0.0000, 0.0133,  ..., 0.0005, 0.0000, 0.0036],
        [0.0045, 0.0000, 0.0133,  ..., 0.0005, 0.0000, 0.0036],
        ...,
        [0.0044, 0.0000, 0.0132,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0132,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0132,  ..., 0.0004, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(755716.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2822.1401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-583.5787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2216.7117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35178.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-939.9354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2876],
        [-0.2773],
        [-0.2707],
        ...,
        [-0.0646],
        [-0.0643],
        [-0.0642]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-290052.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093195.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093089.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0003, -0.0006,  0.0057,  ..., -0.0075, -0.0007,  0.0061],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1323.7117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-175.4995, device='cuda:0')



h[100].sum tensor(-115.8670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(173.6505, device='cuda:0')



h[200].sum tensor(-100.9009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-125.4267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0102,  ..., 0.0000, 0.0000, 0.0109],
        [0.0000, 0.0000, 0.0222,  ..., 0.0000, 0.0000, 0.0237],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(180399.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0077, 0.0540,  ..., 0.0295, 0.0000, 0.0002],
        [0.0012, 0.0191, 0.0903,  ..., 0.0552, 0.0000, 0.0002],
        [0.0003, 0.0352, 0.1398,  ..., 0.0902, 0.0000, 0.0000],
        ...,
        [0.0044, 0.0000, 0.0132,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0132,  ..., 0.0004, 0.0000, 0.0035],
        [0.0044, 0.0000, 0.0132,  ..., 0.0004, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(886052.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2680.8887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-799.1918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2678.6675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(40228.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1124.3479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3863],
        [-0.3904],
        [-0.3942],
        ...,
        [-0.0649],
        [-0.0646],
        [-0.0644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-301842.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093089.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092982.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [-0.0004, -0.0010,  0.0105,  ..., -0.0133, -0.0012,  0.0112],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1156.9995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-142.4326, device='cuda:0')



h[100].sum tensor(-94.1303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.9320, device='cuda:0')



h[200].sum tensor(-81.9565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-101.7943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0204,  ..., 0.0000, 0.0000, 0.0217],
        [0.0000, 0.0000, 0.0105,  ..., 0.0000, 0.0000, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(153171.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.2774e-03, 0.0000e+00, 1.3338e-02,  ..., 2.0631e-05, 0.0000e+00,
         3.1850e-03],
        [3.4022e-03, 3.5950e-03, 2.9292e-02,  ..., 1.1094e-02, 0.0000e+00,
         1.9270e-03],
        [2.2133e-03, 1.0144e-02, 5.1391e-02,  ..., 2.6553e-02, 0.0000e+00,
         1.0235e-03],
        ...,
        [7.9572e-04, 2.7570e-02, 1.0830e-01,  ..., 6.6737e-02, 0.0000e+00,
         0.0000e+00],
        [2.0425e-03, 1.3711e-02, 6.6471e-02,  ..., 3.7271e-02, 0.0000e+00,
         1.4865e-04],
        [3.4446e-03, 3.0596e-03, 2.8715e-02,  ..., 1.0814e-02, 0.0000e+00,
         1.5789e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(765303.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2841.4421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-628.7516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2282.1362, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35371.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-948.1096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1357],
        [-0.1698],
        [-0.2108],
        ...,
        [-0.1623],
        [-0.1412],
        [-0.1146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-274898.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092982.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092875.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1261.2875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-156.0536, device='cuda:0')



h[100].sum tensor(-102.8907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(154.4094, device='cuda:0')



h[200].sum tensor(-89.5671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-111.5290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0327,  ..., 0.0000, 0.0000, 0.0348],
        [0.0000, 0.0000, 0.0445,  ..., 0.0000, 0.0000, 0.0473],
        [0.0000, 0.0000, 0.0325,  ..., 0.0000, 0.0000, 0.0346],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163141.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0999, 0.3036,  ..., 0.2019, 0.0000, 0.0000],
        [0.0000, 0.1056, 0.3200,  ..., 0.2135, 0.0000, 0.0000],
        [0.0000, 0.0870, 0.2697,  ..., 0.1784, 0.0000, 0.0000],
        ...,
        [0.0041, 0.0000, 0.0133,  ..., 0.0000, 0.0000, 0.0028],
        [0.0041, 0.0000, 0.0133,  ..., 0.0000, 0.0000, 0.0028],
        [0.0041, 0.0000, 0.0133,  ..., 0.0000, 0.0000, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(813725.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2839.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-722.8113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2460.3518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37139.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1005.9880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2295],
        [ 0.1617],
        [ 0.0798],
        ...,
        [-0.0646],
        [-0.0644],
        [-0.0643]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-260110.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092875.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092768.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1178.7177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-137.3880, device='cuda:0')



h[100].sum tensor(-90.8044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(135.9405, device='cuda:0')



h[200].sum tensor(-79.0310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-98.1890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(143565.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0000, 0.0172,  ..., 0.0021, 0.0000, 0.0022],
        [0.0037, 0.0002, 0.0208,  ..., 0.0043, 0.0000, 0.0014],
        [0.0031, 0.0045, 0.0351,  ..., 0.0143, 0.0000, 0.0009],
        ...,
        [0.0039, 0.0000, 0.0133,  ..., 0.0000, 0.0000, 0.0025],
        [0.0039, 0.0000, 0.0133,  ..., 0.0000, 0.0000, 0.0025],
        [0.0039, 0.0000, 0.0133,  ..., 0.0000, 0.0000, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(699803.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2994.3374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-605.0378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2178.3171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(32490.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-876.6103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2405],
        [-0.2633],
        [-0.2896],
        ...,
        [-0.0639],
        [-0.0640],
        [-0.0647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-234502.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092768.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 750 loss: tensor(0.0854, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092662., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0011, -0.0026,  0.0279,  ..., -0.0341, -0.0030,  0.0297],
        [-0.0021, -0.0049,  0.0533,  ..., -0.0645, -0.0056,  0.0566],
        [-0.0015, -0.0037,  0.0393,  ..., -0.0477, -0.0042,  0.0418],
        ...,
        [-0.0009, -0.0022,  0.0237,  ..., -0.0290, -0.0025,  0.0252],
        [-0.0003, -0.0008,  0.0079,  ..., -0.0101, -0.0009,  0.0085],
        [-0.0003, -0.0007,  0.0076,  ..., -0.0097, -0.0008,  0.0080]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1321.7682, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-159.3148, device='cuda:0')



h[100].sum tensor(-105.2420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(157.6363, device='cuda:0')



h[200].sum tensor(-91.5795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.8598, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1769,  ..., 0.0000, 0.0000, 0.1879],
        [0.0000, 0.0000, 0.1487,  ..., 0.0000, 0.0000, 0.1579],
        [0.0000, 0.0000, 0.1698,  ..., 0.0000, 0.0000, 0.1803],
        ...,
        [0.0000, 0.0000, 0.0397,  ..., 0.0000, 0.0000, 0.0422],
        [0.0000, 0.0000, 0.0564,  ..., 0.0000, 0.0000, 0.0599],
        [0.0000, 0.0000, 0.0208,  ..., 0.0000, 0.0000, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164116.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2832, 0.7916,  ..., 0.5402, 0.0000, 0.0000],
        [0.0000, 0.2731, 0.7645,  ..., 0.5214, 0.0000, 0.0000],
        [0.0000, 0.2703, 0.7561,  ..., 0.5155, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0856, 0.2664,  ..., 0.1753, 0.0000, 0.0000],
        [0.0000, 0.0726, 0.2290,  ..., 0.1490, 0.0000, 0.0000],
        [0.0009, 0.0370, 0.1286,  ..., 0.0789, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(802776., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2908.0464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-772.9208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2517.1475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36427.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1001.5491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2452],
        [ 0.2646],
        [ 0.2673],
        ...,
        [-0.0527],
        [-0.0833],
        [-0.1060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-230709.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092662., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092555.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0010, -0.0025,  0.0270,  ..., -0.0329, -0.0028,  0.0287],
        [-0.0012, -0.0029,  0.0311,  ..., -0.0378, -0.0033,  0.0330],
        [-0.0006, -0.0014,  0.0144,  ..., -0.0178, -0.0015,  0.0152],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1509.4587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-191.0147, device='cuda:0')



h[100].sum tensor(-125.9453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(189.0022, device='cuda:0')



h[200].sum tensor(-109.5745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-136.5152, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1346,  ..., 0.0000, 0.0000, 0.1430],
        [0.0000, 0.0000, 0.1201,  ..., 0.0000, 0.0000, 0.1275],
        [0.0000, 0.0000, 0.1189,  ..., 0.0000, 0.0000, 0.1263],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(202559.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1946, 0.5507,  ..., 0.3721, 0.0000, 0.0000],
        [0.0000, 0.1994, 0.5650,  ..., 0.3822, 0.0000, 0.0000],
        [0.0000, 0.1977, 0.5616,  ..., 0.3798, 0.0000, 0.0000],
        ...,
        [0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020],
        [0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020],
        [0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1029043.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2779.7290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1077.3733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3136.6255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44701.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1234.2175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0886],
        [ 0.0978],
        [ 0.0779],
        ...,
        [-0.0639],
        [-0.0638],
        [-0.0638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-206782.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092555.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092555.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1338.7495, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-160.5480, device='cuda:0')



h[100].sum tensor(-105.4818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(158.8564, device='cuda:0')



h[200].sum tensor(-91.7709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-114.7411, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166178.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020],
        [0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020],
        [0.0036, 0.0000, 0.0135,  ..., 0.0000, 0.0000, 0.0020],
        ...,
        [0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020],
        [0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020],
        [0.0036, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(824613.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2905.3999, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-808.8040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2567.3711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36992.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1007.9557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1005],
        [-0.1125],
        [-0.1302],
        ...,
        [-0.0616],
        [-0.0614],
        [-0.0613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-211219.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092555.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092448.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0003, -0.0007,  0.0069,  ..., -0.0088, -0.0008,  0.0073],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1310.9996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-152.9767, device='cuda:0')



h[100].sum tensor(-100.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(151.3649, device='cuda:0')



h[200].sum tensor(-87.5241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-109.3300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0251,  ..., 0.0000, 0.0000, 0.0266],
        [0.0000, 0.0000, 0.0124,  ..., 0.0000, 0.0000, 0.0131],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158408.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0450, 0.1512,  ..., 0.0938, 0.0000, 0.0000],
        [0.0015, 0.0224, 0.0867,  ..., 0.0487, 0.0000, 0.0000],
        [0.0028, 0.0067, 0.0381,  ..., 0.0152, 0.0000, 0.0006],
        ...,
        [0.0034, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0018],
        [0.0034, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0018],
        [0.0034, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(782553.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2967.4722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-762.5721, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2462.4731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35611.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-959.4854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1564],
        [-0.1769],
        [-0.1877],
        ...,
        [-0.0612],
        [-0.0609],
        [-0.0608]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-209805.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092448.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092342.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0021,  0.0232,  ..., -0.0283, -0.0024,  0.0246],
        [-0.0003, -0.0008,  0.0086,  ..., -0.0108, -0.0009,  0.0091],
        [-0.0009, -0.0021,  0.0232,  ..., -0.0283, -0.0024,  0.0246],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1262.9264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-141.6519, device='cuda:0')



h[100].sum tensor(-93.3658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.1595, device='cuda:0')



h[200].sum tensor(-81.1991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-101.2364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0499,  ..., 0.0000, 0.0000, 0.0530],
        [0.0000, 0.0000, 0.0909,  ..., 0.0000, 0.0000, 0.0965],
        [0.0000, 0.0000, 0.0272,  ..., 0.0000, 0.0000, 0.0289],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(149632.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.7660e-02, 2.3805e-01,  ..., 1.5372e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.4738e-02, 2.8165e-01,  ..., 1.8397e-01, 0.0000e+00,
         0.0000e+00],
        [7.0452e-05, 6.2161e-02, 1.9273e-01,  ..., 1.2198e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.3266e-03, 0.0000e+00, 1.3399e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5532e-03],
        [3.3266e-03, 0.0000e+00, 1.3399e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5532e-03],
        [3.3266e-03, 0.0000e+00, 1.3399e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5532e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(743973., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3032.5522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-718.9960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2340.8555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33605.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-896.1169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1260],
        [-0.1220],
        [-0.1355],
        ...,
        [-0.0614],
        [-0.0615],
        [-0.0616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-182360.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092342.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092236., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0003, -0.0008,  0.0089,  ..., -0.0111, -0.0009,  0.0094],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1463.6768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-179.0443, device='cuda:0')



h[100].sum tensor(-117.7206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(177.1579, device='cuda:0')



h[200].sum tensor(-102.3609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-127.9601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0378,  ..., 0.0000, 0.0000, 0.0401],
        [0.0000, 0.0000, 0.0184,  ..., 0.0000, 0.0000, 0.0196],
        [0.0000, 0.0000, 0.0142,  ..., 0.0000, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(181664.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0418, 0.1399,  ..., 0.0849, 0.0000, 0.0000],
        [0.0007, 0.0336, 0.1160,  ..., 0.0682, 0.0000, 0.0000],
        [0.0015, 0.0248, 0.0911,  ..., 0.0508, 0.0000, 0.0000],
        ...,
        [0.0032, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0014],
        [0.0032, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0014],
        [0.0032, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(887853.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2995.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-967.7251, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2856.4785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39016.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1093.6709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1894],
        [-0.1960],
        [-0.2048],
        ...,
        [-0.0591],
        [-0.0590],
        [-0.0590]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-170213.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092236., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092129.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004, -0.0010,  0.0105,  ..., -0.0131, -0.0011,  0.0112],
        [-0.0003, -0.0007,  0.0074,  ..., -0.0094, -0.0008,  0.0078],
        [-0.0003, -0.0007,  0.0074,  ..., -0.0094, -0.0008,  0.0078],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1519.7819, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-190.1907, device='cuda:0')



h[100].sum tensor(-124.6051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(188.1868, device='cuda:0')



h[200].sum tensor(-108.3266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-135.9263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0586,  ..., 0.0000, 0.0000, 0.0622],
        [0.0000, 0.0000, 0.0490,  ..., 0.0000, 0.0000, 0.0520],
        [0.0000, 0.0000, 0.0226,  ..., 0.0000, 0.0000, 0.0240],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(196131.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1063, 0.3114,  ..., 0.2039, 0.0000, 0.0000],
        [0.0000, 0.0840, 0.2514,  ..., 0.1621, 0.0000, 0.0000],
        [0.0005, 0.0525, 0.1651,  ..., 0.1020, 0.0000, 0.0000],
        ...,
        [0.0031, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0012],
        [0.0031, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0012],
        [0.0031, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(987065.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2913.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1086.7319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-3094.7368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(42625.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1181.3521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0309],
        [-0.0746],
        [-0.1172],
        ...,
        [-0.0581],
        [-0.0579],
        [-0.0578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-152908.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092129.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092023.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0003, -0.0007,  0.0076,  ..., -0.0096, -0.0008,  0.0081],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1320.8912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-149.9701, device='cuda:0')



h[100].sum tensor(-98.3253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(148.3901, device='cuda:0')



h[200].sum tensor(-85.4638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-107.1813, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0000, 0.0080],
        [0.0000, 0.0000, 0.0380,  ..., 0.0000, 0.0000, 0.0403],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158777.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5555e-03, 2.4307e-02, 8.5789e-02,  ..., 4.6307e-02, 0.0000e+00,
         0.0000e+00],
        [9.7445e-04, 3.9674e-02, 1.2741e-01,  ..., 7.5303e-02, 0.0000e+00,
         0.0000e+00],
        [1.6151e-04, 7.2312e-02, 2.1527e-01,  ..., 1.3641e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [3.0237e-03, 0.0000e+00, 1.3442e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.0443e-03],
        [3.0237e-03, 0.0000e+00, 1.3442e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.0443e-03],
        [3.0237e-03, 0.0000e+00, 1.3442e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.0443e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(796459.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3091.7046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-822.1812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2523.1758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35259.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-943.9539, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0861],
        [-0.0700],
        [-0.0531],
        ...,
        [-0.0574],
        [-0.0573],
        [-0.0574]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-146266.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092023.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091917., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005, -0.0014,  0.0156,  ..., -0.0191, -0.0016,  0.0165],
        [-0.0006, -0.0015,  0.0165,  ..., -0.0202, -0.0017,  0.0175],
        [-0.0007, -0.0018,  0.0199,  ..., -0.0243, -0.0020,  0.0212],
        ...,
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0005,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1329.8851, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-152.0660, device='cuda:0')



h[100].sum tensor(-99.2428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(150.4639, device='cuda:0')



h[200].sum tensor(-86.2449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-108.6792, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0335,  ..., 0.0000, 0.0000, 0.0355],
        [0.0000, 0.0000, 0.0662,  ..., 0.0000, 0.0000, 0.0702],
        [0.0000, 0.0000, 0.0742,  ..., 0.0000, 0.0000, 0.0787],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(157216.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0709, 0.2121,  ..., 0.1340, 0.0000, 0.0000],
        [0.0000, 0.0904, 0.2644,  ..., 0.1703, 0.0000, 0.0000],
        [0.0000, 0.0942, 0.2743,  ..., 0.1772, 0.0000, 0.0000],
        ...,
        [0.0029, 0.0000, 0.0135,  ..., 0.0000, 0.0000, 0.0009],
        [0.0029, 0.0000, 0.0135,  ..., 0.0000, 0.0000, 0.0009],
        [0.0029, 0.0000, 0.0135,  ..., 0.0000, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(779300.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3102.7151, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-820.1516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2508.2339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34555.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-932.2131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0890],
        [-0.0854],
        [-0.0906],
        ...,
        [-0.0561],
        [-0.0559],
        [-0.0559]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-139600.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091917., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091810.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1334.5680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-152.7106, device='cuda:0')



h[100].sum tensor(-99.8316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(151.1017, device='cuda:0')



h[200].sum tensor(-86.7401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-109.1398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158964.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6573e-03, 5.1584e-03, 2.8402e-02,  ..., 6.8920e-03, 0.0000e+00,
         1.1326e-04],
        [2.8286e-03, 1.6378e-03, 1.8275e-02,  ..., 7.5436e-04, 0.0000e+00,
         3.4362e-04],
        [2.7335e-03, 4.2420e-03, 2.6201e-02,  ..., 6.2076e-03, 0.0000e+00,
         3.4362e-04],
        ...,
        [2.8626e-03, 7.4154e-05, 1.3466e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.7829e-04],
        [2.8626e-03, 7.4154e-05, 1.3466e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.7829e-04],
        [2.8626e-03, 7.4154e-05, 1.3466e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.7829e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(796421.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3090.6704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-842.7443, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2543.0864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35129.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-940.9730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0488],
        [-0.0926],
        [-0.1222],
        ...,
        [-0.0549],
        [-0.0547],
        [-0.0546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-131954.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091810.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 900 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091704.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1360.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-158.8949, device='cuda:0')



h[100].sum tensor(-103.6885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(157.2208, device='cuda:0')



h[200].sum tensor(-90.0740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-113.5597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163381., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8138e-03, 7.7949e-04, 1.5650e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.6558e-04],
        [2.8253e-03, 4.0426e-04, 1.4241e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.1572e-04],
        [2.8463e-03, 3.3948e-04, 1.4003e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.4960e-04],
        ...,
        [1.6254e-03, 2.3281e-02, 7.7255e-02,  ..., 4.0798e-02, 0.0000e+00,
         8.5962e-05],
        [2.5465e-03, 5.5193e-03, 2.8583e-02,  ..., 8.1866e-03, 0.0000e+00,
         3.3300e-04],
        [2.7922e-03, 2.1394e-04, 1.3476e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.6600e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(815053.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3120.8435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-886.9597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2622.6982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(35480.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-962.6545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1384],
        [-0.1356],
        [-0.1376],
        ...,
        [-0.0205],
        [-0.0507],
        [-0.0623]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-114540.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091704.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091598.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1310.9872, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-149.2187, device='cuda:0')



h[100].sum tensor(-96.9658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(147.6465, device='cuda:0')



h[200].sum tensor(-84.2180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-106.6442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(153285.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0069, 0.0325,  ..., 0.0096, 0.0000, 0.0001],
        [0.0027, 0.0017, 0.0177,  ..., 0.0004, 0.0000, 0.0003],
        [0.0028, 0.0005, 0.0140,  ..., 0.0000, 0.0000, 0.0005],
        ...,
        [0.0027, 0.0004, 0.0135,  ..., 0.0000, 0.0000, 0.0006],
        [0.0027, 0.0004, 0.0135,  ..., 0.0000, 0.0000, 0.0006],
        [0.0027, 0.0004, 0.0135,  ..., 0.0000, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(766325.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3147.7920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-815.7499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2470.6870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(33869.9023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-902.0450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1192],
        [-0.1240],
        [-0.1225],
        ...,
        [-0.0528],
        [-0.0527],
        [-0.0527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-119328.7109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091598.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091492.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009, -0.0024,  0.0287,  ..., -0.0347, -0.0028,  0.0305],
        [-0.0018, -0.0048,  0.0567,  ..., -0.0679, -0.0055,  0.0602],
        [-0.0018, -0.0048,  0.0571,  ..., -0.0684, -0.0055,  0.0606],
        ...,
        [-0.0005, -0.0013,  0.0147,  ..., -0.0180, -0.0015,  0.0156],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1330.5723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.1086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-154.1822, device='cuda:0')



h[100].sum tensor(-100.3434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(152.5577, device='cuda:0')



h[200].sum tensor(-87.1350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-110.1916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1296,  ..., 0.0000, 0.0000, 0.1375],
        [0.0000, 0.0000, 0.1675,  ..., 0.0000, 0.0000, 0.1778],
        [0.0000, 0.0000, 0.2166,  ..., 0.0000, 0.0000, 0.2298],
        ...,
        [0.0000, 0.0000, 0.0302,  ..., 0.0000, 0.0000, 0.0320],
        [0.0000, 0.0000, 0.0258,  ..., 0.0000, 0.0000, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159282.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.1283e-01, 5.7308e-01,  ..., 3.8411e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.8145e-01, 7.5001e-01,  ..., 5.0731e-01, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3552e-01, 8.8922e-01,  ..., 6.0420e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [8.1528e-06, 8.6233e-02, 2.4673e-01,  ..., 1.5710e-01, 0.0000e+00,
         0.0000e+00],
        [1.8538e-04, 7.2805e-02, 2.0998e-01,  ..., 1.3156e-01, 0.0000e+00,
         0.0000e+00],
        [5.4822e-04, 4.7094e-02, 1.4017e-01,  ..., 8.3053e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(798841.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3128.2234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-869.1857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2574.2251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(34928.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-936.6836, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.4965],
        [0.5589],
        [0.5814],
        ...,
        [0.1528],
        [0.1346],
        [0.1059]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-107928.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091492.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091386., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [-0.0003, -0.0008,  0.0095,  ..., -0.0118, -0.0009,  0.0101]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1244.0951, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-135.2579, device='cuda:0')



h[100].sum tensor(-88.0350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(133.8328, device='cuda:0')



h[200].sum tensor(-76.4321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-96.6667, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0000, 0.0100],
        [0.0000, 0.0000, 0.0170,  ..., 0.0000, 0.0000, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(141733.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5587e-03, 2.7580e-03, 1.9913e-02,  ..., 1.4074e-03, 0.0000e+00,
         2.2070e-05],
        [2.5938e-03, 1.1596e-03, 1.4889e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3476e-04],
        [2.6228e-03, 7.7229e-04, 1.3686e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3603e-04],
        ...,
        [2.3913e-03, 5.6745e-03, 2.7457e-02,  ..., 7.1099e-03, 0.0000e+00,
         2.0204e-04],
        [1.9218e-03, 1.7733e-02, 6.0946e-02,  ..., 2.7823e-02, 0.0000e+00,
         0.0000e+00],
        [1.4229e-03, 3.1242e-02, 9.8677e-02,  ..., 5.4031e-02, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(714603., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3169.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-747.9392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2306.7305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31486.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-823.0775, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1195],
        [-0.1185],
        [-0.1178],
        ...,
        [-0.0637],
        [-0.0592],
        [-0.0483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-100916.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091386., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091386., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0015, -0.0040,  0.0478,  ..., -0.0573, -0.0046,  0.0507],
        [-0.0014, -0.0038,  0.0455,  ..., -0.0545, -0.0044,  0.0482],
        [-0.0020, -0.0054,  0.0649,  ..., -0.0776, -0.0062,  0.0689],
        ...,
        [-0.0006, -0.0016,  0.0186,  ..., -0.0226, -0.0018,  0.0197],
        [-0.0003, -0.0008,  0.0091,  ..., -0.0113, -0.0009,  0.0096],
        [-0.0003, -0.0009,  0.0104,  ..., -0.0129, -0.0010,  0.0111]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1394.5363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-170.7486, device='cuda:0')



h[100].sum tensor(-110.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(168.9496, device='cuda:0')



h[200].sum tensor(-96.1560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-122.0313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1875,  ..., 0.0000, 0.0000, 0.1990],
        [0.0000, 0.0000, 0.2360,  ..., 0.0000, 0.0000, 0.2504],
        [0.0000, 0.0000, 0.1343,  ..., 0.0000, 0.0000, 0.1424],
        ...,
        [0.0000, 0.0000, 0.0559,  ..., 0.0000, 0.0000, 0.0593],
        [0.0000, 0.0000, 0.0533,  ..., 0.0000, 0.0000, 0.0566],
        [0.0000, 0.0000, 0.0244,  ..., 0.0000, 0.0000, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(172353.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3153, 0.8345,  ..., 0.5660, 0.0000, 0.0000],
        [0.0000, 0.3496, 0.9228,  ..., 0.6275, 0.0000, 0.0000],
        [0.0000, 0.2702, 0.7193,  ..., 0.4858, 0.0000, 0.0000],
        ...,
        [0.0000, 0.1070, 0.3038,  ..., 0.1965, 0.0000, 0.0000],
        [0.0000, 0.1014, 0.2888,  ..., 0.1861, 0.0000, 0.0000],
        [0.0000, 0.0843, 0.2433,  ..., 0.1545, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(860772.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3090.9639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-973.1321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2786.1709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(37300.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1018.6018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.6698],
        [0.6606],
        [0.5598],
        ...,
        [0.1302],
        [0.1156],
        [0.1006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-102261.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091386., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091280., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0002, -0.0005,  0.0056,  ..., -0.0072, -0.0006,  0.0059],
        [-0.0007, -0.0018,  0.0215,  ..., -0.0261, -0.0021,  0.0228],
        [-0.0003, -0.0007,  0.0084,  ..., -0.0105, -0.0008,  0.0089],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1113.8531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.3728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-104.8306, device='cuda:0')



h[100].sum tensor(-68.5139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.7261, device='cuda:0')



h[200].sum tensor(-59.4724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-74.9207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0789,  ..., 0.0000, 0.0000, 0.0837],
        [0.0000, 0.0000, 0.0369,  ..., 0.0000, 0.0000, 0.0391],
        [0.0000, 0.0000, 0.0382,  ..., 0.0000, 0.0000, 0.0405],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(116245.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.4577e-02, 2.6824e-01,  ..., 1.7154e-01, 0.0000e+00,
         0.0000e+00],
        [9.5545e-05, 8.1039e-02, 2.3470e-01,  ..., 1.4810e-01, 0.0000e+00,
         0.0000e+00],
        [2.4649e-04, 7.6632e-02, 2.2216e-01,  ..., 1.3938e-01, 0.0000e+00,
         0.0000e+00],
        ...,
        [2.4918e-03, 9.1210e-04, 1.3600e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3164e-04],
        [2.4918e-03, 9.1211e-04, 1.3600e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3164e-04],
        [2.4918e-03, 9.1210e-04, 1.3600e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3164e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(604370.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3203.4482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-572.6367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1913.2861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(26408.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-653.1943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0020],
        [ 0.0018],
        [-0.0023],
        ...,
        [-0.0500],
        [-0.0498],
        [-0.0498]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-81872.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091280., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091173.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-679.8860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(26153.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0011, 0.0137,  ..., 0.0000, 0.0000, 0.0003],
        [0.0025, 0.0011, 0.0137,  ..., 0.0000, 0.0000, 0.0003],
        [0.0025, 0.0011, 0.0138,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0024, 0.0011, 0.0136,  ..., 0.0000, 0.0000, 0.0003],
        [0.0024, 0.0011, 0.0136,  ..., 0.0000, 0.0000, 0.0003],
        [0.0024, 0.0011, 0.0136,  ..., 0.0000, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208513.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3352.0129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(78.8428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-506.1881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9638.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-69.9820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0660],
        [-0.0665],
        [-0.0673],
        ...,
        [-0.0495],
        [-0.0493],
        [-0.0493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-57227.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091173.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091067.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-687.9866, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(26044.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0012, 0.0138,  ..., 0.0000, 0.0000, 0.0003],
        [0.0024, 0.0012, 0.0138,  ..., 0.0000, 0.0000, 0.0003],
        [0.0024, 0.0012, 0.0138,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0024, 0.0012, 0.0137,  ..., 0.0000, 0.0000, 0.0003],
        [0.0024, 0.0012, 0.0137,  ..., 0.0000, 0.0000, 0.0003],
        [0.0024, 0.0012, 0.0137,  ..., 0.0000, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208517.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3327.5762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(74.4843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-510.4210, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9578.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-67.6043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0651],
        [-0.0655],
        [-0.0663],
        ...,
        [-0.0490],
        [-0.0489],
        [-0.0488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-55738.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091067.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090961.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-695.3356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25952.7637, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0013, 0.0138,  ..., 0.0000, 0.0000, 0.0003],
        [0.0023, 0.0013, 0.0138,  ..., 0.0000, 0.0000, 0.0003],
        [0.0024, 0.0014, 0.0138,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0023, 0.0013, 0.0137,  ..., 0.0000, 0.0000, 0.0002],
        [0.0023, 0.0013, 0.0137,  ..., 0.0000, 0.0000, 0.0002],
        [0.0023, 0.0013, 0.0137,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208582.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3302.4197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(69.8559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-514.9423, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9525.8418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-64.9248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0641],
        [-0.0645],
        [-0.0653],
        ...,
        [-0.0485],
        [-0.0483],
        [-0.0483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-54210.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090961.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090855.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-702.0021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25871.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0015, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0023, 0.0015, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0023, 0.0015, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0022, 0.0015, 0.0137,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0015, 0.0137,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0015, 0.0137,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208688.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3276.3799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(65.4322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-519.2677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9478.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-62.3194, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0631],
        [-0.0635],
        [-0.0643],
        ...,
        [-0.0479],
        [-0.0478],
        [-0.0478]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-52673.7461, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090855.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 1050 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090749.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-708.0487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25801.7051, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0016, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0016, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0016, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0022, 0.0016, 0.0137,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0016, 0.0137,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0016, 0.0137,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208770.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3246.8232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(61.0212, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-523.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9431.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-59.9463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0622],
        [-0.0626],
        [-0.0634],
        ...,
        [-0.0474],
        [-0.0473],
        [-0.0473]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-51325.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090749.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090643.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-713.5327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25741.9512, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0017, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0017, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0017, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0021, 0.0017, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0017, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0017, 0.0138,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208855.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3215.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(56.5332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-527.7361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9383.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-57.7819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0614],
        [-0.0619],
        [-0.0626],
        ...,
        [-0.0470],
        [-0.0468],
        [-0.0468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-50178.6797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090643.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090643.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-713.5327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25741.9512, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0017, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0017, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0022, 0.0017, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0021, 0.0017, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0017, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0017, 0.0138,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208855.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3215.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(56.5332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-527.7361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9383.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-57.7819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0614],
        [-0.0619],
        [-0.0626],
        ...,
        [-0.0470],
        [-0.0468],
        [-0.0468]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-50178.6797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090643.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090537., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-718.5057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25687.5410, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0018, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0018, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0018, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0021, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208939.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3186.4929, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(52.4635, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-531.5664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9340.4521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-55.8189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0608],
        [-0.0612],
        [-0.0619],
        ...,
        [-0.0466],
        [-0.0464],
        [-0.0464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-49178.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090537., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090431., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-723.0153, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25637.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0019, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0019, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0021, 0.0019, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0020, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0001],
        [0.0020, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0001],
        [0.0020, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209010.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3159.4072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.7736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-535.0341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9301.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-54.0389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0602],
        [-0.0605],
        [-0.0612],
        ...,
        [-0.0462],
        [-0.0461],
        [-0.0460]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-48295.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090431., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090324.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-727.1038, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25592.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0019, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0020, 0.0019, 0.0139,  ..., 0.0000, 0.0000, 0.0002],
        [0.0020, 0.0019, 0.0140,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0020, 0.0019, 0.0138,  ..., 0.0000, 0.0000, 0.0001],
        [0.0020, 0.0019, 0.0138,  ..., 0.0000, 0.0000, 0.0001],
        [0.0020, 0.0019, 0.0138,  ..., 0.0000, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209070.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3135.2371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(45.4283, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-538.1727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9267.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-52.4248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0596],
        [-0.0600],
        [-0.0607],
        ...,
        [-0.0459],
        [-0.0457],
        [-0.0457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-47460.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090324.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090218.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-730.8101, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25551.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0020, 0.0139,  ..., 0.0000, 0.0000, 0.0001],
        [0.0020, 0.0020, 0.0139,  ..., 0.0000, 0.0000, 0.0001],
        [0.0020, 0.0020, 0.0140,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0019, 0.0020, 0.0138,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0020, 0.0138,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0020, 0.0138,  ..., 0.0000, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209133.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3114.1064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(42.3957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-541.0126, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9236.2598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-50.9613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0591],
        [-0.0595],
        [-0.0602],
        ...,
        [-0.0456],
        [-0.0454],
        [-0.0454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-46696.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090218.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090112.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-734.1702, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25514.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0021, 0.0140,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0021, 0.0140,  ..., 0.0000, 0.0000, 0.0001],
        [0.0020, 0.0021, 0.0140,  ..., 0.0000, 0.0000, 0.0001],
        ...,
        [0.0019, 0.0020, 0.0139,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0020, 0.0139,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0020, 0.0139,  ..., 0.0000, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209197.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3095.2268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(39.6468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-543.5815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9206.1553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-49.6344, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0587],
        [-0.0591],
        [-0.0597],
        ...,
        [-0.0453],
        [-0.0452],
        [-0.0451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-45979.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090112.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090006.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-737.2158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25479.8574, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0021, 0.0140,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0021, 0.0140,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0021, 0.0140,  ..., 0.0000, 0.0000, 0.0001],
        ...,
        [0.0019, 0.0021, 0.0139,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0021, 0.0139,  ..., 0.0000, 0.0000, 0.0001],
        [0.0019, 0.0021, 0.0139,  ..., 0.0000, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209240.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3075.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.1552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-545.9047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9178.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-48.4315, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0583],
        [-0.0586],
        [-0.0593],
        ...,
        [-0.0450],
        [-0.0449],
        [-0.0449]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-45334.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090006.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089900.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-739.9763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25448.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8783e-03, 2.1698e-03, 1.3982e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.1360e-04],
        [1.8783e-03, 2.1698e-03, 1.3982e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.1360e-04],
        [1.8964e-03, 2.1812e-03, 1.4027e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.2410e-04],
        ...,
        [1.8424e-03, 2.1471e-03, 1.3892e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.2313e-05],
        [1.8424e-03, 2.1471e-03, 1.3892e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.2313e-05],
        [1.8424e-03, 2.1471e-03, 1.3892e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.2313e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209293.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3059.4028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(34.8972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-548.0048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9155.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-47.3410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0579],
        [-0.0583],
        [-0.0589],
        ...,
        [-0.0448],
        [-0.0447],
        [-0.0447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-44783.0352, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089900.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 1200 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089794.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-742.4778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25421.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8521e-03, 2.2141e-03, 1.3996e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.0375e-04],
        [1.8521e-03, 2.2141e-03, 1.3996e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.0375e-04],
        [1.8701e-03, 2.2254e-03, 1.4041e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.1416e-04],
        ...,
        [1.8160e-03, 2.1913e-03, 1.3906e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.2938e-05],
        [1.8160e-03, 2.1913e-03, 1.3906e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.2938e-05],
        [1.8160e-03, 2.1913e-03, 1.3906e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.2938e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209303.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3045.1118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.0033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-549.9089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9134.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-46.3554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0576],
        [-0.0580],
        [-0.0586],
        ...,
        [-0.0446],
        [-0.0445],
        [-0.0445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-44259.9102, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089794.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089688.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-744.7440, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25397.9512, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8266e-03, 2.2527e-03, 1.4009e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.5260e-05],
        [1.8266e-03, 2.2527e-03, 1.4009e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.5260e-05],
        [1.8448e-03, 2.2642e-03, 1.4055e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.0562e-04],
        ...,
        [1.7902e-03, 2.2298e-03, 1.3919e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.4540e-05],
        [1.7902e-03, 2.2298e-03, 1.3919e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.4540e-05],
        [1.7902e-03, 2.2298e-03, 1.3919e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.4540e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209312.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3032.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.4687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-551.6368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9118.7510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-45.4652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0574],
        [-0.0577],
        [-0.0584],
        ...,
        [-0.0444],
        [-0.0443],
        [-0.0443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-43799.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089688.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089688.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-744.7440, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25397.9512, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8266e-03, 2.2527e-03, 1.4009e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.5260e-05],
        [1.8266e-03, 2.2527e-03, 1.4009e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.5260e-05],
        [1.8448e-03, 2.2642e-03, 1.4055e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.0562e-04],
        ...,
        [1.7902e-03, 2.2298e-03, 1.3919e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.4540e-05],
        [1.7902e-03, 2.2298e-03, 1.3919e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.4540e-05],
        [1.7902e-03, 2.2298e-03, 1.3919e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.4540e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209312.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3032.1230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.4687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-551.6368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9118.7510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-45.4652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0574],
        [-0.0577],
        [-0.0584],
        ...,
        [-0.0444],
        [-0.0443],
        [-0.0443]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-43799.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089688.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089582.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-746.7980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25376.5449, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8062e-03, 2.2883e-03, 1.4021e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.6017e-05],
        [1.8062e-03, 2.2883e-03, 1.4021e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.6017e-05],
        [1.8241e-03, 2.3000e-03, 1.4066e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.6455e-05],
        ...,
        [1.7702e-03, 2.2650e-03, 1.3931e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.5140e-05],
        [1.7702e-03, 2.2650e-03, 1.3931e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.5140e-05],
        [1.7702e-03, 2.2650e-03, 1.3931e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.5140e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209312.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3021.3792, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(30.0782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-553.1970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9104.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-44.6583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0571],
        [-0.0575],
        [-0.0581],
        ...,
        [-0.0443],
        [-0.0442],
        [-0.0441]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-43342.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089582.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089476.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-748.6590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25356.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7857e-03, 2.3173e-03, 1.4030e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.9090e-05],
        [1.7857e-03, 2.3173e-03, 1.4030e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.9090e-05],
        [1.8038e-03, 2.3290e-03, 1.4075e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.9363e-05],
        ...,
        [1.7496e-03, 2.2940e-03, 1.3940e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8543e-05],
        [1.7496e-03, 2.2940e-03, 1.3940e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8543e-05],
        [1.7496e-03, 2.2940e-03, 1.3940e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8543e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209301.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3010.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(28.8184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.6052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9090.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-43.9269, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0569],
        [-0.0573],
        [-0.0579],
        ...,
        [-0.0441],
        [-0.0440],
        [-0.0440]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-42990.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089476.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089369.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-750.3449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25338.9121, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7664e-03, 2.3448e-03, 1.4039e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.1952e-05],
        [1.7664e-03, 2.3448e-03, 1.4039e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.1952e-05],
        [1.7845e-03, 2.3566e-03, 1.4084e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.2225e-05],
        ...,
        [1.7303e-03, 2.3211e-03, 1.3948e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.1406e-05],
        [1.7303e-03, 2.3211e-03, 1.3948e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.1406e-05],
        [1.7303e-03, 2.3211e-03, 1.3948e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.1406e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209285.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3000.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(27.6770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-555.8756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9078.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-43.2641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0567],
        [-0.0571],
        [-0.0577],
        ...,
        [-0.0440],
        [-0.0439],
        [-0.0438]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-42629.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089369.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089263.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-751.8721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25322.3711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7516e-03, 2.3701e-03, 1.4048e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.4872e-05],
        [1.7516e-03, 2.3701e-03, 1.4048e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.4872e-05],
        [1.7697e-03, 2.3820e-03, 1.4093e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.5057e-05],
        ...,
        [1.7155e-03, 2.3464e-03, 1.3957e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.4502e-05],
        [1.7155e-03, 2.3464e-03, 1.3957e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.4502e-05],
        [1.7155e-03, 2.3464e-03, 1.3957e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.4502e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209263.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2992.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.6430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-557.0211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9066.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-42.6635, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0566],
        [-0.0569],
        [-0.0575],
        ...,
        [-0.0439],
        [-0.0438],
        [-0.0437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-42344.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089263.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089157.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-753.2556, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25307.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7394e-03, 2.3921e-03, 1.4054e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0795e-05],
        [1.7394e-03, 2.3921e-03, 1.4054e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0795e-05],
        [1.7572e-03, 2.4040e-03, 1.4099e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.1053e-05],
        ...,
        [1.7036e-03, 2.3682e-03, 1.3964e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.0279e-05],
        [1.7036e-03, 2.3682e-03, 1.3964e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.0279e-05],
        [1.7036e-03, 2.3682e-03, 1.3964e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.0279e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209248.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2985.5449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(25.7061, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-558.0536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9056.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-42.1191, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0564],
        [-0.0567],
        [-0.0573],
        ...,
        [-0.0438],
        [-0.0436],
        [-0.0436]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-42086.8242, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089157.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089051.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-754.5088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25293.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7235e-03, 2.4123e-03, 1.4060e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.4557e-05],
        [1.7235e-03, 2.4123e-03, 1.4060e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.4557e-05],
        [1.7415e-03, 2.4243e-03, 1.4106e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.4873e-05],
        ...,
        [1.6875e-03, 2.3882e-03, 1.3970e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3925e-05],
        [1.6875e-03, 2.3882e-03, 1.3970e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3925e-05],
        [1.6875e-03, 2.3882e-03, 1.3970e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3925e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209214.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2978.4321, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.8577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-558.9833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9046.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-41.6257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0563],
        [-0.0566],
        [-0.0572],
        ...,
        [-0.0437],
        [-0.0435],
        [-0.0435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-41816.5547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089051.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088945.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-755.6434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25280.2246, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7135e-03, 2.4282e-03, 1.4066e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.0357e-05],
        [1.7135e-03, 2.4282e-03, 1.4066e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.0357e-05],
        [1.7313e-03, 2.4404e-03, 1.4112e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0721e-05],
        ...,
        [1.6778e-03, 2.4039e-03, 1.3976e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.9630e-05],
        [1.6778e-03, 2.4039e-03, 1.3976e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.9630e-05],
        [1.6778e-03, 2.4039e-03, 1.3976e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.9630e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209190.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2972.4861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.0892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-559.8200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9037.6982, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-41.1784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0561],
        [-0.0565],
        [-0.0571],
        ...,
        [-0.0436],
        [-0.0434],
        [-0.0434]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-41597.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088945.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 1350 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088839.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-756.6713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25268.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7026e-03, 2.4447e-03, 1.4071e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.5949e-05],
        [1.7026e-03, 2.4447e-03, 1.4071e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.5949e-05],
        [1.7205e-03, 2.4568e-03, 1.4116e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.6218e-05],
        ...,
        [1.6668e-03, 2.4203e-03, 1.3980e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5411e-05],
        [1.6668e-03, 2.4203e-03, 1.3980e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5411e-05],
        [1.6668e-03, 2.4203e-03, 1.3980e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5411e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209160.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2965.9800, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(23.3931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-560.5725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9029.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-40.7733, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0560],
        [-0.0564],
        [-0.0570],
        ...,
        [-0.0435],
        [-0.0434],
        [-0.0433]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-41427.0977, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088839.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088733.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-757.6018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25257.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6907e-03, 2.4619e-03, 1.4075e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.1572e-05],
        [1.6907e-03, 2.4619e-03, 1.4075e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.1587e-05],
        [1.7087e-03, 2.4742e-03, 1.4120e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.1897e-05],
        ...,
        [1.6548e-03, 2.4379e-03, 1.3985e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0685e-05],
        [1.6548e-03, 2.4379e-03, 1.3985e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0685e-05],
        [1.6548e-03, 2.4379e-03, 1.3985e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0685e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209123.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2960.5073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(22.7626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-561.2486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9022.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-40.4059, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0559],
        [-0.0563],
        [-0.0569],
        ...,
        [-0.0434],
        [-0.0433],
        [-0.0433]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-41246.0547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088733.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088627., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-758.4445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25247.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6818e-03, 2.4753e-03, 1.4079e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.8917e-05],
        [1.6818e-03, 2.4753e-03, 1.4079e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.8917e-05],
        [1.6997e-03, 2.4875e-03, 1.4124e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.9249e-05],
        ...,
        [1.6462e-03, 2.4510e-03, 1.3988e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8257e-05],
        [1.6462e-03, 2.4510e-03, 1.3988e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8257e-05],
        [1.6462e-03, 2.4510e-03, 1.3988e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8257e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209092.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2954.7217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(22.1916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-561.8554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9015.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-40.0730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0559],
        [-0.0562],
        [-0.0568],
        ...,
        [-0.0434],
        [-0.0432],
        [-0.0432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-41099.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088627., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088520.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-759.2075, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25237.5449, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6723e-03, 2.4866e-03, 1.4080e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.5176e-05],
        [1.6723e-03, 2.4866e-03, 1.4080e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.5176e-05],
        [1.6901e-03, 2.4990e-03, 1.4125e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.5303e-05],
        ...,
        [1.6366e-03, 2.4619e-03, 1.3989e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4923e-05],
        [1.6366e-03, 2.4619e-03, 1.3989e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4923e-05],
        [1.6366e-03, 2.4619e-03, 1.3989e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4923e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209033.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2949.4111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.6746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.3996, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9008.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-39.7713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0558],
        [-0.0561],
        [-0.0567],
        ...,
        [-0.0433],
        [-0.0432],
        [-0.0431]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40971.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088520.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088414.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-759.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25228.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6642e-03, 2.4981e-03, 1.4083e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3093e-05],
        [1.6642e-03, 2.4981e-03, 1.4083e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3093e-05],
        [1.6820e-03, 2.5104e-03, 1.4129e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3336e-05],
        ...,
        [1.6287e-03, 2.4735e-03, 1.3992e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.2608e-05],
        [1.6287e-03, 2.4735e-03, 1.3992e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.2608e-05],
        [1.6287e-03, 2.4735e-03, 1.3992e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.2608e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(209005.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2944.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.2063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.8870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9002.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-39.4979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0557],
        [-0.0560],
        [-0.0566],
        ...,
        [-0.0432],
        [-0.0431],
        [-0.0431]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40840.2422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088414.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088308.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-760.5237, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25220.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6555e-03, 2.5065e-03, 1.4085e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2041e-05],
        [1.6555e-03, 2.5065e-03, 1.4085e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2041e-05],
        [1.6733e-03, 2.5189e-03, 1.4130e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.2235e-05],
        ...,
        [1.6201e-03, 2.4816e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.1654e-05],
        [1.6201e-03, 2.4816e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.1654e-05],
        [1.6201e-03, 2.4816e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.1654e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208944.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2939.8364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(20.7823, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-563.3229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8997.2568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-39.2499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0557],
        [-0.0560],
        [-0.0566],
        ...,
        [-0.0432],
        [-0.0431],
        [-0.0430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40746.9023, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088308.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088202.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-761.0899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25212.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6488e-03, 2.5162e-03, 1.4085e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.8408e-05],
        [1.6488e-03, 2.5162e-03, 1.4085e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.8408e-05],
        [1.6667e-03, 2.5286e-03, 1.4131e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.8567e-05],
        ...,
        [1.6130e-03, 2.4914e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.0896e-06],
        [1.6130e-03, 2.4914e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.0896e-06],
        [1.6130e-03, 2.4914e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.0896e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208907.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2935.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(20.3983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-563.7123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8991.9824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-39.0252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0556],
        [-0.0559],
        [-0.0565],
        ...,
        [-0.0432],
        [-0.0430],
        [-0.0430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40642.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088202.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088096., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-761.6025, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25205.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6434e-03, 2.5219e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6489e-05],
        [1.6434e-03, 2.5219e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6452e-05],
        [1.6612e-03, 2.5344e-03, 1.4133e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.6701e-05],
        ...,
        [1.6080e-03, 2.4969e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8395e-06],
        [1.6080e-03, 2.4969e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8395e-06],
        [1.6080e-03, 2.4969e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8395e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208858.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2932.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(20.0506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-564.0594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8987.0400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.8214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0559],
        [-0.0565],
        ...,
        [-0.0431],
        [-0.0430],
        [-0.0430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40563.4180, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088096., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087989.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-762.0663, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25198.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6360e-03, 2.5298e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3630e-05],
        [1.6360e-03, 2.5298e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3630e-05],
        [1.6537e-03, 2.5423e-03, 1.4133e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3928e-05],
        ...,
        [1.6005e-03, 2.5049e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0340e-06],
        [1.6005e-03, 2.5049e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0340e-06],
        [1.6005e-03, 2.5049e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0340e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208817.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2928.0747, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.7357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-564.3683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8982.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.6367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0431],
        [-0.0430],
        [-0.0429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40492.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087989.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087883.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-762.4862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25192.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6313e-03, 2.5364e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.2905e-05],
        [1.6313e-03, 2.5364e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.2905e-05],
        [1.6490e-03, 2.5490e-03, 1.4133e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3102e-05],
        ...,
        [1.5960e-03, 2.5112e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5118e-06],
        [1.5960e-03, 2.5112e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5118e-06],
        [1.5960e-03, 2.5112e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5118e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208775.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2925.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.4507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-564.6427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8978.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.4691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0430],
        [-0.0429],
        [-0.0429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40421.3789, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087883.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 1500 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087777.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-762.8661, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25186.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6249e-03, 2.5423e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3006e-05],
        [1.6249e-03, 2.5423e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.3006e-05],
        [1.6426e-03, 2.5549e-03, 1.4134e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3124e-05],
        ...,
        [1.5894e-03, 2.5171e-03, 1.3998e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.7699e-06],
        [1.5894e-03, 2.5171e-03, 1.3998e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.7699e-06],
        [1.5894e-03, 2.5171e-03, 1.3998e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.7699e-06]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208722.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2921.5176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.1924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-564.8856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8974.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.3172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0430],
        [-0.0429],
        [-0.0429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40393.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087777.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087671.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-763.2100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25181.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6199e-03, 2.5467e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.1151e-05],
        [1.6199e-03, 2.5467e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.1151e-05],
        [1.6378e-03, 2.5591e-03, 1.4133e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.1337e-05],
        ...,
        [1.5840e-03, 2.5219e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.8066e-07],
        [1.5840e-03, 2.5219e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.8066e-07],
        [1.5840e-03, 2.5219e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.8066e-07]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208663.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2918.3994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.9586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.1002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8969.9365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.1793, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0558],
        [-0.0563],
        ...,
        [-0.0430],
        [-0.0429],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40339.1992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087671.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087565., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-763.5211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25175.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6172e-03, 2.5512e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0165e-05],
        [1.6172e-03, 2.5512e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0165e-05],
        [1.6349e-03, 2.5636e-03, 1.4134e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0506e-05],
        ...,
        [1.5818e-03, 2.5264e-03, 1.3998e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5818e-03, 2.5264e-03, 1.3998e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5818e-03, 2.5264e-03, 1.3998e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208610.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2916.5610, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.7469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.2891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8967.2012, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.0542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0430],
        [-0.0429],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40318.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087565., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087458.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-763.8027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25170.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6138e-03, 2.5552e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8987e-05],
        [1.6138e-03, 2.5552e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8987e-05],
        [1.6314e-03, 2.5677e-03, 1.4134e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.9136e-05],
        ...,
        [1.5787e-03, 2.5301e-03, 1.3999e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5787e-03, 2.5301e-03, 1.3999e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5787e-03, 2.5301e-03, 1.3999e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208571.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2913.9277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.5552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.4548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8963.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.9407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0430],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40265.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087458.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087352.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-764.0578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25165.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6093e-03, 2.5584e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8501e-05],
        [1.6093e-03, 2.5584e-03, 1.4088e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8501e-05],
        [1.6269e-03, 2.5709e-03, 1.4133e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.8493e-05],
        ...,
        [1.5739e-03, 2.5335e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5739e-03, 2.5335e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5739e-03, 2.5335e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208502.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2910.9558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.3817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.5994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8960.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.8377, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0430],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40257.4922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087352.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087246.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-764.2885, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25161.2598, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6064e-03, 2.5608e-03, 1.4087e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7474e-05],
        [1.6064e-03, 2.5608e-03, 1.4087e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7474e-05],
        [1.6241e-03, 2.5733e-03, 1.4133e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.7583e-05],
        ...,
        [1.5711e-03, 2.5358e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5711e-03, 2.5358e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5711e-03, 2.5358e-03, 1.3997e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208457.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2908.7725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.2245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.7249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8957.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.7440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40233.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087246.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087140., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-764.4968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25156.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6001e-03, 2.5636e-03, 1.4086e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7935e-05],
        [1.6001e-03, 2.5636e-03, 1.4086e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7935e-05],
        [1.6178e-03, 2.5761e-03, 1.4131e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.8183e-05],
        ...,
        [1.5646e-03, 2.5385e-03, 1.3995e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5646e-03, 2.5385e-03, 1.3995e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5646e-03, 2.5385e-03, 1.3995e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208400.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2904.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.0821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.8331, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8953.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.6591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40214.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087140., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087033.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-764.6855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25152.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5977e-03, 2.5664e-03, 1.4085e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7067e-05],
        [1.5977e-03, 2.5664e-03, 1.4085e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7067e-05],
        [1.6153e-03, 2.5790e-03, 1.4131e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.7229e-05],
        ...,
        [1.5624e-03, 2.5411e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5624e-03, 2.5411e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5624e-03, 2.5411e-03, 1.3994e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208339.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2903.2505, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.9532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.9257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8951.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.5818, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40186.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087033.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086927.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-764.8564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25148.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5949e-03, 2.5669e-03, 1.4084e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6270e-05],
        [1.5949e-03, 2.5669e-03, 1.4084e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6270e-05],
        [1.6125e-03, 2.5796e-03, 1.4129e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6373e-05],
        ...,
        [1.5597e-03, 2.5415e-03, 1.3993e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5597e-03, 2.5415e-03, 1.3993e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5597e-03, 2.5415e-03, 1.3993e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208289.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2901.0601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.8363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.0043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8948.7783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.5116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40187.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086927.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086821.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.0109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25144.7793, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5923e-03, 2.5691e-03, 1.4083e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5462e-05],
        [1.5923e-03, 2.5691e-03, 1.4083e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5462e-05],
        [1.6098e-03, 2.5817e-03, 1.4129e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5659e-05],
        ...,
        [1.5571e-03, 2.5439e-03, 1.3992e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5571e-03, 2.5439e-03, 1.3992e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5571e-03, 2.5439e-03, 1.3992e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208224.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2899.5132, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.7305, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.0701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8946.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.4477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40191.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086821.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 1650 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086715., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.1507, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25141.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5896e-03, 2.5709e-03, 1.4081e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6258e-05],
        [1.5896e-03, 2.5709e-03, 1.4081e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6258e-05],
        [1.6072e-03, 2.5835e-03, 1.4127e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6526e-05],
        ...,
        [1.5543e-03, 2.5457e-03, 1.3991e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5543e-03, 2.5457e-03, 1.3991e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5543e-03, 2.5457e-03, 1.3991e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208170.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2897.2295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.6346, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.1241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8943.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.3897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40195.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086715., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086608.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.2769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25137.4160, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5860e-03, 2.5720e-03, 1.4080e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6344e-05],
        [1.5860e-03, 2.5720e-03, 1.4080e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6344e-05],
        [1.6037e-03, 2.5846e-03, 1.4125e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6594e-05],
        ...,
        [1.5508e-03, 2.5468e-03, 1.3990e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5508e-03, 2.5468e-03, 1.3990e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5508e-03, 2.5468e-03, 1.3990e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208107.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2895.1021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.5478, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.1678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8941.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.3368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40190.1836, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086608.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9895],
        [0.9895],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086502.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.3912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25133.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5837e-03, 2.5739e-03, 1.4079e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5495e-05],
        [1.5837e-03, 2.5739e-03, 1.4079e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5495e-05],
        [1.6012e-03, 2.5864e-03, 1.4125e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5778e-05],
        ...,
        [1.5486e-03, 2.5488e-03, 1.3989e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5486e-03, 2.5488e-03, 1.3989e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5486e-03, 2.5488e-03, 1.3989e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208058.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2893.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.4691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2020, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8938.6611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.2887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40164.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9895],
        [0.9895],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086502.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086396.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.4946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25130.4980, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5819e-03, 2.5754e-03, 1.4078e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6252e-05],
        [1.5819e-03, 2.5754e-03, 1.4078e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6252e-05],
        [1.5995e-03, 2.5879e-03, 1.4123e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6420e-05],
        ...,
        [1.5469e-03, 2.5504e-03, 1.3988e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5469e-03, 2.5504e-03, 1.3988e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5469e-03, 2.5504e-03, 1.3988e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(208002.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2892.0369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.3978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8936.2725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.2446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40204.1523, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086396.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086290., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.5880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25127.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5795e-03, 2.5748e-03, 1.4076e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5452e-05],
        [1.5795e-03, 2.5748e-03, 1.4076e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5452e-05],
        [1.5969e-03, 2.5874e-03, 1.4122e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5550e-05],
        ...,
        [1.5445e-03, 2.5495e-03, 1.3986e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5445e-03, 2.5495e-03, 1.3986e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5445e-03, 2.5495e-03, 1.3986e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207940.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2889.9209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.3331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2456, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8934.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.2046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40197.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086290., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086183.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.6727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25123.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5773e-03, 2.5756e-03, 1.4073e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5124e-05],
        [1.5773e-03, 2.5756e-03, 1.4073e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5124e-05],
        [1.5948e-03, 2.5883e-03, 1.4119e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5324e-05],
        ...,
        [1.5425e-03, 2.5502e-03, 1.3982e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5425e-03, 2.5502e-03, 1.3982e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5425e-03, 2.5502e-03, 1.3982e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207876.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2888.3188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.2744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8932.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.1681, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40198.0547, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086183.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086077.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.7494, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25120.7832, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5746e-03, 2.5765e-03, 1.4071e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4499e-05],
        [1.5746e-03, 2.5765e-03, 1.4071e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4499e-05],
        [1.5921e-03, 2.5891e-03, 1.4116e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.4586e-05],
        ...,
        [1.5395e-03, 2.5514e-03, 1.3981e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5395e-03, 2.5514e-03, 1.3981e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5395e-03, 2.5514e-03, 1.3981e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207809.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2886.2876, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.2213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2608, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8930.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.1346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40202.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086077.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085971.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.8184, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25117.6895, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5718e-03, 2.5772e-03, 1.4070e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4838e-05],
        [1.5718e-03, 2.5772e-03, 1.4070e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4838e-05],
        [1.5893e-03, 2.5898e-03, 1.4116e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.4873e-05],
        ...,
        [1.5368e-03, 2.5520e-03, 1.3980e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5368e-03, 2.5520e-03, 1.3980e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5368e-03, 2.5520e-03, 1.3980e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207756.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2884.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.1730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8928.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.1041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0553],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40208.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085971.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085864.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.8809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25114.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5697e-03, 2.5762e-03, 1.4068e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5322e-05],
        [1.5697e-03, 2.5762e-03, 1.4068e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5322e-05],
        [1.5871e-03, 2.5888e-03, 1.4113e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5615e-05],
        ...,
        [1.5348e-03, 2.5510e-03, 1.3977e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5348e-03, 2.5510e-03, 1.3977e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5348e-03, 2.5510e-03, 1.3977e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207697.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2882.4373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.1293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8925.6699, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.0761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40220.7266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085864.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085758.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.9374, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25111.6777, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5667e-03, 2.5783e-03, 1.4066e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5113e-05],
        [1.5667e-03, 2.5783e-03, 1.4066e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5113e-05],
        [1.5843e-03, 2.5908e-03, 1.4111e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5271e-05],
        ...,
        [1.5315e-03, 2.5533e-03, 1.3975e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5315e-03, 2.5533e-03, 1.3975e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5315e-03, 2.5533e-03, 1.3975e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207637.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2880.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.0895, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8923.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.0504, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40213.8008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085758.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 1800 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085652.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-765.9888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25108.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5642e-03, 2.5781e-03, 1.4065e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4830e-05],
        [1.5642e-03, 2.5781e-03, 1.4065e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4830e-05],
        [1.5818e-03, 2.5906e-03, 1.4110e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.4959e-05],
        ...,
        [1.5290e-03, 2.5530e-03, 1.3975e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5290e-03, 2.5530e-03, 1.3975e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5290e-03, 2.5530e-03, 1.3975e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207577.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2878.7300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.0534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2265, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8921.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.0269, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0558],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40278.9258, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085652.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085545.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.0348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25105.8652, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5627e-03, 2.5779e-03, 1.4063e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4767e-05],
        [1.5627e-03, 2.5779e-03, 1.4063e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4767e-05],
        [1.5803e-03, 2.5905e-03, 1.4108e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.4914e-05],
        ...,
        [1.5277e-03, 2.5523e-03, 1.3972e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5277e-03, 2.5523e-03, 1.3972e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5277e-03, 2.5523e-03, 1.3972e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207513.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2877.1833, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.0207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.2072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8919.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.0052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0557],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40265.0039, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085545.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085439.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.0767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25103.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5604e-03, 2.5770e-03, 1.4061e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4416e-05],
        [1.5604e-03, 2.5770e-03, 1.4061e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4416e-05],
        [1.5780e-03, 2.5896e-03, 1.4106e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.4670e-05],
        ...,
        [1.5252e-03, 2.5519e-03, 1.3970e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5252e-03, 2.5519e-03, 1.3970e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5252e-03, 2.5519e-03, 1.3970e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207448.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2874.8052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.9908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.1844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8917.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.9854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0558],
        [-0.0563],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40290.3320, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085439.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085333.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.1145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25100.2168, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5592e-03, 2.5768e-03, 1.4057e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5679e-05],
        [1.5592e-03, 2.5768e-03, 1.4057e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5679e-05],
        [1.5768e-03, 2.5894e-03, 1.4103e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5876e-05],
        ...,
        [1.5239e-03, 2.5517e-03, 1.3967e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5239e-03, 2.5517e-03, 1.3967e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5239e-03, 2.5517e-03, 1.3967e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207394.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2873.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.9637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.1586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8915.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.9670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40304.1602, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085333.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085226.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.1484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25097.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5569e-03, 2.5776e-03, 1.4056e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6731e-05],
        [1.5569e-03, 2.5776e-03, 1.4056e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6731e-05],
        [1.5744e-03, 2.5901e-03, 1.4102e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6946e-05],
        ...,
        [1.5219e-03, 2.5527e-03, 1.3966e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5219e-03, 2.5527e-03, 1.3966e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5219e-03, 2.5527e-03, 1.3966e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207326.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2872.1997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.9391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.1298, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8913.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.9502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40311.3516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085226.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085120.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.1794, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25094.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5545e-03, 2.5760e-03, 1.4054e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5409e-05],
        [1.5545e-03, 2.5760e-03, 1.4054e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5409e-05],
        [1.5719e-03, 2.5886e-03, 1.4099e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5798e-05],
        ...,
        [1.5196e-03, 2.5510e-03, 1.3963e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5196e-03, 2.5510e-03, 1.3963e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5196e-03, 2.5510e-03, 1.3963e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207268.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2870.6350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.9167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.0986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8911.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.9345, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0554],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40353.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085120.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085014.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.2074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25091.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5530e-03, 2.5765e-03, 1.4052e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4117e-05],
        [1.5530e-03, 2.5765e-03, 1.4052e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4117e-05],
        [1.5704e-03, 2.5890e-03, 1.4098e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.4473e-05],
        ...,
        [1.5181e-03, 2.5516e-03, 1.3962e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5181e-03, 2.5516e-03, 1.3962e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5181e-03, 2.5516e-03, 1.3962e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207203.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2868.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.8962, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.0649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8909.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.9200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40361.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085014.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084908., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.2324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25089.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5525e-03, 2.5749e-03, 1.4051e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5965e-05],
        [1.5525e-03, 2.5749e-03, 1.4051e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.5965e-05],
        [1.5699e-03, 2.5874e-03, 1.4096e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6095e-05],
        ...,
        [1.5178e-03, 2.5498e-03, 1.3960e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5178e-03, 2.5498e-03, 1.3960e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5178e-03, 2.5498e-03, 1.3960e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207148.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2867.8445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.8776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-566.0292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8908.4756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.9066, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40385.8164, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084908., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084801.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.2554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25086.6270, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5491e-03, 2.5742e-03, 1.4049e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6307e-05],
        [1.5491e-03, 2.5742e-03, 1.4049e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6307e-05],
        [1.5665e-03, 2.5868e-03, 1.4094e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6362e-05],
        ...,
        [1.5143e-03, 2.5490e-03, 1.3959e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5143e-03, 2.5490e-03, 1.3959e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5143e-03, 2.5490e-03, 1.3959e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207081.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2865.6321, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.8605, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.9915, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8907.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.8941, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0558],
        [-0.0564],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40403.1289, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084801.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084695.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.2760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25083.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5474e-03, 2.5742e-03, 1.4046e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8235e-05],
        [1.5474e-03, 2.5742e-03, 1.4046e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8235e-05],
        [1.5648e-03, 2.5869e-03, 1.4091e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.8408e-05],
        ...,
        [1.5126e-03, 2.5488e-03, 1.3956e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5126e-03, 2.5488e-03, 1.3956e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5126e-03, 2.5488e-03, 1.3956e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(207022.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2864.0701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.8451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.9522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8905.2236, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.8825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0559],
        [-0.0564],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40421.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084695.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 1950 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084589., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.2948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25081.3457, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5465e-03, 2.5732e-03, 1.4044e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6708e-05],
        [1.5465e-03, 2.5732e-03, 1.4044e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6708e-05],
        [1.5638e-03, 2.5859e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6923e-05],
        ...,
        [1.5118e-03, 2.5479e-03, 1.3953e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5118e-03, 2.5479e-03, 1.3953e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5118e-03, 2.5479e-03, 1.3953e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(206955.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2863.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.8309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.9112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8902.9795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.8716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0559],
        [-0.0565],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40444.6992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084589., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084589., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0000, -0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-766.2948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(25081.3457, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5465e-03, 2.5732e-03, 1.4044e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6708e-05],
        [1.5465e-03, 2.5732e-03, 1.4044e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.6708e-05],
        [1.5638e-03, 2.5859e-03, 1.4089e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.6923e-05],
        ...,
        [1.5118e-03, 2.5479e-03, 1.3953e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5118e-03, 2.5479e-03, 1.3953e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [1.5118e-03, 2.5479e-03, 1.3953e-02,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(206955.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2863.2217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.8309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.9112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8902.9795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-36.8716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0555],
        [-0.0559],
        [-0.0565],
        ...,
        [-0.0429],
        [-0.0428],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-40444.6992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084589., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./Training.py", line 76, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(BatchSize * 6796, 1)
RuntimeError: shape '[101940, 1]' is invalid for input of size 33980

real	1m42.889s
user	0m13.375s
sys	0m9.555s
