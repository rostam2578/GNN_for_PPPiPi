0: cmsgpu001.ihep.ac.cn
GPU 0: NVIDIA A100-PCIE-40GB (UUID: GPU-83673d1f-01b2-490d-5bc6-a84aaf3ddc65)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        465.19.01
supported:      external
license:        NVIDIA
firmware:       nvidia/465.19.01/gsp.bin
retpoline:      Y
rhelversion:    7.8
srcversion:     976AD09EB9C3B8943CBA8C4
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           nv_cap_enable_devfs:Enable (1) or disable (0) nv-caps devfs support. Default: 1 (int)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_EnableS0ixPowerManagement:int
parm:           NVreg_S0ixPowerManagementVideoMemoryThreshold:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableGpuFirmware:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_ExcludedGpus:charp
parm:           rm_firmware_active:charp

nvidia-smi:
Mon Jul 18 15:55:52 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   22C    P0    32W / 250W |      0MiB / 40536MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: NVIDIA A100-PCIE-40GB

 CUDA Device Total Memory [GB]: 42.505273344

 Device capability: (8, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2ac88d23dfa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	1m11.970s
user	0m3.543s
sys	0m3.152s
[15:57:08] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
Matplotlib is building the font cache; this may take a moment.
Exception in thread Thread-2:
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/summary/writer/event_file_writer.py", line 233, in run
    self._record_writer.write(data)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/summary/writer/record_writer.py", line 40, in write
    self._writer.write(header + header_crc + data + footer_crc)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 761, in write
    self.fs.write(self.filename, file_content, self.binary_mode)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 150, in write
    self._write(filename, file_content, "wb" if binary_mode else "w")
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 164, in _write
    with io.open(filename, mode, encoding=encoding) as f:
OSError: [Errno 122] Disk quota exceeded: b'runs/Jul18_15-58-54_cmsgpu001.ihep.ac.cn/events.out.tfevents.1658131134.cmsgpu001.ihep.ac.cn.100565.0'




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-0.0798],
        [-0.8119],
        [-0.0022],
        ...,
        [-0.7731],
        [ 0.5702],
        [-1.1773]], device='cuda:0', requires_grad=True) 
node features sum: tensor(100.3640, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.0342, -0.0192, -0.1436, -0.0311,  0.0275, -0.1073,  0.1439,  0.1119,
         -0.0891, -0.0416, -0.0796, -0.1200, -0.1247, -0.0324, -0.0791,  0.1349,
          0.1117, -0.0594,  0.1019, -0.1218,  0.0420,  0.0985,  0.0558, -0.0933,
         -0.0469, -0.1271,  0.0955, -0.0313,  0.0496,  0.0675, -0.0338, -0.0201,
          0.0660, -0.0823, -0.0206, -0.0755, -0.1098,  0.0015, -0.0329, -0.0474,
         -0.0933, -0.0883, -0.0889,  0.1412, -0.0187,  0.0669,  0.1274,  0.0968,
          0.1378,  0.0361, -0.0508, -0.1196,  0.0286,  0.0461,  0.0082, -0.1262,
         -0.0293,  0.0111, -0.0266, -0.0695,  0.1228,  0.0525, -0.1162, -0.0414,
         -0.1008, -0.1004, -0.1226,  0.0042, -0.0393, -0.0682,  0.1493,  0.0217,
          0.1377,  0.0102,  0.1009, -0.1100,  0.0408, -0.0902, -0.0227, -0.0376,
         -0.0846,  0.0519, -0.0388,  0.1206,  0.1477,  0.1138,  0.1000,  0.1034,
          0.0215,  0.1232,  0.0451,  0.1448,  0.0496,  0.1026,  0.0327, -0.0997,
         -0.1497, -0.0826,  0.0384,  0.0982, -0.0332, -0.0519, -0.0047, -0.0377,
         -0.0304, -0.0110, -0.0539,  0.1331, -0.0107, -0.1248,  0.1298, -0.1170,
         -0.1189, -0.1386, -0.0134, -0.0387,  0.0515, -0.0470,  0.0490, -0.0963,
          0.0958, -0.0382, -0.0467, -0.1131,  0.0609,  0.1119, -0.1310, -0.0493,
         -0.0535,  0.0826, -0.1100, -0.1478,  0.1205,  0.0887,  0.1293, -0.0851,
         -0.0971,  0.0196,  0.1331,  0.0549,  0.0093, -0.0320, -0.0589, -0.0597,
          0.0790, -0.1188, -0.0729,  0.1431, -0.0244,  0.0544, -0.0751,  0.1090,
         -0.0159, -0.0399,  0.1247,  0.1270,  0.1275,  0.0408, -0.0054, -0.0396,
         -0.0156,  0.0640,  0.0649, -0.1328,  0.0045,  0.1301, -0.0925,  0.1239,
          0.0690,  0.1319,  0.1200,  0.0137,  0.0553,  0.0253, -0.0381, -0.0296,
         -0.0027, -0.0968,  0.1195, -0.0697, -0.0021,  0.0467,  0.1295, -0.0177,
         -0.0676,  0.1184, -0.0622,  0.0152, -0.0403, -0.0581,  0.1357,  0.1321,
          0.0187,  0.0797, -0.0106,  0.0554,  0.0706, -0.1384, -0.1466,  0.1130,
         -0.0034,  0.1008, -0.0211, -0.0878, -0.0784, -0.0542,  0.0280, -0.0155,
          0.0639,  0.1032,  0.0584, -0.0536, -0.1444, -0.0581, -0.0627,  0.0729,
         -0.1347, -0.1136,  0.0163, -0.0944, -0.0142, -0.0357,  0.0727, -0.0033,
          0.0182, -0.0474,  0.1436,  0.1085,  0.0636,  0.0828, -0.1347, -0.1245,
          0.1027,  0.0545,  0.0004, -0.0735, -0.0017,  0.0475,  0.1083, -0.0497,
          0.0289,  0.0868, -0.0989, -0.1527,  0.1193,  0.1251,  0.0769, -0.0513,
          0.0739, -0.1160, -0.0126,  0.0635, -0.0014, -0.0170, -0.0465,  0.0523]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0342, -0.0192, -0.1436, -0.0311,  0.0275, -0.1073,  0.1439,  0.1119,
         -0.0891, -0.0416, -0.0796, -0.1200, -0.1247, -0.0324, -0.0791,  0.1349,
          0.1117, -0.0594,  0.1019, -0.1218,  0.0420,  0.0985,  0.0558, -0.0933,
         -0.0469, -0.1271,  0.0955, -0.0313,  0.0496,  0.0675, -0.0338, -0.0201,
          0.0660, -0.0823, -0.0206, -0.0755, -0.1098,  0.0015, -0.0329, -0.0474,
         -0.0933, -0.0883, -0.0889,  0.1412, -0.0187,  0.0669,  0.1274,  0.0968,
          0.1378,  0.0361, -0.0508, -0.1196,  0.0286,  0.0461,  0.0082, -0.1262,
         -0.0293,  0.0111, -0.0266, -0.0695,  0.1228,  0.0525, -0.1162, -0.0414,
         -0.1008, -0.1004, -0.1226,  0.0042, -0.0393, -0.0682,  0.1493,  0.0217,
          0.1377,  0.0102,  0.1009, -0.1100,  0.0408, -0.0902, -0.0227, -0.0376,
         -0.0846,  0.0519, -0.0388,  0.1206,  0.1477,  0.1138,  0.1000,  0.1034,
          0.0215,  0.1232,  0.0451,  0.1448,  0.0496,  0.1026,  0.0327, -0.0997,
         -0.1497, -0.0826,  0.0384,  0.0982, -0.0332, -0.0519, -0.0047, -0.0377,
         -0.0304, -0.0110, -0.0539,  0.1331, -0.0107, -0.1248,  0.1298, -0.1170,
         -0.1189, -0.1386, -0.0134, -0.0387,  0.0515, -0.0470,  0.0490, -0.0963,
          0.0958, -0.0382, -0.0467, -0.1131,  0.0609,  0.1119, -0.1310, -0.0493,
         -0.0535,  0.0826, -0.1100, -0.1478,  0.1205,  0.0887,  0.1293, -0.0851,
         -0.0971,  0.0196,  0.1331,  0.0549,  0.0093, -0.0320, -0.0589, -0.0597,
          0.0790, -0.1188, -0.0729,  0.1431, -0.0244,  0.0544, -0.0751,  0.1090,
         -0.0159, -0.0399,  0.1247,  0.1270,  0.1275,  0.0408, -0.0054, -0.0396,
         -0.0156,  0.0640,  0.0649, -0.1328,  0.0045,  0.1301, -0.0925,  0.1239,
          0.0690,  0.1319,  0.1200,  0.0137,  0.0553,  0.0253, -0.0381, -0.0296,
         -0.0027, -0.0968,  0.1195, -0.0697, -0.0021,  0.0467,  0.1295, -0.0177,
         -0.0676,  0.1184, -0.0622,  0.0152, -0.0403, -0.0581,  0.1357,  0.1321,
          0.0187,  0.0797, -0.0106,  0.0554,  0.0706, -0.1384, -0.1466,  0.1130,
         -0.0034,  0.1008, -0.0211, -0.0878, -0.0784, -0.0542,  0.0280, -0.0155,
          0.0639,  0.1032,  0.0584, -0.0536, -0.1444, -0.0581, -0.0627,  0.0729,
         -0.1347, -0.1136,  0.0163, -0.0944, -0.0142, -0.0357,  0.0727, -0.0033,
          0.0182, -0.0474,  0.1436,  0.1085,  0.0636,  0.0828, -0.1347, -0.1245,
          0.1027,  0.0545,  0.0004, -0.0735, -0.0017,  0.0475,  0.1083, -0.0497,
          0.0289,  0.0868, -0.0989, -0.1527,  0.1193,  0.1251,  0.0769, -0.0513,
          0.0739, -0.1160, -0.0126,  0.0635, -0.0014, -0.0170, -0.0465,  0.0523]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0684,  0.0118, -0.0341,  ..., -0.1020, -0.0834, -0.0852],
        [ 0.0604, -0.0313,  0.0288,  ..., -0.1158,  0.0451,  0.0123],
        [-0.0558,  0.0241,  0.0293,  ..., -0.0723, -0.0076, -0.0851],
        ...,
        [-0.0621,  0.0419,  0.0551,  ..., -0.0511, -0.0815, -0.1142],
        [-0.0688, -0.0696, -0.0322,  ...,  0.0867,  0.0567,  0.0682],
        [-0.0221,  0.0858, -0.0518,  ..., -0.0583,  0.0460,  0.0781]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0684,  0.0118, -0.0341,  ..., -0.1020, -0.0834, -0.0852],
        [ 0.0604, -0.0313,  0.0288,  ..., -0.1158,  0.0451,  0.0123],
        [-0.0558,  0.0241,  0.0293,  ..., -0.0723, -0.0076, -0.0851],
        ...,
        [-0.0621,  0.0419,  0.0551,  ..., -0.0511, -0.0815, -0.1142],
        [-0.0688, -0.0696, -0.0322,  ...,  0.0867,  0.0567,  0.0682],
        [-0.0221,  0.0858, -0.0518,  ..., -0.0583,  0.0460,  0.0781]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1141,  0.0632,  0.0905,  ...,  0.1484,  0.0558, -0.0136],
        [ 0.1669,  0.0877,  0.0744,  ...,  0.0961,  0.0942,  0.1290],
        [-0.0666,  0.0137, -0.1727,  ..., -0.0478, -0.1366, -0.0314],
        ...,
        [ 0.0502, -0.0618,  0.1046,  ...,  0.0882,  0.1099, -0.0795],
        [-0.1354,  0.0909, -0.1492,  ..., -0.1106,  0.1762,  0.1241],
        [ 0.1681, -0.1631, -0.0044,  ..., -0.0173,  0.0601,  0.1181]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1141,  0.0632,  0.0905,  ...,  0.1484,  0.0558, -0.0136],
        [ 0.1669,  0.0877,  0.0744,  ...,  0.0961,  0.0942,  0.1290],
        [-0.0666,  0.0137, -0.1727,  ..., -0.0478, -0.1366, -0.0314],
        ...,
        [ 0.0502, -0.0618,  0.1046,  ...,  0.0882,  0.1099, -0.0795],
        [-0.1354,  0.0909, -0.1492,  ..., -0.1106,  0.1762,  0.1241],
        [ 0.1681, -0.1631, -0.0044,  ..., -0.0173,  0.0601,  0.1181]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-2.1964e-05, -2.0134e-01,  4.7032e-02,  ...,  2.1845e-01,
         -2.4498e-01,  2.4248e-01],
        [ 1.0972e-01, -1.3820e-01,  2.4770e-01,  ...,  1.9706e-02,
          1.1625e-01,  1.8975e-01],
        [-1.1318e-01, -1.6024e-01,  5.6546e-02,  ..., -3.4031e-02,
          1.6136e-01, -4.6802e-02],
        ...,
        [-2.2740e-01,  2.4203e-01, -2.2464e-01,  ..., -8.4334e-02,
         -1.7555e-01, -9.6970e-02],
        [ 1.3374e-01, -4.9780e-02,  6.1628e-02,  ..., -2.1067e-01,
         -4.3545e-02, -2.0080e-01],
        [-1.2916e-01, -7.5297e-02, -5.7097e-02,  ..., -5.9261e-02,
         -8.5598e-02, -1.8799e-01]], device='cuda:0') 
 Parameter containing:
tensor([[-2.1964e-05, -2.0134e-01,  4.7032e-02,  ...,  2.1845e-01,
         -2.4498e-01,  2.4248e-01],
        [ 1.0972e-01, -1.3820e-01,  2.4770e-01,  ...,  1.9706e-02,
          1.1625e-01,  1.8975e-01],
        [-1.1318e-01, -1.6024e-01,  5.6546e-02,  ..., -3.4031e-02,
          1.6136e-01, -4.6802e-02],
        ...,
        [-2.2740e-01,  2.4203e-01, -2.2464e-01,  ..., -8.4334e-02,
         -1.7555e-01, -9.6970e-02],
        [ 1.3374e-01, -4.9780e-02,  6.1628e-02,  ..., -2.1067e-01,
         -4.3545e-02, -2.0080e-01],
        [-1.2916e-01, -7.5297e-02, -5.7097e-02,  ..., -5.9261e-02,
         -8.5598e-02, -1.8799e-01]], device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3091],
        [-0.3052],
        [-0.1907],
        [ 0.0652],
        [ 0.1480],
        [-0.1242],
        [ 0.3897],
        [-0.3742],
        [-0.2800],
        [ 0.1358],
        [ 0.3530],
        [ 0.2951],
        [ 0.2576],
        [ 0.2231],
        [ 0.1211],
        [-0.3677],
        [-0.2992],
        [-0.0662],
        [ 0.3415],
        [ 0.0094],
        [ 0.0206],
        [ 0.3884],
        [ 0.0685],
        [ 0.3274],
        [-0.3935],
        [ 0.3823],
        [ 0.3659],
        [-0.1047],
        [ 0.2896],
        [-0.2492],
        [ 0.0568],
        [ 0.0971]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3091],
        [-0.3052],
        [-0.1907],
        [ 0.0652],
        [ 0.1480],
        [-0.1242],
        [ 0.3897],
        [-0.3742],
        [-0.2800],
        [ 0.1358],
        [ 0.3530],
        [ 0.2951],
        [ 0.2576],
        [ 0.2231],
        [ 0.1211],
        [-0.3677],
        [-0.2992],
        [-0.0662],
        [ 0.3415],
        [ 0.0094],
        [ 0.0206],
        [ 0.3884],
        [ 0.0685],
        [ 0.3274],
        [-0.3935],
        [ 0.3823],
        [ 0.3659],
        [-0.1047],
        [ 0.2896],
        [-0.2492],
        [ 0.0568],
        [ 0.0971]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 1.1306e-01,  3.0967e-03, -5.2315e-02,  4.1413e-02, -1.4888e-01,
          7.2697e-02,  1.3196e-01, -6.7328e-02, -2.8385e-02, -1.4576e-01,
         -1.3667e-01, -1.1358e-01, -1.0948e-01, -1.2385e-01,  3.3394e-02,
         -4.3520e-02, -1.3679e-01,  4.8884e-02, -1.3666e-01, -5.5322e-02,
         -7.2028e-02,  9.0311e-02,  2.0150e-02, -1.2098e-01,  6.2000e-02,
          9.4697e-02, -8.3778e-03, -4.2243e-02,  1.2660e-01,  1.3848e-01,
         -1.7250e-02, -3.4184e-02,  9.7869e-03,  1.4341e-01, -7.2740e-02,
          1.0705e-01,  7.5081e-02, -7.7349e-02,  1.2606e-01, -3.9516e-02,
          5.1872e-02, -4.5257e-02, -1.9194e-03,  1.3043e-01, -1.8524e-03,
          2.6044e-02, -8.1281e-02,  1.2311e-01,  7.7499e-02,  9.0026e-02,
         -1.1480e-01,  5.0998e-03, -4.9400e-02,  5.3709e-04,  1.5039e-01,
          8.2662e-02,  7.2177e-02, -1.8861e-02, -9.6906e-02,  4.3875e-02,
          1.3750e-01,  1.0133e-01, -3.7728e-02, -4.9365e-02, -1.4499e-01,
          1.2115e-01, -1.4454e-01,  1.4851e-01,  9.7347e-02, -1.4207e-01,
          1.4350e-01, -7.7587e-02,  1.2571e-01,  9.7675e-02,  1.0247e-01,
         -1.5143e-01, -2.9027e-02, -1.0756e-01,  3.9553e-02, -8.7653e-02,
          6.8150e-02, -1.9454e-02, -9.9739e-02,  1.0432e-01, -1.0788e-01,
         -1.6005e-02, -2.9737e-02, -8.9774e-02,  6.9842e-03, -1.0655e-01,
          1.1004e-01, -6.5462e-03,  7.2106e-02,  9.9718e-02,  1.2429e-01,
         -1.1709e-01, -7.6902e-02,  1.1166e-01, -9.4683e-04, -6.4314e-02,
         -7.0286e-02, -8.9563e-02, -8.9253e-02, -3.6931e-03,  5.2795e-02,
          3.1114e-02, -8.0882e-02,  8.6513e-02, -7.0583e-02, -9.2214e-02,
         -4.5359e-02,  1.1463e-01, -7.8189e-02,  1.1595e-01, -9.9528e-02,
         -6.0491e-02, -6.2723e-02, -6.7610e-02, -2.5114e-02,  1.1965e-02,
          1.2884e-02, -1.4037e-01,  1.0255e-01,  1.1319e-01,  2.8894e-02,
          6.1758e-02, -3.5914e-03, -1.3607e-01,  4.1882e-03,  4.4480e-02,
         -8.2588e-02, -4.5190e-02, -5.1442e-02,  1.1344e-01, -3.0248e-02,
         -7.1010e-02, -1.2246e-01, -1.3617e-01,  4.4164e-02,  8.0254e-02,
          6.7207e-03,  1.1767e-01,  1.0439e-01,  5.5759e-02,  7.4837e-02,
         -8.5486e-02, -8.6925e-02, -7.2682e-02,  1.7620e-03,  1.0335e-01,
          9.0398e-02, -1.0864e-01,  4.2174e-02, -9.9700e-02,  4.7599e-02,
          2.5233e-02, -1.2478e-01,  1.7052e-02,  7.2926e-02,  3.4452e-03,
          1.1521e-01, -1.1467e-01, -5.1920e-02,  1.3136e-01, -3.3716e-02,
          1.3897e-01, -3.6823e-02,  8.7508e-02, -3.0687e-03, -7.0375e-02,
          5.8643e-02,  8.5445e-04, -7.2351e-02,  2.1024e-02,  8.0504e-02,
          8.9643e-02, -7.8531e-02, -2.5737e-02, -5.9572e-02,  1.1523e-02,
          5.8035e-02,  6.3961e-02,  7.3712e-02,  8.1892e-03, -3.6725e-02,
         -5.1480e-02, -1.5005e-01,  1.0355e-01, -1.2660e-01,  2.1084e-02,
         -1.3711e-01,  1.3712e-01,  1.1163e-01, -5.0328e-02, -6.5434e-03,
         -8.6225e-02, -3.4092e-02,  3.8510e-02, -2.5128e-02, -7.9910e-02,
          2.4951e-02,  7.0634e-02,  1.2290e-01, -6.7553e-02,  1.2519e-01,
         -1.5704e-02, -4.7390e-02, -5.6309e-02,  4.3324e-02, -8.6081e-02,
         -4.8476e-02,  1.2097e-01, -5.2433e-02,  5.1352e-04,  1.4146e-01,
          2.9888e-02,  6.9290e-02,  1.9011e-02, -7.7164e-02,  4.0550e-03,
          6.7758e-06,  8.9390e-02, -3.3995e-02,  5.4126e-03, -1.0501e-01,
         -8.7319e-02,  1.0206e-01,  7.5040e-02,  3.0599e-02,  9.0116e-02,
          7.9755e-02,  7.7052e-02,  5.6585e-02,  9.9900e-03, -4.3285e-03,
          1.0381e-01,  4.6960e-02,  7.7825e-02, -1.5030e-01,  7.2069e-03,
          1.3441e-01, -6.6079e-02, -3.2533e-02,  5.3317e-02,  3.9070e-02,
          8.0548e-02, -9.0866e-02,  6.3535e-02, -9.6514e-03,  9.2363e-02,
          7.5778e-03, -1.4693e-01, -3.1131e-02, -1.1717e-01, -2.7475e-03,
          8.6762e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 1.1306e-01,  3.0967e-03, -5.2315e-02,  4.1413e-02, -1.4888e-01,
          7.2697e-02,  1.3196e-01, -6.7328e-02, -2.8385e-02, -1.4576e-01,
         -1.3667e-01, -1.1358e-01, -1.0948e-01, -1.2385e-01,  3.3394e-02,
         -4.3520e-02, -1.3679e-01,  4.8884e-02, -1.3666e-01, -5.5322e-02,
         -7.2028e-02,  9.0311e-02,  2.0150e-02, -1.2098e-01,  6.2000e-02,
          9.4697e-02, -8.3778e-03, -4.2243e-02,  1.2660e-01,  1.3848e-01,
         -1.7250e-02, -3.4184e-02,  9.7869e-03,  1.4341e-01, -7.2740e-02,
          1.0705e-01,  7.5081e-02, -7.7349e-02,  1.2606e-01, -3.9516e-02,
          5.1872e-02, -4.5257e-02, -1.9194e-03,  1.3043e-01, -1.8524e-03,
          2.6044e-02, -8.1281e-02,  1.2311e-01,  7.7499e-02,  9.0026e-02,
         -1.1480e-01,  5.0998e-03, -4.9400e-02,  5.3709e-04,  1.5039e-01,
          8.2662e-02,  7.2177e-02, -1.8861e-02, -9.6906e-02,  4.3875e-02,
          1.3750e-01,  1.0133e-01, -3.7728e-02, -4.9365e-02, -1.4499e-01,
          1.2115e-01, -1.4454e-01,  1.4851e-01,  9.7347e-02, -1.4207e-01,
          1.4350e-01, -7.7587e-02,  1.2571e-01,  9.7675e-02,  1.0247e-01,
         -1.5143e-01, -2.9027e-02, -1.0756e-01,  3.9553e-02, -8.7653e-02,
          6.8150e-02, -1.9454e-02, -9.9739e-02,  1.0432e-01, -1.0788e-01,
         -1.6005e-02, -2.9737e-02, -8.9774e-02,  6.9842e-03, -1.0655e-01,
          1.1004e-01, -6.5462e-03,  7.2106e-02,  9.9718e-02,  1.2429e-01,
         -1.1709e-01, -7.6902e-02,  1.1166e-01, -9.4683e-04, -6.4314e-02,
         -7.0286e-02, -8.9563e-02, -8.9253e-02, -3.6931e-03,  5.2795e-02,
          3.1114e-02, -8.0882e-02,  8.6513e-02, -7.0583e-02, -9.2214e-02,
         -4.5359e-02,  1.1463e-01, -7.8189e-02,  1.1595e-01, -9.9528e-02,
         -6.0491e-02, -6.2723e-02, -6.7610e-02, -2.5114e-02,  1.1965e-02,
          1.2884e-02, -1.4037e-01,  1.0255e-01,  1.1319e-01,  2.8894e-02,
          6.1758e-02, -3.5914e-03, -1.3607e-01,  4.1882e-03,  4.4480e-02,
         -8.2588e-02, -4.5190e-02, -5.1442e-02,  1.1344e-01, -3.0248e-02,
         -7.1010e-02, -1.2246e-01, -1.3617e-01,  4.4164e-02,  8.0254e-02,
          6.7207e-03,  1.1767e-01,  1.0439e-01,  5.5759e-02,  7.4837e-02,
         -8.5486e-02, -8.6925e-02, -7.2682e-02,  1.7620e-03,  1.0335e-01,
          9.0398e-02, -1.0864e-01,  4.2174e-02, -9.9700e-02,  4.7599e-02,
          2.5233e-02, -1.2478e-01,  1.7052e-02,  7.2926e-02,  3.4452e-03,
          1.1521e-01, -1.1467e-01, -5.1920e-02,  1.3136e-01, -3.3716e-02,
          1.3897e-01, -3.6823e-02,  8.7508e-02, -3.0687e-03, -7.0375e-02,
          5.8643e-02,  8.5445e-04, -7.2351e-02,  2.1024e-02,  8.0504e-02,
          8.9643e-02, -7.8531e-02, -2.5737e-02, -5.9572e-02,  1.1523e-02,
          5.8035e-02,  6.3961e-02,  7.3712e-02,  8.1892e-03, -3.6725e-02,
         -5.1480e-02, -1.5005e-01,  1.0355e-01, -1.2660e-01,  2.1084e-02,
         -1.3711e-01,  1.3712e-01,  1.1163e-01, -5.0328e-02, -6.5434e-03,
         -8.6225e-02, -3.4092e-02,  3.8510e-02, -2.5128e-02, -7.9910e-02,
          2.4951e-02,  7.0634e-02,  1.2290e-01, -6.7553e-02,  1.2519e-01,
         -1.5704e-02, -4.7390e-02, -5.6309e-02,  4.3324e-02, -8.6081e-02,
         -4.8476e-02,  1.2097e-01, -5.2433e-02,  5.1352e-04,  1.4146e-01,
          2.9888e-02,  6.9290e-02,  1.9011e-02, -7.7164e-02,  4.0550e-03,
          6.7758e-06,  8.9390e-02, -3.3995e-02,  5.4126e-03, -1.0501e-01,
         -8.7319e-02,  1.0206e-01,  7.5040e-02,  3.0599e-02,  9.0116e-02,
          7.9755e-02,  7.7052e-02,  5.6585e-02,  9.9900e-03, -4.3285e-03,
          1.0381e-01,  4.6960e-02,  7.7825e-02, -1.5030e-01,  7.2069e-03,
          1.3441e-01, -6.6079e-02, -3.2533e-02,  5.3317e-02,  3.9070e-02,
          8.0548e-02, -9.0866e-02,  6.3535e-02, -9.6514e-03,  9.2363e-02,
          7.5778e-03, -1.4693e-01, -3.1131e-02, -1.1717e-01, -2.7475e-03,
          8.6762e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0138,  0.0753, -0.0474,  ..., -0.0971,  0.0316,  0.0399],
        [ 0.0235, -0.1141, -0.0804,  ...,  0.0561,  0.1182,  0.0126],
        [-0.0853, -0.0347, -0.1159,  ...,  0.0063,  0.1111,  0.0656],
        ...,
        [-0.0390,  0.0416,  0.0966,  ...,  0.1088,  0.0306, -0.0057],
        [-0.0589, -0.0083, -0.0198,  ..., -0.1181, -0.0298, -0.0397],
        [-0.1025, -0.0602,  0.0635,  ..., -0.0976, -0.0460,  0.0268]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0138,  0.0753, -0.0474,  ..., -0.0971,  0.0316,  0.0399],
        [ 0.0235, -0.1141, -0.0804,  ...,  0.0561,  0.1182,  0.0126],
        [-0.0853, -0.0347, -0.1159,  ...,  0.0063,  0.1111,  0.0656],
        ...,
        [-0.0390,  0.0416,  0.0966,  ...,  0.1088,  0.0306, -0.0057],
        [-0.0589, -0.0083, -0.0198,  ..., -0.1181, -0.0298, -0.0397],
        [-0.1025, -0.0602,  0.0635,  ..., -0.0976, -0.0460,  0.0268]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.1704, -0.0105, -0.0272,  ..., -0.0355, -0.0734, -0.1222],
        [ 0.0968,  0.0588, -0.1422,  ...,  0.1089, -0.0262,  0.0162],
        [-0.0707,  0.0703,  0.0787,  ...,  0.1387, -0.0984,  0.1520],
        ...,
        [-0.0900,  0.0761, -0.0353,  ...,  0.0182, -0.0816,  0.0480],
        [-0.0475, -0.1617,  0.0449,  ..., -0.0805,  0.0634,  0.0434],
        [ 0.0477,  0.0693,  0.0359,  ...,  0.1017,  0.1442, -0.0419]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1704, -0.0105, -0.0272,  ..., -0.0355, -0.0734, -0.1222],
        [ 0.0968,  0.0588, -0.1422,  ...,  0.1089, -0.0262,  0.0162],
        [-0.0707,  0.0703,  0.0787,  ...,  0.1387, -0.0984,  0.1520],
        ...,
        [-0.0900,  0.0761, -0.0353,  ...,  0.0182, -0.0816,  0.0480],
        [-0.0475, -0.1617,  0.0449,  ..., -0.0805,  0.0634,  0.0434],
        [ 0.0477,  0.0693,  0.0359,  ...,  0.1017,  0.1442, -0.0419]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0386, -0.0556,  0.1429,  ..., -0.1492,  0.0661,  0.0582],
        [-0.1071, -0.1515, -0.2357,  ...,  0.2072,  0.0099,  0.0692],
        [-0.0354, -0.1183, -0.1644,  ...,  0.1710,  0.1164,  0.1304],
        ...,
        [-0.0432,  0.1620,  0.0860,  ...,  0.1308,  0.2017, -0.1017],
        [-0.0889, -0.2227,  0.0307,  ...,  0.0239, -0.0396, -0.2026],
        [ 0.0289,  0.1382,  0.0081,  ..., -0.2431, -0.0297,  0.0876]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0386, -0.0556,  0.1429,  ..., -0.1492,  0.0661,  0.0582],
        [-0.1071, -0.1515, -0.2357,  ...,  0.2072,  0.0099,  0.0692],
        [-0.0354, -0.1183, -0.1644,  ...,  0.1710,  0.1164,  0.1304],
        ...,
        [-0.0432,  0.1620,  0.0860,  ...,  0.1308,  0.2017, -0.1017],
        [-0.0889, -0.2227,  0.0307,  ...,  0.0239, -0.0396, -0.2026],
        [ 0.0289,  0.1382,  0.0081,  ..., -0.2431, -0.0297,  0.0876]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 2.8493e-01],
        [-9.4996e-02],
        [ 1.8294e-01],
        [-2.5552e-01],
        [ 3.0951e-01],
        [-2.5635e-01],
        [ 2.5886e-01],
        [-1.7464e-02],
        [ 2.5043e-01],
        [-5.7490e-05],
        [ 9.2470e-03],
        [ 2.8184e-01],
        [ 2.3540e-01],
        [-1.9719e-01],
        [ 2.7016e-01],
        [ 1.2051e-01],
        [ 6.9905e-02],
        [-7.2302e-04],
        [-2.6814e-01],
        [-3.1812e-01],
        [-1.1094e-01],
        [-7.5177e-02],
        [ 8.9501e-02],
        [ 2.0892e-02],
        [-7.1536e-02],
        [-2.8702e-01],
        [-3.4381e-01],
        [-3.8477e-02],
        [ 8.7341e-02],
        [-1.3873e-02],
        [-6.4888e-03],
        [ 4.0623e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 2.8493e-01],
        [-9.4996e-02],
        [ 1.8294e-01],
        [-2.5552e-01],
        [ 3.0951e-01],
        [-2.5635e-01],
        [ 2.5886e-01],
        [-1.7464e-02],
        [ 2.5043e-01],
        [-5.7490e-05],
        [ 9.2470e-03],
        [ 2.8184e-01],
        [ 2.3540e-01],
        [-1.9719e-01],
        [ 2.7016e-01],
        [ 1.2051e-01],
        [ 6.9905e-02],
        [-7.2302e-04],
        [-2.6814e-01],
        [-3.1812e-01],
        [-1.1094e-01],
        [-7.5177e-02],
        [ 8.9501e-02],
        [ 2.0892e-02],
        [-7.1536e-02],
        [-2.8702e-01],
        [-3.4381e-01],
        [-3.8477e-02],
        [ 8.7341e-02],
        [-1.3873e-02],
        [-6.4888e-03],
        [ 4.0623e-02]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-31.5326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.5965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(2.6648, device='cuda:0')



h[100].sum tensor(1.4110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.4481, device='cuda:0')



h[200].sum tensor(-0.5244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0.5382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2936.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(13691.6270, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.3165, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-13.7893, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-3.0072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3933],
        [-0.4814],
        [-0.6941],
        ...,
        [-0.1111],
        [-0.1112],
        [-0.0881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-9922.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.3933],
        [-0.4814],
        [-0.6941],
        ...,
        [-0.1111],
        [-0.1112],
        [-0.0881]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(296.0366, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(10.7891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(10.7017, device='cuda:0')



h[100].sum tensor(5.9665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9181, device='cuda:0')



h[200].sum tensor(7.5881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15527.0811, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0155, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(98969.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-20.6083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-189.8763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2656.6763, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(186.8609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1913],
        [0.1174],
        [0.0718],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(11239.6992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.3933],
        [-0.4814],
        [-0.6941],
        ...,
        [-0.1111],
        [-0.1112],
        [-0.0881]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



load_model False 
TraEvN 1998 
BatchSize 5 
EpochNum 30 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0779, -0.0155,  0.1093,  0.0686, -0.1501,  0.0847, -0.1499,  0.1518,
          0.1427, -0.1005, -0.0491,  0.0743, -0.0330, -0.1081,  0.0498,  0.0569,
          0.1278,  0.0354, -0.1405,  0.0706, -0.0894, -0.1083,  0.1304,  0.0533,
         -0.0353, -0.0431, -0.0910, -0.0302,  0.0332,  0.0107,  0.0199, -0.1283,
          0.1111,  0.0253, -0.0477, -0.0506,  0.0522,  0.0993,  0.1343, -0.1382,
         -0.0078,  0.0245, -0.0215,  0.1394,  0.1421, -0.0148, -0.1293,  0.0678,
         -0.1416, -0.0664, -0.1088, -0.1104,  0.1364, -0.1322,  0.1255,  0.0856,
          0.1098,  0.0337,  0.1487, -0.0514,  0.1474,  0.0928, -0.0682, -0.1167,
          0.1088,  0.0378,  0.1349, -0.0764, -0.0880, -0.0658, -0.0500, -0.1013,
         -0.0866, -0.0305,  0.0369,  0.0804,  0.0726, -0.1458, -0.0745, -0.1424,
          0.1498, -0.1316,  0.0550,  0.1308,  0.0575, -0.1383, -0.0260,  0.1497,
          0.0360, -0.0964, -0.1283,  0.0032, -0.0351, -0.0081, -0.0180, -0.0441,
         -0.0055, -0.0201,  0.1413, -0.1389,  0.0658, -0.0718, -0.0264, -0.0264,
         -0.0070, -0.0031, -0.1180, -0.0673,  0.0779, -0.0640, -0.1289, -0.1323,
         -0.0878,  0.0791,  0.1227, -0.0592, -0.0498,  0.0606, -0.0861, -0.1291,
         -0.0584,  0.1029, -0.1208,  0.1396,  0.0306,  0.0707, -0.0133, -0.0098,
          0.0254, -0.1048,  0.1207, -0.0637,  0.1314, -0.0030,  0.0619, -0.1379,
          0.0019, -0.1066, -0.1118, -0.1367, -0.0531,  0.1518,  0.0916,  0.0077,
         -0.0251, -0.0713,  0.0529,  0.0157, -0.1205, -0.0335,  0.1396,  0.0811,
          0.0666,  0.0966,  0.0299,  0.0954,  0.0812,  0.1422, -0.0301, -0.1098,
         -0.0406, -0.0972,  0.0289, -0.0586, -0.1241,  0.1523, -0.0904,  0.1003,
          0.0597, -0.0694, -0.0776, -0.0685,  0.0552, -0.0912, -0.0964, -0.0483,
         -0.1360, -0.0741,  0.1120,  0.0616, -0.0767,  0.1342, -0.0286, -0.0820,
         -0.1048,  0.1266,  0.1145,  0.1445,  0.1479, -0.0474,  0.0095, -0.1198,
         -0.1481,  0.1421,  0.0834, -0.0076,  0.0772, -0.0870,  0.1062,  0.0560,
          0.0668, -0.1517,  0.1024, -0.1342, -0.0016, -0.0662, -0.0523,  0.0515,
          0.0408, -0.0137,  0.0156,  0.1397,  0.0428, -0.0873, -0.1226, -0.0412,
         -0.1200,  0.1337, -0.0282, -0.1528, -0.1064,  0.0125,  0.0497,  0.0546,
         -0.0080,  0.1505,  0.1369,  0.1070,  0.0446,  0.1241, -0.1302,  0.0063,
         -0.0652,  0.0182,  0.1093,  0.1299,  0.0854,  0.1481,  0.1219,  0.1432,
          0.1451, -0.1117, -0.0381, -0.0881,  0.0947, -0.0878, -0.1096,  0.1035,
         -0.0825,  0.0419, -0.0438,  0.1402,  0.1382, -0.1437,  0.1433, -0.0712]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0063,  0.0152, -0.0119,  ...,  0.0555, -0.0520, -0.0646],
        [ 0.0058, -0.0624, -0.0247,  ..., -0.0164,  0.0993, -0.0196],
        [ 0.1177, -0.0912,  0.0645,  ..., -0.0042,  0.0743, -0.0925],
        ...,
        [ 0.0221,  0.0094, -0.0095,  ...,  0.0018, -0.0958,  0.0446],
        [ 0.0637,  0.1158, -0.0296,  ...,  0.0270,  0.0062,  0.0769],
        [-0.0012, -0.0429, -0.0944,  ...,  0.0282, -0.0887, -0.1105]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1568, -0.1723, -0.1564,  ..., -0.1637, -0.1646,  0.1723],
        [ 0.1337, -0.1133,  0.0546,  ...,  0.0173,  0.0103,  0.0990],
        [-0.1011, -0.1083, -0.1123,  ..., -0.1081, -0.1696,  0.1449],
        ...,
        [ 0.0124,  0.0806,  0.1275,  ..., -0.0900,  0.0689,  0.0435],
        [-0.0123,  0.1542, -0.1572,  ..., -0.0634, -0.0045, -0.0464],
        [ 0.1243,  0.0174,  0.0522,  ..., -0.0036, -0.1215,  0.0801]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0883,  0.1140, -0.1883,  ..., -0.1540, -0.1293, -0.1944],
        [ 0.2405,  0.2077, -0.0554,  ..., -0.0422,  0.1521, -0.1979],
        [-0.1607, -0.2002,  0.1543,  ..., -0.0874,  0.2298, -0.1551],
        ...,
        [ 0.0820,  0.0813,  0.2164,  ..., -0.1135,  0.2294, -0.0318],
        [ 0.0840, -0.1815,  0.0092,  ..., -0.2243,  0.1669, -0.0232],
        [-0.1698,  0.2467,  0.1971,  ..., -0.1742,  0.0146, -0.1789]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2123],
        [ 0.1320],
        [ 0.0118],
        [ 0.0849],
        [-0.3031],
        [ 0.3108],
        [-0.0375],
        [-0.1150],
        [-0.4142],
        [ 0.2545],
        [ 0.2714],
        [ 0.0989],
        [-0.2744],
        [ 0.2483],
        [ 0.0601],
        [ 0.0345],
        [ 0.0608],
        [-0.4055],
        [-0.0793],
        [ 0.1237],
        [-0.0927],
        [-0.0975],
        [ 0.0324],
        [ 0.1073],
        [ 0.0968],
        [ 0.0810],
        [-0.0074],
        [-0.0077],
        [ 0.3958],
        [ 0.2451],
        [ 0.0252],
        [-0.1678]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0779, -0.0155,  0.1093,  0.0686, -0.1501,  0.0847, -0.1499,  0.1518,
          0.1427, -0.1005, -0.0491,  0.0743, -0.0330, -0.1081,  0.0498,  0.0569,
          0.1278,  0.0354, -0.1405,  0.0706, -0.0894, -0.1083,  0.1304,  0.0533,
         -0.0353, -0.0431, -0.0910, -0.0302,  0.0332,  0.0107,  0.0199, -0.1283,
          0.1111,  0.0253, -0.0477, -0.0506,  0.0522,  0.0993,  0.1343, -0.1382,
         -0.0078,  0.0245, -0.0215,  0.1394,  0.1421, -0.0148, -0.1293,  0.0678,
         -0.1416, -0.0664, -0.1088, -0.1104,  0.1364, -0.1322,  0.1255,  0.0856,
          0.1098,  0.0337,  0.1487, -0.0514,  0.1474,  0.0928, -0.0682, -0.1167,
          0.1088,  0.0378,  0.1349, -0.0764, -0.0880, -0.0658, -0.0500, -0.1013,
         -0.0866, -0.0305,  0.0369,  0.0804,  0.0726, -0.1458, -0.0745, -0.1424,
          0.1498, -0.1316,  0.0550,  0.1308,  0.0575, -0.1383, -0.0260,  0.1497,
          0.0360, -0.0964, -0.1283,  0.0032, -0.0351, -0.0081, -0.0180, -0.0441,
         -0.0055, -0.0201,  0.1413, -0.1389,  0.0658, -0.0718, -0.0264, -0.0264,
         -0.0070, -0.0031, -0.1180, -0.0673,  0.0779, -0.0640, -0.1289, -0.1323,
         -0.0878,  0.0791,  0.1227, -0.0592, -0.0498,  0.0606, -0.0861, -0.1291,
         -0.0584,  0.1029, -0.1208,  0.1396,  0.0306,  0.0707, -0.0133, -0.0098,
          0.0254, -0.1048,  0.1207, -0.0637,  0.1314, -0.0030,  0.0619, -0.1379,
          0.0019, -0.1066, -0.1118, -0.1367, -0.0531,  0.1518,  0.0916,  0.0077,
         -0.0251, -0.0713,  0.0529,  0.0157, -0.1205, -0.0335,  0.1396,  0.0811,
          0.0666,  0.0966,  0.0299,  0.0954,  0.0812,  0.1422, -0.0301, -0.1098,
         -0.0406, -0.0972,  0.0289, -0.0586, -0.1241,  0.1523, -0.0904,  0.1003,
          0.0597, -0.0694, -0.0776, -0.0685,  0.0552, -0.0912, -0.0964, -0.0483,
         -0.1360, -0.0741,  0.1120,  0.0616, -0.0767,  0.1342, -0.0286, -0.0820,
         -0.1048,  0.1266,  0.1145,  0.1445,  0.1479, -0.0474,  0.0095, -0.1198,
         -0.1481,  0.1421,  0.0834, -0.0076,  0.0772, -0.0870,  0.1062,  0.0560,
          0.0668, -0.1517,  0.1024, -0.1342, -0.0016, -0.0662, -0.0523,  0.0515,
          0.0408, -0.0137,  0.0156,  0.1397,  0.0428, -0.0873, -0.1226, -0.0412,
         -0.1200,  0.1337, -0.0282, -0.1528, -0.1064,  0.0125,  0.0497,  0.0546,
         -0.0080,  0.1505,  0.1369,  0.1070,  0.0446,  0.1241, -0.1302,  0.0063,
         -0.0652,  0.0182,  0.1093,  0.1299,  0.0854,  0.1481,  0.1219,  0.1432,
          0.1451, -0.1117, -0.0381, -0.0881,  0.0947, -0.0878, -0.1096,  0.1035,
         -0.0825,  0.0419, -0.0438,  0.1402,  0.1382, -0.1437,  0.1433, -0.0712]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0063,  0.0152, -0.0119,  ...,  0.0555, -0.0520, -0.0646],
        [ 0.0058, -0.0624, -0.0247,  ..., -0.0164,  0.0993, -0.0196],
        [ 0.1177, -0.0912,  0.0645,  ..., -0.0042,  0.0743, -0.0925],
        ...,
        [ 0.0221,  0.0094, -0.0095,  ...,  0.0018, -0.0958,  0.0446],
        [ 0.0637,  0.1158, -0.0296,  ...,  0.0270,  0.0062,  0.0769],
        [-0.0012, -0.0429, -0.0944,  ...,  0.0282, -0.0887, -0.1105]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1568, -0.1723, -0.1564,  ..., -0.1637, -0.1646,  0.1723],
        [ 0.1337, -0.1133,  0.0546,  ...,  0.0173,  0.0103,  0.0990],
        [-0.1011, -0.1083, -0.1123,  ..., -0.1081, -0.1696,  0.1449],
        ...,
        [ 0.0124,  0.0806,  0.1275,  ..., -0.0900,  0.0689,  0.0435],
        [-0.0123,  0.1542, -0.1572,  ..., -0.0634, -0.0045, -0.0464],
        [ 0.1243,  0.0174,  0.0522,  ..., -0.0036, -0.1215,  0.0801]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0883,  0.1140, -0.1883,  ..., -0.1540, -0.1293, -0.1944],
        [ 0.2405,  0.2077, -0.0554,  ..., -0.0422,  0.1521, -0.1979],
        [-0.1607, -0.2002,  0.1543,  ..., -0.0874,  0.2298, -0.1551],
        ...,
        [ 0.0820,  0.0813,  0.2164,  ..., -0.1135,  0.2294, -0.0318],
        [ 0.0840, -0.1815,  0.0092,  ..., -0.2243,  0.1669, -0.0232],
        [-0.1698,  0.2467,  0.1971,  ..., -0.1742,  0.0146, -0.1789]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.2123],
        [ 0.1320],
        [ 0.0118],
        [ 0.0849],
        [-0.3031],
        [ 0.3108],
        [-0.0375],
        [-0.1150],
        [-0.4142],
        [ 0.2545],
        [ 0.2714],
        [ 0.0989],
        [-0.2744],
        [ 0.2483],
        [ 0.0601],
        [ 0.0345],
        [ 0.0608],
        [-0.4055],
        [-0.0793],
        [ 0.1237],
        [-0.0927],
        [-0.0975],
        [ 0.0324],
        [ 0.1073],
        [ 0.0968],
        [ 0.0810],
        [-0.0074],
        [-0.0077],
        [ 0.3958],
        [ 0.2451],
        [ 0.0252],
        [-0.1678]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0114, -0.0023,  0.0160,  ..., -0.0211,  0.0210, -0.0104],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(111.5446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2661, device='cuda:0')



h[100].sum tensor(19.2508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.4194, device='cuda:0')



h[200].sum tensor(19.5272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0606,  ..., 0.0000, 0.0795, 0.0000],
        [0.0000, 0.0000, 0.0499,  ..., 0.0000, 0.0654, 0.0000],
        [0.0000, 0.0000, 0.0117,  ..., 0.0000, 0.0153, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32927.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3429, 0.5062, 0.0000,  ..., 0.0310, 0.3584, 0.0000],
        [0.2938, 0.4337, 0.0000,  ..., 0.0265, 0.3071, 0.0000],
        [0.2360, 0.3483, 0.0000,  ..., 0.0213, 0.2466, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189540.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4777.1885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(128.8999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3328.1465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-190.7524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(2.8954, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[1.6678e-01],
        [1.8032e-01],
        [1.9908e-01],
        ...,
        [2.0068e-07],
        [2.6345e-08],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(3110.3899, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(67.2512, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0001],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365923.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.0000e-04,  ..., 0.0000e+00, 1.0000e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0000e-04,  ..., 0.0000e+00, 1.0000e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0000e-04,  ..., 0.0000e+00, 1.0000e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0000e-04,  ..., 0.0000e+00, 1.0000e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0000e-04,  ..., 0.0000e+00, 1.0000e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0000e-04,  ..., 0.0000e+00, 1.0000e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96.1785, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3230, device='cuda:0')



h[100].sum tensor(14.3317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3448, device='cuda:0')



h[200].sum tensor(21.4366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7000, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0004, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33166.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0028, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0028, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0014, 0.0033, 0.0004,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0011, 0.0028, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0028, 0.0004,  ..., 0.0000, 0.0000, 0.0000],
        [0.0011, 0.0028, 0.0004,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(200894.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4951.1855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(148.1650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3517.3994, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.3370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(251.3958, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(17.3024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0401],
        [0.0500],
        [0.0647],
        ...,
        [0.0105],
        [0.0105],
        [0.0105]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(24169.6914, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0001],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365923.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0002],
        [0.9999],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365912.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 3.4900e-05,  ..., 0.0000e+00, 2.7279e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.4900e-05,  ..., 0.0000e+00, 2.7279e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.4900e-05,  ..., 0.0000e+00, 2.7279e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.4900e-05,  ..., 0.0000e+00, 2.7279e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.4900e-05,  ..., 0.0000e+00, 2.7279e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.4900e-05,  ..., 0.0000e+00, 2.7279e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22.3425, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8637, device='cuda:0')



h[100].sum tensor(10.4547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7727, device='cuda:0')



h[200].sum tensor(13.8226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0541,  ..., 0.0000, 0.0708, 0.0000],
        [0.0000, 0.0000, 0.0359,  ..., 0.0000, 0.0470, 0.0000],
        [0.0000, 0.0000, 0.0298,  ..., 0.0000, 0.0390, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24505.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2872, 0.4266, 0.0000,  ..., 0.0225, 0.2976, 0.0000],
        [0.2304, 0.3424, 0.0000,  ..., 0.0177, 0.2387, 0.0000],
        [0.1751, 0.2605, 0.0000,  ..., 0.0131, 0.1814, 0.0000],
        ...,
        [0.0005, 0.0014, 0.0004,  ..., 0.0000, 0.0005, 0.0000],
        [0.0005, 0.0014, 0.0004,  ..., 0.0000, 0.0005, 0.0000],
        [0.0005, 0.0014, 0.0004,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155980.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3826.5244, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(105.2504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2586.6899, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.0311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(102.3978, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(26.0864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1523],
        [0.1412],
        [0.1228],
        ...,
        [0.0038],
        [0.0038],
        [0.0038]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(11194.1484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0002],
        [0.9999],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365912.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365904.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2264e-03, -8.2978e-04,  5.9184e-03,  ..., -7.8119e-03,
          7.7548e-03, -3.8618e-03],
        [-1.0246e-02, -2.0115e-03,  1.4396e-02,  ..., -1.8938e-02,
          1.8870e-02, -9.3618e-03],
        [-7.4968e-03, -1.4719e-03,  1.0524e-02,  ..., -1.3857e-02,
          1.3794e-02, -6.8501e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.3913e-05,  ...,  0.0000e+00,
         -4.9819e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.3913e-05,  ...,  0.0000e+00,
         -4.9819e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.3913e-05,  ...,  0.0000e+00,
         -4.9819e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-11.6291, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4398, device='cuda:0')



h[100].sum tensor(12.8102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6443, device='cuda:0')



h[200].sum tensor(12.3945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0514,  ..., 0.0000, 0.0674, 0.0000],
        [0.0000, 0.0000, 0.0331,  ..., 0.0000, 0.0434, 0.0000],
        [0.0000, 0.0000, 0.0278,  ..., 0.0000, 0.0365, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24508.6816, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1922, 0.2871, 0.0000,  ..., 0.0165, 0.2033, 0.0000],
        [0.1717, 0.2566, 0.0000,  ..., 0.0147, 0.1816, 0.0000],
        [0.1475, 0.2206, 0.0000,  ..., 0.0124, 0.1563, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0009, 0.0003],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0009, 0.0003],
        [0.0000, 0.0000, 0.0008,  ..., 0.0000, 0.0009, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149853.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3549.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(93.9449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2420.9521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-168.3956, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.1799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0446],
        [0.0426],
        [0.0384],
        ...,
        [0.0008],
        [0.0008],
        [0.0008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(1758.2786, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365904.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365898.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -5.4636e-05,  ...,  0.0000e+00,
         -1.0058e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4636e-05,  ...,  0.0000e+00,
         -1.0058e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4636e-05,  ...,  0.0000e+00,
         -1.0058e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.4636e-05,  ...,  0.0000e+00,
         -1.0058e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4636e-05,  ...,  0.0000e+00,
         -1.0058e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4636e-05,  ...,  0.0000e+00,
         -1.0058e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-68.6613, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2386, device='cuda:0')



h[100].sum tensor(12.2013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9801, device='cuda:0')



h[200].sum tensor(9.4835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22228.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0006, 0.0003],
        [0.0018, 0.0030, 0.0010,  ..., 0.0000, 0.0028, 0.0003],
        [0.0101, 0.0157, 0.0006,  ..., 0.0003, 0.0119, 0.0002],
        ...,
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0006, 0.0003],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0006, 0.0003],
        [0.0000, 0.0000, 0.0012,  ..., 0.0000, 0.0006, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(131789.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3001.3330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(85.2439, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2030.8092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-167.4257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(26.9486, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0084],
        [-0.0093],
        [-0.0106],
        ...,
        [-0.0041],
        [-0.0035],
        [-0.0035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-2702.1055, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365898.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365892.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1752e-03, -1.3941e-03,  1.0078e-02,  ..., -1.3278e-02,
          1.3144e-02, -6.5546e-03],
        [ 0.0000e+00,  0.0000e+00, -4.7396e-05,  ...,  0.0000e+00,
         -1.3280e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.7396e-05,  ...,  0.0000e+00,
         -1.3280e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.7396e-05,  ...,  0.0000e+00,
         -1.3280e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.7396e-05,  ...,  0.0000e+00,
         -1.3280e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.7396e-05,  ...,  0.0000e+00,
         -1.3280e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-85.1028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2696, device='cuda:0')



h[100].sum tensor(17.8902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8683, device='cuda:0')



h[200].sum tensor(14.0390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.3657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0377,  ..., 0.0000, 0.0493, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0131, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34446.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7065e-01, 2.5577e-01, 0.0000e+00,  ..., 1.4926e-02, 1.8401e-01,
         0.0000e+00],
        [7.5666e-02, 1.1445e-01, 2.0772e-04,  ..., 5.7399e-03, 8.2708e-02,
         0.0000e+00],
        [2.0580e-02, 3.1558e-02, 8.3044e-04,  ..., 1.0458e-03, 2.3063e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.6541e-03,  ..., 0.0000e+00, 2.2036e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6541e-03,  ..., 0.0000e+00, 2.2035e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.6541e-03,  ..., 0.0000e+00, 2.2035e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218715.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5057.5825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(140.4353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3448.3643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-252.1060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.0648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0317],
        [-0.0256],
        [-0.0202],
        ...,
        [-0.0050],
        [-0.0050],
        [-0.0050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-5684.0596, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365892.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0001],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365885.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.6408e-05,  ...,  0.0000e+00,
         -1.4831e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6408e-05,  ...,  0.0000e+00,
         -1.4831e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6408e-05,  ...,  0.0000e+00,
         -1.4831e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6408e-05,  ...,  0.0000e+00,
         -1.4831e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6408e-05,  ...,  0.0000e+00,
         -1.4831e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6408e-05,  ...,  0.0000e+00,
         -1.4831e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-150.9793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5363, device='cuda:0')



h[100].sum tensor(15.0512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3568, device='cuda:0')



h[200].sum tensor(11.0106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30289.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0280, 0.0431, 0.0010,  ..., 0.0014, 0.0315, 0.0000],
        [0.0052, 0.0088, 0.0016,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0021,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184709.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4149.2754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(125.8851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2805.8315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.8622, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.7884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0242],
        [-0.0192],
        [-0.0164],
        ...,
        [-0.0065],
        [-0.0064],
        [-0.0064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-5454.0537, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0001],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365885.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0000],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365878.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3164e-02, -2.5306e-03,  1.8655e-02,  ..., -2.4389e-02,
          2.4265e-02, -1.2022e-02],
        [ 0.0000e+00,  0.0000e+00,  2.8393e-05,  ...,  0.0000e+00,
         -1.5259e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.8393e-05,  ...,  0.0000e+00,
         -1.5259e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.8393e-05,  ...,  0.0000e+00,
         -1.5259e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.8393e-05,  ...,  0.0000e+00,
         -1.5259e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.8393e-05,  ...,  0.0000e+00,
         -1.5259e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-226.7643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3038, device='cuda:0')



h[100].sum tensor(10.2206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0162, device='cuda:0')



h[200].sum tensor(6.7104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0336,  ..., 0.0000, 0.0436, 0.0000],
        [0.0000, 0.0000, 0.0187,  ..., 0.0000, 0.0243, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23846.1426, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.3964e-01, 3.5757e-01, 0.0000e+00,  ..., 2.0038e-02, 2.5793e-01,
         0.0000e+00],
        [1.1884e-01, 1.7823e-01, 1.0279e-04,  ..., 8.4111e-03, 1.2894e-01,
         0.0000e+00],
        [3.7852e-02, 5.7972e-02, 2.9357e-04,  ..., 8.7567e-04, 4.2421e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.4486e-03,  ..., 0.0000e+00, 1.8950e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4487e-03,  ..., 0.0000e+00, 1.8960e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4487e-03,  ..., 0.0000e+00, 1.8957e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149781.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3225.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(103.5165, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2137.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-204.9416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(27.8083, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0326],
        [-0.0227],
        [-0.0166],
        ...,
        [-0.0070],
        [-0.0069],
        [-0.0069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-4262.0815, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0000],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365878.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0000],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365872.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  5.5347e-05,  ...,  0.0000e+00,
         -1.4657e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5347e-05,  ...,  0.0000e+00,
         -1.4657e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5347e-05,  ...,  0.0000e+00,
         -1.4657e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  5.5347e-05,  ...,  0.0000e+00,
         -1.4657e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5347e-05,  ...,  0.0000e+00,
         -1.4657e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5347e-05,  ...,  0.0000e+00,
         -1.4657e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-295.0578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4544, device='cuda:0')



h[100].sum tensor(6.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4404, device='cuda:0')



h[200].sum tensor(3.9708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0267,  ..., 0.0000, 0.0344, 0.0000],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0172, 0.0000],
        [0.0000, 0.0000, 0.0232,  ..., 0.0000, 0.0300, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(19636.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1077, 0.1620, 0.0000,  ..., 0.0059, 0.1171, 0.0000],
        [0.0969, 0.1460, 0.0000,  ..., 0.0050, 0.1057, 0.0000],
        [0.1186, 0.1782, 0.0000,  ..., 0.0072, 0.1289, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0028,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(125974.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2549.6553, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.0195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1705.5515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-186.2992, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2.0509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(24.0873, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0019],
        [ 0.0011],
        [-0.0007],
        ...,
        [-0.0068],
        [-0.0067],
        [-0.0067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-2839.9360, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0000],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365872.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0000],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365868.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  4.0722e-05,  ...,  0.0000e+00,
         -1.3600e-04,  0.0000e+00],
        [-5.2303e-03, -9.9473e-04,  7.4649e-03,  ..., -9.7020e-03,
          9.5920e-03, -4.7756e-03],
        [ 0.0000e+00,  0.0000e+00,  4.0722e-05,  ...,  0.0000e+00,
         -1.3600e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.0722e-05,  ...,  0.0000e+00,
         -1.3600e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.0722e-05,  ...,  0.0000e+00,
         -1.3600e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.0722e-05,  ...,  0.0000e+00,
         -1.3600e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-351.1701, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.0794, device='cuda:0')



h[100].sum tensor(3.3607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6800, device='cuda:0')



h[200].sum tensor(2.9272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0096, 0.0000],
        [0.0000, 0.0000, 0.0107,  ..., 0.0000, 0.0136, 0.0000],
        [0.0000, 0.0000, 0.0360,  ..., 0.0000, 0.0464, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(18572.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.2574e-02, 6.6093e-02, 5.4316e-04,  ..., 5.9950e-04, 4.7873e-02,
         0.0000e+00],
        [6.4330e-02, 9.8495e-02, 1.1706e-04,  ..., 1.3973e-03, 7.0894e-02,
         0.0000e+00],
        [1.0099e-01, 1.5308e-01, 0.0000e+00,  ..., 3.6238e-03, 1.0969e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.8313e-03,  ..., 0.0000e+00, 6.7056e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.8313e-03,  ..., 0.0000e+00, 6.7052e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.8313e-03,  ..., 0.0000e+00, 6.7052e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(123184.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2384.2356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(89.2699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1599.7517, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-188.4949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5.3995, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(21.6554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0308],
        [ 0.0359],
        [ 0.0398],
        ...,
        [-0.0063],
        [-0.0063],
        [-0.0061]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310.6251, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0000],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365868.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [0.9999],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365868.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.0908e-06,  ...,  0.0000e+00,
         -1.2203e-04,  0.0000e+00],
        [-1.7263e-02, -3.2654e-03,  2.4540e-02,  ..., -3.2041e-02,
          3.2031e-02, -1.5760e-02],
        [-1.7488e-02, -3.3080e-03,  2.4860e-02,  ..., -3.2459e-02,
          3.2450e-02, -1.5966e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0908e-06,  ...,  0.0000e+00,
         -1.2203e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0908e-06,  ...,  0.0000e+00,
         -1.2203e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0908e-06,  ...,  0.0000e+00,
         -1.2203e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-366.5927, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4058, device='cuda:0')



h[100].sum tensor(5.9143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6255, device='cuda:0')



h[200].sum tensor(7.5237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0323,  ..., 0.0000, 0.0421, 0.0000],
        [0.0000, 0.0000, 0.0449,  ..., 0.0000, 0.0585, 0.0000],
        [0.0000, 0.0000, 0.0941,  ..., 0.0000, 0.1227, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25363.0684, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1619, 0.2442, 0.0000,  ..., 0.0089, 0.1740, 0.0000],
        [0.2558, 0.3833, 0.0000,  ..., 0.0173, 0.2732, 0.0000],
        [0.3901, 0.5824, 0.0000,  ..., 0.0294, 0.4153, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158095.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3141.9648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(119.9464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2165.0076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-238.1555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14.8160, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(26.5641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0830],
        [ 0.0851],
        [ 0.0880],
        ...,
        [-0.0074],
        [-0.0073],
        [-0.0073]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(2086.0596, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [0.9999],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365868.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(596.0454, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [0.9999],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365870.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0933e-03, -1.1463e-03,  8.6377e-03,  ..., -1.1316e-02,
          1.1261e-02, -5.5622e-03],
        [-7.3776e-03, -1.3879e-03,  1.0466e-02,  ..., -1.3701e-02,
          1.3657e-02, -6.7345e-03],
        [-1.2140e-02, -2.2838e-03,  1.7247e-02,  ..., -2.2545e-02,
          2.2539e-02, -1.1082e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.8519e-05,  ...,  0.0000e+00,
         -1.0374e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.8519e-05,  ...,  0.0000e+00,
         -1.0374e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.8519e-05,  ...,  0.0000e+00,
         -1.0374e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-365.2130, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4620, device='cuda:0')



h[100].sum tensor(10.3475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5277, device='cuda:0')



h[200].sum tensor(13.7738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.2044, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0451,  ..., 0.0000, 0.0589, 0.0000],
        [0.0000, 0.0000, 0.0486,  ..., 0.0000, 0.0634, 0.0000],
        [0.0000, 0.0000, 0.0336,  ..., 0.0000, 0.0438, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33977.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3776e-01, 2.0980e-01, 0.0000e+00,  ..., 5.3427e-03, 1.4754e-01,
         0.0000e+00],
        [1.7771e-01, 2.6925e-01, 0.0000e+00,  ..., 8.5608e-03, 1.8941e-01,
         0.0000e+00],
        [2.1902e-01, 3.3067e-01, 0.0000e+00,  ..., 1.1875e-02, 2.3267e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.6479e-05, 3.0168e-03,  ..., 0.0000e+00, 6.2137e-04,
         0.0000e+00],
        [0.0000e+00, 3.6516e-05, 3.0167e-03,  ..., 0.0000e+00, 6.2133e-04,
         0.0000e+00],
        [0.0000e+00, 3.6425e-05, 3.0167e-03,  ..., 0.0000e+00, 6.2131e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(197525.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4020.1990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(155.9859, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2816.2480, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-300.9679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(23.2674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.9182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0987],
        [ 0.1128],
        [ 0.1274],
        ...,
        [-0.0093],
        [-0.0092],
        [-0.0092]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(4494.8633, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [0.9999],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365870.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [1.0000],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365873.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -6.8817e-05,  ...,  0.0000e+00,
         -8.3318e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.8817e-05,  ...,  0.0000e+00,
         -8.3318e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.8817e-05,  ...,  0.0000e+00,
         -8.3318e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -6.8817e-05,  ...,  0.0000e+00,
         -8.3318e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.8817e-05,  ...,  0.0000e+00,
         -8.3318e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.8817e-05,  ...,  0.0000e+00,
         -8.3318e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-344.4360, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.5205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.0752, device='cuda:0')



h[100].sum tensor(16.6955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.8438, device='cuda:0')



h[200].sum tensor(21.7933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9654, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0120,  ..., 0.0000, 0.0157, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46018.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2838e-03, 4.6060e-03, 3.0024e-03,  ..., 0.0000e+00, 3.9345e-03,
         0.0000e+00],
        [1.3394e-02, 2.2622e-02, 1.8909e-03,  ..., 0.0000e+00, 1.6387e-02,
         0.0000e+00],
        [4.6039e-02, 7.3968e-02, 6.1365e-04,  ..., 3.6037e-04, 5.1956e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.6556e-05, 3.2699e-03,  ..., 0.0000e+00, 7.4945e-04,
         0.0000e+00],
        [0.0000e+00, 4.6850e-05, 3.2700e-03,  ..., 0.0000e+00, 7.4960e-04,
         0.0000e+00],
        [0.0000e+00, 4.6860e-05, 3.2700e-03,  ..., 0.0000e+00, 7.4958e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(268271.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5659.2041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(207.1529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4037.2754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-384.9872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31.7825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(35.6760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0283],
        [ 0.0514],
        [ 0.0795],
        ...,
        [-0.0110],
        [-0.0110],
        [-0.0110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(8340.1455, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [1.0000],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365873.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0000],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365875.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7280e-02, -3.2152e-03,  2.4578e-02,  ..., -3.2131e-02,
          3.2246e-02, -1.5770e-02],
        [-2.5123e-02, -4.6745e-03,  3.5779e-02,  ..., -4.6714e-02,
          4.6914e-02, -2.2928e-02],
        [-1.7960e-02, -3.3416e-03,  2.5548e-02,  ..., -3.3394e-02,
          3.3516e-02, -1.6390e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -9.9765e-05,  ...,  0.0000e+00,
         -7.2275e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.9765e-05,  ...,  0.0000e+00,
         -7.2275e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.9765e-05,  ...,  0.0000e+00,
         -7.2275e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-433.6213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.9976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9236, device='cuda:0')



h[100].sum tensor(9.4050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7830, device='cuda:0')



h[200].sum tensor(14.8328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.5290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1103,  ..., 0.0000, 0.1446, 0.0000],
        [0.0000, 0.0000, 0.1288,  ..., 0.0000, 0.1689, 0.0000],
        [0.0000, 0.0000, 0.1067,  ..., 0.0000, 0.1400, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35851.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.1839e-01, 6.2700e-01, 0.0000e+00,  ..., 2.8747e-02, 4.4143e-01,
         0.0000e+00],
        [5.1057e-01, 7.6373e-01, 0.0000e+00,  ..., 3.6635e-02, 5.3784e-01,
         0.0000e+00],
        [4.9256e-01, 7.3727e-01, 0.0000e+00,  ..., 3.4828e-02, 5.1885e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.4083e-03,  ..., 0.0000e+00, 7.0477e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.4085e-03,  ..., 0.0000e+00, 7.0474e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.4085e-03,  ..., 0.0000e+00, 7.0473e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215559.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4240.3701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(165.8341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3117.3679, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-334.2120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36.0079, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.9652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1798],
        [ 0.2067],
        [ 0.2272],
        ...,
        [-0.0147],
        [-0.0147],
        [-0.0147]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(5534.7432, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0000],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365875.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365878.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-465.0575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6370, device='cuda:0')



h[100].sum tensor(9.3368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0714, device='cuda:0')



h[200].sum tensor(14.5417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35702.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0006, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0037,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225221.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4363.7100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(164.5829, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3299.7515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-342.5256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(30.6282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(35.7266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0127],
        [-0.0079],
        [ 0.0037],
        ...,
        [-0.0054],
        [-0.0147],
        [-0.0173]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(4505.1553, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365878.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365878.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.5823e-03, -1.4029e-03,  1.0712e-02,  ..., -1.4107e-02,
          1.4131e-02, -6.9189e-03],
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        [-1.2900e-02, -2.3869e-03,  1.8316e-02,  ..., -2.4001e-02,
          2.4088e-02, -1.1771e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2984e-04,  ...,  0.0000e+00,
         -6.6362e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-481.8371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4475, device='cuda:0')



h[100].sum tensor(7.3695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8606, device='cuda:0')



h[200].sum tensor(12.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0842, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0115, 0.0000],
        [0.0000, 0.0000, 0.0465,  ..., 0.0000, 0.0613, 0.0000],
        [0.0000, 0.0000, 0.0259,  ..., 0.0000, 0.0342, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33122.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.9140e-02, 9.4681e-02, 4.5063e-04,  ..., 4.4462e-05, 6.6053e-02,
         0.0000e+00],
        [1.2538e-01, 1.9421e-01, 0.0000e+00,  ..., 2.2359e-03, 1.3451e-01,
         0.0000e+00],
        [1.2740e-01, 1.9783e-01, 0.0000e+00,  ..., 1.9947e-03, 1.3625e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.6636e-03,  ..., 0.0000e+00, 5.6450e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6636e-03,  ..., 0.0000e+00, 5.6419e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6636e-03,  ..., 0.0000e+00, 5.6413e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209652.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3907.1265, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(157.0956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3036.9312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-330.5268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(29.9729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(36.1852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0903],
        [ 0.1196],
        [ 0.1409],
        ...,
        [-0.0191],
        [-0.0190],
        [-0.0190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(7856.9150, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365878.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0007],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365881.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7052e-03, -1.2337e-03,  9.4424e-03,  ..., -1.2482e-02,
          1.2506e-02, -6.1177e-03],
        [ 0.0000e+00,  0.0000e+00, -1.5754e-04,  ...,  0.0000e+00,
         -6.3934e-05,  0.0000e+00],
        [-6.7052e-03, -1.2337e-03,  9.4424e-03,  ..., -1.2482e-02,
          1.2506e-02, -6.1177e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5754e-04,  ...,  0.0000e+00,
         -6.3934e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5754e-04,  ...,  0.0000e+00,
         -6.3934e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5754e-04,  ...,  0.0000e+00,
         -6.3934e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-537.8870, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5358, device='cuda:0')



h[100].sum tensor(3.9166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1445, device='cuda:0')



h[200].sum tensor(8.6897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0077,  ..., 0.0000, 0.0102, 0.0000],
        [0.0000, 0.0000, 0.0343,  ..., 0.0000, 0.0454, 0.0000],
        [0.0000, 0.0000, 0.0077,  ..., 0.0000, 0.0102, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25347.0488, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0375, 0.0627, 0.0009,  ..., 0.0000, 0.0442, 0.0000],
        [0.0680, 0.1087, 0.0000,  ..., 0.0000, 0.0757, 0.0000],
        [0.0375, 0.0627, 0.0009,  ..., 0.0000, 0.0442, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0041,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0041,  ..., 0.0000, 0.0006, 0.0000],
        [0.0000, 0.0000, 0.0041,  ..., 0.0000, 0.0006, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164564.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2757.6165, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.0401, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2194.5369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-291.8813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(14.1446, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(27.3439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0084],
        [ 0.0168],
        [ 0.0180],
        ...,
        [-0.0234],
        [-0.0233],
        [-0.0232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-1383.8992, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0007],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365881.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0002],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365885.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4520e-02, -2.6565e-03,  2.0628e-02,  ..., -2.7046e-02,
          2.7186e-02, -1.3246e-02],
        [-1.3424e-02, -2.4560e-03,  1.9057e-02,  ..., -2.5005e-02,
          2.5130e-02, -1.2246e-02],
        [-2.0679e-02, -3.7834e-03,  2.9457e-02,  ..., -3.8520e-02,
          3.8747e-02, -1.8865e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8574e-04,  ...,  0.0000e+00,
         -6.4411e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8574e-04,  ...,  0.0000e+00,
         -6.4411e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8574e-04,  ...,  0.0000e+00,
         -6.4411e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-530.3643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8729, device='cuda:0')



h[100].sum tensor(7.1441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9899, device='cuda:0')



h[200].sum tensor(11.3036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9767, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0822,  ..., 0.0000, 0.1084, 0.0000],
        [0.0000, 0.0000, 0.1068,  ..., 0.0000, 0.1406, 0.0000],
        [0.0000, 0.0000, 0.1290,  ..., 0.0000, 0.1696, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28745.8418, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.8791e-01, 5.8495e-01, 0.0000e+00,  ..., 2.6060e-02, 4.1325e-01,
         0.0000e+00],
        [4.6772e-01, 7.0303e-01, 0.0000e+00,  ..., 3.3642e-02, 4.9760e-01,
         0.0000e+00],
        [5.5903e-01, 8.3811e-01, 0.0000e+00,  ..., 4.2388e-02, 5.9412e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.5189e-03,  ..., 0.0000e+00, 7.0015e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.5189e-03,  ..., 0.0000e+00, 7.0028e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.5187e-03,  ..., 0.0000e+00, 7.0015e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180226.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2983.7964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(133.2587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2436.0283, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-324.3760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4.1168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(16.8854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1149],
        [ 0.1137],
        [ 0.1131],
        ...,
        [-0.0273],
        [-0.0271],
        [-0.0271]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-3416.4917, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0002],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365885.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0008],
        [1.0003],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365890.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.1065e-03, -9.2898e-04,  7.1200e-03,  ..., -9.5177e-03,
          9.5308e-03, -4.6579e-03],
        [-4.7716e-03, -8.6807e-04,  6.6394e-03,  ..., -8.8936e-03,
          8.9016e-03, -4.3525e-03],
        [ 0.0000e+00,  0.0000e+00, -2.0911e-04,  ...,  0.0000e+00,
         -6.4337e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0911e-04,  ...,  0.0000e+00,
         -6.4337e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0911e-04,  ...,  0.0000e+00,
         -6.4337e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0911e-04,  ...,  0.0000e+00,
         -6.4337e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-459.4052, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.7642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0321, device='cuda:0')



h[100].sum tensor(16.7580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.1610, device='cuda:0')



h[200].sum tensor(20.6089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0559,  ..., 0.0000, 0.0741, 0.0000],
        [0.0000, 0.0000, 0.0125,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0000, 0.0274,  ..., 0.0000, 0.0365, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47162.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2121, 0.3250, 0.0000,  ..., 0.0098, 0.2288, 0.0000],
        [0.1209, 0.1894, 0.0000,  ..., 0.0022, 0.1327, 0.0000],
        [0.1337, 0.2088, 0.0000,  ..., 0.0027, 0.1460, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(303656.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5887.1377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.5782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4611.4287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-447.5357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.0568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(9.2505, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1001],
        [ 0.1096],
        [ 0.1186],
        ...,
        [-0.0229],
        [-0.0226],
        [-0.0237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-1652.1661, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0008],
        [1.0003],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365890.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0008],
        [1.0003],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365896.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.3402e-04,  ...,  0.0000e+00,
         -6.9862e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3402e-04,  ...,  0.0000e+00,
         -6.9862e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3402e-04,  ...,  0.0000e+00,
         -6.9862e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.3402e-04,  ...,  0.0000e+00,
         -6.9862e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3402e-04,  ...,  0.0000e+00,
         -6.9862e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3402e-04,  ...,  0.0000e+00,
         -6.9862e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-520.6608, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.6271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3618, device='cuda:0')



h[100].sum tensor(11.4241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.9193, device='cuda:0')



h[200].sum tensor(14.4645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.4305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34995.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(228120., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3835.0315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(158.5452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3253.7634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-389.9261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.5081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-0.7716, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0470],
        [-0.0477],
        [-0.0467],
        ...,
        [-0.0343],
        [-0.0323],
        [-0.0288]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-4889.2236, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0008],
        [1.0003],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365896.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0009],
        [1.0004],
        ...,
        [0.9999],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365903.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3641e-03, -1.1446e-03,  8.8986e-03,  ..., -1.1876e-02,
          1.1908e-02, -5.8037e-03],
        [-6.6703e-03, -1.1997e-03,  9.3391e-03,  ..., -1.2447e-02,
          1.2485e-02, -6.0828e-03],
        [-6.3641e-03, -1.1446e-03,  8.8986e-03,  ..., -1.1876e-02,
          1.1908e-02, -5.8037e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.5724e-04,  ...,  0.0000e+00,
         -7.7133e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5724e-04,  ...,  0.0000e+00,
         -7.7133e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5724e-04,  ...,  0.0000e+00,
         -7.7133e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-492.4088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5712, device='cuda:0')



h[100].sum tensor(15.5147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2471, device='cuda:0')



h[200].sum tensor(17.9058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0413,  ..., 0.0000, 0.0552, 0.0000],
        [0.0000, 0.0000, 0.0401,  ..., 0.0000, 0.0535, 0.0000],
        [0.0000, 0.0000, 0.0166,  ..., 0.0000, 0.0222, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42093.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1138, 0.1801, 0.0000,  ..., 0.0009, 0.1263, 0.0000],
        [0.1148, 0.1818, 0.0000,  ..., 0.0009, 0.1273, 0.0000],
        [0.0869, 0.1398, 0.0005,  ..., 0.0003, 0.0979, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(274837.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4927.3760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.3860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4023.7959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-442.5751, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-7.9089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0654],
        [ 0.0778],
        [ 0.0855],
        ...,
        [-0.0368],
        [-0.0362],
        [-0.0365]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-10167.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0009],
        [1.0004],
        ...,
        [0.9999],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365903.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(530.5594, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0009],
        [1.0005],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365911.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8085e-03, -8.5985e-04,  6.6518e-03,  ..., -8.9787e-03,
          8.9853e-03, -4.3845e-03],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-561.3463, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1671, device='cuda:0')



h[100].sum tensor(9.2611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5996, device='cuda:0')



h[200].sum tensor(11.4644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4804, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0262,  ..., 0.0000, 0.0348, 0.0000],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0090, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28986.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4089e-01, 2.1938e-01, 4.3364e-04,  ..., 4.7565e-03, 1.5573e-01,
         0.0000e+00],
        [6.7191e-02, 1.0886e-01, 2.3757e-03,  ..., 1.0001e-03, 7.8086e-02,
         0.0000e+00],
        [2.0304e-02, 3.4381e-02, 4.6672e-03,  ..., 0.0000e+00, 2.5603e-02,
         5.0165e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 6.5133e-03,  ..., 0.0000e+00, 9.6060e-04,
         8.6549e-05],
        [0.0000e+00, 0.0000e+00, 6.5135e-03,  ..., 0.0000e+00, 9.6038e-04,
         8.6602e-05],
        [0.0000e+00, 0.0000e+00, 6.5135e-03,  ..., 0.0000e+00, 9.6041e-04,
         8.6732e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(192494.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2815.2192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(129.7357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2508.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-374.7047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-13.3235, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0357],
        [ 0.0192],
        [-0.0030],
        ...,
        [-0.0431],
        [-0.0429],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14615.6465, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0009],
        [1.0005],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365911.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0009],
        [1.0005],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365911.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7484e-04,  ...,  0.0000e+00,
         -8.1549e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-581.9734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3622, device='cuda:0')



h[100].sum tensor(7.1329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0485, device='cuda:0')



h[200].sum tensor(9.3170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5077, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0164,  ..., 0.0000, 0.0218, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24674.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.7868e-03, 8.7584e-03, 5.9905e-03,  ..., 0.0000e+00, 7.9477e-03,
         0.0000e+00],
        [1.9064e-02, 3.2998e-02, 4.7138e-03,  ..., 0.0000e+00, 2.5212e-02,
         0.0000e+00],
        [6.6357e-02, 1.0672e-01, 2.4292e-03,  ..., 1.5614e-03, 7.7664e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 6.5133e-03,  ..., 0.0000e+00, 9.6060e-04,
         8.6549e-05],
        [0.0000e+00, 0.0000e+00, 6.5135e-03,  ..., 0.0000e+00, 9.6038e-04,
         8.6602e-05],
        [0.0000e+00, 0.0000e+00, 6.5135e-03,  ..., 0.0000e+00, 9.6041e-04,
         8.6732e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164457.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2212.2964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(109.9010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1960.0181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-346.3282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-13.5305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0039],
        [-0.0036],
        [-0.0089],
        ...,
        [-0.0431],
        [-0.0429],
        [-0.0428]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-17043.8828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0009],
        [1.0005],
        ...,
        [0.9999],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365911.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0005],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365921.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.8552e-04,  ...,  0.0000e+00,
         -8.5271e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8552e-04,  ...,  0.0000e+00,
         -8.5271e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8552e-04,  ...,  0.0000e+00,
         -8.5271e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.8552e-04,  ...,  0.0000e+00,
         -8.5271e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8552e-04,  ...,  0.0000e+00,
         -8.5271e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8552e-04,  ...,  0.0000e+00,
         -8.5271e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-527.7586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7822, device='cuda:0')



h[100].sum tensor(13.4802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1518, device='cuda:0')



h[200].sum tensor(16.1651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37059.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.0294e-02, 5.1671e-02, 3.7890e-03,  ..., 0.0000e+00, 3.7853e-02,
         9.9130e-05],
        [6.2139e-03, 1.3257e-02, 5.9198e-03,  ..., 0.0000e+00, 1.0710e-02,
         2.5526e-04],
        [1.1212e-04, 2.0985e-03, 6.5894e-03,  ..., 0.0000e+00, 2.7943e-03,
         4.0824e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 6.7271e-03,  ..., 0.0000e+00, 8.5674e-04,
         5.0086e-04],
        [0.0000e+00, 0.0000e+00, 6.7265e-03,  ..., 0.0000e+00, 8.5644e-04,
         5.0046e-04],
        [0.0000e+00, 0.0000e+00, 6.7255e-03,  ..., 0.0000e+00, 8.5632e-04,
         5.0018e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244547.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3983.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(166.0929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3451.7119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-434.2900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-16.7461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0189],
        [-0.0211],
        [-0.0135],
        ...,
        [-0.0473],
        [-0.0470],
        [-0.0469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-13401.7266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0005],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365921.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0006],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365932.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.1272e-03, -7.2946e-04,  5.6703e-03,  ..., -7.7157e-03,
          7.7123e-03, -3.7623e-03],
        [-5.2534e-03, -9.2852e-04,  7.2970e-03,  ..., -9.8212e-03,
          9.8412e-03, -4.7889e-03],
        [ 0.0000e+00,  0.0000e+00, -2.9078e-04,  ...,  0.0000e+00,
         -8.9085e-05,  0.0000e+00],
        ...,
        [-6.7755e-03, -1.1975e-03,  9.4954e-03,  ..., -1.2667e-02,
          1.2718e-02, -6.1765e-03],
        [ 0.0000e+00,  0.0000e+00, -2.9078e-04,  ...,  0.0000e+00,
         -8.9085e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.9078e-04,  ...,  0.0000e+00,
         -8.9085e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-552.6624, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3579, device='cuda:0')



h[100].sum tensor(11.5731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8111, device='cuda:0')



h[200].sum tensor(14.9973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0553,  ..., 0.0000, 0.0736, 0.0000],
        [0.0000, 0.0000, 0.0221,  ..., 0.0000, 0.0295, 0.0000],
        [0.0000, 0.0000, 0.0073,  ..., 0.0000, 0.0099, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0699,  ..., 0.0000, 0.0923, 0.0000],
        [0.0000, 0.0000, 0.0618,  ..., 0.0000, 0.0817, 0.0000],
        [0.0000, 0.0000, 0.0444,  ..., 0.0000, 0.0587, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31778.9707, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8863e-01, 2.9224e-01, 0.0000e+00,  ..., 7.5669e-03, 2.0472e-01,
         0.0000e+00],
        [1.2224e-01, 1.9290e-01, 4.7638e-04,  ..., 3.4829e-03, 1.3525e-01,
         0.0000e+00],
        [5.7794e-02, 9.6289e-02, 2.4804e-03,  ..., 6.0435e-04, 6.8022e-02,
         0.0000e+00],
        ...,
        [4.6226e-01, 6.9589e-01, 0.0000e+00,  ..., 3.6094e-02, 4.9569e-01,
         0.0000e+00],
        [3.6819e-01, 5.5669e-01, 0.0000e+00,  ..., 2.6569e-02, 3.9599e-01,
         0.0000e+00],
        [2.5295e-01, 3.8560e-01, 0.0000e+00,  ..., 1.5162e-02, 2.7419e-01,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209292.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3098.0786, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(145.9345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2745.2686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-409.8833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.1020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0617],
        [ 0.0498],
        [ 0.0289],
        ...,
        [-0.0717],
        [-0.0452],
        [-0.0238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-18250.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0006],
        ...,
        [1.0000],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365932.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0006],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365944., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.9039e-04,  ...,  0.0000e+00,
         -9.1926e-05,  0.0000e+00],
        [-7.9948e-03, -1.4048e-03,  1.1273e-02,  ..., -1.4955e-02,
          1.5040e-02, -7.2871e-03],
        [-7.9948e-03, -1.4048e-03,  1.1273e-02,  ..., -1.4955e-02,
          1.5040e-02, -7.2871e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.9039e-04,  ...,  0.0000e+00,
         -9.1926e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.9039e-04,  ...,  0.0000e+00,
         -9.1926e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.9039e-04,  ...,  0.0000e+00,
         -9.1926e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-581.7017, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.8284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3921, device='cuda:0')



h[100].sum tensor(9.2165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1710, device='cuda:0')



h[200].sum tensor(13.7472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9353, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0204,  ..., 0.0000, 0.0273, 0.0000],
        [0.0000, 0.0000, 0.0295,  ..., 0.0000, 0.0395, 0.0000],
        [0.0000, 0.0000, 0.0434,  ..., 0.0000, 0.0576, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28843.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4661e-01, 2.2935e-01, 0.0000e+00,  ..., 4.8547e-03, 1.6058e-01,
         0.0000e+00],
        [2.1457e-01, 3.3030e-01, 0.0000e+00,  ..., 1.0977e-02, 2.3188e-01,
         0.0000e+00],
        [2.9958e-01, 4.5616e-01, 0.0000e+00,  ..., 1.9229e-02, 3.2164e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 6.9949e-03,  ..., 0.0000e+00, 4.5593e-04,
         1.5312e-03],
        [0.0000e+00, 0.0000e+00, 6.9948e-03,  ..., 0.0000e+00, 4.5583e-04,
         1.5314e-03],
        [0.0000e+00, 0.0000e+00, 6.9942e-03,  ..., 0.0000e+00, 4.5628e-04,
         1.5317e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194573.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2769.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.3394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2417.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-400.1842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.4149, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-21.5139, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0100],
        [-0.0305],
        [-0.0507],
        ...,
        [-0.0559],
        [-0.0556],
        [-0.0555]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-21259.6191, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0006],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365944., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0007],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365956.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.8757e-04,  ...,  0.0000e+00,
         -9.6248e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8757e-04,  ...,  0.0000e+00,
         -9.6248e-05,  0.0000e+00],
        [-4.9862e-03, -8.7098e-04,  6.9347e-03,  ..., -9.3330e-03,
          9.3530e-03, -4.5442e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.8757e-04,  ...,  0.0000e+00,
         -9.6248e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8757e-04,  ...,  0.0000e+00,
         -9.6248e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8757e-04,  ...,  0.0000e+00,
         -9.6248e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-578.0070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3139, device='cuda:0')



h[100].sum tensor(9.8152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6807, device='cuda:0')



h[200].sum tensor(15.4735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0069,  ..., 0.0000, 0.0094, 0.0000],
        [0.0000, 0.0000, 0.0160,  ..., 0.0000, 0.0215, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31653.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1499e-02, 2.5471e-02, 4.6505e-03,  ..., 0.0000e+00, 1.6676e-02,
         9.3131e-04],
        [4.9851e-02, 8.6547e-02, 2.2657e-03,  ..., 0.0000e+00, 5.8318e-02,
         1.8206e-04],
        [1.0561e-01, 1.6994e-01, 6.0157e-04,  ..., 2.1119e-03, 1.1574e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 7.0299e-03,  ..., 0.0000e+00, 1.2896e-04,
         2.0490e-03],
        [0.0000e+00, 0.0000e+00, 7.0293e-03,  ..., 0.0000e+00, 1.2866e-04,
         2.0484e-03],
        [0.0000e+00, 0.0000e+00, 7.0286e-03,  ..., 0.0000e+00, 1.2858e-04,
         2.0481e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217468.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3129.9084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(156.7924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2937.0425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-429.3766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.8048, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-26.4362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0500],
        [ 0.0703],
        [ 0.0848],
        ...,
        [-0.0591],
        [-0.0588],
        [-0.0587]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-15060.5918, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0010],
        [1.0007],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365956.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0011],
        [1.0007],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365969.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0139, -0.0024,  0.0199,  ..., -0.0261,  0.0263, -0.0127],
        [-0.0144, -0.0025,  0.0206,  ..., -0.0270,  0.0272, -0.0131],
        [-0.0122, -0.0021,  0.0174,  ..., -0.0228,  0.0230, -0.0111],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-600.1891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4541, device='cuda:0')



h[100].sum tensor(7.8459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0992, device='cuda:0')



h[200].sum tensor(14.4375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0749,  ..., 0.0000, 0.0991, 0.0000],
        [0.0000, 0.0000, 0.0807,  ..., 0.0000, 0.1067, 0.0000],
        [0.0000, 0.0000, 0.0999,  ..., 0.0000, 0.1318, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26636.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.0805e-01, 4.6986e-01, 0.0000e+00,  ..., 1.9601e-02, 3.2567e-01,
         0.0000e+00],
        [3.3693e-01, 5.1263e-01, 0.0000e+00,  ..., 2.2363e-02, 3.5566e-01,
         0.0000e+00],
        [3.6837e-01, 5.5913e-01, 0.0000e+00,  ..., 2.5508e-02, 3.8856e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.1890e-04, 7.0464e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.5320e-03],
        [0.0000e+00, 5.1840e-04, 7.0457e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.5322e-03],
        [0.0000e+00, 5.1800e-04, 7.0453e-03,  ..., 0.0000e+00, 0.0000e+00,
         2.5324e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183736.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2297.5930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(139.3347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2271.6624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-407.5666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.1725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-30.8431, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1076],
        [ 0.1111],
        [ 0.1134],
        ...,
        [-0.0639],
        [-0.0636],
        [-0.0635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-18970.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0011],
        [1.0007],
        ...,
        [1.0001],
        [0.9999],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365969.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0011],
        [1.0008],
        ...,
        [1.0001],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365982.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [-0.0053, -0.0009,  0.0075,  ..., -0.0100,  0.0100, -0.0049],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-576.8821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9839, device='cuda:0')



h[100].sum tensor(10.2481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4983, device='cuda:0')



h[200].sum tensor(17.1569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0326,  ..., 0.0000, 0.0437, 0.0000],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0082, 0.0000],
        [0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0101, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30737.9629, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1156, 0.1859, 0.0000,  ..., 0.0008, 0.1233, 0.0000],
        [0.0660, 0.1120, 0.0010,  ..., 0.0000, 0.0734, 0.0000],
        [0.0348, 0.0656, 0.0022,  ..., 0.0000, 0.0420, 0.0004],
        ...,
        [0.0000, 0.0010, 0.0070,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0010, 0.0070,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0010, 0.0070,  ..., 0.0000, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208803.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2805.7615, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.5078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2779.1594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-442.2735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.3319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-35.5985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1309],
        [ 0.1221],
        [ 0.1074],
        ...,
        [-0.0677],
        [-0.0676],
        [-0.0675]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-16753.0488, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0011],
        [1.0008],
        ...,
        [1.0001],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365982.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0011],
        [1.0008],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365996.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-541.2996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5106, device='cuda:0')



h[100].sum tensor(13.6665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4486, device='cuda:0')



h[200].sum tensor(20.3956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0130,  ..., 0.0000, 0.0175, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0095,  ..., 0.0000, 0.0126, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35742.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6423e-03, 1.0436e-02, 5.8984e-03,  ..., 0.0000e+00, 5.5456e-03,
         2.5215e-03],
        [3.1262e-02, 5.6415e-02, 3.1775e-03,  ..., 0.0000e+00, 3.6491e-02,
         1.5414e-03],
        [1.2323e-01, 1.9642e-01, 9.0819e-04,  ..., 4.8462e-03, 1.3250e-01,
         1.0063e-05],
        ...,
        [5.0827e-04, 4.7402e-03, 6.5360e-03,  ..., 0.0000e+00, 2.1761e-03,
         3.1665e-03],
        [2.2961e-02, 4.1482e-02, 3.4824e-03,  ..., 0.0000e+00, 2.7035e-02,
         1.8244e-03],
        [7.1458e-02, 1.1971e-01, 1.3197e-03,  ..., 8.7476e-04, 7.9651e-02,
         3.8015e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(238896.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3418.5894, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(184.4365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3418.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-480.6393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.3397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-38.5766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0178],
        [ 0.0342],
        [ 0.0397],
        ...,
        [-0.0260],
        [ 0.0059],
        [ 0.0399]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-12096.4258, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0011],
        [1.0008],
        ...,
        [1.0002],
        [1.0000],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365996.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0012],
        [1.0009],
        ...,
        [1.0002],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366008.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-597.9387, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3203, device='cuda:0')



h[100].sum tensor(9.0666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4722, device='cuda:0')



h[200].sum tensor(14.4056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25665.2012, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.2500e-05, 3.8003e-03, 6.5690e-03,  ..., 0.0000e+00, 1.7326e-03,
         3.8474e-03],
        [0.0000e+00, 8.0978e-04, 6.8926e-03,  ..., 0.0000e+00, 0.0000e+00,
         4.2490e-03],
        [0.0000e+00, 8.0632e-04, 6.9052e-03,  ..., 0.0000e+00, 0.0000e+00,
         4.2581e-03],
        ...,
        [0.0000e+00, 8.1894e-04, 6.8596e-03,  ..., 0.0000e+00, 0.0000e+00,
         4.2243e-03],
        [0.0000e+00, 8.1940e-04, 6.8592e-03,  ..., 0.0000e+00, 0.0000e+00,
         4.2236e-03],
        [0.0000e+00, 8.1953e-04, 6.8585e-03,  ..., 0.0000e+00, 0.0000e+00,
         4.2232e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181778.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2057.3079, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(139.4302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2159.6711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-425.7066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.1087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-43.7804, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0172],
        [-0.0492],
        [-0.0720],
        ...,
        [-0.0785],
        [-0.0781],
        [-0.0780]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-24536.6133, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0012],
        [1.0009],
        ...,
        [1.0002],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366008.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(501.7603, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0012],
        [1.0010],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366021.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0001,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-555.3857, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2683, device='cuda:0')



h[100].sum tensor(13.3211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6555, device='cuda:0')



h[200].sum tensor(16.9780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31583.1191, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0027, 0.0064,  ..., 0.0000, 0.0009, 0.0047],
        [0.0000, 0.0011, 0.0067,  ..., 0.0000, 0.0002, 0.0048],
        [0.0000, 0.0004, 0.0068,  ..., 0.0000, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0004, 0.0068,  ..., 0.0000, 0.0000, 0.0048],
        [0.0000, 0.0004, 0.0068,  ..., 0.0000, 0.0000, 0.0048],
        [0.0000, 0.0004, 0.0068,  ..., 0.0000, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218446.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2807.5361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(163.2713, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2902.3496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-469.0943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.8155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-51.6865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0164],
        [-0.0092],
        [-0.0319],
        ...,
        [-0.0843],
        [-0.0837],
        [-0.0835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-21178.3730, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0012],
        [1.0010],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366021.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0013],
        [1.0010],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366034.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [-0.0058, -0.0010,  0.0082,  ..., -0.0109,  0.0109, -0.0053],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-521.5989, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4257, device='cuda:0')



h[100].sum tensor(16.5978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8486, device='cuda:0')



h[200].sum tensor(18.4115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0689, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0145,  ..., 0.0000, 0.0193, 0.0000],
        [0.0000, 0.0000, 0.0368,  ..., 0.0000, 0.0487, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36445.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0365, 0.0634, 0.0024,  ..., 0.0000, 0.0418, 0.0018],
        [0.1073, 0.1702, 0.0005,  ..., 0.0019, 0.1146, 0.0000],
        [0.2123, 0.3242, 0.0000,  ..., 0.0083, 0.2219, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0054],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254591.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3701.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(178.9156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3500.5059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-500.9081, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.5276, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-58.9685, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0697],
        [ 0.1004],
        [ 0.1197],
        ...,
        [-0.0775],
        [-0.0880],
        [-0.0906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-25813.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0013],
        [1.0010],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366034.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0013],
        [1.0011],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366047.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-573.0850, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2398, device='cuda:0')



h[100].sum tensor(12.4416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4277, device='cuda:0')



h[200].sum tensor(12.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26558.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193981.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2107.9111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.0500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2346.0840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-448.1050, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1663, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-68.9392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1320],
        [-0.1416],
        [-0.1485],
        ...,
        [-0.0991],
        [-0.0986],
        [-0.0984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30152.4258, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0013],
        [1.0011],
        ...,
        [1.0003],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366047.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0014],
        [1.0012],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366060.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0214, -0.0036,  0.0311,  ..., -0.0403,  0.0408, -0.0195],
        [-0.0228, -0.0038,  0.0332,  ..., -0.0430,  0.0436, -0.0208],
        [-0.0182, -0.0030,  0.0264,  ..., -0.0343,  0.0348, -0.0166],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-444.6536, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.9194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0768, device='cuda:0')



h[100].sum tensor(23.8594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.9737, device='cuda:0')



h[200].sum tensor(21.3616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.0433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1296,  ..., 0.0000, 0.1702, 0.0000],
        [0.0000, 0.0000, 0.1335,  ..., 0.0000, 0.1754, 0.0000],
        [0.0000, 0.0000, 0.1465,  ..., 0.0000, 0.1923, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46090.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5951, 0.8795, 0.0000,  ..., 0.0449, 0.6183, 0.0000],
        [0.6499, 0.9594, 0.0000,  ..., 0.0506, 0.6754, 0.0000],
        [0.6848, 1.0104, 0.0000,  ..., 0.0543, 0.7121, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0000, 0.0064],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(327605.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5480.7397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.3831, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4752.3037, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-569.2312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-77.2133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0295],
        [ 0.0177],
        [ 0.0162],
        ...,
        [-0.1065],
        [-0.1060],
        [-0.1058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33724.7539, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0014],
        [1.0012],
        ...,
        [1.0004],
        [1.0002],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366060.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0014],
        [1.0012],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366074.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056, -0.0009,  0.0080,  ..., -0.0106,  0.0106, -0.0051],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-544.6531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7013, device='cuda:0')



h[100].sum tensor(16.5595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6829, device='cuda:0')



h[200].sum tensor(11.0141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0087, 0.0000],
        [0.0000, 0.0000, 0.0080,  ..., 0.0000, 0.0106, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27693.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.8919e-02, 5.2737e-02, 1.2566e-03,  ..., 0.0000e+00, 3.5719e-02,
         9.7901e-04],
        [2.0758e-02, 4.0761e-02, 2.7101e-03,  ..., 0.0000e+00, 2.7973e-02,
         2.4249e-03],
        [4.9313e-03, 1.2404e-02, 5.0180e-03,  ..., 0.0000e+00, 8.7364e-03,
         4.9585e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 6.5942e-03,  ..., 0.0000e+00, 2.6795e-05,
         6.8716e-03],
        [0.0000e+00, 0.0000e+00, 6.5938e-03,  ..., 0.0000e+00, 2.6476e-05,
         6.8708e-03],
        [0.0000e+00, 0.0000e+00, 6.5944e-03,  ..., 0.0000e+00, 2.6806e-05,
         6.8722e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206352.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2247.6162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(139.4525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2491.9568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-470.6031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-86.3309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0737],
        [-0.0780],
        [-0.0775],
        ...,
        [-0.1138],
        [-0.1132],
        [-0.1131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-37254.0352, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0014],
        [1.0012],
        ...,
        [1.0004],
        [1.0002],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366074.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0015],
        [1.0013],
        ...,
        [1.0005],
        [1.0003],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366088.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0103, -0.0017,  0.0149,  ..., -0.0194,  0.0196, -0.0094],
        [-0.0118, -0.0019,  0.0170,  ..., -0.0222,  0.0224, -0.0107],
        [-0.0330, -0.0054,  0.0482,  ..., -0.0622,  0.0632, -0.0301],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-450.3601, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4130, device='cuda:0')



h[100].sum tensor(24.9004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.9476, device='cuda:0')



h[200].sum tensor(17.0827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.4666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1172,  ..., 0.0000, 0.1539, 0.0000],
        [0.0000, 0.0000, 0.1330,  ..., 0.0000, 0.1747, 0.0000],
        [0.0000, 0.0000, 0.0805,  ..., 0.0000, 0.1060, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41501.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.4670e-01, 8.0459e-01, 0.0000e+00,  ..., 3.7574e-02, 5.6504e-01,
         0.0000e+00],
        [5.8706e-01, 8.6323e-01, 0.0000e+00,  ..., 4.1685e-02, 6.0694e-01,
         0.0000e+00],
        [4.8498e-01, 7.1500e-01, 0.0000e+00,  ..., 3.1356e-02, 5.0133e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 6.7712e-03,  ..., 0.0000e+00, 1.7573e-05,
         7.1588e-03],
        [0.0000e+00, 0.0000e+00, 6.7712e-03,  ..., 0.0000e+00, 1.7331e-05,
         7.1581e-03],
        [0.0000e+00, 0.0000e+00, 6.7713e-03,  ..., 0.0000e+00, 1.7189e-05,
         7.1576e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(298330.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4550.2021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.7676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4195.0840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-560.0876, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-96.0937, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0328],
        [ 0.0269],
        [ 0.0317],
        ...,
        [-0.1201],
        [-0.1195],
        [-0.1193]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-38480.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0015],
        [1.0013],
        ...,
        [1.0005],
        [1.0003],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366088.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0015],
        [1.0013],
        ...,
        [1.0006],
        [1.0003],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366102.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-447.5429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7901, device='cuda:0')



h[100].sum tensor(25.6109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6031, device='cuda:0')



h[200].sum tensor(15.7450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.0285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0201,  ..., 0.0000, 0.0266, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38893.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0083, 0.0173, 0.0050,  ..., 0.0000, 0.0124, 0.0044],
        [0.0207, 0.0358, 0.0034,  ..., 0.0000, 0.0245, 0.0035],
        [0.0725, 0.1153, 0.0006,  ..., 0.0000, 0.0778, 0.0003],
        ...,
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0000, 0.0074],
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0000, 0.0074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(280681.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4041.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(186.3939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3960.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-555.2275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-106.4484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0055],
        [ 0.0252],
        [ 0.0670],
        ...,
        [-0.1259],
        [-0.1254],
        [-0.1252]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-35977.2852, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0015],
        [1.0013],
        ...,
        [1.0006],
        [1.0003],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366102.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0016],
        [1.0014],
        ...,
        [1.0006],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366116.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058, -0.0009,  0.0083,  ..., -0.0109,  0.0109, -0.0053],
        [-0.0037, -0.0006,  0.0052,  ..., -0.0070,  0.0070, -0.0034],
        [-0.0095, -0.0015,  0.0138,  ..., -0.0179,  0.0181, -0.0087],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-501.1273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6233, device='cuda:0')



h[100].sum tensor(21.8391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6398, device='cuda:0')



h[200].sum tensor(9.7196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9880, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0164,  ..., 0.0000, 0.0217, 0.0000],
        [0.0000, 0.0000, 0.0456,  ..., 0.0000, 0.0600, 0.0000],
        [0.0000, 0.0000, 0.0274,  ..., 0.0000, 0.0361, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29004.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0992, 0.1536, 0.0003,  ..., 0.0000, 0.1040, 0.0000],
        [0.1522, 0.2303, 0.0000,  ..., 0.0000, 0.1563, 0.0000],
        [0.1364, 0.2074, 0.0000,  ..., 0.0000, 0.1404, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0074,  ..., 0.0000, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0074,  ..., 0.0000, 0.0000, 0.0076],
        [0.0000, 0.0000, 0.0074,  ..., 0.0000, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221108.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2531.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(144.4123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2644.5051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-500.7559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-109.7835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0993],
        [ 0.1110],
        [ 0.1067],
        ...,
        [-0.1183],
        [-0.1086],
        [-0.1005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-47627.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0016],
        [1.0014],
        ...,
        [1.0006],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366116.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0017],
        [1.0014],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366131.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-325.0303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5200, device='cuda:0')



h[100].sum tensor(35.7113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.8778, device='cuda:0')



h[200].sum tensor(21.7966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.4650, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0043,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48584.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 7.7812e-03,  ..., 0.0000e+00, 1.6806e-05,
         7.5413e-03],
        [0.0000e+00, 0.0000e+00, 7.7835e-03,  ..., 0.0000e+00, 1.6910e-05,
         7.5436e-03],
        [1.5409e-03, 4.4620e-03, 7.2630e-03,  ..., 0.0000e+00, 3.1201e-03,
         6.7182e-03],
        ...,
        [1.3065e-03, 6.7393e-03, 6.6437e-03,  ..., 0.0000e+00, 4.5090e-03,
         6.3013e-03],
        [6.7694e-03, 1.7249e-02, 5.0166e-03,  ..., 0.0000e+00, 1.1307e-02,
         4.5152e-03],
        [1.7522e-02, 3.4966e-02, 2.8591e-03,  ..., 0.0000e+00, 2.2280e-02,
         2.4234e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(329272.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5202.3115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.6476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4827.0918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-624.6639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-119.1476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1567],
        [-0.1172],
        [-0.0694],
        ...,
        [-0.0874],
        [-0.0531],
        [-0.0228]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43033.1836, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0017],
        [1.0014],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366131.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0017],
        [1.0015],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366145.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-413.5813, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9371, device='cuda:0')



h[100].sum tensor(29.2299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5784, device='cuda:0')



h[200].sum tensor(13.2412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7252, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0164,  ..., 0.0000, 0.0215, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35786.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.7800e-02, 1.3647e-01, 1.8441e-04,  ..., 0.0000e+00, 9.4452e-02,
         0.0000e+00],
        [4.3178e-02, 7.2123e-02, 2.3836e-03,  ..., 0.0000e+00, 5.0229e-02,
         1.3341e-03],
        [2.0845e-02, 3.9933e-02, 3.3732e-03,  ..., 0.0000e+00, 2.7784e-02,
         1.5011e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 8.0948e-03,  ..., 0.0000e+00, 3.1542e-05,
         7.5583e-03],
        [0.0000e+00, 0.0000e+00, 8.0946e-03,  ..., 0.0000e+00, 3.1393e-05,
         7.5586e-03],
        [0.0000e+00, 0.0000e+00, 8.0950e-03,  ..., 0.0000e+00, 3.1373e-05,
         7.5588e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(261763.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3448.9834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(175.3723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3599.5647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-557.5038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-125.9869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0805],
        [ 0.0717],
        [ 0.0677],
        ...,
        [-0.1405],
        [-0.1398],
        [-0.1396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44819.0430, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0017],
        [1.0015],
        ...,
        [1.0007],
        [1.0004],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366145.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(576.9171, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0018],
        [1.0015],
        ...,
        [1.0007],
        [1.0005],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366159.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0039, -0.0006,  0.0056,  ..., -0.0074,  0.0073, -0.0035],
        [-0.0039, -0.0006,  0.0056,  ..., -0.0074,  0.0073, -0.0035],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-415.1587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6855, device='cuda:0')



h[100].sum tensor(29.5721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3333, device='cuda:0')



h[200].sum tensor(11.2650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1417, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0133, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0133, 0.0000],
        [0.0000, 0.0000, 0.0101,  ..., 0.0000, 0.0133, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33173.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.6451e-02, 9.0684e-02, 0.0000e+00,  ..., 0.0000e+00, 5.7713e-02,
         0.0000e+00],
        [4.1077e-02, 6.8633e-02, 8.0190e-04,  ..., 0.0000e+00, 4.3546e-02,
         3.8570e-04],
        [2.8495e-02, 5.0635e-02, 2.2980e-03,  ..., 0.0000e+00, 3.2348e-02,
         1.4890e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 8.2781e-03,  ..., 0.0000e+00, 3.3711e-05,
         7.5740e-03],
        [0.0000e+00, 0.0000e+00, 8.2772e-03,  ..., 0.0000e+00, 3.3633e-05,
         7.5740e-03],
        [0.0000e+00, 0.0000e+00, 8.2780e-03,  ..., 0.0000e+00, 3.3587e-05,
         7.5745e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(245840.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2949.2854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(166.0742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3357.0779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-548.5634, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.3638, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0862],
        [ 0.0400],
        [-0.0171],
        ...,
        [-0.1455],
        [-0.1448],
        [-0.1446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44016.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0018],
        [1.0015],
        ...,
        [1.0007],
        [1.0005],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366159.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0018],
        [1.0016],
        ...,
        [1.0008],
        [1.0005],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366173.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [-0.0176, -0.0028,  0.0259,  ..., -0.0333,  0.0339, -0.0160],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-317.4776, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.2256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5039, device='cuda:0')



h[100].sum tensor(37.1467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1039, device='cuda:0')



h[200].sum tensor(16.8862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9371, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0407,  ..., 0.0000, 0.0531, 0.0000],
        [0.0000, 0.0000, 0.0508,  ..., 0.0000, 0.0663, 0.0000],
        [0.0000, 0.0000, 0.1092,  ..., 0.0000, 0.1426, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44377.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7130e-01, 2.5627e-01, 0.0000e+00,  ..., 2.6904e-03, 1.7820e-01,
         0.0000e+00],
        [2.3136e-01, 3.4274e-01, 0.0000e+00,  ..., 4.0208e-03, 2.3853e-01,
         0.0000e+00],
        [2.8646e-01, 4.2214e-01, 0.0000e+00,  ..., 8.2738e-03, 2.9451e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 8.4694e-03,  ..., 0.0000e+00, 9.6179e-06,
         7.6522e-03],
        [0.0000e+00, 0.0000e+00, 8.4694e-03,  ..., 0.0000e+00, 9.8204e-06,
         7.6527e-03],
        [0.0000e+00, 0.0000e+00, 8.4698e-03,  ..., 0.0000e+00, 9.7120e-06,
         7.6522e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(318112.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4775.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(214.1906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4829.9512, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-622.0779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-141.7043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0198],
        [ 0.0245],
        [ 0.0186],
        ...,
        [-0.1505],
        [-0.1499],
        [-0.1496]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-43102.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0018],
        [1.0016],
        ...,
        [1.0008],
        [1.0005],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366173.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0019],
        [1.0017],
        ...,
        [1.0008],
        [1.0005],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366187.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0287, -0.0045,  0.0424,  ..., -0.0543,  0.0554, -0.0261],
        [-0.0271, -0.0043,  0.0401,  ..., -0.0513,  0.0524, -0.0247],
        [-0.0274, -0.0043,  0.0406,  ..., -0.0519,  0.0530, -0.0250],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-340.5302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8688, device='cuda:0')



h[100].sum tensor(35.7400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.0936, device='cuda:0')



h[200].sum tensor(13.3745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.3805, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1604,  ..., 0.0000, 0.2094, 0.0000],
        [0.0000, 0.0000, 0.1747,  ..., 0.0000, 0.2282, 0.0000],
        [0.0000, 0.0000, 0.1714,  ..., 0.0000, 0.2238, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38496.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.7721, 1.1204, 0.0000,  ..., 0.0520, 0.7877, 0.0000],
        [0.7983, 1.1583, 0.0000,  ..., 0.0546, 0.8147, 0.0000],
        [0.7557, 1.0969, 0.0000,  ..., 0.0502, 0.7708, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0000, 0.0078],
        [0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0000, 0.0078],
        [0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0000, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(284042.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3809.6296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.9369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4319.5371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-595.3282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-148.1482, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0100],
        [-0.0083],
        [ 0.0040],
        ...,
        [-0.1556],
        [-0.1549],
        [-0.1546]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-41778.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0019],
        [1.0017],
        ...,
        [1.0008],
        [1.0005],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366187.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0020],
        [1.0018],
        ...,
        [1.0009],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366200.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000, -0.0002,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-282.8014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1849, device='cuda:0')



h[100].sum tensor(39.9847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.9275, device='cuda:0')



h[200].sum tensor(15.5291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.7127, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42499.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0197, 0.0344, 0.0044,  ..., 0.0000, 0.0229, 0.0036],
        [0.0005, 0.0058, 0.0077,  ..., 0.0000, 0.0037, 0.0067],
        [0.0000, 0.0020, 0.0083,  ..., 0.0000, 0.0014, 0.0074],
        ...,
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0079],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0079],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0000, 0.0079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310053.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4449.3179, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(207.9041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4636.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-619.1382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.4270, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0399],
        [-0.0047],
        [-0.0525],
        ...,
        [-0.1601],
        [-0.1594],
        [-0.1592]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-51372.9727, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0020],
        [1.0018],
        ...,
        [1.0009],
        [1.0006],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366200.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0020],
        [1.0018],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366214.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -7.9653e-05,  ...,  0.0000e+00,
         -1.9916e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.9653e-05,  ...,  0.0000e+00,
         -1.9916e-04,  0.0000e+00],
        [-7.6756e-03, -1.1915e-03,  1.1336e-02,  ..., -1.4536e-02,
          1.4708e-02, -6.9782e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.9653e-05,  ...,  0.0000e+00,
         -1.9916e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.9653e-05,  ...,  0.0000e+00,
         -1.9916e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.9653e-05,  ...,  0.0000e+00,
         -1.9916e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-348.7621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0345, device='cuda:0')



h[100].sum tensor(35.2791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4202, device='cuda:0')



h[200].sum tensor(8.9002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9805, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0198,  ..., 0.0000, 0.0258, 0.0000],
        [0.0000, 0.0000, 0.0114,  ..., 0.0000, 0.0147, 0.0000],
        [0.0000, 0.0000, 0.0334,  ..., 0.0000, 0.0435, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32081.6777, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3561e-01, 2.0366e-01, 0.0000e+00,  ..., 2.3145e-04, 1.4046e-01,
         0.0000e+00],
        [1.2671e-01, 1.9088e-01, 0.0000e+00,  ..., 0.0000e+00, 1.3131e-01,
         0.0000e+00],
        [1.8063e-01, 2.6828e-01, 0.0000e+00,  ..., 1.6287e-03, 1.8570e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 8.9979e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.0636e-03],
        [0.0000e+00, 0.0000e+00, 8.9976e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.0641e-03],
        [0.0000e+00, 0.0000e+00, 8.9978e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.0640e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(253284.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2899.3936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(168.5858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3660.8586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.7430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.1187, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0696],
        [ 0.0663],
        [ 0.0589],
        ...,
        [-0.1648],
        [-0.1640],
        [-0.1638]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53029.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0020],
        [1.0018],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366214.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0019],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366228.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.4834e-03, -1.4626e-03,  1.4072e-02,  ..., -1.7971e-02,
          1.8243e-02, -8.6206e-03],
        [-4.5956e-03, -7.0878e-04,  6.7920e-03,  ..., -8.7087e-03,
          8.7377e-03, -4.1775e-03],
        [ 0.0000e+00,  0.0000e+00, -5.2660e-05,  ...,  0.0000e+00,
         -1.9948e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.2660e-05,  ...,  0.0000e+00,
         -1.9948e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.2660e-05,  ...,  0.0000e+00,
         -1.9948e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.2660e-05,  ...,  0.0000e+00,
         -1.9948e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-213.3266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7967, device='cuda:0')



h[100].sum tensor(44.8358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.8188, device='cuda:0')



h[200].sum tensor(16.8097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8464, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0603,  ..., 0.0000, 0.0782, 0.0000],
        [0.0000, 0.0000, 0.0362,  ..., 0.0000, 0.0469, 0.0000],
        [0.0000, 0.0000, 0.0068,  ..., 0.0000, 0.0088, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45518.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2716, 0.3980, 0.0000,  ..., 0.0041, 0.2743, 0.0000],
        [0.1985, 0.2934, 0.0000,  ..., 0.0014, 0.2020, 0.0000],
        [0.1207, 0.1820, 0.0000,  ..., 0.0000, 0.1241, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0000, 0.0084],
        [0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0000, 0.0084],
        [0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0000, 0.0084]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(327325.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4733.2856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(229.4073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5379.4194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-649.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.3217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0109],
        [ 0.0199],
        [ 0.0466],
        ...,
        [-0.1699],
        [-0.1692],
        [-0.1689]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46856.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0019],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366228.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0020],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366243.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.6309e-05,  ...,  0.0000e+00,
         -1.9897e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6309e-05,  ...,  0.0000e+00,
         -1.9897e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6309e-05,  ...,  0.0000e+00,
         -1.9897e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.6309e-05,  ...,  0.0000e+00,
         -1.9897e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6309e-05,  ...,  0.0000e+00,
         -1.9897e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6309e-05,  ...,  0.0000e+00,
         -1.9897e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-304.8650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2877, device='cuda:0')



h[100].sum tensor(38.5483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5602, device='cuda:0')



h[200].sum tensor(8.7971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31805.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.3383e-02, 1.4073e-01, 3.4415e-03,  ..., 5.9393e-05, 9.8180e-02,
         2.2273e-03],
        [2.2336e-02, 3.6985e-02, 5.7653e-03,  ..., 0.0000e+00, 2.5561e-02,
         4.3128e-03],
        [1.6555e-03, 6.5376e-03, 8.4394e-03,  ..., 0.0000e+00, 4.1327e-03,
         7.4270e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 9.5224e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.9204e-03],
        [0.0000e+00, 0.0000e+00, 9.5223e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.9213e-03],
        [0.0000e+00, 0.0000e+00, 9.5218e-03,  ..., 0.0000e+00, 0.0000e+00,
         8.9216e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(245808.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2523.2524, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(174.5792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3713.9355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-571.2767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.4646, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0450],
        [ 0.0507],
        [ 0.0602],
        ...,
        [-0.1740],
        [-0.1735],
        [-0.1734]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-54552.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0021],
        [1.0020],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366243.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0022],
        [1.0020],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366258.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  9.9665e-07,  ...,  0.0000e+00,
         -1.9739e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  9.9665e-07,  ...,  0.0000e+00,
         -1.9739e-04,  0.0000e+00],
        [-5.3577e-03, -8.1556e-04,  8.0039e-03,  ..., -1.0165e-02,
          1.0249e-02, -4.8690e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  9.9665e-07,  ...,  0.0000e+00,
         -1.9739e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  9.9665e-07,  ...,  0.0000e+00,
         -1.9739e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  9.9665e-07,  ...,  0.0000e+00,
         -1.9739e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-252.9660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5441, device='cuda:0')



h[100].sum tensor(42.0218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3610, device='cuda:0')



h[200].sum tensor(11.1847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4488, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.5495e-02,  ..., 0.0000e+00, 2.0024e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.0213e-03,  ..., 0.0000e+00, 1.0268e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.7697e-02,  ..., 0.0000e+00, 2.2700e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.9926e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.9928e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.9930e-06,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35927.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.9205e-02, 1.0812e-01, 1.9651e-04,  ..., 0.0000e+00, 7.2744e-02,
         0.0000e+00],
        [7.1735e-02, 1.1160e-01, 4.9333e-04,  ..., 0.0000e+00, 7.4043e-02,
         0.0000e+00],
        [1.0834e-01, 1.6369e-01, 2.5632e-05,  ..., 0.0000e+00, 1.0890e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 9.9770e-03,  ..., 0.0000e+00, 0.0000e+00,
         9.6389e-03],
        [0.0000e+00, 0.0000e+00, 9.9774e-03,  ..., 0.0000e+00, 0.0000e+00,
         9.6394e-03],
        [0.0000e+00, 0.0000e+00, 9.9778e-03,  ..., 0.0000e+00, 0.0000e+00,
         9.6396e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(271312.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3145.3628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.4099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4236.6768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-599.2864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.3811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0994],
        [ 0.1090],
        [ 0.1180],
        ...,
        [-0.1802],
        [-0.1794],
        [-0.1791]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56466.4336, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0022],
        [1.0020],
        ...,
        [1.0010],
        [1.0007],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366258.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0021],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366273.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  2.4959e-05,  ...,  0.0000e+00,
         -1.9654e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.4959e-05,  ...,  0.0000e+00,
         -1.9654e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.4959e-05,  ...,  0.0000e+00,
         -1.9654e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.4959e-05,  ...,  0.0000e+00,
         -1.9654e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.4959e-05,  ...,  0.0000e+00,
         -1.9654e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.4959e-05,  ...,  0.0000e+00,
         -1.9654e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-261.8903, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2879, device='cuda:0')



h[100].sum tensor(41.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1134, device='cuda:0')



h[200].sum tensor(9.3627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 9.9974e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0002e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.2218e-02,  ..., 0.0000e+00, 1.5619e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0000e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0001e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0001e-04,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33267.1055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0089, 0.0091,  ..., 0.0000, 0.0053, 0.0082],
        [0.0145, 0.0258, 0.0074,  ..., 0.0000, 0.0174, 0.0056],
        [0.0524, 0.0842, 0.0037,  ..., 0.0000, 0.0573, 0.0022],
        ...,
        [0.0000, 0.0000, 0.0105,  ..., 0.0000, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0105,  ..., 0.0000, 0.0000, 0.0103],
        [0.0000, 0.0000, 0.0105,  ..., 0.0000, 0.0000, 0.0103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259110.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2703.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(187.7735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4163.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-591.2361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.6544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0050],
        [-0.0141],
        [-0.0230],
        ...,
        [-0.1856],
        [-0.1848],
        [-0.1845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53843.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0021],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366273.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0022],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366288.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  3.9166e-05,  ...,  0.0000e+00,
         -1.9523e-04,  0.0000e+00],
        [-1.0749e-02, -1.6148e-03,  1.6142e-02,  ..., -2.0421e-02,
          2.0820e-02, -9.7663e-03],
        [ 0.0000e+00,  0.0000e+00,  3.9166e-05,  ...,  0.0000e+00,
         -1.9523e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.9166e-05,  ...,  0.0000e+00,
         -1.9523e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.9166e-05,  ...,  0.0000e+00,
         -1.9523e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.9166e-05,  ...,  0.0000e+00,
         -1.9523e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-257.3269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9012, device='cuda:0')



h[100].sum tensor(41.7384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8995, device='cuda:0')



h[200].sum tensor(8.9399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5900, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0231,  ..., 0.0000, 0.0296, 0.0000],
        [0.0000, 0.0000, 0.0133,  ..., 0.0000, 0.0170, 0.0000],
        [0.0000, 0.0000, 0.0758,  ..., 0.0000, 0.0979, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34349.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1054, 0.1590, 0.0000,  ..., 0.0000, 0.1064, 0.0000],
        [0.1077, 0.1626, 0.0000,  ..., 0.0000, 0.1103, 0.0000],
        [0.2001, 0.2942, 0.0000,  ..., 0.0012, 0.2025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0000, 0.0113],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0000, 0.0113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(275997.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3053.2251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(200.5816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4567.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-601.9924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.7546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1037],
        [ 0.0828],
        [ 0.0583],
        ...,
        [-0.1921],
        [-0.1913],
        [-0.1910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-53388.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0022],
        [1.0022],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366288.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(572.5905, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0023],
        [1.0023],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366304.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  4.4915e-05,  ...,  0.0000e+00,
         -1.9434e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.4915e-05,  ...,  0.0000e+00,
         -1.9434e-04,  0.0000e+00],
        [-6.5935e-03, -9.8391e-04,  9.9364e-03,  ..., -1.2534e-02,
          1.2713e-02, -5.9897e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.4915e-05,  ...,  0.0000e+00,
         -1.9434e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.4915e-05,  ...,  0.0000e+00,
         -1.9434e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.4915e-05,  ...,  0.0000e+00,
         -1.9434e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62.7666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.2453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.6198, device='cuda:0')



h[100].sum tensor(63.0103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(23.5691, device='cuda:0')



h[200].sum tensor(29.4801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0155,  ..., 0.0000, 0.0196, 0.0000],
        [0.0000, 0.0000, 0.0358,  ..., 0.0000, 0.0459, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70862.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.2183e-02, 5.5185e-02, 5.2084e-03,  ..., 0.0000e+00, 3.5224e-02,
         4.7656e-03],
        [9.7412e-02, 1.4748e-01, 2.0908e-03,  ..., 0.0000e+00, 9.7284e-02,
         1.2636e-03],
        [1.8582e-01, 2.7287e-01, 0.0000e+00,  ..., 7.6618e-05, 1.8328e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.1495e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.2581e-02],
        [0.0000e+00, 0.0000e+00, 1.1495e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.2581e-02],
        [0.0000e+00, 0.0000e+00, 1.1495e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.2582e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(500438.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8888.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(362.5216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9221.3467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-829.0883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.3194, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0841],
        [ 0.0861],
        [ 0.0869],
        ...,
        [-0.1992],
        [-0.1982],
        [-0.1980]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-46240.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0023],
        [1.0023],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366304.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0023],
        [1.0023],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366320.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  5.5812e-05,  ...,  0.0000e+00,
         -1.9217e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5812e-05,  ...,  0.0000e+00,
         -1.9217e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5812e-05,  ...,  0.0000e+00,
         -1.9217e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  5.5812e-05,  ...,  0.0000e+00,
         -1.9217e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5812e-05,  ...,  0.0000e+00,
         -1.9217e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5812e-05,  ...,  0.0000e+00,
         -1.9217e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-176.8945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3180, device='cuda:0')



h[100].sum tensor(48.1330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3420, device='cuda:0')



h[200].sum tensor(12.7178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6964, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43556.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0102, 0.0230, 0.0086,  ..., 0.0000, 0.0138, 0.0085],
        [0.0066, 0.0166, 0.0095,  ..., 0.0000, 0.0101, 0.0098],
        [0.0005, 0.0047, 0.0112,  ..., 0.0000, 0.0027, 0.0123],
        ...,
        [0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0000, 0.0134],
        [0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0000, 0.0134],
        [0.0000, 0.0000, 0.0119,  ..., 0.0000, 0.0000, 0.0134]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(347799.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4909.8936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.8414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5710.9038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-661.7207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.3832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0312],
        [ 0.0128],
        [-0.0112],
        ...,
        [-0.2061],
        [-0.2052],
        [-0.2048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69646.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0023],
        [1.0023],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366320.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0024],
        [1.0024],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366334.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3249e-02, -1.9508e-03,  1.9996e-02,  ..., -2.5218e-02,
          2.5821e-02, -1.2033e-02],
        [-6.6470e-03, -9.7869e-04,  1.0062e-02,  ..., -1.2651e-02,
          1.2861e-02, -6.0367e-03],
        [-6.6381e-03, -9.7738e-04,  1.0049e-02,  ..., -1.2634e-02,
          1.2843e-02, -6.0286e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  6.1532e-05,  ...,  0.0000e+00,
         -1.8766e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.1532e-05,  ...,  0.0000e+00,
         -1.8766e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.1532e-05,  ...,  0.0000e+00,
         -1.8766e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-183.9695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8029, device='cuda:0')



h[100].sum tensor(49.1065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5042, device='cuda:0')



h[200].sum tensor(11.3416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.6308, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0529,  ..., 0.0000, 0.0680, 0.0000],
        [0.0000, 0.0000, 0.0547,  ..., 0.0000, 0.0704, 0.0000],
        [0.0000, 0.0000, 0.0184,  ..., 0.0000, 0.0234, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39225.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8419e-01, 2.6901e-01, 0.0000e+00,  ..., 0.0000e+00, 1.8221e-01,
         0.0000e+00],
        [1.6446e-01, 2.4116e-01, 0.0000e+00,  ..., 0.0000e+00, 1.6315e-01,
         0.0000e+00],
        [9.7013e-02, 1.4583e-01, 1.2297e-03,  ..., 0.0000e+00, 9.7126e-02,
         2.5676e-04],
        ...,
        [0.0000e+00, 0.0000e+00, 1.2383e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4344e-02],
        [0.0000e+00, 0.0000e+00, 1.2383e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4344e-02],
        [0.0000e+00, 0.0000e+00, 1.2383e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4345e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(309239.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3746.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.8885, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5232.1025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-645.4268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.4644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1142],
        [ 0.1108],
        [ 0.1026],
        ...,
        [-0.2119],
        [-0.2112],
        [-0.2113]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61647.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0024],
        [1.0024],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366334.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0024],
        [1.0024],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366349.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3586e-02, -1.9869e-03,  2.0536e-02,  ..., -2.5874e-02,
          2.6523e-02, -1.2337e-02],
        [-9.4151e-03, -1.3769e-03,  1.4252e-02,  ..., -1.7931e-02,
          1.8324e-02, -8.5496e-03],
        [-1.3308e-02, -1.9462e-03,  2.0117e-02,  ..., -2.5345e-02,
          2.5977e-02, -1.2084e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  6.5312e-05,  ...,  0.0000e+00,
         -1.8380e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.5312e-05,  ...,  0.0000e+00,
         -1.8380e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.5312e-05,  ...,  0.0000e+00,
         -1.8380e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-177.3969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2759, device='cuda:0')



h[100].sum tensor(50.5183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2127, device='cuda:0')



h[200].sum tensor(10.7707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0511,  ..., 0.0000, 0.0656, 0.0000],
        [0.0000, 0.0000, 0.0573,  ..., 0.0000, 0.0737, 0.0000],
        [0.0000, 0.0000, 0.0359,  ..., 0.0000, 0.0459, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37569.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2546, 0.3670, 0.0000,  ..., 0.0000, 0.2484, 0.0000],
        [0.2480, 0.3578, 0.0000,  ..., 0.0000, 0.2427, 0.0000],
        [0.2042, 0.2960, 0.0000,  ..., 0.0000, 0.1999, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150],
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150],
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(296072.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3370.2808, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.3293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4940.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-640.8575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.0645, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0867],
        [ 0.0932],
        [ 0.0973],
        ...,
        [-0.2204],
        [-0.2194],
        [-0.2191]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66988.5078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0024],
        [1.0024],
        ...,
        [1.0011],
        [1.0008],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366349.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0025],
        [1.0025],
        ...,
        [1.0012],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366365.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  6.6104e-05,  ...,  0.0000e+00,
         -1.8123e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.6104e-05,  ...,  0.0000e+00,
         -1.8123e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.6104e-05,  ...,  0.0000e+00,
         -1.8123e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  6.6104e-05,  ...,  0.0000e+00,
         -1.8123e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.6104e-05,  ...,  0.0000e+00,
         -1.8123e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.6104e-05,  ...,  0.0000e+00,
         -1.8123e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-192.8068, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4024, device='cuda:0')



h[100].sum tensor(49.9082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6237, device='cuda:0')



h[200].sum tensor(8.5862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34107.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0128,  ..., 0.0000, 0.0000, 0.0153],
        ...,
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0000, 0.0152],
        [0.0000, 0.0000, 0.0127,  ..., 0.0000, 0.0000, 0.0152]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(278992.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2950.3472, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.4544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4359.5869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-620.3698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.8643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2515],
        [-0.2339],
        [-0.1981],
        ...,
        [-0.2250],
        [-0.2239],
        [-0.2236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76026.2266, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0025],
        [1.0025],
        ...,
        [1.0012],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366365.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0025],
        [1.0025],
        ...,
        [1.0012],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366380.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  6.8326e-05,  ...,  0.0000e+00,
         -1.7878e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.8326e-05,  ...,  0.0000e+00,
         -1.7878e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.8326e-05,  ...,  0.0000e+00,
         -1.7878e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  6.8326e-05,  ...,  0.0000e+00,
         -1.7878e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.8326e-05,  ...,  0.0000e+00,
         -1.7878e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.8326e-05,  ...,  0.0000e+00,
         -1.7878e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-126.1968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6994, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3886, device='cuda:0')



h[100].sum tensor(53.6645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8281, device='cuda:0')



h[200].sum tensor(11.3925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37489.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150],
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150],
        [0.0118, 0.0205, 0.0103,  ..., 0.0000, 0.0136, 0.0107],
        ...,
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150],
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150],
        [0.0000, 0.0000, 0.0126,  ..., 0.0000, 0.0000, 0.0150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(288596.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3181.2119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.1516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4510.1631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-642.2655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.5498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0479],
        [-0.0617],
        [-0.0568],
        ...,
        [-0.2278],
        [-0.2268],
        [-0.2265]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78727.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0025],
        [1.0025],
        ...,
        [1.0012],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366380.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0026],
        [1.0026],
        ...,
        [1.0013],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366396.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  7.3656e-05,  ...,  0.0000e+00,
         -1.7515e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.3656e-05,  ...,  0.0000e+00,
         -1.7515e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.3656e-05,  ...,  0.0000e+00,
         -1.7515e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  7.3656e-05,  ...,  0.0000e+00,
         -1.7515e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.3656e-05,  ...,  0.0000e+00,
         -1.7515e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.3656e-05,  ...,  0.0000e+00,
         -1.7515e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-5.7525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9309, device='cuda:0')



h[100].sum tensor(60.1643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.9990, device='cuda:0')



h[200].sum tensor(17.3775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50611.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0293, 0.0476, 0.0060,  ..., 0.0000, 0.0286, 0.0044],
        [0.0172, 0.0309, 0.0081,  ..., 0.0000, 0.0178, 0.0079],
        [0.0104, 0.0199, 0.0096,  ..., 0.0000, 0.0116, 0.0102],
        ...,
        [0.0000, 0.0000, 0.0123,  ..., 0.0000, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0124,  ..., 0.0000, 0.0000, 0.0146],
        [0.0000, 0.0000, 0.0124,  ..., 0.0000, 0.0000, 0.0146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375479.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5608.7817, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.8441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6670.5142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-725.6910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.9277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0975],
        [ 0.0692],
        [ 0.0338],
        ...,
        [-0.2290],
        [-0.2279],
        [-0.2276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62069.3867, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0026],
        [1.0026],
        ...,
        [1.0013],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366396.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0026],
        [1.0027],
        ...,
        [1.0013],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366411.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.8244e-02, -2.5964e-03,  2.7726e-02,  ..., -3.4834e-02,
          3.5892e-02, -1.6558e-02],
        [-5.0952e-03, -7.2512e-04,  7.7982e-03,  ..., -9.7283e-03,
          9.8992e-03, -4.6244e-03],
        [-1.5968e-02, -2.2725e-03,  2.4277e-02,  ..., -3.0488e-02,
          3.1394e-02, -1.4493e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  7.5993e-05,  ...,  0.0000e+00,
         -1.7320e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.5993e-05,  ...,  0.0000e+00,
         -1.7320e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.5993e-05,  ...,  0.0000e+00,
         -1.7320e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(121.8669, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.4646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.7129, device='cuda:0')



h[100].sum tensor(67.6006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(20.3026, device='cuda:0')



h[200].sum tensor(23.5707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0415,  ..., 0.0000, 0.0533, 0.0000],
        [0.0000, 0.0000, 0.1018,  ..., 0.0000, 0.1317, 0.0000],
        [0.0000, 0.0000, 0.0649,  ..., 0.0000, 0.0836, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65603.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.7407e-01, 3.8666e-01, 0.0000e+00,  ..., 1.3610e-03, 2.6267e-01,
         0.0000e+00],
        [4.1954e-01, 5.8951e-01, 0.0000e+00,  ..., 7.5050e-03, 4.0530e-01,
         0.0000e+00],
        [4.3295e-01, 6.0806e-01, 0.0000e+00,  ..., 7.9838e-03, 4.1786e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.7826e-05, 1.2256e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4380e-02],
        [8.0119e-03, 1.4522e-02, 1.0528e-02,  ..., 0.0000e+00, 8.8241e-03,
         1.1070e-02],
        [3.8533e-02, 5.8728e-02, 5.8822e-03,  ..., 0.0000e+00, 3.8908e-02,
         4.8391e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493367.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8815.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(361.3519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8928.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-813.2328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.8811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0755],
        [ 0.0648],
        [ 0.0579],
        ...,
        [-0.1234],
        [-0.0615],
        [-0.0060]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62317.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0026],
        [1.0027],
        ...,
        [1.0013],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366411.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0027],
        [1.0027],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366426.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  8.0845e-05,  ...,  0.0000e+00,
         -1.7358e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.0845e-05,  ...,  0.0000e+00,
         -1.7358e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.0845e-05,  ...,  0.0000e+00,
         -1.7358e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  8.0845e-05,  ...,  0.0000e+00,
         -1.7358e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.0845e-05,  ...,  0.0000e+00,
         -1.7358e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.0845e-05,  ...,  0.0000e+00,
         -1.7358e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(10.5053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7790, device='cuda:0')



h[100].sum tensor(60.6900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.8090, device='cuda:0')



h[200].sum tensor(15.1104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47662.0430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0049, 0.0114,  ..., 0.0000, 0.0012, 0.0131],
        [0.0000, 0.0009, 0.0121,  ..., 0.0000, 0.0000, 0.0140],
        [0.0000, 0.0003, 0.0123,  ..., 0.0000, 0.0000, 0.0142],
        ...,
        [0.0000, 0.0003, 0.0122,  ..., 0.0000, 0.0000, 0.0142],
        [0.0000, 0.0003, 0.0122,  ..., 0.0000, 0.0000, 0.0142],
        [0.0000, 0.0003, 0.0122,  ..., 0.0000, 0.0000, 0.0142]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(365819.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5420.4561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(287.9434, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6467.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-707.2258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.4168, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0931],
        [-0.1819],
        [-0.2511],
        ...,
        [-0.2341],
        [-0.2331],
        [-0.2327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-61031.8984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0027],
        [1.0027],
        ...,
        [1.0013],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366426.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0027],
        [1.0028],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366441.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  8.1315e-05,  ...,  0.0000e+00,
         -1.7373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.1315e-05,  ...,  0.0000e+00,
         -1.7373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.1315e-05,  ...,  0.0000e+00,
         -1.7373e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  8.1315e-05,  ...,  0.0000e+00,
         -1.7373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.1315e-05,  ...,  0.0000e+00,
         -1.7373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  8.1315e-05,  ...,  0.0000e+00,
         -1.7373e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-73.5451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1269, device='cuda:0')



h[100].sum tensor(56.1618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5773, device='cuda:0')



h[200].sum tensor(8.4305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4521, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38092.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0122,  ..., 0.0000, 0.0000, 0.0140],
        [0.0000, 0.0004, 0.0122,  ..., 0.0000, 0.0000, 0.0140],
        [0.0000, 0.0004, 0.0122,  ..., 0.0000, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0004, 0.0122,  ..., 0.0000, 0.0000, 0.0141],
        [0.0000, 0.0004, 0.0122,  ..., 0.0000, 0.0000, 0.0141],
        [0.0000, 0.0004, 0.0122,  ..., 0.0000, 0.0000, 0.0141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(309106.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3863.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.4532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5253.0825, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-651.7276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.1089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3307],
        [-0.3162],
        [-0.2857],
        ...,
        [-0.2381],
        [-0.2371],
        [-0.2367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70206.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0027],
        [1.0028],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366441.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(571.2941, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0028],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366456.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  7.8710e-05,  ...,  0.0000e+00,
         -1.7329e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.8710e-05,  ...,  0.0000e+00,
         -1.7329e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.8710e-05,  ...,  0.0000e+00,
         -1.7329e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  7.8710e-05,  ...,  0.0000e+00,
         -1.7329e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.8710e-05,  ...,  0.0000e+00,
         -1.7329e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.8710e-05,  ...,  0.0000e+00,
         -1.7329e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(0.8550, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9947, device='cuda:0')



h[100].sum tensor(61.1146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7162, device='cuda:0')



h[200].sum tensor(11.3136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1724, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0081,  ..., 0.0000, 0.0100, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41814.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0038, 0.0117,  ..., 0.0000, 0.0013, 0.0133],
        [0.0179, 0.0272, 0.0086,  ..., 0.0000, 0.0162, 0.0079],
        [0.0536, 0.0771, 0.0038,  ..., 0.0000, 0.0481, 0.0033],
        ...,
        [0.0000, 0.0005, 0.0122,  ..., 0.0000, 0.0000, 0.0141],
        [0.0000, 0.0005, 0.0122,  ..., 0.0000, 0.0000, 0.0141],
        [0.0000, 0.0005, 0.0122,  ..., 0.0000, 0.0000, 0.0141]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(323979.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4286.8311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.6628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5405.9624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-673.2697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.1057, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0506],
        [ 0.0167],
        [ 0.0784],
        ...,
        [-0.2431],
        [-0.2420],
        [-0.2417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77332.8047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0028],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366456.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0028],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366471.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2137e-02, -1.6800e-03,  1.8572e-02,  ..., -2.3232e-02,
          2.3952e-02, -1.1010e-02],
        [-1.6039e-02, -2.2202e-03,  2.4520e-02,  ..., -3.0701e-02,
          3.1708e-02, -1.4549e-02],
        [-7.9264e-03, -1.0972e-03,  1.2155e-02,  ..., -1.5172e-02,
          1.5582e-02, -7.1900e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  7.4275e-05,  ...,  0.0000e+00,
         -1.7224e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.4275e-05,  ...,  0.0000e+00,
         -1.7224e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.4275e-05,  ...,  0.0000e+00,
         -1.7224e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-96.5519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6539, device='cuda:0')



h[100].sum tensor(56.1883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1037, device='cuda:0')



h[200].sum tensor(4.3124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3062, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1110,  ..., 0.0000, 0.1437, 0.0000],
        [0.0000, 0.0000, 0.0928,  ..., 0.0000, 0.1199, 0.0000],
        [0.0000, 0.0000, 0.0775,  ..., 0.0000, 0.1001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32308.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.3564e-01, 8.7613e-01, 0.0000e+00,  ..., 2.2561e-02, 6.1458e-01,
         0.0000e+00],
        [5.7509e-01, 7.9278e-01, 0.0000e+00,  ..., 1.8038e-02, 5.5523e-01,
         0.0000e+00],
        [4.5212e-01, 6.2406e-01, 0.0000e+00,  ..., 1.0996e-02, 4.3618e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.6392e-04, 1.2291e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4368e-02],
        [0.0000e+00, 4.6420e-04, 1.2293e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4371e-02],
        [0.0000e+00, 4.6461e-04, 1.2293e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4371e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(279867.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3049.6853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.2896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4633.5542, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-620.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-173.9400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1174],
        [-0.1086],
        [-0.0850],
        ...,
        [-0.2488],
        [-0.2478],
        [-0.2475]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-73990.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0028],
        ...,
        [1.0014],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366471.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0029],
        ...,
        [1.0015],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366486.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8755e-03, -5.3269e-04,  5.9797e-03,  ..., -7.4229e-03,
          7.5462e-03, -3.5150e-03],
        [-9.5081e-03, -1.3069e-03,  1.4576e-02,  ..., -1.8211e-02,
          1.8757e-02, -8.6236e-03],
        [ 0.0000e+00,  0.0000e+00,  6.4970e-05,  ...,  0.0000e+00,
         -1.6746e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  6.4970e-05,  ...,  0.0000e+00,
         -1.6746e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.4970e-05,  ...,  0.0000e+00,
         -1.6746e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.4970e-05,  ...,  0.0000e+00,
         -1.6746e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(147.2002, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4323, device='cuda:0')



h[100].sum tensor(70.8673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.3823, device='cuda:0')



h[200].sum tensor(17.2811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0523,  ..., 0.0000, 0.0672, 0.0000],
        [0.0000, 0.0000, 0.0231,  ..., 0.0000, 0.0292, 0.0000],
        [0.0000, 0.0000, 0.0197,  ..., 0.0000, 0.0250, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50682.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.7634e-01, 2.4213e-01, 0.0000e+00,  ..., 0.0000e+00, 1.6205e-01,
         0.0000e+00],
        [1.3320e-01, 1.8329e-01, 5.3862e-05,  ..., 0.0000e+00, 1.2089e-01,
         0.0000e+00],
        [9.8440e-02, 1.3656e-01, 1.8695e-03,  ..., 0.0000e+00, 8.9532e-02,
         1.6099e-03],
        ...,
        [0.0000e+00, 4.2449e-04, 1.2388e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4811e-02],
        [0.0000e+00, 4.2532e-04, 1.2391e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4814e-02],
        [0.0000e+00, 4.2475e-04, 1.2390e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.4815e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(369995., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5437.0396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(297.7519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5924.0225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-724.3514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.3849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0922],
        [ 0.0927],
        [ 0.0774],
        ...,
        [-0.2517],
        [-0.2423],
        [-0.2236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93391.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0029],
        ...,
        [1.0015],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366486.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0029],
        ...,
        [1.0015],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366501.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  5.9525e-05,  ...,  0.0000e+00,
         -1.5829e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.9525e-05,  ...,  0.0000e+00,
         -1.5829e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.9525e-05,  ...,  0.0000e+00,
         -1.5829e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  5.9525e-05,  ...,  0.0000e+00,
         -1.5829e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.9525e-05,  ...,  0.0000e+00,
         -1.5829e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.9525e-05,  ...,  0.0000e+00,
         -1.5829e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-42.7581, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9457, device='cuda:0')



h[100].sum tensor(60.1258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3711, device='cuda:0')



h[200].sum tensor(5.6202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34172.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0074, 0.0114,  ..., 0.0000, 0.0028, 0.0136],
        [0.0000, 0.0011, 0.0124,  ..., 0.0000, 0.0000, 0.0151],
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0153],
        ...,
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0153],
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0153],
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287006.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3173.2510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.1100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4559.3667, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-633.5957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.0012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0837],
        [-0.1233],
        [-0.1496],
        ...,
        [-0.2606],
        [-0.2595],
        [-0.2591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-83437.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0028],
        [1.0029],
        ...,
        [1.0015],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366501.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0029],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366516.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  5.5370e-05,  ...,  0.0000e+00,
         -1.4762e-04,  0.0000e+00],
        [-2.3327e-02, -3.1615e-03,  3.5767e-02,  ..., -4.4736e-02,
          4.6425e-02, -2.1152e-02],
        [-1.0990e-02, -1.4894e-03,  1.6880e-02,  ..., -2.1076e-02,
          2.1794e-02, -9.9649e-03],
        ...,
        [-8.3955e-03, -1.1378e-03,  1.2908e-02,  ..., -1.6101e-02,
          1.6614e-02, -7.6125e-03],
        [-5.8930e-03, -7.9866e-04,  9.0771e-03,  ..., -1.1302e-02,
          1.1618e-02, -5.3434e-03],
        [-8.3955e-03, -1.1378e-03,  1.2908e-02,  ..., -1.6101e-02,
          1.6614e-02, -7.6125e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(21.6690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0224, device='cuda:0')



h[100].sum tensor(63.4207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6255, device='cuda:0')



h[200].sum tensor(8.4466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7852, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1201,  ..., 0.0000, 0.1558, 0.0000],
        [0.0000, 0.0000, 0.0463,  ..., 0.0000, 0.0597, 0.0000],
        [0.0000, 0.0000, 0.1213,  ..., 0.0000, 0.1573, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0198,  ..., 0.0000, 0.0252, 0.0000],
        [0.0000, 0.0000, 0.0545,  ..., 0.0000, 0.0702, 0.0000],
        [0.0000, 0.0000, 0.0437,  ..., 0.0000, 0.0561, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37775.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.9909e-01, 6.8403e-01, 0.0000e+00,  ..., 1.0129e-02, 4.7615e-01,
         0.0000e+00],
        [3.7942e-01, 5.2001e-01, 0.0000e+00,  ..., 1.6240e-03, 3.5961e-01,
         0.0000e+00],
        [5.1424e-01, 7.0472e-01, 0.0000e+00,  ..., 1.0580e-02, 4.9046e-01,
         0.0000e+00],
        ...,
        [9.7755e-02, 1.3551e-01, 1.4427e-03,  ..., 0.0000e+00, 8.9203e-02,
         3.5301e-04],
        [1.6163e-01, 2.2196e-01, 0.0000e+00,  ..., 0.0000e+00, 1.4876e-01,
         0.0000e+00],
        [1.7780e-01, 2.4374e-01, 0.0000e+00,  ..., 0.0000e+00, 1.6362e-01,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(301064.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3488.7642, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.5961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4626.9468, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-651.6302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.2008, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0193],
        [0.0288],
        [0.0177],
        ...,
        [0.0063],
        [0.0356],
        [0.0465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89992.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0029],
        ...,
        [1.0015],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366516.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0029],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366531.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  5.1850e-05,  ...,  0.0000e+00,
         -1.3906e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.1850e-05,  ...,  0.0000e+00,
         -1.3906e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.1850e-05,  ...,  0.0000e+00,
         -1.3906e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  5.1850e-05,  ...,  0.0000e+00,
         -1.3906e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.1850e-05,  ...,  0.0000e+00,
         -1.3906e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.1850e-05,  ...,  0.0000e+00,
         -1.3906e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-22.4958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1099, device='cuda:0')



h[100].sum tensor(60.2521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4619, device='cuda:0')



h[200].sum tensor(5.1412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35195.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0522, 0.0735, 0.0048,  ..., 0.0000, 0.0478, 0.0053],
        [0.0167, 0.0254, 0.0091,  ..., 0.0000, 0.0145, 0.0099],
        [0.0054, 0.0101, 0.0109,  ..., 0.0000, 0.0039, 0.0137],
        ...,
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0161],
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0161],
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297281.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3351.7288, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.8540, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4606.5449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-637.3984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.1497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0653],
        [ 0.0543],
        [ 0.0581],
        ...,
        [-0.2685],
        [-0.2674],
        [-0.2670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89205.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0029],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366531.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0030],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366547.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  4.6229e-05,  ...,  0.0000e+00,
         -1.3214e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.6229e-05,  ...,  0.0000e+00,
         -1.3214e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.6229e-05,  ...,  0.0000e+00,
         -1.3214e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.6229e-05,  ...,  0.0000e+00,
         -1.3214e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.6229e-05,  ...,  0.0000e+00,
         -1.3214e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.6229e-05,  ...,  0.0000e+00,
         -1.3214e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28.9009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7941, device='cuda:0')



h[100].sum tensor(62.3954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9463, device='cuda:0')



h[200].sum tensor(6.9920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39476.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0290, 0.0416, 0.0075,  ..., 0.0000, 0.0270, 0.0082],
        [0.0069, 0.0115, 0.0111,  ..., 0.0000, 0.0061, 0.0136],
        [0.0082, 0.0141, 0.0107,  ..., 0.0000, 0.0071, 0.0129],
        ...,
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0164],
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0164],
        [0.0000, 0.0004, 0.0125,  ..., 0.0000, 0.0000, 0.0164]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322713.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4084.8369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(262.9752, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5468.5068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-665.9814, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.8633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0309],
        [-0.0302],
        [-0.0049],
        ...,
        [-0.2717],
        [-0.2708],
        [-0.2706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72856.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0029],
        [1.0030],
        ...,
        [1.0016],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366547.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0030],
        [1.0030],
        ...,
        [1.0017],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366561.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  3.3524e-05,  ...,  0.0000e+00,
         -1.1920e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3524e-05,  ...,  0.0000e+00,
         -1.1920e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3524e-05,  ...,  0.0000e+00,
         -1.1920e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.3524e-05,  ...,  0.0000e+00,
         -1.1920e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3524e-05,  ...,  0.0000e+00,
         -1.1920e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3524e-05,  ...,  0.0000e+00,
         -1.1920e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(-51.9419, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.7319, device='cuda:0')



h[100].sum tensor(58.7950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5938, device='cuda:0')



h[200].sum tensor(1.7500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0053,  ..., 0.0000, 0.0066, 0.0000],
        [0.0000, 0.0000, 0.0104,  ..., 0.0000, 0.0131, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29658.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0305, 0.0432, 0.0064,  ..., 0.0000, 0.0258, 0.0069],
        [0.0530, 0.0736, 0.0034,  ..., 0.0000, 0.0450, 0.0032],
        [0.0737, 0.1011, 0.0021,  ..., 0.0000, 0.0632, 0.0020],
        ...,
        [0.0000, 0.0003, 0.0128,  ..., 0.0000, 0.0000, 0.0171],
        [0.0000, 0.0003, 0.0128,  ..., 0.0000, 0.0000, 0.0171],
        [0.0000, 0.0003, 0.0128,  ..., 0.0000, 0.0000, 0.0171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(264566.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2296.0896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.9375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3648.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-605.4381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.8640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0927],
        [ 0.1235],
        [ 0.1429],
        ...,
        [-0.2803],
        [-0.2791],
        [-0.2787]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101907.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0030],
        [1.0030],
        ...,
        [1.0017],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366561.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0030],
        [1.0030],
        ...,
        [1.0017],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366576.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.1103e-03, -1.1999e-03,  1.4053e-02,  ..., -1.7516e-02,
          1.8193e-02, -8.2561e-03],
        [-3.7625e-03, -4.9555e-04,  5.8173e-03,  ..., -7.2340e-03,
          7.4524e-03, -3.4097e-03],
        [-5.3478e-03, -7.0435e-04,  8.2587e-03,  ..., -1.0282e-02,
          1.0636e-02, -4.8464e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.3231e-05,  ...,  0.0000e+00,
         -1.0446e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.3231e-05,  ...,  0.0000e+00,
         -1.0446e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.3231e-05,  ...,  0.0000e+00,
         -1.0446e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49.4496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0062, device='cuda:0')



h[100].sum tensor(64.9476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0636, device='cuda:0')



h[200].sum tensor(6.5759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.3347e-02,  ..., 0.0000e+00, 3.0015e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.2592e-02,  ..., 0.0000e+00, 6.8053e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.3385e-02,  ..., 0.0000e+00, 3.0168e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 9.3336e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.3355e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.3358e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38072.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3536e-01, 1.8426e-01, 1.5472e-04,  ..., 0.0000e+00, 1.2084e-01,
         0.0000e+00],
        [1.7922e-01, 2.4357e-01, 0.0000e+00,  ..., 0.0000e+00, 1.6204e-01,
         0.0000e+00],
        [1.4444e-01, 1.9720e-01, 0.0000e+00,  ..., 0.0000e+00, 1.3075e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.7702e-04, 1.3122e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7951e-02],
        [0.0000e+00, 1.7711e-04, 1.3125e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7954e-02],
        [0.0000e+00, 1.7706e-04, 1.3125e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.7955e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313378.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3609.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(251.8809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4482.2759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-660.5990, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.1829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1560],
        [ 0.1621],
        [ 0.1599],
        ...,
        [-0.2869],
        [-0.2860],
        [-0.2858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106251.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0030],
        [1.0030],
        ...,
        [1.0017],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366576.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0031],
        [1.0031],
        ...,
        [1.0017],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366591.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  1.3161e-05,  ...,  0.0000e+00,
         -9.2789e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3161e-05,  ...,  0.0000e+00,
         -9.2789e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3161e-05,  ...,  0.0000e+00,
         -9.2789e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.3161e-05,  ...,  0.0000e+00,
         -9.2789e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3161e-05,  ...,  0.0000e+00,
         -9.2789e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3161e-05,  ...,  0.0000e+00,
         -9.2789e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49.7093, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9945, device='cuda:0')



h[100].sum tensor(65.4771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5041, device='cuda:0')



h[200].sum tensor(5.4105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3590, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 5.2729e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.2764e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.2810e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.2885e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.2896e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.2898e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38901.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0219, 0.0320, 0.0086,  ..., 0.0000, 0.0191, 0.0104],
        [0.0047, 0.0083, 0.0120,  ..., 0.0000, 0.0040, 0.0163],
        [0.0000, 0.0001, 0.0133,  ..., 0.0000, 0.0000, 0.0184],
        ...,
        [0.0000, 0.0001, 0.0133,  ..., 0.0000, 0.0000, 0.0184],
        [0.0000, 0.0001, 0.0133,  ..., 0.0000, 0.0000, 0.0184],
        [0.0000, 0.0001, 0.0133,  ..., 0.0000, 0.0000, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(327889.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4015.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.3247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5316.8003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-677.1827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.0397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0210],
        [-0.0462],
        [-0.1087],
        ...,
        [-0.2942],
        [-0.2929],
        [-0.2925]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-85141.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0031],
        [1.0031],
        ...,
        [1.0017],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366591.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(555.3911, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0031],
        [1.0031],
        ...,
        [1.0017],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366605.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3435e-02, -1.7441e-03,  2.0756e-02,  ..., -2.5865e-02,
          2.6983e-02, -1.2172e-02],
        [-1.7927e-02, -2.3272e-03,  2.7693e-02,  ..., -3.4512e-02,
          3.6031e-02, -1.6242e-02],
        [-2.3104e-02, -2.9993e-03,  3.5687e-02,  ..., -4.4478e-02,
          4.6458e-02, -2.0932e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  7.3931e-06,  ...,  0.0000e+00,
         -8.0618e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.3931e-06,  ...,  0.0000e+00,
         -8.0618e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.3931e-06,  ...,  0.0000e+00,
         -8.0618e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(184.6597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3677, device='cuda:0')



h[100].sum tensor(72.6612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5816, device='cuda:0')



h[200].sum tensor(11.2165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.5446, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 9.3312e-02,  ..., 0.0000e+00, 1.2135e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0216e-01,  ..., 0.0000e+00, 1.3289e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.2155e-01,  ..., 0.0000e+00, 1.5818e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.9712e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9718e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9719e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47442.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.9380e-01, 5.3532e-01, 0.0000e+00,  ..., 8.2174e-04, 3.7088e-01,
         0.0000e+00],
        [4.7227e-01, 6.4180e-01, 0.0000e+00,  ..., 2.7149e-03, 4.4608e-01,
         0.0000e+00],
        [5.4128e-01, 7.3529e-01, 0.0000e+00,  ..., 5.0887e-03, 5.1188e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.0624e-04, 1.3343e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8563e-02],
        [0.0000e+00, 1.0712e-04, 1.3347e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8567e-02],
        [0.0000e+00, 1.0788e-04, 1.3347e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8568e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368600.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5104.2339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.3721, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6128.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-728.1813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.1942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0608],
        [ 0.0699],
        [ 0.0808],
        ...,
        [-0.2990],
        [-0.2977],
        [-0.2973]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-87972.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0031],
        [1.0031],
        ...,
        [1.0017],
        [1.0013],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366605.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0032],
        [1.0032],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366619.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  4.3070e-06,  ...,  0.0000e+00,
         -7.0558e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3070e-06,  ...,  0.0000e+00,
         -7.0558e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3070e-06,  ...,  0.0000e+00,
         -7.0558e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.3070e-06,  ...,  0.0000e+00,
         -7.0558e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3070e-06,  ...,  0.0000e+00,
         -7.0558e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3070e-06,  ...,  0.0000e+00,
         -7.0558e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(117.7466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6607, device='cuda:0')



h[100].sum tensor(68.6493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4255, device='cuda:0')



h[200].sum tensor(6.1419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5308, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.7996e-02,  ..., 0.0000e+00, 2.3311e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.7269e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.7284e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.7312e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.7316e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.7316e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40866.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0940, 0.1284, 0.0015,  ..., 0.0000, 0.0839, 0.0014],
        [0.0409, 0.0571, 0.0053,  ..., 0.0000, 0.0362, 0.0069],
        [0.0152, 0.0232, 0.0100,  ..., 0.0000, 0.0135, 0.0125],
        ...,
        [0.0000, 0.0002, 0.0133,  ..., 0.0000, 0.0000, 0.0184],
        [0.0000, 0.0002, 0.0133,  ..., 0.0000, 0.0000, 0.0184],
        [0.0000, 0.0002, 0.0133,  ..., 0.0000, 0.0000, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339448.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4245.1753, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.8115, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5610.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-689.7068, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.5124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0911],
        [ 0.0421],
        [-0.0257],
        ...,
        [-0.3025],
        [-0.3012],
        [-0.3002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-88472.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0032],
        [1.0032],
        ...,
        [1.0017],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366619.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0033],
        [1.0032],
        ...,
        [1.0018],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366634.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  6.3708e-06,  ...,  0.0000e+00,
         -6.1215e-05,  0.0000e+00],
        [-7.4568e-03, -9.5399e-04,  1.1554e-02,  ..., -1.4374e-02,
          1.5002e-02, -6.7540e-03],
        [-5.3923e-03, -6.8987e-04,  8.3570e-03,  ..., -1.0394e-02,
          1.0832e-02, -4.8841e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  6.3708e-06,  ...,  0.0000e+00,
         -6.1215e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.3708e-06,  ...,  0.0000e+00,
         -6.1215e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.3708e-06,  ...,  0.0000e+00,
         -6.1215e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(138.6304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7162, device='cuda:0')



h[100].sum tensor(68.7636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4562, device='cuda:0')



h[200].sum tensor(5.8551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5699, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.7409e-02,  ..., 0.0000e+00, 3.5597e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4686e-02,  ..., 0.0000e+00, 3.2045e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.7332e-02,  ..., 0.0000e+00, 7.4508e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.5611e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.5617e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.5618e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42389.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.3141e-01, 1.7973e-01, 0.0000e+00,  ..., 0.0000e+00, 1.2133e-01,
         0.0000e+00],
        [1.4159e-01, 1.9291e-01, 0.0000e+00,  ..., 0.0000e+00, 1.2998e-01,
         0.0000e+00],
        [2.0556e-01, 2.7850e-01, 0.0000e+00,  ..., 0.0000e+00, 1.8937e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.3554e-04, 1.3284e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8115e-02],
        [0.0000e+00, 2.3661e-04, 1.3287e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8118e-02],
        [0.0000e+00, 2.3657e-04, 1.3287e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8119e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(353497., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4598.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.8755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5978.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-696.3435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.2303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0763],
        [ 0.0812],
        [ 0.0881],
        ...,
        [-0.3047],
        [-0.3035],
        [-0.3030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89317.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0033],
        [1.0032],
        ...,
        [1.0018],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366634.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0033],
        [1.0033],
        ...,
        [1.0018],
        [1.0014],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366649.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  1.4276e-05,  ...,  0.0000e+00,
         -5.1315e-05,  0.0000e+00],
        [-3.5218e-03, -4.4727e-04,  5.4757e-03,  ..., -6.7929e-03,
          7.0729e-03, -3.1894e-03],
        [-3.5218e-03, -4.4727e-04,  5.4757e-03,  ..., -6.7929e-03,
          7.0729e-03, -3.1894e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.4276e-05,  ...,  0.0000e+00,
         -5.1315e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.4276e-05,  ...,  0.0000e+00,
         -5.1315e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.4276e-05,  ...,  0.0000e+00,
         -5.1315e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(175.7297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8874, device='cuda:0')



h[100].sum tensor(69.1051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1039, device='cuda:0')



h[200].sum tensor(6.3867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.3936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 9.9945e-03,  ..., 0.0000e+00, 1.2860e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0002e-02,  ..., 0.0000e+00, 1.2870e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0011e-02,  ..., 0.0000e+00, 1.2882e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.7401e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.7413e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.7416e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41623.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0352, 0.0492, 0.0054,  ..., 0.0000, 0.0286, 0.0056],
        [0.0577, 0.0788, 0.0031,  ..., 0.0000, 0.0487, 0.0024],
        [0.1088, 0.1474, 0.0009,  ..., 0.0000, 0.0969, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0131,  ..., 0.0000, 0.0000, 0.0176],
        [0.0000, 0.0004, 0.0131,  ..., 0.0000, 0.0000, 0.0176],
        [0.0000, 0.0004, 0.0131,  ..., 0.0000, 0.0000, 0.0176]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343269.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4287.6064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(253.4082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5977.4004, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-686.6703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.0072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0496],
        [ 0.0214],
        [ 0.0567],
        ...,
        [-0.3044],
        [-0.3031],
        [-0.3027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84169.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0033],
        [1.0033],
        ...,
        [1.0018],
        [1.0014],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366649.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0034],
        [1.0033],
        ...,
        [1.0018],
        [1.0014],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366664.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  2.5485e-05,  ...,  0.0000e+00,
         -4.0462e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.5485e-05,  ...,  0.0000e+00,
         -4.0462e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.5485e-05,  ...,  0.0000e+00,
         -4.0462e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.5485e-05,  ...,  0.0000e+00,
         -4.0462e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.5485e-05,  ...,  0.0000e+00,
         -4.0462e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.5485e-05,  ...,  0.0000e+00,
         -4.0462e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(121.8165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7790, device='cuda:0')



h[100].sum tensor(64.6533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2789, device='cuda:0')



h[200].sum tensor(2.2599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8008, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34669.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0130,  ..., 0.0000, 0.0000, 0.0168],
        [0.0000, 0.0005, 0.0130,  ..., 0.0000, 0.0000, 0.0168],
        [0.0000, 0.0005, 0.0130,  ..., 0.0000, 0.0000, 0.0168],
        ...,
        [0.0000, 0.0005, 0.0130,  ..., 0.0000, 0.0000, 0.0169],
        [0.0000, 0.0005, 0.0130,  ..., 0.0000, 0.0000, 0.0169],
        [0.0000, 0.0005, 0.0130,  ..., 0.0000, 0.0000, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(300686.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3022.2122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.4347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4720.5029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-632.3841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.0704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4197],
        [-0.4123],
        [-0.3955],
        ...,
        [-0.3030],
        [-0.3018],
        [-0.3013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-108259.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0034],
        [1.0033],
        ...,
        [1.0018],
        [1.0014],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366664.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0034],
        [1.0034],
        ...,
        [1.0019],
        [1.0014],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366679.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  3.3339e-05,  ...,  0.0000e+00,
         -3.1320e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3339e-05,  ...,  0.0000e+00,
         -3.1320e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3339e-05,  ...,  0.0000e+00,
         -3.1320e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.3339e-05,  ...,  0.0000e+00,
         -3.1320e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3339e-05,  ...,  0.0000e+00,
         -3.1320e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.3339e-05,  ...,  0.0000e+00,
         -3.1320e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(188.7393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0224, device='cuda:0')



h[100].sum tensor(66.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0726, device='cuda:0')



h[200].sum tensor(4.5919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0225,  ..., 0.0000, 0.0291, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43325.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1126, 0.1520, 0.0013,  ..., 0.0000, 0.0995, 0.0009],
        [0.0412, 0.0575, 0.0055,  ..., 0.0000, 0.0347, 0.0067],
        [0.0130, 0.0197, 0.0098,  ..., 0.0000, 0.0095, 0.0114],
        ...,
        [0.0000, 0.0006, 0.0129,  ..., 0.0000, 0.0000, 0.0165],
        [0.0000, 0.0006, 0.0129,  ..., 0.0000, 0.0000, 0.0165],
        [0.0000, 0.0006, 0.0129,  ..., 0.0000, 0.0000, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(377619.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5125.9033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(250.9041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6705.5630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-684.4742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.6786, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0878],
        [ 0.0194],
        [-0.0725],
        ...,
        [-0.3011],
        [-0.2998],
        [-0.2993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89576.6953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0034],
        [1.0034],
        ...,
        [1.0019],
        [1.0014],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366679.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0035],
        [1.0034],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366694.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  3.2124e-05,  ...,  0.0000e+00,
         -2.2390e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.2124e-05,  ...,  0.0000e+00,
         -2.2390e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.2124e-05,  ...,  0.0000e+00,
         -2.2390e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.2124e-05,  ...,  0.0000e+00,
         -2.2390e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.2124e-05,  ...,  0.0000e+00,
         -2.2390e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.2124e-05,  ...,  0.0000e+00,
         -2.2390e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(164.7179, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8437, device='cuda:0')



h[100].sum tensor(64.2071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8677, device='cuda:0')



h[200].sum tensor(2.6861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5496, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0140, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38565.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0612, 0.0855, 0.0048,  ..., 0.0000, 0.0541, 0.0049],
        [0.0150, 0.0223, 0.0098,  ..., 0.0000, 0.0125, 0.0107],
        [0.0026, 0.0048, 0.0123,  ..., 0.0000, 0.0016, 0.0152],
        ...,
        [0.0000, 0.0008, 0.0129,  ..., 0.0000, 0.0000, 0.0164],
        [0.0000, 0.0008, 0.0129,  ..., 0.0000, 0.0000, 0.0164],
        [0.0000, 0.0008, 0.0129,  ..., 0.0000, 0.0000, 0.0164]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(338380.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4020.8232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(232.7349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6208.4341, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-659.5482, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.3634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0073],
        [-0.1003],
        [-0.1994],
        ...,
        [-0.3016],
        [-0.3004],
        [-0.3000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82347.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0035],
        [1.0034],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366694.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0035],
        [1.0035],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366709.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  1.8719e-05,  ...,  0.0000e+00,
         -1.1349e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.8719e-05,  ...,  0.0000e+00,
         -1.1349e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.8719e-05,  ...,  0.0000e+00,
         -1.1349e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.8719e-05,  ...,  0.0000e+00,
         -1.1349e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.8719e-05,  ...,  0.0000e+00,
         -1.1349e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.8719e-05,  ...,  0.0000e+00,
         -1.1349e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(136.3699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8126, device='cuda:0')



h[100].sum tensor(62.9811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7445, device='cuda:0')



h[200].sum tensor(0.7996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 7.5010e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.5068e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4539e-02,  ..., 0.0000e+00, 3.1890e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 7.5309e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.5327e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.5330e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35026.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0144, 0.0110,  ..., 0.0000, 0.0068, 0.0131],
        [0.0438, 0.0622, 0.0062,  ..., 0.0000, 0.0388, 0.0078],
        [0.1328, 0.1810, 0.0015,  ..., 0.0000, 0.1199, 0.0012],
        ...,
        [0.0000, 0.0010, 0.0129,  ..., 0.0000, 0.0000, 0.0167],
        [0.0000, 0.0010, 0.0129,  ..., 0.0000, 0.0000, 0.0168],
        [0.0000, 0.0010, 0.0129,  ..., 0.0000, 0.0000, 0.0168]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(314229.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3243.1694, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.3120, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5589.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-639.6082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.9371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1185],
        [-0.0312],
        [ 0.0391],
        ...,
        [-0.3080],
        [-0.3067],
        [-0.3062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89606.9922, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0035],
        [1.0035],
        ...,
        [1.0019],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366709.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0036],
        [1.0036],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366723.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5287e-03, -5.5416e-04,  7.0817e-03,  ..., -8.7632e-03,
          9.2250e-03, -4.0985e-03],
        [ 0.0000e+00,  0.0000e+00,  9.8091e-06,  ...,  0.0000e+00,
         -6.6498e-07,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  9.8091e-06,  ...,  0.0000e+00,
         -6.6498e-07,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  9.8091e-06,  ...,  0.0000e+00,
         -6.6498e-07,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  9.8091e-06,  ...,  0.0000e+00,
         -6.6498e-07,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  9.8091e-06,  ...,  0.0000e+00,
         -6.6498e-07,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(141.9049, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.7158, device='cuda:0')



h[100].sum tensor(63.6932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6910, device='cuda:0')



h[200].sum tensor(0.5971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.3381e-02,  ..., 0.0000e+00, 3.0449e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.1279e-03,  ..., 0.0000e+00, 9.2467e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.8707e-02,  ..., 0.0000e+00, 5.0442e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.9469e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.9478e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.9480e-05,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34936.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1760, 0.2388, 0.0000,  ..., 0.0000, 0.1600, 0.0000],
        [0.1446, 0.1977, 0.0000,  ..., 0.0000, 0.1319, 0.0000],
        [0.2165, 0.2949, 0.0000,  ..., 0.0000, 0.2010, 0.0000],
        ...,
        [0.0000, 0.0010, 0.0130,  ..., 0.0000, 0.0000, 0.0173],
        [0.0000, 0.0010, 0.0130,  ..., 0.0000, 0.0000, 0.0173],
        [0.0000, 0.0010, 0.0130,  ..., 0.0000, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(312616.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3125.8394, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.8387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5558.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-645.8854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.6693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0990],
        [ 0.0865],
        [ 0.0714],
        ...,
        [-0.3131],
        [-0.3119],
        [-0.3118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92829.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0036],
        [1.0036],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366723.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0036],
        [1.0036],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366737.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.2371e-06,  ...,  0.0000e+00,
          9.1171e-06,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2371e-06,  ...,  0.0000e+00,
          9.1171e-06,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2371e-06,  ...,  0.0000e+00,
          9.1171e-06,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.2371e-06,  ...,  0.0000e+00,
          9.1171e-06,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2371e-06,  ...,  0.0000e+00,
          9.1171e-06,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2371e-06,  ...,  0.0000e+00,
          9.1171e-06,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(158.4701, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5763, device='cuda:0')



h[100].sum tensor(65.0224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1668, device='cuda:0')



h[200].sum tensor(1.0352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6583, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6535e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6564e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6600e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6690e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6699e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6700e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36674.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0132,  ..., 0.0000, 0.0000, 0.0179],
        [0.0014, 0.0040, 0.0128,  ..., 0.0000, 0.0007, 0.0172],
        [0.0197, 0.0296, 0.0091,  ..., 0.0000, 0.0170, 0.0104],
        ...,
        [0.0000, 0.0011, 0.0132,  ..., 0.0000, 0.0000, 0.0180],
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0180],
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(325961.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3358.1494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(220.1156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5393.9775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-658.3625, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.4719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2943],
        [-0.1871],
        [-0.0648],
        ...,
        [-0.3240],
        [-0.3226],
        [-0.3222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109794.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0036],
        [1.0036],
        ...,
        [1.0020],
        [1.0015],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366737.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(550.8968, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0037],
        ...,
        [1.0020],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366752.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -5.3429e-06,  ...,  0.0000e+00,
          2.0054e-05,  0.0000e+00],
        [-9.4797e-03, -1.1426e-03,  1.4840e-02,  ..., -1.8367e-02,
          1.9386e-02, -8.5769e-03],
        [-4.7921e-03, -5.7762e-04,  7.4991e-03,  ..., -9.2849e-03,
          9.8098e-03, -4.3358e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.3429e-06,  ...,  0.0000e+00,
          2.0054e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.3429e-06,  ...,  0.0000e+00,
          2.0054e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.3429e-06,  ...,  0.0000e+00,
          2.0054e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(224.3954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1811, device='cuda:0')



h[100].sum tensor(68.1431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1603, device='cuda:0')



h[200].sum tensor(4.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1935, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.1001e-02,  ..., 0.0000e+00, 2.7491e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.3950e-02,  ..., 0.0000e+00, 3.1345e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.6426e-02,  ..., 0.0000e+00, 1.1285e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.0714e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.0734e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.0737e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38856.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.1573e-01, 1.5940e-01, 2.0019e-03,  ..., 0.0000e+00, 1.0414e-01,
         2.1061e-03],
        [2.0384e-01, 2.7671e-01, 3.4816e-05,  ..., 0.0000e+00, 1.8638e-01,
         0.0000e+00],
        [3.7977e-01, 5.1348e-01, 0.0000e+00,  ..., 0.0000e+00, 3.5434e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.0768e-03, 1.3326e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8589e-02],
        [0.0000e+00, 1.0764e-03, 1.3330e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8593e-02],
        [0.0000e+00, 1.0768e-03, 1.3330e-02,  ..., 0.0000e+00, 0.0000e+00,
         1.8593e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(329294.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3438.6147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(233.3901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5763.5186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-682.1642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.8413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0659],
        [ 0.0848],
        [ 0.0869],
        ...,
        [-0.3301],
        [-0.3288],
        [-0.3283]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97852.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0037],
        [1.0037],
        ...,
        [1.0020],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366752.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0037],
        [1.0038],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366767.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.9115e-06,  ...,  0.0000e+00,
          2.7666e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9115e-06,  ...,  0.0000e+00,
          2.7666e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9115e-06,  ...,  0.0000e+00,
          2.7666e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.9115e-06,  ...,  0.0000e+00,
          2.7666e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9115e-06,  ...,  0.0000e+00,
          2.7666e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9115e-06,  ...,  0.0000e+00,
          2.7666e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(288.5378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8496, device='cuda:0')



h[100].sum tensor(70.8017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1890, device='cuda:0')



h[200].sum tensor(6.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48073.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0190],
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0190],
        [0.0000, 0.0010, 0.0133,  ..., 0.0000, 0.0000, 0.0190],
        ...,
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0191],
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0191],
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398045.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5297.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(272.8264, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7223.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-740.8867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.9496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2516],
        [-0.2978],
        [-0.3377],
        ...,
        [-0.3354],
        [-0.3340],
        [-0.3336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94296.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0037],
        [1.0038],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366767.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0038],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366781.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 6.1015e-06,  ..., 0.0000e+00, 3.8509e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1015e-06,  ..., 0.0000e+00, 3.8509e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1015e-06,  ..., 0.0000e+00, 3.8509e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 6.1015e-06,  ..., 0.0000e+00, 3.8509e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1015e-06,  ..., 0.0000e+00, 3.8509e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.1015e-06,  ..., 0.0000e+00, 3.8509e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(368.1141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8735, device='cuda:0')



h[100].sum tensor(73.9294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.4143, device='cuda:0')



h[200].sum tensor(10.2375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.6036, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 5.3855e-03,  ..., 0.0000e+00, 7.1469e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0757e-02,  ..., 0.0000e+00, 1.4153e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3964e-03,  ..., 0.0000e+00, 7.1614e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.4565e-05,  ..., 0.0000e+00, 1.5504e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4571e-05,  ..., 0.0000e+00, 1.5508e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.4572e-05,  ..., 0.0000e+00, 1.5509e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51050.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0482, 0.0684, 0.0029,  ..., 0.0000, 0.0391, 0.0033],
        [0.0561, 0.0784, 0.0019,  ..., 0.0000, 0.0458, 0.0025],
        [0.0538, 0.0757, 0.0019,  ..., 0.0000, 0.0442, 0.0015],
        ...,
        [0.0000, 0.0012, 0.0133,  ..., 0.0000, 0.0000, 0.0194],
        [0.0000, 0.0012, 0.0133,  ..., 0.0000, 0.0000, 0.0194],
        [0.0000, 0.0012, 0.0133,  ..., 0.0000, 0.0000, 0.0194]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409911.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5576.5967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.1767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7224.2339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-756.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.7757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1457],
        [ 0.1473],
        [ 0.1449],
        ...,
        [-0.3390],
        [-0.3376],
        [-0.3372]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104910.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0038],
        [1.0038],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366781.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0038],
        [1.0039],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366795.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.4558e-06,  ...,  0.0000e+00,
          4.8167e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4558e-06,  ...,  0.0000e+00,
          4.8167e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4558e-06,  ...,  0.0000e+00,
          4.8167e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.4558e-06,  ...,  0.0000e+00,
          4.8167e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4558e-06,  ...,  0.0000e+00,
          4.8167e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4558e-06,  ...,  0.0000e+00,
          4.8167e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(213.0497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7572, device='cuda:0')



h[100].sum tensor(66.2593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3729, device='cuda:0')



h[200].sum tensor(2.6679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1921, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39466.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0848, 0.1194, 0.0029,  ..., 0.0000, 0.0761, 0.0044],
        [0.0502, 0.0723, 0.0038,  ..., 0.0000, 0.0446, 0.0050],
        [0.0339, 0.0525, 0.0062,  ..., 0.0000, 0.0297, 0.0080],
        ...,
        [0.0000, 0.0013, 0.0133,  ..., 0.0000, 0.0000, 0.0199],
        [0.0000, 0.0013, 0.0134,  ..., 0.0000, 0.0000, 0.0199],
        [0.0000, 0.0013, 0.0133,  ..., 0.0000, 0.0000, 0.0199]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(346627.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3778.5830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(239.4502, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5955.4951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-691.2662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.4025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0548],
        [ 0.0476],
        [ 0.0444],
        ...,
        [-0.3391],
        [-0.3376],
        [-0.3351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104064.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0038],
        [1.0039],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366795.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0039],
        ...,
        [1.0020],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366810.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6635e-02, -3.1140e-03,  4.1958e-02,  ..., -5.1741e-02,
          5.4777e-02, -2.4086e-02],
        [-1.9125e-02, -2.2359e-03,  3.0127e-02,  ..., -3.7152e-02,
          3.9347e-02, -1.7294e-02],
        [-1.0090e-02, -1.1797e-03,  1.5894e-02,  ..., -1.9601e-02,
          2.0785e-02, -9.1245e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6387e-06,  ...,  0.0000e+00,
          5.2960e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6387e-06,  ...,  0.0000e+00,
          5.2960e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6387e-06,  ...,  0.0000e+00,
          5.2960e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(213.7980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2313, device='cuda:0')



h[100].sum tensor(65.9528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0821, device='cuda:0')



h[200].sum tensor(2.4427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8222, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1288,  ..., 0.0000, 0.1682, 0.0000],
        [0.0000, 0.0000, 0.1184,  ..., 0.0000, 0.1546, 0.0000],
        [0.0000, 0.0000, 0.0980,  ..., 0.0000, 0.1281, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37509.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.5918e-01, 7.5447e-01, 0.0000e+00,  ..., 4.0820e-04, 5.2532e-01,
         0.0000e+00],
        [4.8838e-01, 6.5927e-01, 0.0000e+00,  ..., 7.9105e-04, 4.5765e-01,
         0.0000e+00],
        [3.9585e-01, 5.3425e-01, 0.0000e+00,  ..., 0.0000e+00, 3.6831e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.3209e-03, 1.3398e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0292e-02],
        [0.0000e+00, 1.3215e-03, 1.3402e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0299e-02],
        [0.0000e+00, 1.3208e-03, 1.3402e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0299e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(330413.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3247.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(231.2460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5404.8799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-678.7001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.6110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0576],
        [ 0.0760],
        [ 0.0974],
        ...,
        [-0.3483],
        [-0.3468],
        [-0.3464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111693., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0039],
        ...,
        [1.0020],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366810.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0040],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366824.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.9879e-02, -3.4664e-03,  4.7142e-02,  ..., -5.8080e-02,
          6.1532e-02, -2.7015e-02],
        [-9.1023e-03, -1.0560e-03,  1.4362e-02,  ..., -1.7693e-02,
          1.8783e-02, -8.2298e-03],
        [ 0.0000e+00,  0.0000e+00,  1.1167e-06,  ...,  0.0000e+00,
          5.3982e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.1167e-06,  ...,  0.0000e+00,
          5.3982e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.1167e-06,  ...,  0.0000e+00,
          5.3982e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.1167e-06,  ...,  0.0000e+00,
          5.3982e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(215.0036, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2652, device='cuda:0')



h[100].sum tensor(65.7619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1008, device='cuda:0')



h[200].sum tensor(2.3808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 7.9299e-02,  ..., 0.0000e+00, 1.0363e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.4131e-02,  ..., 0.0000e+00, 1.2297e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.7681e-02,  ..., 0.0000e+00, 4.9351e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.4978e-06,  ..., 0.0000e+00, 2.1743e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4990e-06,  ..., 0.0000e+00, 2.1749e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4991e-06,  ..., 0.0000e+00, 2.1750e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38551.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4079, 0.5494, 0.0000,  ..., 0.0000, 0.3784, 0.0000],
        [0.4078, 0.5497, 0.0000,  ..., 0.0000, 0.3789, 0.0000],
        [0.2984, 0.4033, 0.0000,  ..., 0.0000, 0.2753, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0135,  ..., 0.0000, 0.0000, 0.0208],
        [0.0000, 0.0013, 0.0135,  ..., 0.0000, 0.0000, 0.0208],
        [0.0000, 0.0013, 0.0135,  ..., 0.0000, 0.0000, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341662.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3543.2593, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(237.1638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5754.3682, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-691.0044, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.4898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0663],
        [ 0.0810],
        [ 0.0959],
        ...,
        [-0.3530],
        [-0.3516],
        [-0.3511]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-106197.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0039],
        [1.0040],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366824.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0040],
        [1.0040],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366839.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.8540e-05,  ..., 0.0000e+00, 5.1910e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8540e-05,  ..., 0.0000e+00, 5.1910e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8540e-05,  ..., 0.0000e+00, 5.1910e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.8540e-05,  ..., 0.0000e+00, 5.1910e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8540e-05,  ..., 0.0000e+00, 5.1910e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8540e-05,  ..., 0.0000e+00, 5.1910e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(398.1971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.4832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3092, device='cuda:0')



h[100].sum tensor(73.5932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6552, device='cuda:0')



h[200].sum tensor(10.3563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.9101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 7.4311e-05,  ..., 0.0000e+00, 2.0807e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.4375e-05,  ..., 0.0000e+00, 2.0825e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.4450e-05,  ..., 0.0000e+00, 2.0846e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 7.4686e-05,  ..., 0.0000e+00, 2.0912e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.4707e-05,  ..., 0.0000e+00, 2.0918e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.4710e-05,  ..., 0.0000e+00, 2.0918e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51579.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0012, 0.0133,  ..., 0.0000, 0.0000, 0.0207],
        [0.0000, 0.0012, 0.0133,  ..., 0.0000, 0.0000, 0.0207],
        [0.0000, 0.0012, 0.0134,  ..., 0.0000, 0.0000, 0.0207],
        ...,
        [0.0000, 0.0012, 0.0134,  ..., 0.0000, 0.0000, 0.0208],
        [0.0000, 0.0012, 0.0134,  ..., 0.0000, 0.0000, 0.0208],
        [0.0000, 0.0012, 0.0134,  ..., 0.0000, 0.0000, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423400.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5851.0693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.5918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7637.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-771.4167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-152.3609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4402],
        [-0.4608],
        [-0.4704],
        ...,
        [-0.3531],
        [-0.3516],
        [-0.3510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93688.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0040],
        [1.0040],
        ...,
        [1.0020],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366839.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0040],
        [1.0041],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366852.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0783e-02, -1.2317e-03,  1.7091e-02,  ..., -2.0988e-02,
          2.2291e-02, -9.7467e-03],
        [-2.2469e-02, -2.5666e-03,  3.5583e-02,  ..., -4.3733e-02,
          4.6403e-02, -2.0310e-02],
        [-1.7953e-02, -2.0507e-03,  2.8437e-02,  ..., -3.4943e-02,
          3.7085e-02, -1.6228e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.7511e-05,  ...,  0.0000e+00,
          4.2715e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7511e-05,  ...,  0.0000e+00,
          4.2715e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7511e-05,  ...,  0.0000e+00,
          4.2715e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(370.3879, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9899, device='cuda:0')



h[100].sum tensor(72.5135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.8196, device='cuda:0')



h[200].sum tensor(8.2003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.5756, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 8.7653e-02,  ..., 0.0000e+00, 1.1432e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.3157e-02,  ..., 0.0000e+00, 1.2149e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1528e-01,  ..., 0.0000e+00, 1.5034e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.1085e-04,  ..., 0.0000e+00, 1.7210e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1088e-04,  ..., 0.0000e+00, 1.7215e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1088e-04,  ..., 0.0000e+00, 1.7215e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48756.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3361, 0.4497, 0.0000,  ..., 0.0000, 0.3092, 0.0000],
        [0.4120, 0.5502, 0.0000,  ..., 0.0000, 0.3800, 0.0000],
        [0.4669, 0.6231, 0.0000,  ..., 0.0000, 0.4318, 0.0000],
        ...,
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0208],
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0208],
        [0.0000, 0.0011, 0.0133,  ..., 0.0000, 0.0000, 0.0208]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402056.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5268.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.6158, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7333.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-758.6404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.7063, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0641],
        [ 0.0500],
        [ 0.0344],
        ...,
        [-0.3616],
        [-0.3601],
        [-0.3597]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92442.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0040],
        [1.0041],
        ...,
        [1.0021],
        [1.0016],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366852.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0041],
        [1.0042],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366866.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2040e-03, -4.7648e-04,  6.7011e-03,  ..., -8.1879e-03,
          8.7254e-03, -3.7995e-03],
        [-1.3930e-02, -1.5788e-03,  2.2115e-02,  ..., -2.7131e-02,
          2.8822e-02, -1.2590e-02],
        [ 0.0000e+00,  0.0000e+00,  3.8534e-05,  ...,  0.0000e+00,
          3.8819e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.8534e-05,  ...,  0.0000e+00,
          3.8819e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.8534e-05,  ...,  0.0000e+00,
          3.8819e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.8534e-05,  ...,  0.0000e+00,
          3.8819e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(510.8591, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.0602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1259, device='cuda:0')



h[100].sum tensor(78.8975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.7659, device='cuda:0')



h[200].sum tensor(14.0206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5944, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0711,  ..., 0.0000, 0.0927, 0.0000],
        [0.0000, 0.0000, 0.0293,  ..., 0.0000, 0.0381, 0.0000],
        [0.0000, 0.0000, 0.0480,  ..., 0.0000, 0.0625, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56753.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3269, 0.4339, 0.0000,  ..., 0.0000, 0.2972, 0.0000],
        [0.2514, 0.3341, 0.0000,  ..., 0.0000, 0.2264, 0.0000],
        [0.2199, 0.2928, 0.0000,  ..., 0.0000, 0.1974, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0134,  ..., 0.0000, 0.0000, 0.0209],
        [0.0000, 0.0009, 0.0134,  ..., 0.0000, 0.0000, 0.0209],
        [0.0000, 0.0009, 0.0134,  ..., 0.0000, 0.0000, 0.0209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445944.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6547.2637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(303.9539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8234.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-808.8575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.7108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0815],
        [ 0.0798],
        [ 0.0751],
        ...,
        [-0.3664],
        [-0.3650],
        [-0.3645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93665.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0041],
        [1.0042],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366866.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0041],
        [1.0043],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366880.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  4.4816e-05,  ...,  0.0000e+00,
          3.0816e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.4816e-05,  ...,  0.0000e+00,
          3.0816e-05,  0.0000e+00],
        [-4.4695e-03, -5.0261e-04,  7.1384e-03,  ..., -8.7106e-03,
          9.2789e-03, -4.0388e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.4816e-05,  ...,  0.0000e+00,
          3.0816e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.4816e-05,  ...,  0.0000e+00,
          3.0816e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.4816e-05,  ...,  0.0000e+00,
          3.0816e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(496.9885, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.4663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0804, device='cuda:0')



h[100].sum tensor(78.5028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.6347, device='cuda:0')



h[200].sum tensor(12.6021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0073,  ..., 0.0000, 0.0094, 0.0000],
        [0.0000, 0.0000, 0.0242,  ..., 0.0000, 0.0315, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56222.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0234, 0.0337, 0.0085,  ..., 0.0000, 0.0192, 0.0121],
        [0.0683, 0.0931, 0.0042,  ..., 0.0000, 0.0573, 0.0061],
        [0.1316, 0.1748, 0.0005,  ..., 0.0000, 0.1142, 0.0003],
        ...,
        [0.0000, 0.0007, 0.0134,  ..., 0.0000, 0.0000, 0.0209],
        [0.0000, 0.0007, 0.0134,  ..., 0.0000, 0.0000, 0.0209],
        [0.0000, 0.0007, 0.0135,  ..., 0.0000, 0.0000, 0.0209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(452779.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6723.1597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(297.7202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8271.2363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-807.7466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.1243, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0544],
        [ 0.0435],
        [ 0.0982],
        ...,
        [-0.3718],
        [-0.3703],
        [-0.3699]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100264.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0041],
        [1.0043],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366880.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(566.2062, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0042],
        [1.0043],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366893.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 4.7380e-05,  ..., 0.0000e+00, 2.5326e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.7380e-05,  ..., 0.0000e+00, 2.5326e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.7380e-05,  ..., 0.0000e+00, 2.5326e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.7380e-05,  ..., 0.0000e+00, 2.5326e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.7380e-05,  ..., 0.0000e+00, 2.5326e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.7380e-05,  ..., 0.0000e+00, 2.5326e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(334.1593, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1225, device='cuda:0')



h[100].sum tensor(71.5538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1279, device='cuda:0')



h[200].sum tensor(4.5751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44933.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0079, 0.0139, 0.0112,  ..., 0.0000, 0.0042, 0.0173],
        [0.0040, 0.0080, 0.0121,  ..., 0.0000, 0.0016, 0.0189],
        [0.0205, 0.0300, 0.0089,  ..., 0.0000, 0.0153, 0.0131],
        ...,
        [0.0000, 0.0007, 0.0135,  ..., 0.0000, 0.0000, 0.0209],
        [0.0000, 0.0007, 0.0135,  ..., 0.0000, 0.0000, 0.0209],
        [0.0000, 0.0007, 0.0135,  ..., 0.0000, 0.0000, 0.0209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(403281.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5301.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.0473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7308.4614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-743.9726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.3984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0186],
        [ 0.0113],
        [ 0.0287],
        ...,
        [-0.3759],
        [-0.3744],
        [-0.3739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100372.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0042],
        [1.0043],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366893.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0042],
        [1.0044],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366907.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 5.5070e-05,  ..., 0.0000e+00, 2.0807e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.5070e-05,  ..., 0.0000e+00, 2.0807e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.5070e-05,  ..., 0.0000e+00, 2.0807e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.5070e-05,  ..., 0.0000e+00, 2.0807e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.5070e-05,  ..., 0.0000e+00, 2.0807e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.5070e-05,  ..., 0.0000e+00, 2.0807e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(302.8916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8858, device='cuda:0')



h[100].sum tensor(69.8488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8910, device='cuda:0')



h[200].sum tensor(2.5453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5792, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.2075e-04,  ..., 0.0000e+00, 8.3406e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.2097e-04,  ..., 0.0000e+00, 8.3488e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.2121e-04,  ..., 0.0000e+00, 8.3576e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.2201e-04,  ..., 0.0000e+00, 8.3881e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.2208e-04,  ..., 0.0000e+00, 8.3905e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.2209e-04,  ..., 0.0000e+00, 8.3910e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37839.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0296, 0.0412, 0.0079,  ..., 0.0000, 0.0254, 0.0108],
        [0.0164, 0.0243, 0.0100,  ..., 0.0000, 0.0115, 0.0143],
        [0.0415, 0.0566, 0.0059,  ..., 0.0000, 0.0338, 0.0085],
        ...,
        [0.0000, 0.0007, 0.0135,  ..., 0.0000, 0.0000, 0.0207],
        [0.0000, 0.0007, 0.0135,  ..., 0.0000, 0.0000, 0.0207],
        [0.0000, 0.0007, 0.0135,  ..., 0.0000, 0.0000, 0.0207]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339860.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3422.0088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(214.3554, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5742.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-696.0483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.8434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0022],
        [ 0.0104],
        [ 0.0330],
        ...,
        [-0.3762],
        [-0.3738],
        [-0.3672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117767.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0042],
        [1.0044],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366907.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0042],
        [1.0045],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366921.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8596e-03, -5.3368e-04,  7.8143e-03,  ..., -9.4894e-03,
          1.0121e-02, -4.3895e-03],
        [-3.8647e-03, -4.2442e-04,  6.2282e-03,  ..., -7.5466e-03,
          8.0535e-03, -3.4908e-03],
        [-8.7242e-03, -9.5809e-04,  1.3975e-02,  ..., -1.7036e-02,
          1.8152e-02, -7.8803e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  6.7222e-05,  ...,  0.0000e+00,
          2.2128e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.7222e-05,  ...,  0.0000e+00,
          2.2128e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  6.7222e-05,  ...,  0.0000e+00,
          2.2128e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(267.2375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0094, device='cuda:0')



h[100].sum tensor(67.5680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3003, device='cuda:0')



h[200].sum tensor(0.4445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5562, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.3395e-02,  ..., 0.0000e+00, 3.0235e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.0885e-02,  ..., 0.0000e+00, 6.6070e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.3447e-02,  ..., 0.0000e+00, 3.0301e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.7104e-04,  ..., 0.0000e+00, 8.9221e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.7112e-04,  ..., 0.0000e+00, 8.9247e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.7114e-04,  ..., 0.0000e+00, 8.9253e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34698.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1293, 0.1687, 0.0007,  ..., 0.0000, 0.1096, 0.0005],
        [0.1844, 0.2394, 0.0000,  ..., 0.0000, 0.1591, 0.0000],
        [0.1413, 0.1835, 0.0000,  ..., 0.0000, 0.1197, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0135,  ..., 0.0000, 0.0000, 0.0204],
        [0.0000, 0.0008, 0.0135,  ..., 0.0000, 0.0000, 0.0204],
        [0.0000, 0.0008, 0.0135,  ..., 0.0000, 0.0000, 0.0204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(327141.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3104.2561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(198.1789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5684.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-677.3390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.6699, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1209],
        [ 0.1298],
        [ 0.1151],
        ...,
        [-0.3714],
        [-0.3623],
        [-0.3354]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110033.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0042],
        [1.0045],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366921.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0046],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366935.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  7.4511e-05,  ...,  0.0000e+00,
          2.3357e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.4511e-05,  ...,  0.0000e+00,
          2.3357e-05,  0.0000e+00],
        [-5.3281e-03, -5.8049e-04,  8.5811e-03,  ..., -1.0411e-02,
          1.1112e-02, -4.8120e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  7.4511e-05,  ...,  0.0000e+00,
          2.3357e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.4511e-05,  ...,  0.0000e+00,
          2.3357e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.4511e-05,  ...,  0.0000e+00,
          2.3357e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(439.7102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0121, device='cuda:0')



h[100].sum tensor(74.1826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2789, device='cuda:0')



h[200].sum tensor(7.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.8879, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.5313e-02,  ..., 0.0000e+00, 3.2700e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.8350e-03,  ..., 0.0000e+00, 1.1221e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.5089e-02,  ..., 0.0000e+00, 1.9372e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.0048e-04,  ..., 0.0000e+00, 9.4191e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.0057e-04,  ..., 0.0000e+00, 9.4220e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.0059e-04,  ..., 0.0000e+00, 9.4227e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46373.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4407e-01, 1.9101e-01, 4.2256e-04,  ..., 0.0000e+00, 1.2726e-01,
         0.0000e+00],
        [9.0568e-02, 1.2006e-01, 7.9090e-04,  ..., 0.0000e+00, 7.6322e-02,
         1.7209e-04],
        [1.1110e-01, 1.4522e-01, 1.2150e-03,  ..., 0.0000e+00, 9.3143e-02,
         1.7225e-04],
        ...,
        [0.0000e+00, 9.2699e-04, 1.3578e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0301e-02],
        [0.0000e+00, 9.2684e-04, 1.3581e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0307e-02],
        [0.0000e+00, 9.2707e-04, 1.3582e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.0310e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(395289.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5046.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.0426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7033.2510, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-743.2208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.6723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0835],
        [ 0.0996],
        [ 0.1188],
        ...,
        [-0.3780],
        [-0.3765],
        [-0.3760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110901.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0043],
        [1.0046],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366935.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0043],
        [1.0046],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366948.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  7.8480e-05,  ...,  0.0000e+00,
          2.3257e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.8480e-05,  ...,  0.0000e+00,
          2.3257e-05,  0.0000e+00],
        [-5.5559e-03, -6.0049e-04,  8.9619e-03,  ..., -1.0863e-02,
          1.1602e-02, -5.0171e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  7.8480e-05,  ...,  0.0000e+00,
          2.3257e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.8480e-05,  ...,  0.0000e+00,
          2.3257e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  7.8480e-05,  ...,  0.0000e+00,
          2.3257e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(484.1477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5548, device='cuda:0')



h[100].sum tensor(75.4940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1320, device='cuda:0')



h[200].sum tensor(9.1924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9729, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 3.1461e-04,  ..., 0.0000e+00, 9.3233e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.2297e-03,  ..., 0.0000e+00, 1.1713e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.6017e-03,  ..., 0.0000e+00, 9.5910e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.1652e-04,  ..., 0.0000e+00, 9.3800e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.1663e-04,  ..., 0.0000e+00, 9.3830e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.1665e-04,  ..., 0.0000e+00, 9.3837e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50973.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0127, 0.0186, 0.0107,  ..., 0.0000, 0.0074, 0.0156],
        [0.0386, 0.0514, 0.0062,  ..., 0.0000, 0.0270, 0.0085],
        [0.0596, 0.0775, 0.0035,  ..., 0.0000, 0.0448, 0.0047],
        ...,
        [0.0000, 0.0010, 0.0137,  ..., 0.0000, 0.0000, 0.0204],
        [0.0000, 0.0010, 0.0137,  ..., 0.0000, 0.0000, 0.0204],
        [0.0000, 0.0010, 0.0137,  ..., 0.0000, 0.0000, 0.0204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431985.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6162.3721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(257.5835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7984.8506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-773.6312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.7545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0823],
        [ 0.0367],
        [ 0.1247],
        ...,
        [-0.3790],
        [-0.3776],
        [-0.3771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100446.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0043],
        [1.0046],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366948.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0047],
        ...,
        [1.0021],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366961.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 7.2274e-05,  ..., 0.0000e+00, 2.8822e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.2274e-05,  ..., 0.0000e+00, 2.8822e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.2274e-05,  ..., 0.0000e+00, 2.8822e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 7.2274e-05,  ..., 0.0000e+00, 2.8822e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.2274e-05,  ..., 0.0000e+00, 2.8822e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.2274e-05,  ..., 0.0000e+00, 2.8822e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(597.3660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3795, device='cuda:0')



h[100].sum tensor(80.3353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.8001, device='cuda:0')



h[200].sum tensor(13.4528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56007.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0137,  ..., 0.0000, 0.0000, 0.0202],
        [0.0000, 0.0010, 0.0137,  ..., 0.0000, 0.0000, 0.0203],
        [0.0000, 0.0010, 0.0138,  ..., 0.0000, 0.0000, 0.0203],
        ...,
        [0.0000, 0.0011, 0.0138,  ..., 0.0000, 0.0000, 0.0204],
        [0.0000, 0.0011, 0.0138,  ..., 0.0000, 0.0000, 0.0204],
        [0.0000, 0.0011, 0.0138,  ..., 0.0000, 0.0000, 0.0204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450243., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6676.0518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(274.1891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8511.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-807.9014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.8081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5066],
        [-0.4980],
        [-0.4736],
        ...,
        [-0.3784],
        [-0.3783],
        [-0.3782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93183.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0047],
        ...,
        [1.0021],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366961.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0048],
        ...,
        [1.0021],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366974.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5564e-03, -4.8460e-04,  7.3618e-03,  ..., -8.9208e-03,
          9.5586e-03, -4.1134e-03],
        [ 0.0000e+00,  0.0000e+00,  5.5743e-05,  ...,  0.0000e+00,
          3.4675e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5743e-05,  ...,  0.0000e+00,
          3.4675e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  5.5743e-05,  ...,  0.0000e+00,
          3.4675e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5743e-05,  ...,  0.0000e+00,
          3.4675e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.5743e-05,  ...,  0.0000e+00,
          3.4675e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(398.2333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8632, device='cuda:0')



h[100].sum tensor(72.5252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9845, device='cuda:0')



h[200].sum tensor(4.5845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0247,  ..., 0.0000, 0.0320, 0.0000],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0097, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0001, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41363.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1395, 0.1799, 0.0007,  ..., 0.0000, 0.1166, 0.0003],
        [0.0721, 0.0941, 0.0047,  ..., 0.0000, 0.0570, 0.0062],
        [0.0277, 0.0375, 0.0087,  ..., 0.0000, 0.0198, 0.0110],
        ...,
        [0.0000, 0.0010, 0.0140,  ..., 0.0000, 0.0000, 0.0207],
        [0.0000, 0.0010, 0.0140,  ..., 0.0000, 0.0000, 0.0207],
        [0.0000, 0.0010, 0.0140,  ..., 0.0000, 0.0000, 0.0207]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(365156.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4093.6162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.2349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6563.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-723.4494, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.5461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1149],
        [ 0.0596],
        [-0.0268],
        ...,
        [-0.3901],
        [-0.3885],
        [-0.3881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109140.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0044],
        [1.0048],
        ...,
        [1.0021],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366974.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0045],
        [1.0049],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366987.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6448e-02, -2.7902e-03,  4.2510e-02,  ..., -5.1815e-02,
          5.5405e-02, -2.3873e-02],
        [ 0.0000e+00,  0.0000e+00,  4.0751e-05,  ...,  0.0000e+00,
          4.2125e-05,  0.0000e+00],
        [-1.5675e-02, -1.6536e-03,  2.5210e-02,  ..., -3.0709e-02,
          3.2853e-02, -1.4148e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.0751e-05,  ...,  0.0000e+00,
          4.2125e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.0751e-05,  ...,  0.0000e+00,
          4.2125e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.0751e-05,  ...,  0.0000e+00,
          4.2125e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(456.5772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3419, device='cuda:0')



h[100].sum tensor(75.2703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3553, device='cuda:0')



h[200].sum tensor(6.8821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0750,  ..., 0.0000, 0.0978, 0.0000],
        [0.0000, 0.0000, 0.1093,  ..., 0.0000, 0.1424, 0.0000],
        [0.0000, 0.0000, 0.0208,  ..., 0.0000, 0.0271, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45602.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5429e-01, 5.9306e-01, 0.0000e+00,  ..., 0.0000e+00, 4.1183e-01,
         0.0000e+00],
        [3.6671e-01, 4.7925e-01, 0.0000e+00,  ..., 0.0000e+00, 3.3112e-01,
         0.0000e+00],
        [1.6969e-01, 2.2338e-01, 4.8238e-04,  ..., 0.0000e+00, 1.4972e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 9.3166e-04, 1.4155e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.1244e-02],
        [0.0000e+00, 9.3108e-04, 1.4159e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.1251e-02],
        [0.0000e+00, 9.3103e-04, 1.4161e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.1253e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(399142.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5060.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(229.9385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7120.4199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-751.5469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-147.0421, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0579],
        [ 0.0515],
        [ 0.0274],
        ...,
        [-0.3967],
        [-0.3952],
        [-0.3947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118941.6484, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0045],
        [1.0049],
        ...,
        [1.0021],
        [1.0016],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366987.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0045],
        [1.0049],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367000.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  2.8042e-05,  ...,  0.0000e+00,
          5.3604e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.8042e-05,  ...,  0.0000e+00,
          5.3604e-05,  0.0000e+00],
        [-7.4047e-03, -7.7486e-04,  1.1935e-02,  ..., -1.4516e-02,
          1.5577e-02, -6.6828e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.8042e-05,  ...,  0.0000e+00,
          5.3604e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.8042e-05,  ...,  0.0000e+00,
          5.3604e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.8042e-05,  ...,  0.0000e+00,
          5.3604e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(540.4642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.9222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9650, device='cuda:0')



h[100].sum tensor(78.9846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3588, device='cuda:0')



h[200].sum tensor(10.1327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0121,  ..., 0.0000, 0.0158, 0.0000],
        [0.0000, 0.0000, 0.0417,  ..., 0.0000, 0.0544, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53075.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0519, 0.0692, 0.0057,  ..., 0.0000, 0.0418, 0.0085],
        [0.1566, 0.2049, 0.0028,  ..., 0.0000, 0.1358, 0.0043],
        [0.3359, 0.4381, 0.0000,  ..., 0.0000, 0.3013, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0142,  ..., 0.0000, 0.0000, 0.0217],
        [0.0000, 0.0009, 0.0142,  ..., 0.0000, 0.0000, 0.0217],
        [0.0000, 0.0009, 0.0142,  ..., 0.0000, 0.0000, 0.0217]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(445355.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6364.3760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.8032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8125.4189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-800.7185, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.0867, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0097],
        [ 0.0179],
        [ 0.0086],
        ...,
        [-0.4027],
        [-0.4011],
        [-0.4006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112639.1172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0045],
        [1.0049],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367000.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0045],
        [1.0049],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367013.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 1.8323e-05,  ..., 0.0000e+00, 6.7090e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8323e-05,  ..., 0.0000e+00, 6.7090e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8323e-05,  ..., 0.0000e+00, 6.7090e-05,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.8323e-05,  ..., 0.0000e+00, 6.7090e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8323e-05,  ..., 0.0000e+00, 6.7090e-05,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.8323e-05,  ..., 0.0000e+00, 6.7090e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(318.8327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6320, device='cuda:0')



h[100].sum tensor(69.7154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0916, device='cuda:0')



h[200].sum tensor(0.6552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 7.3462e-05,  ..., 0.0000e+00, 2.6899e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.3547e-05,  ..., 0.0000e+00, 2.6930e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.5978e-02,  ..., 0.0000e+00, 3.4041e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 7.3948e-05,  ..., 0.0000e+00, 2.7077e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.3974e-05,  ..., 0.0000e+00, 2.7086e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.3981e-05,  ..., 0.0000e+00, 2.7089e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34619.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0073, 0.0109, 0.0128,  ..., 0.0000, 0.0050, 0.0193],
        [0.0323, 0.0440, 0.0086,  ..., 0.0000, 0.0264, 0.0110],
        [0.1126, 0.1493, 0.0032,  ..., 0.0000, 0.0968, 0.0031],
        ...,
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0220],
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0220],
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(333366.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3026.5698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.4863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5650.3174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-688.4315, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.9976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2948],
        [-0.1385],
        [ 0.0021],
        ...,
        [-0.3468],
        [-0.3895],
        [-0.4011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127142.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0045],
        [1.0049],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367013.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(527.8656, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0050],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367026.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00],
        [-4.4813e-03, -4.6134e-04,  7.2406e-03,  ..., -8.7969e-03,
          9.5040e-03, -4.0433e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(345.4906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5626, device='cuda:0')



h[100].sum tensor(70.4697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6062, device='cuda:0')



h[200].sum tensor(1.6945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 5.3403e-05,  ..., 0.0000e+00, 3.2732e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.3091e-03,  ..., 0.0000e+00, 9.7870e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.9838e-03,  ..., 0.0000e+00, 8.0595e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.3761e-05,  ..., 0.0000e+00, 3.2951e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3780e-05,  ..., 0.0000e+00, 3.2963e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3785e-05,  ..., 0.0000e+00, 3.2966e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35964.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0085, 0.0128, 0.0123,  ..., 0.0000, 0.0038, 0.0192],
        [0.0284, 0.0386, 0.0085,  ..., 0.0000, 0.0174, 0.0125],
        [0.0392, 0.0517, 0.0064,  ..., 0.0000, 0.0260, 0.0092],
        ...,
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0224],
        [0.0002, 0.0022, 0.0140,  ..., 0.0000, 0.0000, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(342081.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3283.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(189.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5969.1724, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-699.7746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.1896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2315],
        [-0.1072],
        [ 0.0044],
        ...,
        [-0.4008],
        [-0.3818],
        [-0.3569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122990.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0050],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367026.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0050],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367026.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8603e-03, -5.0036e-04,  7.8518e-03,  ..., -9.5409e-03,
          1.0301e-02, -4.3853e-03],
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00],
        [-4.8603e-03, -5.0036e-04,  7.8518e-03,  ..., -9.5409e-03,
          1.0301e-02, -4.3853e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.3319e-05,  ...,  0.0000e+00,
          8.1633e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(609.3993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.0541, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3493, device='cuda:0')



h[100].sum tensor(81.3681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.2304, device='cuda:0')



h[200].sum tensor(12.6129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.4228e-02,  ..., 0.0000e+00, 3.1844e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.0119e-02,  ..., 0.0000e+00, 5.2562e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.2235e-02,  ..., 0.0000e+00, 1.6210e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.3761e-05,  ..., 0.0000e+00, 3.2951e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3780e-05,  ..., 0.0000e+00, 3.2963e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3785e-05,  ..., 0.0000e+00, 3.2966e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56546.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1535, 0.1963, 0.0000,  ..., 0.0000, 0.1267, 0.0000],
        [0.1532, 0.1952, 0.0000,  ..., 0.0000, 0.1254, 0.0000],
        [0.0917, 0.1179, 0.0020,  ..., 0.0000, 0.0720, 0.0025],
        ...,
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0224],
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466164., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7057.5635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(273.1210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9060.6396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-832.1311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.1028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1604],
        [ 0.1494],
        [ 0.1164],
        ...,
        [-0.4111],
        [-0.4094],
        [-0.4089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90148.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0046],
        [1.0050],
        ...,
        [1.0022],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367026.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0046],
        [1.0051],
        ...,
        [1.0021],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367039.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.1339e-07,  ...,  0.0000e+00,
          9.9518e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1339e-07,  ...,  0.0000e+00,
          9.9518e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1339e-07,  ...,  0.0000e+00,
          9.9518e-05,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.1339e-07,  ...,  0.0000e+00,
          9.9518e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1339e-07,  ...,  0.0000e+00,
          9.9518e-05,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1339e-07,  ...,  0.0000e+00,
          9.9518e-05,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(517.5292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7743, device='cuda:0')



h[100].sum tensor(77.7117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1474, device='cuda:0')



h[200].sum tensor(8.5006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0091, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0004, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47239.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0149, 0.0210, 0.0107,  ..., 0.0000, 0.0044, 0.0176],
        [0.0203, 0.0284, 0.0099,  ..., 0.0000, 0.0104, 0.0157],
        [0.0470, 0.0623, 0.0064,  ..., 0.0000, 0.0337, 0.0092],
        ...,
        [0.0024, 0.0045, 0.0137,  ..., 0.0000, 0.0003, 0.0219],
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0228],
        [0.0000, 0.0010, 0.0142,  ..., 0.0000, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407274.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5189.5796, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(232.9443, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7442.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-773.4027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.2689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0234],
        [ 0.0330],
        [ 0.0538],
        ...,
        [-0.3680],
        [-0.3993],
        [-0.4120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118261.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0046],
        [1.0051],
        ...,
        [1.0021],
        [1.0016],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367039.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0046],
        [1.0051],
        ...,
        [1.0021],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367051.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -3.1181e-05,  ...,  0.0000e+00,
          1.3191e-04,  0.0000e+00],
        [-6.7989e-03, -6.8850e-04,  1.0964e-02,  ..., -1.3364e-02,
          1.4471e-02, -6.1327e-03],
        [-5.8396e-03, -5.9134e-04,  9.4123e-03,  ..., -1.1478e-02,
          1.2447e-02, -5.2673e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.1181e-05,  ...,  0.0000e+00,
          1.3191e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.1181e-05,  ...,  0.0000e+00,
          1.3191e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.1181e-05,  ...,  0.0000e+00,
          1.3191e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(334.0558, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.7085, device='cuda:0')



h[100].sum tensor(70.9122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5809, device='cuda:0')



h[200].sum tensor(0.4925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6413, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0110,  ..., 0.0000, 0.0149, 0.0000],
        [0.0000, 0.0000, 0.0262,  ..., 0.0000, 0.0347, 0.0000],
        [0.0000, 0.0000, 0.0648,  ..., 0.0000, 0.0852, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33902.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1167, 0.1527, 0.0000,  ..., 0.0000, 0.0976, 0.0000],
        [0.1726, 0.2239, 0.0000,  ..., 0.0000, 0.1475, 0.0000],
        [0.2711, 0.3500, 0.0000,  ..., 0.0000, 0.2363, 0.0000],
        ...,
        [0.0000, 0.0011, 0.0142,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0011, 0.0142,  ..., 0.0000, 0.0000, 0.0231],
        [0.0000, 0.0011, 0.0142,  ..., 0.0000, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(332696.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2879.2998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(173.0614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5716.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-692.5035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.9056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0701],
        [ 0.0753],
        [ 0.0778],
        ...,
        [-0.4235],
        [-0.4212],
        [-0.4168]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132464.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0046],
        [1.0051],
        ...,
        [1.0021],
        [1.0015],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367051.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0047],
        [1.0052],
        ...,
        [1.0021],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367063.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.7015e-03, -8.7389e-04,  1.4035e-02,  ..., -1.7115e-02,
          1.8543e-02, -7.8477e-03],
        [-1.4771e-02, -1.4834e-03,  2.3864e-02,  ..., -2.9052e-02,
          3.1362e-02, -1.3321e-02],
        [-1.5376e-02, -1.5442e-03,  2.4844e-02,  ..., -3.0244e-02,
          3.2641e-02, -1.3867e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.5393e-05,  ...,  0.0000e+00,
          1.6407e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.5393e-05,  ...,  0.0000e+00,
          1.6407e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.5393e-05,  ...,  0.0000e+00,
          1.6407e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(427.6737, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9284, device='cuda:0')



h[100].sum tensor(74.8447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9146, device='cuda:0')



h[200].sum tensor(3.9298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6092, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0743,  ..., 0.0000, 0.0979, 0.0000],
        [0.0000, 0.0000, 0.0992,  ..., 0.0000, 0.1303, 0.0000],
        [0.0000, 0.0000, 0.0877,  ..., 0.0000, 0.1154, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0007, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40505.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4199, 0.5426, 0.0000,  ..., 0.0000, 0.3721, 0.0000],
        [0.4461, 0.5779, 0.0000,  ..., 0.0000, 0.3978, 0.0000],
        [0.3761, 0.4877, 0.0000,  ..., 0.0000, 0.3341, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0232],
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0232],
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(380392.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4266.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.4574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6797.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-733.3893, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.5182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0785],
        [ 0.0716],
        [ 0.0632],
        ...,
        [-0.4270],
        [-0.4254],
        [-0.4249]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128814.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0047],
        [1.0052],
        ...,
        [1.0021],
        [1.0015],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367063.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0047],
        [1.0053],
        ...,
        [1.0020],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367076.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1224e-02, -1.1179e-03,  1.8129e-02,  ..., -2.2092e-02,
          2.3939e-02, -1.0121e-02],
        [-6.5896e-03, -6.5630e-04,  1.0613e-02,  ..., -1.2970e-02,
          1.4134e-02, -5.9421e-03],
        [-2.3318e-02, -2.3224e-03,  3.7741e-02,  ..., -4.5895e-02,
          4.9522e-02, -2.1027e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.2277e-05,  ...,  0.0000e+00,
          1.9495e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.2277e-05,  ...,  0.0000e+00,
          1.9495e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.2277e-05,  ...,  0.0000e+00,
          1.9495e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(458.4260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8356, device='cuda:0')



h[100].sum tensor(75.7611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4162, device='cuda:0')



h[200].sum tensor(4.9195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2472, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0535,  ..., 0.0000, 0.0710, 0.0000],
        [0.0000, 0.0000, 0.0945,  ..., 0.0000, 0.1245, 0.0000],
        [0.0000, 0.0000, 0.0416,  ..., 0.0000, 0.0553, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0008, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40858.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2180, 0.2832, 0.0000,  ..., 0.0000, 0.1884, 0.0000],
        [0.3027, 0.3936, 0.0000,  ..., 0.0000, 0.2670, 0.0000],
        [0.2439, 0.3172, 0.0000,  ..., 0.0000, 0.2128, 0.0000],
        ...,
        [0.0000, 0.0015, 0.0140,  ..., 0.0000, 0.0000, 0.0232],
        [0.0000, 0.0015, 0.0140,  ..., 0.0000, 0.0000, 0.0233],
        [0.0000, 0.0015, 0.0140,  ..., 0.0000, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(374530.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4110.3569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.5492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6943.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-737.7936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.5169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0013],
        [ 0.0263],
        [ 0.0451],
        ...,
        [-0.4296],
        [-0.4279],
        [-0.4275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-118585.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0047],
        [1.0053],
        ...,
        [1.0020],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367076.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0048],
        [1.0053],
        ...,
        [1.0020],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367089.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -7.7936e-05,  ...,  0.0000e+00,
          2.2544e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7936e-05,  ...,  0.0000e+00,
          2.2544e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7936e-05,  ...,  0.0000e+00,
          2.2544e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.7936e-05,  ...,  0.0000e+00,
          2.2544e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7936e-05,  ...,  0.0000e+00,
          2.2544e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7936e-05,  ...,  0.0000e+00,
          2.2544e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(436.1917, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1846, device='cuda:0')



h[100].sum tensor(73.9469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5032, device='cuda:0')



h[200].sum tensor(3.8735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0861, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0120, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0009, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38573.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0553, 0.0726, 0.0049,  ..., 0.0000, 0.0392, 0.0080],
        [0.0196, 0.0278, 0.0099,  ..., 0.0000, 0.0120, 0.0165],
        [0.0032, 0.0072, 0.0129,  ..., 0.0000, 0.0000, 0.0217],
        ...,
        [0.0000, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0232],
        [0.0000, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0232],
        [0.0000, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359751., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3668.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(186.5146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6757.6318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-722.2262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.6792, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0608],
        [-0.0113],
        [-0.0711],
        ...,
        [-0.4306],
        [-0.4291],
        [-0.4285]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115687.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0048],
        [1.0053],
        ...,
        [1.0020],
        [1.0014],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367089.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0049],
        [1.0054],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367102.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.6489e-03, -3.5739e-04,  5.8629e-03,  ..., -7.1913e-03,
          7.9896e-03, -3.2895e-03],
        [-3.6489e-03, -3.5739e-04,  5.8629e-03,  ..., -7.1913e-03,
          7.9896e-03, -3.2895e-03],
        [ 0.0000e+00,  0.0000e+00, -7.2059e-05,  ...,  0.0000e+00,
          2.4745e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.2059e-05,  ...,  0.0000e+00,
          2.4745e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.2059e-05,  ...,  0.0000e+00,
          2.4745e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.2059e-05,  ...,  0.0000e+00,
          2.4745e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(780.3870, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8884, device='cuda:0')



h[100].sum tensor(86.2951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.7405, device='cuda:0')



h[200].sum tensor(17.6036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0376,  ..., 0.0000, 0.0504, 0.0000],
        [0.0000, 0.0000, 0.0222,  ..., 0.0000, 0.0302, 0.0000],
        [0.0000, 0.0000, 0.0445,  ..., 0.0000, 0.0595, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64254.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2045, 0.2599, 0.0000,  ..., 0.0000, 0.1676, 0.0000],
        [0.1843, 0.2344, 0.0000,  ..., 0.0000, 0.1499, 0.0000],
        [0.2161, 0.2757, 0.0000,  ..., 0.0000, 0.1792, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0137,  ..., 0.0000, 0.0000, 0.0234],
        [0.0000, 0.0018, 0.0137,  ..., 0.0000, 0.0000, 0.0234],
        [0.0000, 0.0018, 0.0137,  ..., 0.0000, 0.0000, 0.0234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525178.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8460.4941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.5626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10013.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-871.2102, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.2476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1758],
        [ 0.1740],
        [ 0.1634],
        ...,
        [-0.4315],
        [-0.4299],
        [-0.4294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117291.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0049],
        [1.0054],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367102.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0049],
        [1.0055],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367115.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -6.5331e-05,  ...,  0.0000e+00,
          2.5884e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.5331e-05,  ...,  0.0000e+00,
          2.5884e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.5331e-05,  ...,  0.0000e+00,
          2.5884e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -6.5331e-05,  ...,  0.0000e+00,
          2.5884e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.5331e-05,  ...,  0.0000e+00,
          2.5884e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.5331e-05,  ...,  0.0000e+00,
          2.5884e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(561.6274, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9727, device='cuda:0')



h[100].sum tensor(76.6327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7041, device='cuda:0')



h[200].sum tensor(8.9803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1569, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0264,  ..., 0.0000, 0.0355, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46979.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0261, 0.0364, 0.0088,  ..., 0.0000, 0.0143, 0.0148],
        [0.0571, 0.0762, 0.0053,  ..., 0.0000, 0.0443, 0.0091],
        [0.1442, 0.1885, 0.0020,  ..., 0.0000, 0.1206, 0.0035],
        ...,
        [0.0000, 0.0018, 0.0137,  ..., 0.0000, 0.0000, 0.0237],
        [0.0000, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0238],
        [0.0000, 0.0018, 0.0138,  ..., 0.0000, 0.0000, 0.0238]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410126.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5161.0029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.3497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8004.1138, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-773.9582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.3766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1152],
        [ 0.1168],
        [ 0.1260],
        ...,
        [-0.3884],
        [-0.4018],
        [-0.4146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107074.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0049],
        [1.0055],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367115.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0056],
        ...,
        [1.0020],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367128.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -6.1322e-05,  ...,  0.0000e+00,
          2.6125e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.1322e-05,  ...,  0.0000e+00,
          2.6125e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.1322e-05,  ...,  0.0000e+00,
          2.6125e-04,  0.0000e+00],
        ...,
        [-9.8298e-03, -9.4665e-04,  1.5975e-02,  ..., -1.9398e-02,
          2.1178e-02, -8.8590e-03],
        [ 0.0000e+00,  0.0000e+00, -6.1322e-05,  ...,  0.0000e+00,
          2.6125e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.1322e-05,  ...,  0.0000e+00,
          2.6125e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(661.9762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5091, device='cuda:0')



h[100].sum tensor(80.1603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2127, device='cuda:0')



h[200].sum tensor(13.0142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0260,  ..., 0.0000, 0.0352, 0.0000],
        [0.0000, 0.0000, 0.0227,  ..., 0.0000, 0.0308, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56221.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0134, 0.0120,  ..., 0.0000, 0.0046, 0.0212],
        [0.0000, 0.0016, 0.0137,  ..., 0.0000, 0.0000, 0.0242],
        [0.0000, 0.0015, 0.0138,  ..., 0.0000, 0.0000, 0.0242],
        ...,
        [0.1735, 0.2217, 0.0000,  ..., 0.0000, 0.1412, 0.0000],
        [0.1259, 0.1624, 0.0019,  ..., 0.0000, 0.1006, 0.0035],
        [0.0412, 0.0553, 0.0067,  ..., 0.0000, 0.0307, 0.0118]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478264.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7138.4990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.2288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9394.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-833.2554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.5665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0735],
        [-0.1846],
        [-0.2646],
        ...,
        [ 0.1116],
        [ 0.0525],
        [-0.0808]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103840.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0050],
        [1.0056],
        ...,
        [1.0020],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367128.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(477.6159, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0051],
        [1.0057],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367141.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.6337e-03, -3.4699e-04,  5.8618e-03,  ..., -7.1757e-03,
          8.0120e-03, -3.2744e-03],
        [-4.6601e-03, -4.4500e-04,  7.5385e-03,  ..., -9.2026e-03,
          1.0199e-02, -4.1993e-03],
        [ 0.0000e+00,  0.0000e+00, -7.4102e-05,  ...,  0.0000e+00,
          2.6911e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.4102e-05,  ...,  0.0000e+00,
          2.6911e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.4102e-05,  ...,  0.0000e+00,
          2.6911e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.4102e-05,  ...,  0.0000e+00,
          2.6911e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(562.3894, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1013, device='cuda:0')



h[100].sum tensor(76.6197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7752, device='cuda:0')



h[200].sum tensor(9.0083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0587,  ..., 0.0000, 0.0781, 0.0000],
        [0.0000, 0.0000, 0.0241,  ..., 0.0000, 0.0327, 0.0000],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0111, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46501.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2391, 0.3064, 0.0000,  ..., 0.0000, 0.2008, 0.0000],
        [0.1587, 0.2045, 0.0007,  ..., 0.0000, 0.1299, 0.0014],
        [0.0798, 0.1046, 0.0047,  ..., 0.0000, 0.0612, 0.0085],
        ...,
        [0.0000, 0.0015, 0.0139,  ..., 0.0000, 0.0000, 0.0250],
        [0.0000, 0.0015, 0.0139,  ..., 0.0000, 0.0000, 0.0250],
        [0.0000, 0.0015, 0.0139,  ..., 0.0000, 0.0000, 0.0250]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410892.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5115.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(227.6131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8001.7769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-780.0039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.2167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1372],
        [ 0.1131],
        [ 0.0596],
        ...,
        [-0.4483],
        [-0.4467],
        [-0.4461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111496.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0051],
        [1.0057],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367141.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0052],
        [1.0057],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367153.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -8.1786e-05,  ...,  0.0000e+00,
          2.7739e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1786e-05,  ...,  0.0000e+00,
          2.7739e-04,  0.0000e+00],
        [-3.6354e-03, -3.4420e-04,  5.8648e-03,  ..., -7.1837e-03,
          8.0345e-03, -3.2754e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.1786e-05,  ...,  0.0000e+00,
          2.7739e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1786e-05,  ...,  0.0000e+00,
          2.7739e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1786e-05,  ...,  0.0000e+00,
          2.7739e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(705.2119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0168, device='cuda:0')



h[100].sum tensor(82.8034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.0465, device='cuda:0')



h[200].sum tensor(14.0661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.4078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0048,  ..., 0.0000, 0.0074, 0.0000],
        [0.0000, 0.0000, 0.0248,  ..., 0.0000, 0.0338, 0.0000],
        [0.0000, 0.0000, 0.0431,  ..., 0.0000, 0.0577, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57598.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0737, 0.0951, 0.0044,  ..., 0.0000, 0.0535, 0.0087],
        [0.1527, 0.1942, 0.0005,  ..., 0.0000, 0.1223, 0.0016],
        [0.2156, 0.2745, 0.0000,  ..., 0.0000, 0.1787, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0138,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0013, 0.0138,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0013, 0.0138,  ..., 0.0000, 0.0000, 0.0254]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(484405.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7297.4155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.0501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9620.6836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-856.1359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.2691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0291],
        [ 0.0923],
        [ 0.1225],
        ...,
        [-0.4576],
        [-0.4560],
        [-0.4555]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-107220.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0052],
        [1.0057],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367153.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0058],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367165.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -8.1676e-05,  ...,  0.0000e+00,
          2.8400e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1676e-05,  ...,  0.0000e+00,
          2.8400e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1676e-05,  ...,  0.0000e+00,
          2.8400e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.1676e-05,  ...,  0.0000e+00,
          2.8400e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1676e-05,  ...,  0.0000e+00,
          2.8400e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1676e-05,  ...,  0.0000e+00,
          2.8400e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(492.1208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2912, device='cuda:0')



h[100].sum tensor(74.9489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1152, device='cuda:0')



h[200].sum tensor(5.3164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0011, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41231.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0137,  ..., 0.0000, 0.0000, 0.0256],
        [0.0000, 0.0011, 0.0137,  ..., 0.0000, 0.0000, 0.0256],
        [0.0000, 0.0010, 0.0137,  ..., 0.0000, 0.0000, 0.0256],
        ...,
        [0.0000, 0.0011, 0.0138,  ..., 0.0000, 0.0000, 0.0258],
        [0.0000, 0.0011, 0.0138,  ..., 0.0000, 0.0000, 0.0258],
        [0.0000, 0.0011, 0.0138,  ..., 0.0000, 0.0000, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(383525.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4129.4072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.3658, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7099.8931, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-752.9741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.0303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5583],
        [-0.4962],
        [-0.3969],
        ...,
        [-0.4644],
        [-0.4628],
        [-0.4623]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135459.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0053],
        [1.0058],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367165.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0059],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367177.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -8.2444e-05,  ...,  0.0000e+00,
          2.9752e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.2444e-05,  ...,  0.0000e+00,
          2.9752e-04,  0.0000e+00],
        [-4.4031e-03, -4.0980e-04,  7.1402e-03,  ..., -8.7125e-03,
          9.7193e-03, -3.9660e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.2444e-05,  ...,  0.0000e+00,
          2.9752e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.2444e-05,  ...,  0.0000e+00,
          2.9752e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.2444e-05,  ...,  0.0000e+00,
          2.9752e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(593.1856, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5492, device='cuda:0')



h[100].sum tensor(78.7090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4699, device='cuda:0')



h[200].sum tensor(8.7486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8591, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0072,  ..., 0.0000, 0.0107, 0.0000],
        [0.0000, 0.0000, 0.0107,  ..., 0.0000, 0.0153, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0012, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47409.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0154, 0.0213, 0.0107,  ..., 0.0000, 0.0085, 0.0207],
        [0.0536, 0.0696, 0.0055,  ..., 0.0000, 0.0373, 0.0111],
        [0.0900, 0.1144, 0.0022,  ..., 0.0000, 0.0677, 0.0050],
        ...,
        [0.0000, 0.0011, 0.0137,  ..., 0.0000, 0.0000, 0.0259],
        [0.0000, 0.0011, 0.0137,  ..., 0.0000, 0.0000, 0.0259],
        [0.0000, 0.0011, 0.0137,  ..., 0.0000, 0.0000, 0.0259]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422118.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5384.9458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.2608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8368.1318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-798.6687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.1196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2676],
        [-0.1071],
        [ 0.0164],
        ...,
        [-0.4686],
        [-0.4669],
        [-0.4664]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113426.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0054],
        [1.0059],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367177.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0060],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367189.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -7.7459e-05,  ...,  0.0000e+00,
          3.1600e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7459e-05,  ...,  0.0000e+00,
          3.1600e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7459e-05,  ...,  0.0000e+00,
          3.1600e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.7459e-05,  ...,  0.0000e+00,
          3.1600e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7459e-05,  ...,  0.0000e+00,
          3.1600e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7459e-05,  ...,  0.0000e+00,
          3.1600e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(569.0303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.8101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8146, device='cuda:0')



h[100].sum tensor(76.9520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5107, device='cuda:0')



h[200].sum tensor(7.5335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.6391, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0190, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43254.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0063, 0.0097, 0.0123,  ..., 0.0000, 0.0021, 0.0234],
        [0.0273, 0.0361, 0.0084,  ..., 0.0000, 0.0176, 0.0169],
        [0.0811, 0.1025, 0.0034,  ..., 0.0000, 0.0591, 0.0070],
        ...,
        [0.0000, 0.0012, 0.0137,  ..., 0.0000, 0.0000, 0.0258],
        [0.0000, 0.0012, 0.0137,  ..., 0.0000, 0.0000, 0.0258],
        [0.0000, 0.0012, 0.0137,  ..., 0.0000, 0.0000, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393106.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4502.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(198.2865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7689.8662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-766.9277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.9859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3056],
        [-0.1366],
        [ 0.0032],
        ...,
        [-0.4580],
        [-0.4535],
        [-0.4517]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-121120.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0055],
        [1.0060],
        ...,
        [1.0019],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367189.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0061],
        ...,
        [1.0020],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367202.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3480e-02, -2.1479e-03,  3.8559e-02,  ..., -4.6522e-02,
          5.0720e-02, -2.1143e-02],
        [-5.8817e-03, -5.3804e-04,  9.6066e-03,  ..., -1.1654e-02,
          1.2954e-02, -5.2962e-03],
        [-1.9083e-02, -1.7456e-03,  3.1325e-02,  ..., -3.7810e-02,
          4.1284e-02, -1.7183e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -6.9932e-05,  ...,  0.0000e+00,
          3.3167e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.9932e-05,  ...,  0.0000e+00,
          3.3167e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.9932e-05,  ...,  0.0000e+00,
          3.3167e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(848.8842, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2505, device='cuda:0')



h[100].sum tensor(86.2388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.3878, device='cuda:0')



h[200].sum tensor(17.8783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.3854, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0692,  ..., 0.0000, 0.0920, 0.0000],
        [0.0000, 0.0000, 0.1560,  ..., 0.0000, 0.2052, 0.0000],
        [0.0000, 0.0000, 0.0989,  ..., 0.0000, 0.1308, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0013, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63227.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4611, 0.5879, 0.0000,  ..., 0.0000, 0.4038, 0.0000],
        [0.6222, 0.7969, 0.0000,  ..., 0.0000, 0.5536, 0.0000],
        [0.5705, 0.7299, 0.0000,  ..., 0.0000, 0.5057, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0136,  ..., 0.0000, 0.0000, 0.0255],
        [0.0000, 0.0013, 0.0136,  ..., 0.0000, 0.0000, 0.0255],
        [0.0000, 0.0013, 0.0136,  ..., 0.0000, 0.0000, 0.0255]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517328.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8353.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.1949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10622.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-886.6769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.6802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0446],
        [ 0.0257],
        [ 0.0092],
        ...,
        [-0.4596],
        [-0.4523],
        [-0.4459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101022.9141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0061],
        ...,
        [1.0020],
        [1.0013],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367202.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0061],
        ...,
        [1.0020],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367214.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        [-1.3550e-02, -1.2288e-03,  2.2266e-02,  ..., -2.6865e-02,
          2.9466e-02, -1.2199e-02],
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(544.6191, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0179, device='cuda:0')



h[100].sum tensor(73.7187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5171, device='cuda:0')



h[200].sum tensor(6.3972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0223,  ..., 0.0000, 0.0306, 0.0000],
        [0.0000, 0.0000, 0.0183,  ..., 0.0000, 0.0253, 0.0000],
        [0.0000, 0.0000, 0.0813,  ..., 0.0000, 0.1078, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42016.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.3998e-02, 1.2087e-01, 3.2098e-03,  ..., 0.0000e+00, 7.3055e-02,
         6.1188e-03],
        [1.2066e-01, 1.5448e-01, 6.3340e-04,  ..., 0.0000e+00, 9.6662e-02,
         1.5810e-03],
        [2.0691e-01, 2.6377e-01, 0.0000e+00,  ..., 0.0000e+00, 1.7375e-01,
         0.0000e+00],
        ...,
        [1.3709e-04, 1.4144e-03, 1.3542e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5480e-02],
        [1.3528e-04, 1.4125e-03, 1.3545e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5489e-02],
        [1.3619e-04, 1.4122e-03, 1.3547e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.5491e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389517.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4439.8433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(196.8972, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7793.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-750.6434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.2582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0975],
        [-0.0143],
        [ 0.0247],
        ...,
        [-0.4661],
        [-0.4644],
        [-0.4639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-117298.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0061],
        ...,
        [1.0020],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367214.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0061],
        ...,
        [1.0020],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367214.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.9828e-05,  ...,  0.0000e+00,
          3.4538e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(558.0626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3455, device='cuda:0')



h[100].sum tensor(74.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6982, device='cuda:0')



h[200].sum tensor(6.9000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41948.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0002, 0.0014, 0.0134,  ..., 0.0000, 0.0000, 0.0253],
        [0.0008, 0.0024, 0.0132,  ..., 0.0000, 0.0000, 0.0251],
        [0.0029, 0.0048, 0.0127,  ..., 0.0000, 0.0000, 0.0245],
        ...,
        [0.0001, 0.0014, 0.0135,  ..., 0.0000, 0.0000, 0.0255],
        [0.0001, 0.0014, 0.0135,  ..., 0.0000, 0.0000, 0.0255],
        [0.0001, 0.0014, 0.0135,  ..., 0.0000, 0.0000, 0.0255]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386752.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4388.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(197.2041, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7874.4253, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-753.5560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.9053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3348],
        [-0.3691],
        [-0.3659],
        ...,
        [-0.4449],
        [-0.4602],
        [-0.4632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111991.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0056],
        [1.0061],
        ...,
        [1.0020],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367214.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0057],
        [1.0062],
        ...,
        [1.0020],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367226.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -4.8948e-05,  ...,  0.0000e+00,
          3.5557e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.8948e-05,  ...,  0.0000e+00,
          3.5557e-04,  0.0000e+00],
        [-3.6084e-03, -3.2439e-04,  5.9053e-03,  ..., -7.1590e-03,
          8.1218e-03, -3.2483e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.8948e-05,  ...,  0.0000e+00,
          3.5557e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.8948e-05,  ...,  0.0000e+00,
          3.5557e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.8948e-05,  ...,  0.0000e+00,
          3.5557e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(521.0607, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8643, device='cuda:0')



h[100].sum tensor(71.9263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8791, device='cuda:0')



h[200].sum tensor(5.6307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5641, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0108,  ..., 0.0000, 0.0156, 0.0000],
        [0.0000, 0.0000, 0.0153,  ..., 0.0000, 0.0216, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0014, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39019.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0245, 0.0309, 0.0088,  ..., 0.0000, 0.0135, 0.0181],
        [0.0712, 0.0874, 0.0037,  ..., 0.0000, 0.0472, 0.0079],
        [0.1023, 0.1246, 0.0010,  ..., 0.0000, 0.0725, 0.0030],
        ...,
        [0.0003, 0.0015, 0.0135,  ..., 0.0000, 0.0000, 0.0256],
        [0.0003, 0.0015, 0.0136,  ..., 0.0000, 0.0000, 0.0256],
        [0.0003, 0.0015, 0.0136,  ..., 0.0000, 0.0000, 0.0256]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(367131.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3652.1265, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(186.3815, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7093.5322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-723.2142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.1546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0455],
        [ 0.0714],
        [ 0.1415],
        ...,
        [-0.4656],
        [-0.4640],
        [-0.4635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130782.7891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0057],
        [1.0062],
        ...,
        [1.0020],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367226.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0063],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367239.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -3.7616e-05,  ...,  0.0000e+00,
          3.6513e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.7616e-05,  ...,  0.0000e+00,
          3.6513e-04,  0.0000e+00],
        [-4.3399e-03, -3.8675e-04,  7.1347e-03,  ..., -8.6162e-03,
          9.7197e-03, -3.9062e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.7616e-05,  ...,  0.0000e+00,
          3.6513e-04,  0.0000e+00],
        [-1.1625e-02, -1.0359e-03,  1.9174e-02,  ..., -2.3079e-02,
          2.5422e-02, -1.0463e-02],
        [ 0.0000e+00,  0.0000e+00, -3.7616e-05,  ...,  0.0000e+00,
          3.6513e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(664.3649, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3180, device='cuda:0')



h[100].sum tensor(76.4782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.4480, device='cuda:0')



h[200].sum tensor(11.1547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.1030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0015, 0.0000],
        [0.0000, 0.0000, 0.0072,  ..., 0.0000, 0.0109, 0.0000],
        [0.0000, 0.0000, 0.0239,  ..., 0.0000, 0.0328, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0194,  ..., 0.0000, 0.0268, 0.0000],
        [0.0000, 0.0000, 0.0158,  ..., 0.0000, 0.0222, 0.0000],
        [0.0000, 0.0000, 0.0705,  ..., 0.0000, 0.0936, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50357.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0499, 0.0626, 0.0046,  ..., 0.0000, 0.0308, 0.0106],
        [0.0847, 0.1048, 0.0030,  ..., 0.0000, 0.0607, 0.0066],
        [0.1442, 0.1783, 0.0006,  ..., 0.0000, 0.1106, 0.0019],
        ...,
        [0.0833, 0.1058, 0.0037,  ..., 0.0000, 0.0616, 0.0075],
        [0.1072, 0.1356, 0.0010,  ..., 0.0000, 0.0824, 0.0026],
        [0.1835, 0.2316, 0.0000,  ..., 0.0000, 0.1497, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(443670.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6269.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.0642, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9448.0420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-806.3599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.8236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1459],
        [ 0.1635],
        [ 0.1798],
        ...,
        [-0.0753],
        [ 0.0358],
        [ 0.0717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-86455.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0058],
        [1.0063],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367239.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(512.5153, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0059],
        [1.0064],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367250.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -5.1277e-05,  ...,  0.0000e+00,
          3.9094e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.1277e-05,  ...,  0.0000e+00,
          3.9094e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.1277e-05,  ...,  0.0000e+00,
          3.9094e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.1277e-05,  ...,  0.0000e+00,
          3.9094e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.1277e-05,  ...,  0.0000e+00,
          3.9094e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.1277e-05,  ...,  0.0000e+00,
          3.9094e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(560.6230, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0224, device='cuda:0')



h[100].sum tensor(73.6484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0725, device='cuda:0')



h[200].sum tensor(7.7738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0016, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45434.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0063, 0.0093, 0.0124,  ..., 0.0000, 0.0018, 0.0250],
        [0.0003, 0.0018, 0.0136,  ..., 0.0000, 0.0000, 0.0268],
        [0.0002, 0.0014, 0.0137,  ..., 0.0000, 0.0000, 0.0269],
        ...,
        [0.0002, 0.0014, 0.0138,  ..., 0.0000, 0.0000, 0.0271],
        [0.0002, 0.0014, 0.0138,  ..., 0.0000, 0.0000, 0.0271],
        [0.0002, 0.0014, 0.0138,  ..., 0.0000, 0.0000, 0.0271]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419198.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5229.0918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.3154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8259.0908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-771.6499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.6743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3057],
        [-0.4636],
        [-0.5654],
        ...,
        [-0.4801],
        [-0.4785],
        [-0.4779]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-122014.7578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0059],
        [1.0064],
        ...,
        [1.0020],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367250.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0065],
        ...,
        [1.0021],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367260.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -7.8789e-05,  ...,  0.0000e+00,
          4.2282e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.8789e-05,  ...,  0.0000e+00,
          4.2282e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.8789e-05,  ...,  0.0000e+00,
          4.2282e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.8789e-05,  ...,  0.0000e+00,
          4.2282e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.8789e-05,  ...,  0.0000e+00,
          4.2282e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.8789e-05,  ...,  0.0000e+00,
          4.2282e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(488.5841, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0249, device='cuda:0')



h[100].sum tensor(72.3098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4149, device='cuda:0')



h[200].sum tensor(5.4754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9738, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0072,  ..., 0.0000, 0.0111, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0144,  ..., 0.0000, 0.0206, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40731.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0348, 0.0450, 0.0071,  ..., 0.0000, 0.0175, 0.0173],
        [0.0280, 0.0374, 0.0086,  ..., 0.0000, 0.0126, 0.0191],
        [0.0635, 0.0821, 0.0051,  ..., 0.0000, 0.0440, 0.0115],
        ...,
        [0.0000, 0.0015, 0.0139,  ..., 0.0000, 0.0000, 0.0281],
        [0.0000, 0.0016, 0.0139,  ..., 0.0000, 0.0000, 0.0281],
        [0.0000, 0.0016, 0.0139,  ..., 0.0000, 0.0000, 0.0281]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390586.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4253.7676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.7789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7566.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-750.7580, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.9382, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3323],
        [-0.3392],
        [-0.2858],
        ...,
        [-0.4913],
        [-0.4894],
        [-0.4887]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125793.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0060],
        [1.0065],
        ...,
        [1.0021],
        [1.0014],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367260.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0061],
        [1.0066],
        ...,
        [1.0021],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367271.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -9.4834e-05,  ...,  0.0000e+00,
          4.4403e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.4834e-05,  ...,  0.0000e+00,
          4.4403e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.4834e-05,  ...,  0.0000e+00,
          4.4403e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -9.4834e-05,  ...,  0.0000e+00,
          4.4403e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.4834e-05,  ...,  0.0000e+00,
          4.4403e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.4834e-05,  ...,  0.0000e+00,
          4.4403e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(621.8018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2265, device='cuda:0')



h[100].sum tensor(78.1490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8444, device='cuda:0')



h[200].sum tensor(10.6231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.3354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47314.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0046, 0.0134,  ..., 0.0000, 0.0000, 0.0280],
        [0.0032, 0.0064, 0.0130,  ..., 0.0000, 0.0000, 0.0276],
        [0.0148, 0.0202, 0.0107,  ..., 0.0000, 0.0051, 0.0243],
        ...,
        [0.0000, 0.0015, 0.0140,  ..., 0.0000, 0.0000, 0.0290],
        [0.0000, 0.0015, 0.0140,  ..., 0.0000, 0.0000, 0.0290],
        [0.0000, 0.0015, 0.0140,  ..., 0.0000, 0.0000, 0.0290]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422038.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5307.0962, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(221.3210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8582.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-804.5664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.4086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3427],
        [-0.2334],
        [-0.0834],
        ...,
        [-0.5008],
        [-0.4991],
        [-0.4987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104737.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0061],
        [1.0066],
        ...,
        [1.0021],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367271.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0068],
        ...,
        [1.0021],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367282.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(517.1399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8357, device='cuda:0')



h[100].sum tensor(75.8931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4163, device='cuda:0')



h[200].sum tensor(7.0676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0623,  ..., 0.0000, 0.0838, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0152, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41474.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.0221e-01, 3.8732e-01, 0.0000e+00,  ..., 0.0000e+00, 2.5855e-01,
         0.0000e+00],
        [1.8497e-01, 2.3780e-01, 2.1813e-04,  ..., 0.0000e+00, 1.5308e-01,
         1.9340e-03],
        [1.3270e-01, 1.7176e-01, 2.2002e-03,  ..., 0.0000e+00, 1.0679e-01,
         5.4446e-03],
        ...,
        [0.0000e+00, 1.4783e-03, 1.4050e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.9819e-02],
        [0.0000e+00, 1.4773e-03, 1.4055e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.9830e-02],
        [0.0000e+00, 1.4780e-03, 1.4054e-02,  ..., 0.0000e+00, 0.0000e+00,
         2.9832e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390323., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4138.1299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(191.7086, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7454.2290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-766.0623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.8113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0760],
        [ 0.0733],
        [ 0.0671],
        ...,
        [-0.5117],
        [-0.5098],
        [-0.5092]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131640.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0062],
        [1.0068],
        ...,
        [1.0021],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367282.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0063],
        [1.0069],
        ...,
        [1.0021],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367294.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(522.1735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0686, device='cuda:0')



h[100].sum tensor(76.9482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5451, device='cuda:0')



h[200].sum tensor(7.4298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4111, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43137.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0131, 0.0191, 0.0114,  ..., 0.0000, 0.0055, 0.0259],
        [0.0028, 0.0056, 0.0134,  ..., 0.0000, 0.0000, 0.0292],
        [0.0000, 0.0014, 0.0140,  ..., 0.0000, 0.0000, 0.0302],
        ...,
        [0.0000, 0.0014, 0.0141,  ..., 0.0000, 0.0000, 0.0304],
        [0.0000, 0.0014, 0.0141,  ..., 0.0000, 0.0000, 0.0304],
        [0.0000, 0.0014, 0.0141,  ..., 0.0000, 0.0000, 0.0304]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406952.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4601.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.3265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7781.8003, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-777.9659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.8031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3182],
        [-0.4908],
        [-0.6090],
        ...,
        [-0.5184],
        [-0.5166],
        [-0.5160]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129709.5703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0063],
        [1.0069],
        ...,
        [1.0021],
        [1.0014],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367294.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0064],
        [1.0070],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367306.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(537.9561, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6884, device='cuda:0')



h[100].sum tensor(77.7469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8878, device='cuda:0')



h[200].sum tensor(8.2701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42626.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0427, 0.0565, 0.0062,  ..., 0.0000, 0.0263, 0.0175],
        [0.0306, 0.0411, 0.0084,  ..., 0.0000, 0.0176, 0.0212],
        [0.0179, 0.0252, 0.0106,  ..., 0.0000, 0.0078, 0.0250],
        ...,
        [0.0000, 0.0013, 0.0142,  ..., 0.0000, 0.0000, 0.0308],
        [0.0000, 0.0013, 0.0142,  ..., 0.0000, 0.0000, 0.0308],
        [0.0000, 0.0013, 0.0142,  ..., 0.0000, 0.0000, 0.0308]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(396826.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4307.4165, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.3931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7682.3657, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-781.0397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.2161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0428],
        [ 0.0305],
        [ 0.0145],
        ...,
        [-0.5222],
        [-0.5206],
        [-0.5205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127883.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0064],
        [1.0070],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367306.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0064],
        [1.0070],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367306.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(527.3449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1882, device='cuda:0')



h[100].sum tensor(77.3698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6112, device='cuda:0')



h[200].sum tensor(7.8918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4952, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41564.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0013, 0.0140,  ..., 0.0000, 0.0000, 0.0306],
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0306],
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0306],
        ...,
        [0.0000, 0.0013, 0.0142,  ..., 0.0000, 0.0000, 0.0308],
        [0.0000, 0.0013, 0.0142,  ..., 0.0000, 0.0000, 0.0308],
        [0.0000, 0.0013, 0.0142,  ..., 0.0000, 0.0000, 0.0308]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391775.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4054.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(187.2225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7109.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-764.6095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.6133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5005],
        [-0.5254],
        [-0.5373],
        ...,
        [-0.5236],
        [-0.5217],
        [-0.5211]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151957.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0064],
        [1.0070],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367306.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0071],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367318.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(529.3949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1516, device='cuda:0')



h[100].sum tensor(77.0706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5910, device='cuda:0')



h[200].sum tensor(8.2310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4695, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42651.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0062, 0.0099, 0.0128,  ..., 0.0000, 0.0003, 0.0285],
        [0.0286, 0.0387, 0.0089,  ..., 0.0000, 0.0162, 0.0217],
        [0.0556, 0.0729, 0.0048,  ..., 0.0000, 0.0377, 0.0135],
        ...,
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0309],
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0309],
        [0.0000, 0.0013, 0.0141,  ..., 0.0000, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(402842.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4485.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.6463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7727.8311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-778.6583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.2892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0289],
        [ 0.0123],
        [ 0.0522],
        ...,
        [-0.5265],
        [-0.5246],
        [-0.5240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131870.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0065],
        [1.0071],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367318.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0072],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367330.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -8.6361e-05,  ...,  0.0000e+00,
          5.3373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.6361e-05,  ...,  0.0000e+00,
          5.3373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.6361e-05,  ...,  0.0000e+00,
          5.3373e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.6361e-05,  ...,  0.0000e+00,
          5.3373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.6361e-05,  ...,  0.0000e+00,
          5.3373e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.6361e-05,  ...,  0.0000e+00,
          5.3373e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(536.5506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3472, device='cuda:0')



h[100].sum tensor(76.5358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6992, device='cuda:0')



h[200].sum tensor(8.6417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42047.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0040, 0.0064, 0.0129,  ..., 0.0000, 0.0000, 0.0291],
        [0.0036, 0.0060, 0.0130,  ..., 0.0000, 0.0000, 0.0293],
        [0.0070, 0.0104, 0.0125,  ..., 0.0000, 0.0007, 0.0282],
        ...,
        [0.0098, 0.0140, 0.0123,  ..., 0.0000, 0.0028, 0.0275],
        [0.0297, 0.0392, 0.0088,  ..., 0.0000, 0.0163, 0.0214],
        [0.0399, 0.0519, 0.0070,  ..., 0.0000, 0.0225, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(394432.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4231.3442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(192.0621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7442.9424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-769.2143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.6115, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3478],
        [-0.2845],
        [-0.1791],
        ...,
        [-0.3064],
        [-0.1638],
        [-0.0731]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139852.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0066],
        [1.0072],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367330.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0067],
        [1.0073],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367342.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -5.4086e-05,  ...,  0.0000e+00,
          5.3054e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4086e-05,  ...,  0.0000e+00,
          5.3054e-04,  0.0000e+00],
        [-1.0782e-02, -8.8669e-04,  1.7993e-02,  ..., -2.1537e-02,
          2.4090e-02, -9.6923e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.4086e-05,  ...,  0.0000e+00,
          5.3054e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4086e-05,  ...,  0.0000e+00,
          5.3054e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4086e-05,  ...,  0.0000e+00,
          5.3054e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(878.7767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.7710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.6707, device='cuda:0')



h[100].sum tensor(87.5525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.6202, device='cuda:0')



h[200].sum tensor(20.7866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6809, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0329,  ..., 0.0000, 0.0452, 0.0000],
        [0.0000, 0.0000, 0.0329,  ..., 0.0000, 0.0452, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60999.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.5332e-02, 1.0701e-01, 9.1782e-06,  ..., 0.0000e+00, 5.9810e-02,
         4.4218e-03],
        [1.6457e-01, 2.0758e-01, 0.0000e+00,  ..., 0.0000e+00, 1.3051e-01,
         1.5560e-03],
        [2.0788e-01, 2.6248e-01, 0.0000e+00,  ..., 0.0000e+00, 1.6909e-01,
         1.0533e-03],
        ...,
        [3.1803e-04, 1.4166e-03, 1.3895e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0314e-02],
        [3.1835e-04, 1.4156e-03, 1.3901e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0326e-02],
        [3.1992e-04, 1.4174e-03, 1.3901e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0328e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508701.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7786.5728, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(268.6575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10130.1914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-888.9363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.7888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1765],
        [ 0.1726],
        [ 0.1706],
        ...,
        [-0.5239],
        [-0.5220],
        [-0.5213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115403.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0067],
        [1.0073],
        ...,
        [1.0021],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367342.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(509.1992, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0074],
        ...,
        [1.0022],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367354.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.5924e-03, -2.9274e-04,  6.0016e-03,  ..., -7.1803e-03,
          8.3772e-03, -3.2287e-03],
        [-1.5768e-02, -1.2849e-03,  2.6412e-02,  ..., -3.1516e-02,
          3.5017e-02, -1.4171e-02],
        [-2.4469e-02, -1.9939e-03,  4.0998e-02,  ..., -4.8907e-02,
          5.4054e-02, -2.1992e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0578e-05,  ...,  0.0000e+00,
          5.1712e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0578e-05,  ...,  0.0000e+00,
          5.1712e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0578e-05,  ...,  0.0000e+00,
          5.1712e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(711.0737, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8689, device='cuda:0')



h[100].sum tensor(80.9419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3057, device='cuda:0')



h[200].sum tensor(14.8854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.1938, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0868,  ..., 0.0000, 0.1155, 0.0000],
        [0.0000, 0.0000, 0.0974,  ..., 0.0000, 0.1293, 0.0000],
        [0.0000, 0.0000, 0.1293,  ..., 0.0000, 0.1710, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53364.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.2669e-01, 6.6315e-01, 0.0000e+00,  ..., 0.0000e+00, 4.5139e-01,
         0.0000e+00],
        [5.7093e-01, 7.1953e-01, 0.0000e+00,  ..., 0.0000e+00, 4.9135e-01,
         0.0000e+00],
        [6.4214e-01, 8.1060e-01, 0.0000e+00,  ..., 0.0000e+00, 5.5607e-01,
         0.0000e+00],
        ...,
        [6.9474e-04, 1.3530e-03, 1.3665e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0081e-02],
        [6.9658e-04, 1.3565e-03, 1.3671e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0092e-02],
        [6.9600e-04, 1.3560e-03, 1.3673e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0094e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(472290.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6734.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.0044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9478.8242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-844.4293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.5015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1986],
        [ 0.1963],
        [ 0.1893],
        ...,
        [-0.5245],
        [-0.5226],
        [-0.5220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116262.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0068],
        [1.0074],
        ...,
        [1.0022],
        [1.0015],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367354.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0075],
        ...,
        [1.0022],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367366.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.0273e-03, -7.2895e-04,  1.5168e-02,  ..., -1.8056e-02,
          2.0274e-02, -8.1123e-03],
        [ 0.0000e+00,  0.0000e+00,  1.0634e-05,  ...,  0.0000e+00,
          4.9610e-04,  0.0000e+00],
        [-2.4729e-02, -1.9969e-03,  4.1531e-02,  ..., -4.9461e-02,
          5.4675e-02, -2.2222e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.0634e-05,  ...,  0.0000e+00,
          4.9610e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.0634e-05,  ...,  0.0000e+00,
          4.9610e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.0634e-05,  ...,  0.0000e+00,
          4.9610e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(879.1892, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.7609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2737, device='cuda:0')



h[100].sum tensor(86.3403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.8476, device='cuda:0')



h[200].sum tensor(20.6769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.2469e-02,  ..., 0.0000e+00, 1.8208e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.1704e-02,  ..., 0.0000e+00, 1.2160e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.5543e-02,  ..., 0.0000e+00, 1.1356e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.3102e-05,  ..., 0.0000e+00, 2.0107e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.3118e-05,  ..., 0.0000e+00, 2.0115e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.3121e-05,  ..., 0.0000e+00, 2.0116e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57833.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1560, 0.1932, 0.0015,  ..., 0.0000, 0.1214, 0.0058],
        [0.3544, 0.4424, 0.0000,  ..., 0.0000, 0.2972, 0.0000],
        [0.4474, 0.5600, 0.0000,  ..., 0.0000, 0.3803, 0.0000],
        ...,
        [0.0011, 0.0010, 0.0136,  ..., 0.0000, 0.0000, 0.0301],
        [0.0122, 0.0144, 0.0115,  ..., 0.0000, 0.0025, 0.0269],
        [0.0286, 0.0340, 0.0085,  ..., 0.0000, 0.0125, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474694.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6794.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(255.0265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9490.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-872.2253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.7625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1342],
        [ 0.1642],
        [ 0.1729],
        ...,
        [-0.4520],
        [-0.3394],
        [-0.1815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-123047.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0069],
        [1.0075],
        ...,
        [1.0022],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367366.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0069],
        [1.0076],
        ...,
        [1.0022],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367378.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  2.6128e-05,  ...,  0.0000e+00,
          4.7409e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6128e-05,  ...,  0.0000e+00,
          4.7409e-04,  0.0000e+00],
        [-8.7556e-03, -7.0056e-04,  1.4749e-02,  ..., -1.7524e-02,
          1.9682e-02, -7.8669e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.6128e-05,  ...,  0.0000e+00,
          4.7409e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6128e-05,  ...,  0.0000e+00,
          4.7409e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6128e-05,  ...,  0.0000e+00,
          4.7409e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(482.8377, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1187, device='cuda:0')



h[100].sum tensor(72.8255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3607, device='cuda:0')



h[200].sum tensor(7.0009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0203,  ..., 0.0000, 0.0283, 0.0000],
        [0.0000, 0.0000, 0.0189,  ..., 0.0000, 0.0264, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37361.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0321, 0.0374, 0.0078,  ..., 0.0000, 0.0179, 0.0210],
        [0.0968, 0.1156, 0.0026,  ..., 0.0000, 0.0662, 0.0084],
        [0.1230, 0.1462, 0.0006,  ..., 0.0000, 0.0868, 0.0034],
        ...,
        [0.0013, 0.0006, 0.0136,  ..., 0.0000, 0.0000, 0.0304],
        [0.0013, 0.0006, 0.0136,  ..., 0.0000, 0.0000, 0.0304],
        [0.0013, 0.0006, 0.0136,  ..., 0.0000, 0.0000, 0.0304]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(373291.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3646.8662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(173.7740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7310.6460, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-749.5784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.8962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0606],
        [ 0.0686],
        [ 0.1485],
        ...,
        [-0.5347],
        [-0.5328],
        [-0.5322]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139192.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0069],
        [1.0076],
        ...,
        [1.0022],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367378.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0077],
        ...,
        [1.0022],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367390.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5776e-02, -1.2507e-03,  2.6605e-02,  ..., -3.1597e-02,
          3.5110e-02, -1.4173e-02],
        [-1.5890e-02, -1.2597e-03,  2.6796e-02,  ..., -3.1825e-02,
          3.5360e-02, -1.4275e-02],
        [ 0.0000e+00,  0.0000e+00,  3.7498e-05,  ...,  0.0000e+00,
          4.5478e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.7498e-05,  ...,  0.0000e+00,
          4.5478e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.7498e-05,  ...,  0.0000e+00,
          4.5478e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.7498e-05,  ...,  0.0000e+00,
          4.5478e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(718.0383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8202, device='cuda:0')



h[100].sum tensor(81.0116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7258, device='cuda:0')



h[200].sum tensor(15.1355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.4563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1096,  ..., 0.0000, 0.1446, 0.0000],
        [0.0000, 0.0000, 0.0790,  ..., 0.0000, 0.1047, 0.0000],
        [0.0000, 0.0000, 0.0728,  ..., 0.0000, 0.0966, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50969.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.5760e-01, 5.6716e-01, 0.0000e+00,  ..., 0.0000e+00, 3.8826e-01,
         0.0000e+00],
        [4.6047e-01, 5.7000e-01, 0.0000e+00,  ..., 0.0000e+00, 3.8989e-01,
         0.0000e+00],
        [4.3919e-01, 5.4184e-01, 0.0000e+00,  ..., 0.0000e+00, 3.6922e-01,
         0.0000e+00],
        ...,
        [1.3641e-03, 4.4769e-04, 1.3553e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0698e-02],
        [1.3650e-03, 4.4818e-04, 1.3558e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0710e-02],
        [1.3641e-03, 4.4793e-04, 1.3559e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.0712e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456142.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6259.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(226.9214, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9265.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-842.4417, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.2542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1194],
        [ 0.1224],
        [ 0.1259],
        ...,
        [-0.5412],
        [-0.5392],
        [-0.5387]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125379.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0070],
        [1.0077],
        ...,
        [1.0022],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367390.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0071],
        [1.0078],
        ...,
        [1.0023],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367402.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.3995e-03, -2.6703e-04,  5.7731e-03,  ..., -6.8134e-03,
          7.9163e-03, -3.0536e-03],
        [-4.6461e-03, -3.6495e-04,  7.8755e-03,  ..., -9.3118e-03,
          1.0658e-02, -4.1733e-03],
        [ 0.0000e+00,  0.0000e+00,  3.9742e-05,  ...,  0.0000e+00,
          4.3832e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.9742e-05,  ...,  0.0000e+00,
          4.3832e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.9742e-05,  ...,  0.0000e+00,
          4.3832e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.9742e-05,  ...,  0.0000e+00,
          4.3832e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(549.1177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0221, device='cuda:0')



h[100].sum tensor(75.5120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4134, device='cuda:0')



h[200].sum tensor(9.3675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9718, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0517,  ..., 0.0000, 0.0690, 0.0000],
        [0.0000, 0.0000, 0.0171,  ..., 0.0000, 0.0238, 0.0000],
        [0.0000, 0.0000, 0.0081,  ..., 0.0000, 0.0121, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40289.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2544, 0.3085, 0.0000,  ..., 0.0000, 0.2039, 0.0000],
        [0.1401, 0.1677, 0.0015,  ..., 0.0000, 0.1042, 0.0061],
        [0.0654, 0.0768, 0.0051,  ..., 0.0000, 0.0415, 0.0139],
        ...,
        [0.0013, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0311],
        [0.0013, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0311],
        [0.0013, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0311]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(392506.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4264.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.5278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7937.5122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-779.9895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.5100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1108],
        [ 0.0791],
        [ 0.0138],
        ...,
        [-0.5485],
        [-0.5466],
        [-0.5459]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134376.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0071],
        [1.0078],
        ...,
        [1.0023],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367402.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0071],
        [1.0078],
        ...,
        [1.0023],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367414.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 3.8129e-05,  ..., 0.0000e+00, 4.3448e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.8129e-05,  ..., 0.0000e+00, 4.3448e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.8129e-05,  ..., 0.0000e+00, 4.3448e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.8129e-05,  ..., 0.0000e+00, 4.3448e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.8129e-05,  ..., 0.0000e+00, 4.3448e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.8129e-05,  ..., 0.0000e+00, 4.3448e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(598.2865, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7629, device='cuda:0')



h[100].sum tensor(77.5285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3760, device='cuda:0')



h[200].sum tensor(10.8888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1961, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0146, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42839.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0359, 0.0411, 0.0072,  ..., 0.0000, 0.0191, 0.0210],
        [0.0590, 0.0691, 0.0042,  ..., 0.0000, 0.0388, 0.0141],
        [0.1033, 0.1225, 0.0019,  ..., 0.0000, 0.0732, 0.0074],
        ...,
        [0.0015, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0315],
        [0.0015, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0315],
        [0.0015, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0315]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415093.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4938.2285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(189.2808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8392.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-795.3152, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.5557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0409],
        [ 0.0612],
        [ 0.0851],
        ...,
        [-0.5554],
        [-0.5535],
        [-0.5528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136985.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0071],
        [1.0078],
        ...,
        [1.0023],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367414.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0079],
        ...,
        [1.0023],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367426.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 3.2388e-05,  ..., 0.0000e+00, 4.3001e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.2388e-05,  ..., 0.0000e+00, 4.3001e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.2388e-05,  ..., 0.0000e+00, 4.3001e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.2388e-05,  ..., 0.0000e+00, 4.3001e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.2388e-05,  ..., 0.0000e+00, 4.3001e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.2388e-05,  ..., 0.0000e+00, 4.3001e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(608.0771, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2221, device='cuda:0')



h[100].sum tensor(78.1028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0770, device='cuda:0')



h[200].sum tensor(11.0897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41859.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0104, 0.0097, 0.0116,  ..., 0.0000, 0.0000, 0.0293],
        [0.0046, 0.0030, 0.0127,  ..., 0.0000, 0.0000, 0.0310],
        [0.0019, 0.0004, 0.0133,  ..., 0.0000, 0.0000, 0.0317],
        ...,
        [0.0014, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0320],
        [0.0014, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0320],
        [0.0014, 0.0003, 0.0135,  ..., 0.0000, 0.0000, 0.0320]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(401895.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4516.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.2414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8163.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-792.2739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.3687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1120],
        [-0.2079],
        [-0.3307],
        ...,
        [-0.5555],
        [-0.5567],
        [-0.5576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141583.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0079],
        ...,
        [1.0023],
        [1.0016],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367426.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0080],
        ...,
        [1.0023],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367438.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.7463e-03, -5.9172e-04,  1.3150e-02,  ..., -1.5557e-02,
          1.7541e-02, -6.9549e-03],
        [-8.7547e-03, -6.6875e-04,  1.4858e-02,  ..., -1.7582e-02,
          1.9769e-02, -7.8603e-03],
        [-8.9539e-03, -6.8396e-04,  1.5195e-02,  ..., -1.7982e-02,
          2.0209e-02, -8.0392e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.7935e-05,  ...,  0.0000e+00,
          4.2902e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7935e-05,  ...,  0.0000e+00,
          4.2902e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.7935e-05,  ...,  0.0000e+00,
          4.2902e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(634.7562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0252, device='cuda:0')



h[100].sum tensor(78.9605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5211, device='cuda:0')



h[200].sum tensor(11.9869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0483,  ..., 0.0000, 0.0646, 0.0000],
        [0.0000, 0.0000, 0.0563,  ..., 0.0000, 0.0750, 0.0000],
        [0.0000, 0.0000, 0.0712,  ..., 0.0000, 0.0944, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43151.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.0748e-01, 4.9499e-01, 0.0000e+00,  ..., 0.0000e+00, 3.3970e-01,
         0.0000e+00],
        [3.7216e-01, 4.5050e-01, 0.0000e+00,  ..., 0.0000e+00, 3.0747e-01,
         0.0000e+00],
        [3.3085e-01, 3.9962e-01, 0.0000e+00,  ..., 0.0000e+00, 2.7104e-01,
         0.0000e+00],
        ...,
        [1.2775e-03, 2.4576e-04, 1.3584e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2494e-02],
        [1.2782e-03, 2.4582e-04, 1.3588e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2504e-02],
        [1.2795e-03, 2.4696e-04, 1.3589e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2507e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(413309.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4842.8799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(188.3730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8512.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-803.5590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.9411, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0474],
        [ 0.0637],
        [ 0.0718],
        ...,
        [-0.5684],
        [-0.5663],
        [-0.5656]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136364.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0072],
        [1.0080],
        ...,
        [1.0023],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367438.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0081],
        ...,
        [1.0024],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367450.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.2009e-03, -6.2060e-04,  1.3937e-02,  ..., -1.6481e-02,
          1.8576e-02, -7.3620e-03],
        [-3.4581e-03, -2.6169e-04,  5.8912e-03,  ..., -6.9496e-03,
          8.0840e-03, -3.1043e-03],
        [-4.7428e-03, -3.5891e-04,  8.0708e-03,  ..., -9.5316e-03,
          1.0926e-02, -4.2577e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.4752e-05,  ...,  0.0000e+00,
          4.3385e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.4752e-05,  ...,  0.0000e+00,
          4.3385e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.4752e-05,  ...,  0.0000e+00,
          4.3385e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(602.9629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1264, device='cuda:0')



h[100].sum tensor(77.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4711, device='cuda:0')



h[200].sum tensor(10.9132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0232,  ..., 0.0000, 0.0319, 0.0000],
        [0.0000, 0.0000, 0.0519,  ..., 0.0000, 0.0692, 0.0000],
        [0.0000, 0.0000, 0.0233,  ..., 0.0000, 0.0320, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39669.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5579e-01, 1.8356e-01, 9.6171e-05,  ..., 0.0000e+00, 1.1618e-01,
         2.8079e-03],
        [2.0303e-01, 2.4149e-01, 0.0000e+00,  ..., 0.0000e+00, 1.5762e-01,
         0.0000e+00],
        [1.4241e-01, 1.6839e-01, 7.4002e-04,  ..., 0.0000e+00, 1.0577e-01,
         4.3101e-03],
        ...,
        [1.0957e-03, 2.7566e-04, 1.3682e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2878e-02],
        [1.0972e-03, 2.7616e-04, 1.3687e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2891e-02],
        [1.0973e-03, 2.7620e-04, 1.3689e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.2893e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390224.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3876.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(174.4789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7659.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-775.3614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-210.9018, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1169],
        [ 0.1182],
        [ 0.0816],
        ...,
        [-0.5740],
        [-0.5720],
        [-0.5714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162641.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0081],
        ...,
        [1.0024],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367450.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0081],
        ...,
        [1.0024],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367461.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.7733e-03, -2.8286e-04,  6.4377e-03,  ..., -7.5882e-03,
          8.8007e-03, -3.3868e-03],
        [-8.5292e-03, -6.3939e-04,  1.4518e-02,  ..., -1.7153e-02,
          1.9337e-02, -7.6556e-03],
        [ 0.0000e+00,  0.0000e+00,  2.6861e-05,  ...,  0.0000e+00,
          4.4109e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.6861e-05,  ...,  0.0000e+00,
          4.4109e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6861e-05,  ...,  0.0000e+00,
          4.4109e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6861e-05,  ...,  0.0000e+00,
          4.4109e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(890.4283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.4012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6118, device='cuda:0')



h[100].sum tensor(86.5052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.8225, device='cuda:0')



h[200].sum tensor(20.4615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.1229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0506,  ..., 0.0000, 0.0676, 0.0000],
        [0.0000, 0.0000, 0.0231,  ..., 0.0000, 0.0318, 0.0000],
        [0.0000, 0.0000, 0.0200,  ..., 0.0000, 0.0277, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60818.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.0324e-01, 2.4157e-01, 0.0000e+00,  ..., 0.0000e+00, 1.5741e-01,
         0.0000e+00],
        [1.5380e-01, 1.8125e-01, 2.3619e-04,  ..., 0.0000e+00, 1.1431e-01,
         3.0686e-03],
        [1.0903e-01, 1.2846e-01, 2.5668e-03,  ..., 0.0000e+00, 7.7266e-02,
         9.2802e-03],
        ...,
        [9.5547e-04, 3.4998e-04, 1.3803e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3093e-02],
        [9.5416e-04, 3.4939e-04, 1.3807e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3105e-02],
        [9.5441e-04, 3.4926e-04, 1.3808e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3108e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537862.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8621.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(260.7973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11189.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-911.1671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.7517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1214],
        [ 0.0983],
        [ 0.0376],
        ...,
        [-0.5771],
        [-0.5750],
        [-0.5740]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129291.3203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0081],
        ...,
        [1.0024],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367461.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0081],
        ...,
        [1.0024],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367461.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7407e-02, -1.3049e-03,  2.9602e-02,  ..., -3.5007e-02,
          3.9006e-02, -1.5624e-02],
        [-3.4791e-02, -2.6081e-03,  5.9137e-02,  ..., -6.9966e-02,
          7.7520e-02, -3.1227e-02],
        [-1.7511e-02, -1.3127e-03,  2.9777e-02,  ..., -3.5215e-02,
          3.9236e-02, -1.5717e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.6861e-05,  ...,  0.0000e+00,
          4.4109e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6861e-05,  ...,  0.0000e+00,
          4.4109e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6861e-05,  ...,  0.0000e+00,
          4.4109e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(600.9700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6969, device='cuda:0')



h[100].sum tensor(76.9509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2335, device='cuda:0')



h[200].sum tensor(10.8788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7430, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.4509e-01,  ..., 0.0000e+00, 1.9083e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.3517e-01,  ..., 0.0000e+00, 1.7790e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.4725e-01,  ..., 0.0000e+00, 1.9364e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0899e-04,  ..., 0.0000e+00, 1.7899e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0903e-04,  ..., 0.0000e+00, 1.7904e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0904e-04,  ..., 0.0000e+00, 1.7906e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39256.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.8924e-01, 7.2399e-01, 0.0000e+00,  ..., 0.0000e+00, 5.0597e-01,
         0.0000e+00],
        [6.2681e-01, 7.7041e-01, 0.0000e+00,  ..., 0.0000e+00, 5.3934e-01,
         0.0000e+00],
        [6.2666e-01, 7.6990e-01, 0.0000e+00,  ..., 0.0000e+00, 5.3887e-01,
         0.0000e+00],
        ...,
        [9.5547e-04, 3.4998e-04, 1.3803e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3093e-02],
        [9.5416e-04, 3.4939e-04, 1.3807e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3105e-02],
        [9.5441e-04, 3.4926e-04, 1.3808e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3108e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389621.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3913.3242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(175.5906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7858.3296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-776.3859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.5174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0686],
        [ 0.0630],
        [ 0.0673],
        ...,
        [-0.5774],
        [-0.5754],
        [-0.5748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154161.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0073],
        [1.0081],
        ...,
        [1.0024],
        [1.0016],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367461.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0073],
        [1.0082],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367473.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  3.4727e-05,  ...,  0.0000e+00,
          4.5111e-04,  0.0000e+00],
        [-4.2747e-03, -3.1744e-04,  7.3084e-03,  ..., -8.6025e-03,
          9.9355e-03, -3.8363e-03],
        [ 0.0000e+00,  0.0000e+00,  3.4727e-05,  ...,  0.0000e+00,
          4.5111e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.4727e-05,  ...,  0.0000e+00,
          4.5111e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.4727e-05,  ...,  0.0000e+00,
          4.5111e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.4727e-05,  ...,  0.0000e+00,
          4.5111e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.4393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0949, device='cuda:0')



h[100].sum tensor(77.6168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0066, device='cuda:0')



h[200].sum tensor(12.4123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7263, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0381,  ..., 0.0000, 0.0514, 0.0000],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0096, 0.0000],
        [0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0114, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41167.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1386, 0.1627, 0.0000,  ..., 0.0000, 0.1006, 0.0018],
        [0.0686, 0.0797, 0.0038,  ..., 0.0000, 0.0422, 0.0129],
        [0.0605, 0.0706, 0.0038,  ..., 0.0000, 0.0360, 0.0152],
        ...,
        [0.0009, 0.0004, 0.0139,  ..., 0.0000, 0.0000, 0.0331],
        [0.0009, 0.0004, 0.0139,  ..., 0.0000, 0.0000, 0.0332],
        [0.0009, 0.0004, 0.0140,  ..., 0.0000, 0.0000, 0.0332]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398460.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4102.0771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(185.1782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8036.0010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-782.6870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.0197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0625],
        [-0.1540],
        [-0.1962],
        ...,
        [-0.5787],
        [-0.5767],
        [-0.5761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154721.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0073],
        [1.0082],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367473.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367485.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00],
        [-1.0983e-02, -8.0793e-04,  1.8761e-02,  ..., -2.2118e-02,
          2.4868e-02, -9.8554e-03],
        [-1.6429e-02, -1.2085e-03,  2.8040e-02,  ..., -3.3084e-02,
          3.6967e-02, -1.4741e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(742.4269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.8520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4910, device='cuda:0')



h[100].sum tensor(79.7319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8847, device='cuda:0')



h[200].sum tensor(15.5499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1148, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0358,  ..., 0.0000, 0.0483, 0.0000],
        [0.0000, 0.0000, 0.0511,  ..., 0.0000, 0.0683, 0.0000],
        [0.0000, 0.0000, 0.0781,  ..., 0.0000, 0.1035, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46206.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.4906e-01, 4.2323e-01, 0.0000e+00,  ..., 0.0000e+00, 2.8683e-01,
         0.0000e+00],
        [4.1360e-01, 5.0311e-01, 0.0000e+00,  ..., 0.0000e+00, 3.4408e-01,
         0.0000e+00],
        [4.9291e-01, 6.0074e-01, 0.0000e+00,  ..., 0.0000e+00, 4.1392e-01,
         0.0000e+00],
        ...,
        [8.7542e-04, 5.5501e-04, 1.4127e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3149e-02],
        [8.7420e-04, 5.5524e-04, 1.4131e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3158e-02],
        [8.7472e-04, 5.5524e-04, 1.4134e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3161e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(428495.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5162.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.0131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9101.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-819.9319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.3156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0009],
        [ 0.0085],
        [ 0.0169],
        ...,
        [-0.5777],
        [-0.5755],
        [-0.5745]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132991.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367485.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367485.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00],
        [-4.5727e-03, -3.3636e-04,  7.8357e-03,  ..., -9.2084e-03,
          1.0623e-02, -4.1031e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  4.3055e-05,  ...,  0.0000e+00,
          4.6251e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(663.2469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3685, device='cuda:0')



h[100].sum tensor(77.1476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1579, device='cuda:0')



h[200].sum tensor(12.9560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0080,  ..., 0.0000, 0.0121, 0.0000],
        [0.0000, 0.0000, 0.0430,  ..., 0.0000, 0.0577, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42282.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.3101e-02, 6.2899e-02, 6.0339e-03,  ..., 0.0000e+00, 3.3877e-02,
         1.7042e-02],
        [1.2289e-01, 1.4686e-01, 2.6893e-03,  ..., 0.0000e+00, 8.9891e-02,
         9.1683e-03],
        [2.5293e-01, 3.0578e-01, 0.0000e+00,  ..., 0.0000e+00, 2.0313e-01,
         1.3678e-04],
        ...,
        [8.7542e-04, 5.5501e-04, 1.4127e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3149e-02],
        [8.7420e-04, 5.5524e-04, 1.4131e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3158e-02],
        [8.7472e-04, 5.5524e-04, 1.4134e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3161e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410250.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4536.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(192.4286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8600.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-792.9619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.1701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0288],
        [ 0.0819],
        [ 0.1105],
        ...,
        [-0.5785],
        [-0.5765],
        [-0.5758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139872.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367485.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367497.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 2.9881e-05,  ..., 0.0000e+00, 4.6397e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9881e-05,  ..., 0.0000e+00, 4.6397e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9881e-05,  ..., 0.0000e+00, 4.6397e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.9881e-05,  ..., 0.0000e+00, 4.6397e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9881e-05,  ..., 0.0000e+00, 4.6397e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9881e-05,  ..., 0.0000e+00, 4.6397e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(537.0560, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.9664, device='cuda:0')



h[100].sum tensor(72.8318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1705, device='cuda:0')



h[200].sum tensor(8.9754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0357,  ..., 0.0000, 0.0482, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35978.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1529, 0.1856, 0.0009,  ..., 0.0000, 0.1175, 0.0050],
        [0.0512, 0.0620, 0.0065,  ..., 0.0000, 0.0339, 0.0178],
        [0.0192, 0.0233, 0.0110,  ..., 0.0000, 0.0098, 0.0277],
        ...,
        [0.0005, 0.0006, 0.0143,  ..., 0.0000, 0.0000, 0.0337],
        [0.0005, 0.0006, 0.0143,  ..., 0.0000, 0.0000, 0.0337],
        [0.0005, 0.0006, 0.0143,  ..., 0.0000, 0.0000, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(378266., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3505.0623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(171.3815, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7954.8608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-758.0545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.1787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0384],
        [-0.1212],
        [-0.3214],
        ...,
        [-0.5845],
        [-0.5825],
        [-0.5819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141446.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367497.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367497.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9568e-03, -6.5262e-04,  1.5316e-02,  ..., -1.8050e-02,
          2.0396e-02, -8.0358e-03],
        [-1.8069e-02, -1.3166e-03,  3.0868e-02,  ..., -3.6413e-02,
          4.0674e-02, -1.6211e-02],
        [-2.6194e-02, -1.9086e-03,  4.4735e-02,  ..., -5.2786e-02,
          5.8756e-02, -2.3501e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.9881e-05,  ...,  0.0000e+00,
          4.6397e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.9881e-05,  ...,  0.0000e+00,
          4.6397e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.9881e-05,  ...,  0.0000e+00,
          4.6397e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(572.3482, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5140, device='cuda:0')



h[100].sum tensor(73.9774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0264, device='cuda:0')



h[200].sum tensor(10.1255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 1.0057e-01,  ..., 0.0000e+00, 1.3284e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.4296e-01,  ..., 0.0000e+00, 1.8812e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.1132e-01,  ..., 0.0000e+00, 2.7726e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.2131e-04,  ..., 0.0000e+00, 1.8835e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.2134e-04,  ..., 0.0000e+00, 1.8841e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.2136e-04,  ..., 0.0000e+00, 1.8843e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36551.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.4054e-01, 7.8581e-01, 0.0000e+00,  ..., 0.0000e+00, 5.4643e-01,
         0.0000e+00],
        [9.0494e-01, 1.1170e+00, 0.0000e+00,  ..., 1.7644e-03, 7.8515e-01,
         0.0000e+00],
        [1.1815e+00, 1.4635e+00, 0.0000e+00,  ..., 6.7521e-03, 1.0349e+00,
         0.0000e+00],
        ...,
        [5.0406e-04, 6.1211e-04, 1.4329e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3679e-02],
        [5.0397e-04, 6.1143e-04, 1.4331e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3689e-02],
        [5.0371e-04, 6.1157e-04, 1.4332e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.3693e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(375307.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3276.7290, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(173.2920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7617.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-755.0618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.2397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0718],
        [ 0.0237],
        [-0.0227],
        ...,
        [-0.5843],
        [-0.5822],
        [-0.5816]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156705.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0074],
        [1.0083],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367497.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0075],
        [1.0084],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367508.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 5.3019e-06,  ..., 0.0000e+00, 4.6191e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3019e-06,  ..., 0.0000e+00, 4.6191e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3019e-06,  ..., 0.0000e+00, 4.6191e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.3019e-06,  ..., 0.0000e+00, 4.6191e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3019e-06,  ..., 0.0000e+00, 4.6191e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.3019e-06,  ..., 0.0000e+00, 4.6191e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(677.9142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8197, device='cuda:0')



h[100].sum tensor(77.4717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4075, device='cuda:0')



h[200].sum tensor(13.7152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 8.7521e-03,  ..., 0.0000e+00, 1.3241e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.1338e-05,  ..., 0.0000e+00, 1.8590e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0820e-02,  ..., 0.0000e+00, 1.5944e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.1527e-05,  ..., 0.0000e+00, 1.8754e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.1534e-05,  ..., 0.0000e+00, 1.8760e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.1536e-05,  ..., 0.0000e+00, 1.8762e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43549.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0424, 0.0505, 0.0070,  ..., 0.0000, 0.0206, 0.0215],
        [0.0299, 0.0359, 0.0091,  ..., 0.0000, 0.0105, 0.0252],
        [0.0653, 0.0786, 0.0055,  ..., 0.0000, 0.0407, 0.0163],
        ...,
        [0.0002, 0.0006, 0.0146,  ..., 0.0000, 0.0000, 0.0345],
        [0.0002, 0.0006, 0.0146,  ..., 0.0000, 0.0000, 0.0345],
        [0.0002, 0.0006, 0.0146,  ..., 0.0000, 0.0000, 0.0345]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424203., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4855.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.3137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8727.0430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-805.5747, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.3441, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1969],
        [-0.1215],
        [-0.0105],
        ...,
        [-0.5934],
        [-0.5914],
        [-0.5907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147080.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0075],
        [1.0084],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367508.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0075],
        [1.0085],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367519.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.3420e-03, -2.3889e-04,  5.6805e-03,  ..., -6.7441e-03,
          7.9092e-03, -2.9975e-03],
        [ 0.0000e+00,  0.0000e+00, -3.9398e-05,  ...,  0.0000e+00,
          4.4979e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.9398e-05,  ...,  0.0000e+00,
          4.4979e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.9398e-05,  ...,  0.0000e+00,
          4.4979e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.9398e-05,  ...,  0.0000e+00,
          4.4979e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.9398e-05,  ...,  0.0000e+00,
          4.4979e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(839.9478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9276, device='cuda:0')



h[100].sum tensor(83.2255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7852, device='cuda:0')



h[200].sum tensor(19.2131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.5318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0200,  ..., 0.0000, 0.0279, 0.0000],
        [0.0000, 0.0000, 0.0365,  ..., 0.0000, 0.0495, 0.0000],
        [0.0000, 0.0000, 0.0609,  ..., 0.0000, 0.0814, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52304.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5375e-01, 3.0794e-01, 0.0000e+00,  ..., 0.0000e+00, 2.0275e-01,
         0.0000e+00],
        [3.3771e-01, 4.1162e-01, 0.0000e+00,  ..., 0.0000e+00, 2.7690e-01,
         0.0000e+00],
        [4.3086e-01, 5.2759e-01, 0.0000e+00,  ..., 0.0000e+00, 3.6020e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.0218e-04, 1.4964e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.5646e-02],
        [0.0000e+00, 5.0318e-04, 1.4970e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.5657e-02],
        [0.0000e+00, 5.0243e-04, 1.4971e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.5661e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471606.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6237.4673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.8030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9621.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-865.7952, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.3598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1203],
        [ 0.1165],
        [ 0.1132],
        ...,
        [-0.6034],
        [-0.6012],
        [-0.6006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144924.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0075],
        [1.0085],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367519.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0075],
        [1.0085],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367530.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -8.0426e-05,  ...,  0.0000e+00,
          4.3622e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0426e-05,  ...,  0.0000e+00,
          4.3622e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0426e-05,  ...,  0.0000e+00,
          4.3622e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.0426e-05,  ...,  0.0000e+00,
          4.3622e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0426e-05,  ...,  0.0000e+00,
          4.3622e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.0426e-05,  ...,  0.0000e+00,
          4.3622e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(589.1626, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.2486, device='cuda:0')



h[100].sum tensor(75.8596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4326, device='cuda:0')



h[200].sum tensor(11.2241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0090,  ..., 0.0000, 0.0137, 0.0000],
        [0.0000, 0.0000, 0.0045,  ..., 0.0000, 0.0077, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39270.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0448, 0.0522, 0.0061,  ..., 0.0000, 0.0203, 0.0233],
        [0.0312, 0.0369, 0.0088,  ..., 0.0000, 0.0106, 0.0271],
        [0.0154, 0.0189, 0.0119,  ..., 0.0000, 0.0031, 0.0316],
        ...,
        [0.0000, 0.0004, 0.0152,  ..., 0.0000, 0.0000, 0.0366],
        [0.0000, 0.0004, 0.0153,  ..., 0.0000, 0.0000, 0.0366],
        [0.0000, 0.0004, 0.0153,  ..., 0.0000, 0.0000, 0.0366]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(400957.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3998.7717, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.4514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8133.5713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-793.0896, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.7056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4120],
        [-0.4633],
        [-0.5359],
        ...,
        [-0.6169],
        [-0.6146],
        [-0.6139]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148515.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0075],
        [1.0085],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367530.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0076],
        [1.0086],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367542., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0042, -0.0003,  0.0071,  ..., -0.0085,  0.0098, -0.0038],
        [-0.0041, -0.0003,  0.0069,  ..., -0.0082,  0.0096, -0.0037],
        [-0.0083, -0.0006,  0.0141,  ..., -0.0167,  0.0189, -0.0074],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(720.6553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1596, device='cuda:0')



h[100].sum tensor(80.8252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1484, device='cuda:0')



h[200].sum tensor(15.3105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1784, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0233,  ..., 0.0000, 0.0324, 0.0000],
        [0.0000, 0.0000, 0.0484,  ..., 0.0000, 0.0654, 0.0000],
        [0.0000, 0.0000, 0.0233,  ..., 0.0000, 0.0325, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44554.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5868e-01, 1.9227e-01, 1.2503e-03,  ..., 0.0000e+00, 1.2024e-01,
         4.9471e-03],
        [2.2921e-01, 2.7733e-01, 0.0000e+00,  ..., 0.0000e+00, 1.8024e-01,
         0.0000e+00],
        [2.0536e-01, 2.4803e-01, 0.0000e+00,  ..., 0.0000e+00, 1.5930e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.5986e-04, 1.5536e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.7301e-02],
        [0.0000e+00, 2.6072e-04, 1.5540e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.7314e-02],
        [0.0000e+00, 2.6149e-04, 1.5542e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.7318e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424001.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4668.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.4474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8555.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-830.0374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.2014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1340],
        [ 0.1508],
        [ 0.1601],
        ...,
        [-0.6261],
        [-0.6239],
        [-0.6232]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150923.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0076],
        [1.0086],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367542., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(869.4731, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0076],
        [1.0087],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367553.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(695.2711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0950, device='cuda:0')



h[100].sum tensor(80.5927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5597, device='cuda:0')



h[200].sum tensor(14.2959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4297, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43261.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0156,  ..., 0.0000, 0.0000, 0.0374],
        [0.0002, 0.0009, 0.0154,  ..., 0.0000, 0.0000, 0.0372],
        [0.0019, 0.0030, 0.0149,  ..., 0.0000, 0.0000, 0.0367],
        ...,
        [0.0000, 0.0002, 0.0157,  ..., 0.0000, 0.0000, 0.0378],
        [0.0000, 0.0002, 0.0157,  ..., 0.0000, 0.0000, 0.0378],
        [0.0000, 0.0002, 0.0157,  ..., 0.0000, 0.0000, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423497.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4401.4019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(203.4922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7997.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-815.6729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.5654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5162],
        [-0.5324],
        [-0.5325],
        ...,
        [-0.6333],
        [-0.6316],
        [-0.6314]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-180860.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0076],
        [1.0087],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367553.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0077],
        [1.0087],
        ...,
        [1.0025],
        [1.0017],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367565.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(611.5726, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0697, device='cuda:0')



h[100].sum tensor(77.7765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3337, device='cuda:0')



h[200].sum tensor(11.4083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5987, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0044,  ..., 0.0000, 0.0076, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0136, 0.0000],
        [0.0000, 0.0000, 0.0044,  ..., 0.0000, 0.0077, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39538.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0440, 0.0525, 0.0070,  ..., 0.0000, 0.0204, 0.0241],
        [0.0567, 0.0671, 0.0046,  ..., 0.0000, 0.0302, 0.0205],
        [0.0411, 0.0491, 0.0075,  ..., 0.0000, 0.0187, 0.0250],
        ...,
        [0.0000, 0.0003, 0.0158,  ..., 0.0000, 0.0000, 0.0376],
        [0.0000, 0.0003, 0.0158,  ..., 0.0000, 0.0000, 0.0377],
        [0.0000, 0.0003, 0.0158,  ..., 0.0000, 0.0000, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(409599.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4116.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(186.7074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8099.8018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-799.1105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.4996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0542],
        [ 0.0254],
        [ 0.0437],
        ...,
        [-0.6378],
        [-0.6356],
        [-0.6349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161104.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0077],
        [1.0087],
        ...,
        [1.0025],
        [1.0017],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367565.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0077],
        [1.0088],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367577.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [-0.0087, -0.0006,  0.0149,  ..., -0.0177,  0.0200, -0.0078],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(864.6973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9657, device='cuda:0')



h[100].sum tensor(84.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2532, device='cuda:0')



h[200].sum tensor(19.0013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.8553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0080,  ..., 0.0000, 0.0124, 0.0000],
        [0.0000, 0.0000, 0.0441,  ..., 0.0000, 0.0599, 0.0000],
        [0.0000, 0.0000, 0.0669,  ..., 0.0000, 0.0896, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50079.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.4370e-01, 1.7450e-01, 2.9926e-03,  ..., 0.0000e+00, 1.0687e-01,
         1.0083e-02],
        [3.1742e-01, 3.8705e-01, 0.0000e+00,  ..., 0.0000e+00, 2.5812e-01,
         3.8798e-04],
        [4.6598e-01, 5.6998e-01, 0.0000e+00,  ..., 0.0000e+00, 3.8894e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 6.2830e-04, 1.5731e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.6608e-02],
        [0.0000e+00, 6.2865e-04, 1.5736e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.6619e-02],
        [0.0000e+00, 6.2927e-04, 1.5738e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.6624e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456657.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5669.6802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(224.1512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9228.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-857.7024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.4380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1097],
        [ 0.1433],
        [ 0.1557],
        ...,
        [-0.6343],
        [-0.6322],
        [-0.6316]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-150779.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0077],
        [1.0088],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367577.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0078],
        [1.0089],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367589.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058, -0.0004,  0.0098,  ..., -0.0117,  0.0135, -0.0052],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(666.1847, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3154, device='cuda:0')



h[100].sum tensor(78.0447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4695, device='cuda:0')



h[200].sum tensor(12.2460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0317,  ..., 0.0000, 0.0436, 0.0000],
        [0.0000, 0.0000, 0.0099,  ..., 0.0000, 0.0149, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38817.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2310, 0.2805, 0.0010,  ..., 0.0000, 0.1815, 0.0037],
        [0.1093, 0.1325, 0.0046,  ..., 0.0000, 0.0777, 0.0122],
        [0.0361, 0.0440, 0.0090,  ..., 0.0000, 0.0179, 0.0239],
        ...,
        [0.0003, 0.0013, 0.0155,  ..., 0.0000, 0.0000, 0.0353],
        [0.0003, 0.0013, 0.0155,  ..., 0.0000, 0.0000, 0.0353],
        [0.0003, 0.0013, 0.0155,  ..., 0.0000, 0.0000, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(396332.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3804.5225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(172.7325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8006.2134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-780.5258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.5537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1489],
        [ 0.0999],
        [ 0.0222],
        ...,
        [-0.6295],
        [-0.6273],
        [-0.6266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152937.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0078],
        [1.0089],
        ...,
        [1.0024],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367589.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0079],
        [1.0090],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367601.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [-0.0155, -0.0010,  0.0266,  ..., -0.0314,  0.0354, -0.0139],
        [-0.0181, -0.0012,  0.0312,  ..., -0.0368,  0.0413, -0.0162],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(714.3583, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8583, device='cuda:0')



h[100].sum tensor(79.2629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7698, device='cuda:0')



h[200].sum tensor(13.2028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0267,  ..., 0.0000, 0.0369, 0.0000],
        [0.0000, 0.0000, 0.0590,  ..., 0.0000, 0.0792, 0.0000],
        [0.0000, 0.0000, 0.1400,  ..., 0.0000, 0.1854, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40681.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1875, 0.2284, 0.0024,  ..., 0.0000, 0.1450, 0.0062],
        [0.3453, 0.4207, 0.0000,  ..., 0.0000, 0.2818, 0.0000],
        [0.5357, 0.6532, 0.0000,  ..., 0.0000, 0.4474, 0.0000],
        ...,
        [0.0013, 0.0021, 0.0153,  ..., 0.0000, 0.0000, 0.0342],
        [0.0013, 0.0021, 0.0154,  ..., 0.0000, 0.0000, 0.0342],
        [0.0013, 0.0021, 0.0154,  ..., 0.0000, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411596.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4221.3677, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(170.2984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8245.0342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-782.5396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.8803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1567],
        [ 0.1884],
        [ 0.2067],
        ...,
        [-0.6245],
        [-0.6224],
        [-0.6218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162170.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0079],
        [1.0090],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367601.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0080],
        [1.0091],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367612.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(952.1834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1111, device='cuda:0')



h[100].sum tensor(86.5330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3337, device='cuda:0')



h[200].sum tensor(19.8546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.9576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49097.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0461, 0.0541, 0.0071,  ..., 0.0000, 0.0251, 0.0194],
        [0.0161, 0.0186, 0.0124,  ..., 0.0000, 0.0033, 0.0289],
        [0.0041, 0.0048, 0.0146,  ..., 0.0000, 0.0000, 0.0326],
        ...,
        [0.0021, 0.0027, 0.0152,  ..., 0.0000, 0.0000, 0.0335],
        [0.0021, 0.0027, 0.0152,  ..., 0.0000, 0.0000, 0.0335],
        [0.0021, 0.0027, 0.0152,  ..., 0.0000, 0.0000, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447745.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5669.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.5790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9526.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-840.0999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.6014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0527],
        [-0.0625],
        [-0.1996],
        ...,
        [-0.6183],
        [-0.6166],
        [-0.6163]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129518.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0080],
        [1.0091],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367612.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0080],
        [1.0093],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367624.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(786.7737, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2989, device='cuda:0')



h[100].sum tensor(82.2099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5664, device='cuda:0')



h[200].sum tensor(13.6408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41930.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0030, 0.0152,  ..., 0.0000, 0.0000, 0.0331],
        [0.0091, 0.0113, 0.0140,  ..., 0.0000, 0.0020, 0.0309],
        [0.0180, 0.0220, 0.0126,  ..., 0.0000, 0.0055, 0.0280],
        ...,
        [0.0023, 0.0030, 0.0153,  ..., 0.0000, 0.0000, 0.0334],
        [0.0023, 0.0030, 0.0153,  ..., 0.0000, 0.0000, 0.0334],
        [0.0023, 0.0030, 0.0153,  ..., 0.0000, 0.0000, 0.0334]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415900.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4644.1123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.9057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8854.6182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-795.4822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.7129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5948],
        [-0.5150],
        [-0.3484],
        ...,
        [-0.6278],
        [-0.6257],
        [-0.6251]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135298.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0080],
        [1.0093],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367624.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0081],
        [1.0093],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367635.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048, -0.0003,  0.0080,  ..., -0.0097,  0.0113, -0.0043],
        [-0.0112, -0.0007,  0.0193,  ..., -0.0229,  0.0259, -0.0101],
        [-0.0142, -0.0009,  0.0244,  ..., -0.0289,  0.0327, -0.0127],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1296.6000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.8855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.2837, device='cuda:0')



h[100].sum tensor(98.6348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.5122, device='cuda:0')



h[200].sum tensor(28.4165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8153, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0367,  ..., 0.0000, 0.0503, 0.0000],
        [0.0000, 0.0000, 0.0821,  ..., 0.0000, 0.1103, 0.0000],
        [0.0000, 0.0000, 0.0995,  ..., 0.0000, 0.1330, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68826.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2795, 0.3372, 0.0000,  ..., 0.0000, 0.2203, 0.0000],
        [0.4534, 0.5491, 0.0000,  ..., 0.0000, 0.3713, 0.0000],
        [0.5767, 0.7000, 0.0000,  ..., 0.0000, 0.4792, 0.0000],
        ...,
        [0.0021, 0.0031, 0.0155,  ..., 0.0000, 0.0000, 0.0337],
        [0.0021, 0.0031, 0.0155,  ..., 0.0000, 0.0000, 0.0337],
        [0.0021, 0.0031, 0.0155,  ..., 0.0000, 0.0000, 0.0337]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577646.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9372.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.1612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11799.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-953.8719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.8092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1485],
        [ 0.1337],
        [ 0.1172],
        ...,
        [-0.6260],
        [-0.6290],
        [-0.6299]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149712.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0081],
        [1.0093],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367635.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0082],
        [1.0094],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367647.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0134, -0.0009,  0.0230,  ..., -0.0273,  0.0309, -0.0120],
        [-0.0235, -0.0015,  0.0406,  ..., -0.0478,  0.0538, -0.0211],
        [-0.0127, -0.0008,  0.0218,  ..., -0.0259,  0.0293, -0.0114],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(783.3464, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7862, device='cuda:0')



h[100].sum tensor(83.5947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2829, device='cuda:0')



h[200].sum tensor(12.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8059, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1138,  ..., 0.0000, 0.1517, 0.0000],
        [0.0000, 0.0000, 0.1177,  ..., 0.0000, 0.1568, 0.0000],
        [0.0000, 0.0000, 0.1246,  ..., 0.0000, 0.1659, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40867., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4698, 0.5712, 0.0000,  ..., 0.0000, 0.3877, 0.0000],
        [0.5487, 0.6666, 0.0000,  ..., 0.0000, 0.4553, 0.0000],
        [0.5626, 0.6829, 0.0000,  ..., 0.0000, 0.4666, 0.0000],
        ...,
        [0.0018, 0.0031, 0.0156,  ..., 0.0000, 0.0000, 0.0341],
        [0.0018, 0.0031, 0.0156,  ..., 0.0000, 0.0000, 0.0341],
        [0.0018, 0.0031, 0.0156,  ..., 0.0000, 0.0000, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410264.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4221.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(146.4222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8517.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-790.4061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.0265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1047],
        [ 0.1356],
        [ 0.1544],
        ...,
        [-0.6437],
        [-0.6415],
        [-0.6409]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151670.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0082],
        [1.0094],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367647.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0083],
        [1.0095],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367659.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0170, -0.0011,  0.0293,  ..., -0.0346,  0.0391, -0.0152],
        [-0.0331, -0.0021,  0.0573,  ..., -0.0673,  0.0757, -0.0296],
        [-0.0324, -0.0021,  0.0561,  ..., -0.0659,  0.0740, -0.0290],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(798.3958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2370, device='cuda:0')



h[100].sum tensor(84.5160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5322, device='cuda:0')



h[200].sum tensor(12.0903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1455,  ..., 0.0000, 0.1931, 0.0000],
        [0.0000, 0.0000, 0.1930,  ..., 0.0000, 0.2552, 0.0000],
        [0.0000, 0.0000, 0.2023,  ..., 0.0000, 0.2673, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42714.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6794, 0.8309, 0.0000,  ..., 0.0000, 0.5737, 0.0000],
        [0.8803, 1.0770, 0.0000,  ..., 0.0000, 0.7495, 0.0000],
        [0.9128, 1.1164, 0.0000,  ..., 0.0000, 0.7773, 0.0000],
        ...,
        [0.0013, 0.0030, 0.0158,  ..., 0.0000, 0.0000, 0.0346],
        [0.0013, 0.0030, 0.0158,  ..., 0.0000, 0.0000, 0.0346],
        [0.0013, 0.0030, 0.0158,  ..., 0.0000, 0.0000, 0.0346]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427913.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4733.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(151.7148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8957.9229, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-808.6614, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.0788, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0868],
        [ 0.1090],
        [ 0.1262],
        ...,
        [-0.6514],
        [-0.6490],
        [-0.6476]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148820.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0083],
        [1.0095],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367659.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(534.7014, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0084],
        [1.0096],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367671.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070, -0.0004,  0.0119,  ..., -0.0143,  0.0164, -0.0063],
        [-0.0169, -0.0011,  0.0291,  ..., -0.0344,  0.0388, -0.0151],
        [-0.0145, -0.0009,  0.0249,  ..., -0.0294,  0.0333, -0.0129],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(998.8417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8738, device='cuda:0')



h[100].sum tensor(90.7318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7554, device='cuda:0')



h[200].sum tensor(17.9457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.4939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0814,  ..., 0.0000, 0.1095, 0.0000],
        [0.0000, 0.0000, 0.0808,  ..., 0.0000, 0.1088, 0.0000],
        [0.0000, 0.0000, 0.0843,  ..., 0.0000, 0.1134, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51726.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3328, 0.4048, 0.0000,  ..., 0.0000, 0.2672, 0.0000],
        [0.3726, 0.4530, 0.0000,  ..., 0.0000, 0.3012, 0.0000],
        [0.3701, 0.4498, 0.0000,  ..., 0.0000, 0.2989, 0.0000],
        ...,
        [0.0006, 0.0028, 0.0160,  ..., 0.0000, 0.0000, 0.0353],
        [0.0006, 0.0029, 0.0160,  ..., 0.0000, 0.0000, 0.0353],
        [0.0006, 0.0029, 0.0160,  ..., 0.0000, 0.0000, 0.0353]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481079.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6324.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(187.6139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10054.4834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-868.1700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.5693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1671],
        [ 0.1754],
        [ 0.1781],
        ...,
        [-0.6597],
        [-0.6585],
        [-0.6584]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145950.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0084],
        [1.0096],
        ...,
        [1.0024],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367671.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0085],
        [1.0097],
        ...,
        [1.0025],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367683.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0094, -0.0006,  0.0162,  ..., -0.0192,  0.0219, -0.0084],
        [-0.0085, -0.0005,  0.0145,  ..., -0.0173,  0.0198, -0.0076],
        [-0.0092, -0.0006,  0.0158,  ..., -0.0188,  0.0215, -0.0083],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [-0.0106, -0.0007,  0.0182,  ..., -0.0216,  0.0246, -0.0095],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(926.3915, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5741, device='cuda:0')



h[100].sum tensor(88.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4837, device='cuda:0')



h[200].sum tensor(15.6882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8766, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0559,  ..., 0.0000, 0.0763, 0.0000],
        [0.0000, 0.0000, 0.0574,  ..., 0.0000, 0.0783, 0.0000],
        [0.0000, 0.0000, 0.0556,  ..., 0.0000, 0.0760, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0185,  ..., 0.0000, 0.0263, 0.0000],
        [0.0000, 0.0000, 0.0151,  ..., 0.0000, 0.0218, 0.0000],
        [0.0000, 0.0000, 0.0671,  ..., 0.0000, 0.0910, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48136.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2826, 0.3419, 0.0000,  ..., 0.0000, 0.2204, 0.0000],
        [0.2814, 0.3409, 0.0000,  ..., 0.0000, 0.2198, 0.0000],
        [0.2773, 0.3359, 0.0000,  ..., 0.0000, 0.2163, 0.0000],
        ...,
        [0.0841, 0.1046, 0.0058,  ..., 0.0000, 0.0543, 0.0141],
        [0.1084, 0.1338, 0.0025,  ..., 0.0000, 0.0748, 0.0069],
        [0.1860, 0.2276, 0.0000,  ..., 0.0000, 0.1408, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456021.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5601.7603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(177.0679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9764.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-857.7558, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.5838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2216],
        [ 0.2217],
        [ 0.2207],
        ...,
        [-0.0750],
        [-0.0046],
        [ 0.0184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132417.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0085],
        [1.0097],
        ...,
        [1.0025],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367683.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0086],
        [1.0098],
        ...,
        [1.0025],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367696.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1019.4419, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.7067, device='cuda:0')



h[100].sum tensor(90.9464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2160, device='cuda:0')



h[200].sum tensor(18.4754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52803.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0282, 0.0363, 0.0112,  ..., 0.0000, 0.0099, 0.0273],
        [0.0118, 0.0168, 0.0141,  ..., 0.0000, 0.0000, 0.0326],
        [0.0059, 0.0098, 0.0152,  ..., 0.0000, 0.0000, 0.0346],
        ...,
        [0.0000, 0.0023, 0.0166,  ..., 0.0000, 0.0000, 0.0369],
        [0.0000, 0.0023, 0.0166,  ..., 0.0000, 0.0000, 0.0369],
        [0.0000, 0.0023, 0.0166,  ..., 0.0000, 0.0000, 0.0369]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481682.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6355.8096, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(197.9148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10300.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-890.7134, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.6063, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0164],
        [-0.0917],
        [-0.2288],
        ...,
        [-0.6803],
        [-0.6780],
        [-0.6772]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133985.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0086],
        [1.0098],
        ...,
        [1.0025],
        [1.0016],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367696.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0088],
        [1.0099],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367708.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [-0.0083, -0.0005,  0.0143,  ..., -0.0170,  0.0195, -0.0075],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(982.5225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.5271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2869, device='cuda:0')



h[100].sum tensor(90.2528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.4308, device='cuda:0')



h[200].sum tensor(17.1221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0811, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0017, 0.0000],
        [0.0000, 0.0000, 0.0262,  ..., 0.0000, 0.0367, 0.0000],
        [0.0000, 0.0000, 0.0361,  ..., 0.0000, 0.0501, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0189,  ..., 0.0000, 0.0273, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49320.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0552, 0.0704, 0.0073,  ..., 0.0000, 0.0338, 0.0186],
        [0.1583, 0.1946, 0.0022,  ..., 0.0000, 0.1169, 0.0069],
        [0.2450, 0.2988, 0.0000,  ..., 0.0000, 0.1898, 0.0007],
        ...,
        [0.0156, 0.0215, 0.0138,  ..., 0.0000, 0.0048, 0.0325],
        [0.0482, 0.0608, 0.0080,  ..., 0.0000, 0.0273, 0.0218],
        [0.1256, 0.1534, 0.0025,  ..., 0.0000, 0.0869, 0.0079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(459841.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5438.9248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(180.5383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9643.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-871.0555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.3801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0407],
        [ 0.1030],
        [ 0.1577],
        ...,
        [-0.3534],
        [-0.1326],
        [ 0.0526]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144694.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0088],
        [1.0099],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367708.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0089],
        [1.0100],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367720.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0076, -0.0005,  0.0130,  ..., -0.0155,  0.0178, -0.0068],
        [-0.0202, -0.0012,  0.0352,  ..., -0.0413,  0.0467, -0.0181],
        [-0.0034, -0.0002,  0.0057,  ..., -0.0070,  0.0083, -0.0031],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(785.3656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9961, device='cuda:0')



h[100].sum tensor(84.5966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3990, device='cuda:0')



h[200].sum tensor(11.1381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9535, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1253,  ..., 0.0000, 0.1670, 0.0000],
        [0.0000, 0.0000, 0.0627,  ..., 0.0000, 0.0852, 0.0000],
        [0.0000, 0.0000, 0.0700,  ..., 0.0000, 0.0948, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39783.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5130, 0.6283, 0.0000,  ..., 0.0000, 0.4238, 0.0000],
        [0.4009, 0.4896, 0.0000,  ..., 0.0000, 0.3248, 0.0000],
        [0.3346, 0.4088, 0.0000,  ..., 0.0000, 0.2677, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0169,  ..., 0.0000, 0.0000, 0.0386],
        [0.0000, 0.0018, 0.0169,  ..., 0.0000, 0.0000, 0.0386],
        [0.0000, 0.0018, 0.0169,  ..., 0.0000, 0.0000, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407629.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3509.2798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(141.0295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8139.2666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-809.8936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.4245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1770],
        [ 0.1767],
        [ 0.1646],
        ...,
        [-0.7001],
        [-0.6978],
        [-0.6971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182349.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0089],
        [1.0100],
        ...,
        [1.0025],
        [1.0017],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367720.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0072],
        [1.0090],
        [1.0101],
        ...,
        [1.0025],
        [1.0017],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367733.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0067, -0.0004,  0.0114,  ..., -0.0136,  0.0157, -0.0060],
        [-0.0134, -0.0008,  0.0232,  ..., -0.0273,  0.0311, -0.0120],
        [-0.0067, -0.0004,  0.0115,  ..., -0.0137,  0.0158, -0.0060],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0004,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1044.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0251, device='cuda:0')



h[100].sum tensor(92.0531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3921, device='cuda:0')



h[200].sum tensor(18.7955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.3037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0770,  ..., 0.0000, 0.1034, 0.0000],
        [0.0000, 0.0000, 0.0767,  ..., 0.0000, 0.1035, 0.0000],
        [0.0000, 0.0000, 0.0763,  ..., 0.0000, 0.1026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55096.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5796, 0.7121, 0.0000,  ..., 0.0000, 0.4842, 0.0000],
        [0.6357, 0.7803, 0.0000,  ..., 0.0000, 0.5323, 0.0000],
        [0.6096, 0.7487, 0.0000,  ..., 0.0000, 0.5101, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0170,  ..., 0.0000, 0.0000, 0.0393],
        [0.0000, 0.0016, 0.0170,  ..., 0.0000, 0.0000, 0.0393],
        [0.0000, 0.0016, 0.0170,  ..., 0.0000, 0.0000, 0.0393]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503657.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6569.1499, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(200.8136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10374.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-911.2259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-210.9918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1337],
        [ 0.1351],
        [ 0.1358],
        ...,
        [-0.7077],
        [-0.7053],
        [-0.7045]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161650.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0072],
        [1.0090],
        [1.0101],
        ...,
        [1.0025],
        [1.0017],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367733.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0092],
        [1.0103],
        ...,
        [1.0026],
        [1.0017],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367745.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(829.7828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0023, device='cuda:0')



h[100].sum tensor(85.6233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9554, device='cuda:0')



h[200].sum tensor(12.3343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0018, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43329.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0073, 0.0158,  ..., 0.0000, 0.0000, 0.0376],
        [0.0002, 0.0026, 0.0166,  ..., 0.0000, 0.0000, 0.0390],
        [0.0000, 0.0016, 0.0169,  ..., 0.0000, 0.0000, 0.0392],
        ...,
        [0.0000, 0.0017, 0.0170,  ..., 0.0000, 0.0000, 0.0396],
        [0.0000, 0.0017, 0.0170,  ..., 0.0000, 0.0000, 0.0396],
        [0.0000, 0.0017, 0.0170,  ..., 0.0000, 0.0000, 0.0396]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(439314.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4588.2681, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(151.9413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9215.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-844.6424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.6691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3346],
        [-0.5392],
        [-0.7007],
        ...,
        [-0.7129],
        [-0.7105],
        [-0.7098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160418.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0092],
        [1.0103],
        ...,
        [1.0026],
        [1.0017],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367745.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0093],
        [1.0104],
        ...,
        [1.0026],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367758.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(803.0211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2665, device='cuda:0')



h[100].sum tensor(84.3726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9955, device='cuda:0')



h[200].sum tensor(11.4597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0019, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40538.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0168,  ..., 0.0000, 0.0000, 0.0392],
        [0.0000, 0.0018, 0.0168,  ..., 0.0000, 0.0000, 0.0392],
        [0.0000, 0.0017, 0.0168,  ..., 0.0000, 0.0000, 0.0392],
        ...,
        [0.0000, 0.0018, 0.0170,  ..., 0.0000, 0.0000, 0.0396],
        [0.0000, 0.0018, 0.0170,  ..., 0.0000, 0.0000, 0.0396],
        [0.0000, 0.0018, 0.0170,  ..., 0.0000, 0.0000, 0.0396]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(421526.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3978.3767, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.1288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8870.8027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-827.0312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.1795, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8289],
        [-0.8740],
        [-0.9094],
        ...,
        [-0.7154],
        [-0.7131],
        [-0.7123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-166633.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0093],
        [1.0104],
        ...,
        [1.0026],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367758.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0095],
        [1.0105],
        ...,
        [1.0027],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367771.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0094, -0.0005,  0.0164,  ..., -0.0193,  0.0222, -0.0084],
        [-0.0158, -0.0009,  0.0277,  ..., -0.0324,  0.0369, -0.0141],
        [-0.0108, -0.0006,  0.0188,  ..., -0.0221,  0.0253, -0.0096],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1491.5112, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.5521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.2687, device='cuda:0')



h[100].sum tensor(103.5488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.2689, device='cuda:0')



h[200].sum tensor(31.4929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0454,  ..., 0.0000, 0.0623, 0.0000],
        [0.0000, 0.0000, 0.0667,  ..., 0.0000, 0.0904, 0.0000],
        [0.0000, 0.0000, 0.1288,  ..., 0.0000, 0.1715, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80705.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3417, 0.4145, 0.0000,  ..., 0.0000, 0.2714, 0.0000],
        [0.4800, 0.5835, 0.0000,  ..., 0.0000, 0.3913, 0.0000],
        [0.6755, 0.8241, 0.0000,  ..., 0.0000, 0.5628, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0168,  ..., 0.0000, 0.0000, 0.0394],
        [0.0000, 0.0019, 0.0168,  ..., 0.0000, 0.0000, 0.0394],
        [0.0000, 0.0019, 0.0168,  ..., 0.0000, 0.0000, 0.0394]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(711705.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13161.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(290.2117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15284.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1079.4769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.3492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2063],
        [ 0.1957],
        [ 0.1841],
        ...,
        [-0.7162],
        [-0.7139],
        [-0.7132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130713.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0095],
        [1.0105],
        ...,
        [1.0027],
        [1.0018],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367771.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0107],
        ...,
        [1.0027],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367783.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0082, -0.0005,  0.0143,  ..., -0.0169,  0.0195, -0.0074],
        [-0.0037, -0.0002,  0.0064,  ..., -0.0077,  0.0091, -0.0033],
        [-0.0143, -0.0008,  0.0251,  ..., -0.0294,  0.0336, -0.0128],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1075.6954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0806, device='cuda:0')



h[100].sum tensor(91.2731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3168, device='cuda:0')



h[200].sum tensor(19.0185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.9361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0319,  ..., 0.0000, 0.0448, 0.0000],
        [0.0000, 0.0000, 0.0579,  ..., 0.0000, 0.0788, 0.0000],
        [0.0000, 0.0000, 0.0323,  ..., 0.0000, 0.0451, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55034.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2756, 0.3294, 0.0000,  ..., 0.0000, 0.2116, 0.0000],
        [0.2733, 0.3271, 0.0000,  ..., 0.0000, 0.2101, 0.0000],
        [0.2270, 0.2717, 0.0000,  ..., 0.0000, 0.1710, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0167,  ..., 0.0000, 0.0000, 0.0391],
        [0.0000, 0.0018, 0.0167,  ..., 0.0000, 0.0000, 0.0391],
        [0.0000, 0.0018, 0.0167,  ..., 0.0000, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527675.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7599.1006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(184.0074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11891.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-925.7739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.8650, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1923],
        [ 0.1952],
        [ 0.1926],
        ...,
        [-0.7183],
        [-0.7160],
        [-0.7153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131402.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0107],
        ...,
        [1.0027],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367783.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0107],
        ...,
        [1.0027],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367783.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0061, -0.0004,  0.0106,  ..., -0.0125,  0.0146, -0.0055],
        [-0.0062, -0.0004,  0.0108,  ..., -0.0128,  0.0149, -0.0056],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1057.8948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7201, device='cuda:0')



h[100].sum tensor(90.7603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1174, device='cuda:0')



h[200].sum tensor(18.5025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6826, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0591,  ..., 0.0000, 0.0804, 0.0000],
        [0.0000, 0.0000, 0.0195,  ..., 0.0000, 0.0281, 0.0000],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0166, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53131.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2355, 0.2843, 0.0000,  ..., 0.0000, 0.1809, 0.0000],
        [0.1313, 0.1591, 0.0029,  ..., 0.0000, 0.0922, 0.0092],
        [0.0685, 0.0842, 0.0071,  ..., 0.0000, 0.0409, 0.0188],
        ...,
        [0.0000, 0.0018, 0.0167,  ..., 0.0000, 0.0000, 0.0391],
        [0.0000, 0.0018, 0.0167,  ..., 0.0000, 0.0000, 0.0391],
        [0.0000, 0.0018, 0.0167,  ..., 0.0000, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(514668.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7050.0186, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(176.6471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11400.2275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-909.1998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.0631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0472],
        [-0.1136],
        [-0.3444],
        ...,
        [-0.7183],
        [-0.7160],
        [-0.7153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144955.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0097],
        [1.0107],
        ...,
        [1.0027],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367783.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0108],
        ...,
        [1.0028],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367796.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [-0.0187, -0.0011,  0.0329,  ..., -0.0383,  0.0437, -0.0167],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1068.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0605, device='cuda:0')



h[100].sum tensor(90.7257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7527, device='cuda:0')



h[200].sum tensor(18.6436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0748,  ..., 0.0000, 0.1007, 0.0000],
        [0.0000, 0.0000, 0.0268,  ..., 0.0000, 0.0376, 0.0000],
        [0.0000, 0.0000, 0.0781,  ..., 0.0000, 0.1051, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0020, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51788.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2401, 0.2890, 0.0000,  ..., 0.0000, 0.1855, 0.0000],
        [0.1921, 0.2312, 0.0000,  ..., 0.0000, 0.1444, 0.0000],
        [0.2473, 0.2978, 0.0000,  ..., 0.0000, 0.1917, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0165,  ..., 0.0000, 0.0000, 0.0389],
        [0.0000, 0.0016, 0.0165,  ..., 0.0000, 0.0000, 0.0389],
        [0.0000, 0.0016, 0.0165,  ..., 0.0000, 0.0000, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493619.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6329.6260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(167.1851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11045.3691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-898.1517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-231.0706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0049],
        [-0.0084],
        [-0.0322],
        ...,
        [-0.7126],
        [-0.7107],
        [-0.7110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157222.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0098],
        [1.0108],
        ...,
        [1.0028],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367796.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0109],
        ...,
        [1.0028],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367809.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048, -0.0003,  0.0083,  ..., -0.0099,  0.0116, -0.0043],
        [-0.0144, -0.0008,  0.0253,  ..., -0.0295,  0.0337, -0.0128],
        [-0.0177, -0.0010,  0.0312,  ..., -0.0363,  0.0415, -0.0158],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(991.7979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6040, device='cuda:0')



h[100].sum tensor(88.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8412, device='cuda:0')



h[200].sum tensor(16.3305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0369,  ..., 0.0000, 0.0507, 0.0000],
        [0.0000, 0.0000, 0.0871,  ..., 0.0000, 0.1167, 0.0000],
        [0.0000, 0.0000, 0.1448,  ..., 0.0000, 0.1919, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46528.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.9300e-01, 3.5065e-01, 0.0000e+00,  ..., 0.0000e+00, 2.3018e-01,
         3.8796e-04],
        [4.9220e-01, 5.9116e-01, 0.0000e+00,  ..., 0.0000e+00, 4.0196e-01,
         0.0000e+00],
        [7.0958e-01, 8.5507e-01, 0.0000e+00,  ..., 0.0000e+00, 5.9093e-01,
         0.0000e+00],
        ...,
        [1.2490e-04, 1.4535e-03, 1.6459e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.8837e-02],
        [1.2484e-04, 1.4547e-03, 1.6463e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.8849e-02],
        [1.2440e-04, 1.4553e-03, 1.6463e-02,  ..., 0.0000e+00, 0.0000e+00,
         3.8851e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(463527.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5579.9404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(142.4775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10908.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-871.6012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.9350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1303],
        [ 0.1089],
        [ 0.0849],
        ...,
        [-0.7234],
        [-0.7211],
        [-0.7203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-137128.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0099],
        [1.0109],
        ...,
        [1.0028],
        [1.0019],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367809.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0100],
        [1.0111],
        ...,
        [1.0028],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367822.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0073, -0.0004,  0.0129,  ..., -0.0150,  0.0175, -0.0065],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(987.8317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0850, device='cuda:0')



h[100].sum tensor(87.4961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5542, device='cuda:0')



h[200].sum tensor(16.0257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0261,  ..., 0.0000, 0.0365, 0.0000],
        [0.0000, 0.0000, 0.0130,  ..., 0.0000, 0.0192, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44586.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1706, 0.2029, 0.0014,  ..., 0.0000, 0.1252, 0.0056],
        [0.0959, 0.1136, 0.0045,  ..., 0.0000, 0.0615, 0.0137],
        [0.0381, 0.0444, 0.0096,  ..., 0.0000, 0.0148, 0.0268],
        ...,
        [0.0003, 0.0014, 0.0165,  ..., 0.0000, 0.0000, 0.0390],
        [0.0003, 0.0014, 0.0165,  ..., 0.0000, 0.0000, 0.0390],
        [0.0003, 0.0014, 0.0165,  ..., 0.0000, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(448299.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4867.7563, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(133.3334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10372.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-851.2653, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.3571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0910],
        [ 0.0368],
        [-0.0537],
        ...,
        [-0.7270],
        [-0.7247],
        [-0.7239]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158585.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0100],
        [1.0111],
        ...,
        [1.0028],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367822.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0101],
        [1.0112],
        ...,
        [1.0028],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367835.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0079, -0.0004,  0.0140,  ..., -0.0163,  0.0190, -0.0071],
        [-0.0035, -0.0002,  0.0060,  ..., -0.0071,  0.0086, -0.0031],
        [-0.0045, -0.0002,  0.0078,  ..., -0.0092,  0.0109, -0.0040],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(955.5526, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4397, device='cuda:0')



h[100].sum tensor(86.2350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6443, device='cuda:0')



h[200].sum tensor(14.8085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0231,  ..., 0.0000, 0.0328, 0.0000],
        [0.0000, 0.0000, 0.0514,  ..., 0.0000, 0.0699, 0.0000],
        [0.0000, 0.0000, 0.0233,  ..., 0.0000, 0.0329, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42581.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1635, 0.1904, 0.0007,  ..., 0.0000, 0.1156, 0.0037],
        [0.2140, 0.2502, 0.0000,  ..., 0.0000, 0.1581, 0.0000],
        [0.1526, 0.1779, 0.0014,  ..., 0.0000, 0.1068, 0.0053],
        ...,
        [0.0003, 0.0012, 0.0166,  ..., 0.0000, 0.0000, 0.0396],
        [0.0003, 0.0012, 0.0166,  ..., 0.0000, 0.0000, 0.0396],
        [0.0003, 0.0012, 0.0166,  ..., 0.0000, 0.0000, 0.0396]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440065.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4495.7119, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(127.3678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10187.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-839.2806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.6041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1278],
        [ 0.1554],
        [ 0.1461],
        ...,
        [-0.7336],
        [-0.7313],
        [-0.7306]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168142.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0101],
        [1.0112],
        ...,
        [1.0028],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367835.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0102],
        [1.0113],
        ...,
        [1.0028],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367848.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040, -0.0002,  0.0070,  ..., -0.0082,  0.0098, -0.0035],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0005,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(998.8723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8396, device='cuda:0')



h[100].sum tensor(87.0909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4185, device='cuda:0')



h[200].sum tensor(15.6525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.2501, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0238,  ..., 0.0000, 0.0335, 0.0000],
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0114, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0021, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45244.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5696e-01, 1.8311e-01, 1.3944e-03,  ..., 0.0000e+00, 1.1042e-01,
         5.2503e-03],
        [8.4926e-02, 9.8722e-02, 5.4397e-03,  ..., 0.0000e+00, 5.3075e-02,
         1.5366e-02],
        [3.3439e-02, 3.8908e-02, 1.0682e-02,  ..., 0.0000e+00, 1.5488e-02,
         2.9600e-02],
        ...,
        [1.4017e-04, 1.1666e-03, 1.6832e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.0413e-02],
        [1.3846e-04, 1.1663e-03, 1.6835e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.0424e-02],
        [1.4018e-04, 1.1678e-03, 1.6835e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.0426e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455228.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5100.2173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(142.9870, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10719.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-861.7017, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.5179, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1495],
        [ 0.0829],
        [-0.0116],
        ...,
        [-0.7212],
        [-0.7314],
        [-0.7343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155683.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0102],
        [1.0113],
        ...,
        [1.0028],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367848.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0103],
        [1.0114],
        ...,
        [1.0029],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367860.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.7746e-03, -2.0329e-04,  6.6430e-03,  ..., -7.7672e-03,
          9.3227e-03, -3.3711e-03],
        [ 0.0000e+00,  0.0000e+00, -9.1653e-05,  ...,  0.0000e+00,
          5.3789e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.1653e-05,  ...,  0.0000e+00,
          5.3789e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -9.1653e-05,  ...,  0.0000e+00,
          5.3789e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.1653e-05,  ...,  0.0000e+00,
          5.3789e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.1653e-05,  ...,  0.0000e+00,
          5.3789e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1011.3699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0353, device='cuda:0')



h[100].sum tensor(87.3807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5267, device='cuda:0')



h[200].sum tensor(15.3906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0166,  ..., 0.0000, 0.0242, 0.0000],
        [0.0000, 0.0000, 0.0121,  ..., 0.0000, 0.0183, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47630.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1200, 0.1376, 0.0019,  ..., 0.0000, 0.0774, 0.0066],
        [0.0863, 0.0992, 0.0046,  ..., 0.0000, 0.0504, 0.0147],
        [0.0341, 0.0393, 0.0107,  ..., 0.0000, 0.0142, 0.0308],
        ...,
        [0.0000, 0.0010, 0.0172,  ..., 0.0000, 0.0000, 0.0416],
        [0.0000, 0.0010, 0.0172,  ..., 0.0000, 0.0000, 0.0416],
        [0.0000, 0.0010, 0.0172,  ..., 0.0000, 0.0000, 0.0416]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479123.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5764.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(159.3143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11125.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-879.0961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.0129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1575],
        [ 0.0880],
        [-0.0420],
        ...,
        [-0.7530],
        [-0.7505],
        [-0.7498]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158131.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0103],
        [1.0114],
        ...,
        [1.0029],
        [1.0020],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367860.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0104],
        [1.0115],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367873.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6671e-02, -8.8833e-04,  2.9712e-02,  ..., -3.4328e-02,
          3.9402e-02, -1.4886e-02],
        [-2.6801e-02, -1.4281e-03,  4.7815e-02,  ..., -5.5188e-02,
          6.3015e-02, -2.3932e-02],
        [-1.9031e-02, -1.0141e-03,  3.3930e-02,  ..., -3.9188e-02,
          4.4903e-02, -1.6994e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.8036e-05,  ...,  0.0000e+00,
          5.4268e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.8036e-05,  ...,  0.0000e+00,
          5.4268e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.8036e-05,  ...,  0.0000e+00,
          5.4268e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(867.9288, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4560, device='cuda:0')



h[100].sum tensor(83.8967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9943, device='cuda:0')



h[200].sum tensor(10.9937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1292,  ..., 0.0000, 0.1712, 0.0000],
        [0.0000, 0.0000, 0.1659,  ..., 0.0000, 0.2190, 0.0000],
        [0.0000, 0.0000, 0.1758,  ..., 0.0000, 0.2319, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0022, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38954.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.2279e-01, 7.4543e-01, 0.0000e+00,  ..., 0.0000e+00, 5.1255e-01,
         0.0000e+00],
        [7.8324e-01, 9.3817e-01, 0.0000e+00,  ..., 0.0000e+00, 6.5021e-01,
         0.0000e+00],
        [8.0969e-01, 9.6914e-01, 0.0000e+00,  ..., 0.0000e+00, 6.7208e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.4984e-04, 1.7488e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3098e-02],
        [0.0000e+00, 7.4991e-04, 1.7493e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3108e-02],
        [0.0000e+00, 7.5044e-04, 1.7493e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3111e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420811.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3677.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(132.4802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9501.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-829.7122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.4565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1578],
        [ 0.1674],
        [ 0.1772],
        ...,
        [-0.7679],
        [-0.7654],
        [-0.7646]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184713.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0104],
        [1.0115],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367873.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0105],
        [1.0116],
        ...,
        [1.0028],
        [1.0019],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367885.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.9324e-03, -4.1821e-04,  1.4134e-02,  ..., -1.6346e-02,
          1.9073e-02, -7.0822e-03],
        [-3.8313e-03, -2.0199e-04,  6.7935e-03,  ..., -7.8951e-03,
          9.4984e-03, -3.4207e-03],
        [-4.1011e-03, -2.1622e-04,  7.2765e-03,  ..., -8.4511e-03,
          1.0128e-02, -3.6616e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -6.3804e-05,  ...,  0.0000e+00,
          5.5351e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.3804e-05,  ...,  0.0000e+00,
          5.5351e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.3804e-05,  ...,  0.0000e+00,
          5.5351e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1061.5636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9988, device='cuda:0')



h[100].sum tensor(89.5638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6125, device='cuda:0')



h[200].sum tensor(15.8877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0234,  ..., 0.0000, 0.0330, 0.0000],
        [0.0000, 0.0000, 0.0492,  ..., 0.0000, 0.0667, 0.0000],
        [0.0000, 0.0000, 0.0235,  ..., 0.0000, 0.0331, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48983., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1580, 0.1840, 0.0011,  ..., 0.0000, 0.1107, 0.0049],
        [0.2031, 0.2372, 0.0000,  ..., 0.0000, 0.1484, 0.0000],
        [0.1434, 0.1679, 0.0019,  ..., 0.0000, 0.0993, 0.0091],
        ...,
        [0.0000, 0.0007, 0.0178,  ..., 0.0000, 0.0000, 0.0442],
        [0.0000, 0.0007, 0.0178,  ..., 0.0000, 0.0000, 0.0442],
        [0.0000, 0.0007, 0.0178,  ..., 0.0000, 0.0000, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480996.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5865.0830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(175.1886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11091.1396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-903.3472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.7877, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1571],
        [ 0.1524],
        [ 0.0758],
        ...,
        [-0.7791],
        [-0.7765],
        [-0.7756]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157953.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0105],
        [1.0116],
        ...,
        [1.0028],
        [1.0019],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367885.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0107],
        [1.0117],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367897.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2811e-02, -6.6825e-04,  2.2910e-02,  ..., -2.6419e-02,
          3.0520e-02, -1.1437e-02],
        [ 0.0000e+00,  0.0000e+00, -5.4591e-05,  ...,  0.0000e+00,
          5.6304e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4591e-05,  ...,  0.0000e+00,
          5.6304e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.4591e-05,  ...,  0.0000e+00,
          5.6304e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4591e-05,  ...,  0.0000e+00,
          5.6304e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.4591e-05,  ...,  0.0000e+00,
          5.6304e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1113.7468, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3969, device='cuda:0')



h[100].sum tensor(91.5114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3857, device='cuda:0')



h[200].sum tensor(16.5957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7519, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0849,  ..., 0.0000, 0.1133, 0.0000],
        [0.0000, 0.0000, 0.0577,  ..., 0.0000, 0.0777, 0.0000],
        [0.0000, 0.0000, 0.0344,  ..., 0.0000, 0.0474, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50317.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5257, 0.6248, 0.0000,  ..., 0.0000, 0.4264, 0.0000],
        [0.4427, 0.5248, 0.0000,  ..., 0.0000, 0.3547, 0.0000],
        [0.3620, 0.4270, 0.0000,  ..., 0.0000, 0.2846, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0180,  ..., 0.0000, 0.0000, 0.0449],
        [0.0000, 0.0006, 0.0180,  ..., 0.0000, 0.0000, 0.0449],
        [0.0000, 0.0006, 0.0180,  ..., 0.0000, 0.0000, 0.0449]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486996.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6087.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(178.8531, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11154.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-911.9457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.1375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1734],
        [ 0.1824],
        [ 0.1905],
        ...,
        [-0.7893],
        [-0.7866],
        [-0.7859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-157921.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0107],
        [1.0117],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367897.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(1070.3691, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0108],
        [1.0118],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367908.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -4.3513e-05,  ...,  0.0000e+00,
          5.7188e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.3513e-05,  ...,  0.0000e+00,
          5.7188e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.3513e-05,  ...,  0.0000e+00,
          5.7188e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.3513e-05,  ...,  0.0000e+00,
          5.7188e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.3513e-05,  ...,  0.0000e+00,
          5.7188e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.3513e-05,  ...,  0.0000e+00,
          5.7188e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1080.7029, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9749, device='cuda:0')



h[100].sum tensor(90.9464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5993, device='cuda:0')



h[200].sum tensor(14.8478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7519, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0121, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0023, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48145.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0057, 0.0075, 0.0166,  ..., 0.0000, 0.0000, 0.0429],
        [0.0218, 0.0258, 0.0139,  ..., 0.0000, 0.0069, 0.0379],
        [0.0677, 0.0788, 0.0069,  ..., 0.0000, 0.0392, 0.0234],
        ...,
        [0.0000, 0.0007, 0.0181,  ..., 0.0000, 0.0000, 0.0453],
        [0.0000, 0.0007, 0.0181,  ..., 0.0000, 0.0000, 0.0453],
        [0.0000, 0.0007, 0.0181,  ..., 0.0000, 0.0000, 0.0453]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479265.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5617.6333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(166.0534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10584.6572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-891.8864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.4944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3526],
        [-0.1474],
        [ 0.0123],
        ...,
        [-0.7975],
        [-0.7950],
        [-0.7941]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187754.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0108],
        [1.0118],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367908.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0109],
        [1.0120],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367920.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8482e-03, -2.4749e-04,  8.6860e-03,  ..., -1.0012e-02,
          1.1952e-02, -4.3265e-03],
        [ 0.0000e+00,  0.0000e+00, -3.0779e-05,  ...,  0.0000e+00,
          5.8126e-04,  0.0000e+00],
        [-4.8482e-03, -2.4749e-04,  8.6860e-03,  ..., -1.0012e-02,
          1.1952e-02, -4.3265e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.0779e-05,  ...,  0.0000e+00,
          5.8126e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.0779e-05,  ...,  0.0000e+00,
          5.8126e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.0779e-05,  ...,  0.0000e+00,
          5.8126e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1658.5042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.5972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.0633, device='cuda:0')



h[100].sum tensor(106.9947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.6023, device='cuda:0')



h[200].sum tensor(30.1033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0181,  ..., 0.0000, 0.0261, 0.0000],
        [0.0000, 0.0000, 0.0374,  ..., 0.0000, 0.0512, 0.0000],
        [0.0000, 0.0000, 0.0072,  ..., 0.0000, 0.0117, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(80207.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1405, 0.1623, 0.0017,  ..., 0.0000, 0.0962, 0.0094],
        [0.1606, 0.1854, 0.0000,  ..., 0.0000, 0.1126, 0.0031],
        [0.0977, 0.1129, 0.0040,  ..., 0.0000, 0.0610, 0.0167],
        ...,
        [0.0000, 0.0007, 0.0182,  ..., 0.0000, 0.0000, 0.0457],
        [0.0000, 0.0007, 0.0182,  ..., 0.0000, 0.0000, 0.0457],
        [0.0000, 0.0007, 0.0182,  ..., 0.0000, 0.0000, 0.0457]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701331.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12357.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(286.5242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14738.0332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1079.0518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.0612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1465],
        [ 0.1435],
        [ 0.0991],
        ...,
        [-0.8057],
        [-0.8031],
        [-0.8023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195302.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0109],
        [1.0120],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367920.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0110],
        [1.0121],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367932.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.7632e-03, -1.9004e-04,  6.7653e-03,  ..., -7.7769e-03,
          9.4404e-03, -3.3578e-03],
        [-3.7632e-03, -1.9004e-04,  6.7653e-03,  ..., -7.7769e-03,
          9.4404e-03, -3.3578e-03],
        [ 0.0000e+00,  0.0000e+00, -1.2064e-05,  ...,  0.0000e+00,
          5.9992e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.2064e-05,  ...,  0.0000e+00,
          5.9992e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2064e-05,  ...,  0.0000e+00,
          5.9992e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2064e-05,  ...,  0.0000e+00,
          5.9992e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1137.2078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5285, device='cuda:0')



h[100].sum tensor(92.5066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9054, device='cuda:0')



h[200].sum tensor(14.9713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0218,  ..., 0.0000, 0.0308, 0.0000],
        [0.0000, 0.0000, 0.0171,  ..., 0.0000, 0.0248, 0.0000],
        [0.0000, 0.0000, 0.0124,  ..., 0.0000, 0.0187, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48457.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5298e-01, 1.7433e-01, 0.0000e+00,  ..., 0.0000e+00, 1.0421e-01,
         3.5499e-03],
        [1.2943e-01, 1.4703e-01, 1.4815e-03,  ..., 0.0000e+00, 8.4740e-02,
         6.8590e-03],
        [9.6364e-02, 1.0943e-01, 3.3335e-03,  ..., 0.0000e+00, 5.8092e-02,
         1.5053e-02],
        ...,
        [5.1114e-05, 9.3604e-04, 1.8238e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.5549e-02],
        [5.2006e-05, 9.4005e-04, 1.8245e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.5563e-02],
        [5.2454e-05, 9.4063e-04, 1.8248e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.5564e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(479467.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5809.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.1968, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10869.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-899.7343, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.0910, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2012],
        [ 0.1792],
        [ 0.1322],
        ...,
        [-0.8095],
        [-0.8068],
        [-0.8058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173648.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0110],
        [1.0121],
        ...,
        [1.0029],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367932.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0112],
        [1.0122],
        ...,
        [1.0030],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367944.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  1.1392e-05,  ...,  0.0000e+00,
          6.2332e-04,  0.0000e+00],
        [-9.7654e-03, -4.8782e-04,  1.7628e-02,  ..., -2.0195e-02,
          2.3601e-02, -8.7119e-03],
        [ 0.0000e+00,  0.0000e+00,  1.1392e-05,  ...,  0.0000e+00,
          6.2332e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.1392e-05,  ...,  0.0000e+00,
          6.2332e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.1392e-05,  ...,  0.0000e+00,
          6.2332e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.1392e-05,  ...,  0.0000e+00,
          6.2332e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1049.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7590, device='cuda:0')



h[100].sum tensor(89.5314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8209, device='cuda:0')



h[200].sum tensor(11.8034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4901, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 3.7320e-02,  ..., 0.0000e+00, 5.1130e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.4570e-02,  ..., 0.0000e+00, 2.1462e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.5197e-02,  ..., 0.0000e+00, 6.1412e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.6498e-05,  ..., 0.0000e+00, 2.5442e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.6510e-05,  ..., 0.0000e+00, 2.5448e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.6511e-05,  ..., 0.0000e+00, 2.5449e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44096.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1812, 0.2081, 0.0000,  ..., 0.0000, 0.1279, 0.0008],
        [0.1630, 0.1871, 0.0006,  ..., 0.0000, 0.1129, 0.0047],
        [0.2138, 0.2464, 0.0000,  ..., 0.0000, 0.1551, 0.0000],
        ...,
        [0.0006, 0.0014, 0.0183,  ..., 0.0000, 0.0000, 0.0452],
        [0.0006, 0.0014, 0.0183,  ..., 0.0000, 0.0000, 0.0452],
        [0.0006, 0.0014, 0.0183,  ..., 0.0000, 0.0000, 0.0452]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456724.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5069.6787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(138.9491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10454.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-870.4275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.1314, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2441],
        [ 0.2445],
        [ 0.2467],
        ...,
        [-0.8107],
        [-0.8075],
        [-0.8055]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175721.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0112],
        [1.0122],
        ...,
        [1.0030],
        [1.0020],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367944.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0113],
        [1.0123],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367956.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9074e-03, -4.4014e-04,  1.6130e-02,  ..., -1.8434e-02,
          2.1629e-02, -7.9453e-03],
        [-7.8621e-03, -3.8849e-04,  1.4241e-02,  ..., -1.6271e-02,
          1.9166e-02, -7.0129e-03],
        [ 0.0000e+00,  0.0000e+00,  3.4742e-05,  ...,  0.0000e+00,
          6.3772e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  3.4742e-05,  ...,  0.0000e+00,
          6.3772e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.4742e-05,  ...,  0.0000e+00,
          6.3772e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  3.4742e-05,  ...,  0.0000e+00,
          6.3772e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1179.0254, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4948, device='cuda:0')



h[100].sum tensor(92.4064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8868, device='cuda:0')



h[200].sum tensor(14.4863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0654,  ..., 0.0000, 0.0877, 0.0000],
        [0.0000, 0.0000, 0.0281,  ..., 0.0000, 0.0390, 0.0000],
        [0.0000, 0.0000, 0.0145,  ..., 0.0000, 0.0213, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47282.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2420, 0.2829, 0.0000,  ..., 0.0000, 0.1812, 0.0000],
        [0.1622, 0.1892, 0.0019,  ..., 0.0000, 0.1146, 0.0073],
        [0.0952, 0.1107, 0.0061,  ..., 0.0000, 0.0593, 0.0178],
        ...,
        [0.0013, 0.0020, 0.0183,  ..., 0.0000, 0.0000, 0.0445],
        [0.0013, 0.0020, 0.0183,  ..., 0.0000, 0.0000, 0.0445],
        [0.0013, 0.0020, 0.0183,  ..., 0.0000, 0.0000, 0.0445]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465927.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5195.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(144.9422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10534.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-881.4383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.9863, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0383],
        [ 0.0164],
        [-0.0491],
        ...,
        [-0.8120],
        [-0.8096],
        [-0.8088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188570.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0113],
        [1.0123],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367956.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0114],
        [1.0124],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367968.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.5520e-03, -1.7360e-04,  6.4884e-03,  ..., -7.3561e-03,
          9.0316e-03, -3.1678e-03],
        [-8.6665e-03, -4.2358e-04,  1.5745e-02,  ..., -1.7948e-02,
          2.1102e-02, -7.7291e-03],
        [-4.6709e-03, -2.2829e-04,  8.5133e-03,  ..., -9.6732e-03,
          1.1672e-02, -4.1657e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00,  5.9955e-05,  ...,  0.0000e+00,
          6.4848e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.9955e-05,  ...,  0.0000e+00,
          6.4848e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.9955e-05,  ...,  0.0000e+00,
          6.4848e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1064.6168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5206, device='cuda:0')



h[100].sum tensor(88.5641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1360, device='cuda:0')



h[200].sum tensor(10.4960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0459,  ..., 0.0000, 0.0622, 0.0000],
        [0.0000, 0.0000, 0.0282,  ..., 0.0000, 0.0391, 0.0000],
        [0.0000, 0.0000, 0.0467,  ..., 0.0000, 0.0633, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42882.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1853, 0.2121, 0.0000,  ..., 0.0000, 0.1300, 0.0003],
        [0.1928, 0.2195, 0.0000,  ..., 0.0000, 0.1351, 0.0000],
        [0.2376, 0.2722, 0.0000,  ..., 0.0000, 0.1725, 0.0000],
        ...,
        [0.0054, 0.0063, 0.0177,  ..., 0.0000, 0.0000, 0.0426],
        [0.0022, 0.0026, 0.0182,  ..., 0.0000, 0.0000, 0.0437],
        [0.0022, 0.0026, 0.0182,  ..., 0.0000, 0.0000, 0.0437]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(452321.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5086.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.8280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10738.2705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-857.9903, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.2642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0376],
        [ 0.0604],
        [ 0.1033],
        ...,
        [-0.5530],
        [-0.6765],
        [-0.7568]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163719.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0114],
        [1.0124],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367968.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0114],
        [1.0124],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367968.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 5.9955e-05,  ..., 0.0000e+00, 6.4848e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.9955e-05,  ..., 0.0000e+00, 6.4848e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.9955e-05,  ..., 0.0000e+00, 6.4848e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 5.9955e-05,  ..., 0.0000e+00, 6.4848e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.9955e-05,  ..., 0.0000e+00, 6.4848e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 5.9955e-05,  ..., 0.0000e+00, 6.4848e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1000.2221, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4230, device='cuda:0')



h[100].sum tensor(86.8288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9760, device='cuda:0')



h[200].sum tensor(8.7467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1438, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40386.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0026, 0.0180,  ..., 0.0000, 0.0000, 0.0432],
        [0.0047, 0.0055, 0.0176,  ..., 0.0000, 0.0000, 0.0425],
        [0.0086, 0.0098, 0.0169,  ..., 0.0000, 0.0000, 0.0412],
        ...,
        [0.0022, 0.0026, 0.0182,  ..., 0.0000, 0.0000, 0.0437],
        [0.0022, 0.0026, 0.0182,  ..., 0.0000, 0.0000, 0.0437],
        [0.0022, 0.0026, 0.0182,  ..., 0.0000, 0.0000, 0.0437]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438682., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4584.8901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(112.1160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10378.1602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-841.0266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.2239, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7642],
        [-0.5975],
        [-0.3758],
        ...,
        [-0.8034],
        [-0.7957],
        [-0.7897]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170876.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0114],
        [1.0124],
        ...,
        [1.0030],
        [1.0021],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367968.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0115],
        [1.0126],
        ...,
        [1.0030],
        [1.0021],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367979.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 6.6208e-05,  ..., 0.0000e+00, 6.5454e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.6208e-05,  ..., 0.0000e+00, 6.5454e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.6208e-05,  ..., 0.0000e+00, 6.5454e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 6.6208e-05,  ..., 0.0000e+00, 6.5454e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.6208e-05,  ..., 0.0000e+00, 6.5454e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 6.6208e-05,  ..., 0.0000e+00, 6.5454e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1073.4048, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1473, device='cuda:0')



h[100].sum tensor(88.4207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9296, device='cuda:0')



h[200].sum tensor(10.0467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42389.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0030, 0.0180,  ..., 0.0000, 0.0000, 0.0429],
        [0.0025, 0.0030, 0.0180,  ..., 0.0000, 0.0000, 0.0430],
        [0.0025, 0.0030, 0.0180,  ..., 0.0000, 0.0000, 0.0430],
        ...,
        [0.0025, 0.0030, 0.0182,  ..., 0.0000, 0.0000, 0.0434],
        [0.0025, 0.0030, 0.0182,  ..., 0.0000, 0.0000, 0.0434],
        [0.0025, 0.0030, 0.0182,  ..., 0.0000, 0.0000, 0.0434]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449076.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4880.1455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(115.7632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10628.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-849.6965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.4289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7381],
        [-0.8462],
        [-0.9272],
        ...,
        [-0.8118],
        [-0.8095],
        [-0.8089]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169223.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0115],
        [1.0126],
        ...,
        [1.0030],
        [1.0021],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367979.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0127],
        ...,
        [1.0031],
        [1.0021],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367991.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00,  5.2625e-05,  ...,  0.0000e+00,
          6.5241e-04,  0.0000e+00],
        [-8.2387e-03, -3.9391e-04,  1.5009e-02,  ..., -1.7086e-02,
          2.0155e-02, -7.3453e-03],
        [-1.2286e-02, -5.8744e-04,  2.2356e-02,  ..., -2.5481e-02,
          2.9736e-02, -1.0954e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00,  5.2625e-05,  ...,  0.0000e+00,
          6.5241e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.2625e-05,  ...,  0.0000e+00,
          6.5241e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  5.2625e-05,  ...,  0.0000e+00,
          6.5241e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1027.6326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3750, device='cuda:0')



h[100].sum tensor(87.3703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9495, device='cuda:0')



h[200].sum tensor(8.3724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0426,  ..., 0.0000, 0.0578, 0.0000],
        [0.0000, 0.0000, 0.0351,  ..., 0.0000, 0.0481, 0.0000],
        [0.0000, 0.0000, 0.0338,  ..., 0.0000, 0.0464, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39994.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.8167e-01, 2.0884e-01, 7.1153e-05,  ..., 0.0000e+00, 1.2752e-01,
         2.0345e-03],
        [2.1177e-01, 2.4515e-01, 0.0000e+00,  ..., 0.0000e+00, 1.5333e-01,
         1.5144e-03],
        [2.3822e-01, 2.7591e-01, 0.0000e+00,  ..., 0.0000e+00, 1.7508e-01,
         0.0000e+00],
        ...,
        [2.4245e-03, 3.1969e-03, 1.8263e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3621e-02],
        [2.4229e-03, 3.1967e-03, 1.8266e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3632e-02],
        [2.4234e-03, 3.1976e-03, 1.8264e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.3633e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438181.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4434.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(104.6790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10363.7061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-835.2920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.9323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1999],
        [ 0.2369],
        [ 0.2534],
        ...,
        [-0.8236],
        [-0.8210],
        [-0.8202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179272.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0117],
        [1.0127],
        ...,
        [1.0031],
        [1.0021],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367991.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0118],
        [1.0128],
        ...,
        [1.0031],
        [1.0021],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368002.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 2.9923e-05,  ..., 0.0000e+00, 6.5166e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9923e-05,  ..., 0.0000e+00, 6.5166e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9923e-05,  ..., 0.0000e+00, 6.5166e-04,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.9923e-05,  ..., 0.0000e+00, 6.5166e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9923e-05,  ..., 0.0000e+00, 6.5166e-04,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.9923e-05,  ..., 0.0000e+00, 6.5166e-04,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1128.7798, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3945, device='cuda:0')



h[100].sum tensor(90.3800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6193, device='cuda:0')



h[200].sum tensor(10.7636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46586.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0107, 0.0170,  ..., 0.0000, 0.0000, 0.0414],
        [0.0229, 0.0266, 0.0146,  ..., 0.0000, 0.0038, 0.0368],
        [0.0302, 0.0347, 0.0134,  ..., 0.0000, 0.0069, 0.0344],
        ...,
        [0.0021, 0.0033, 0.0184,  ..., 0.0000, 0.0000, 0.0441],
        [0.0021, 0.0033, 0.0184,  ..., 0.0000, 0.0000, 0.0441],
        [0.0021, 0.0033, 0.0184,  ..., 0.0000, 0.0000, 0.0441]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(486710.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5957.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(129.0208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11382.3477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-877.3430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.5446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0961],
        [-0.0891],
        [-0.0467],
        ...,
        [-0.8342],
        [-0.8316],
        [-0.8309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174527.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0118],
        [1.0128],
        ...,
        [1.0031],
        [1.0021],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368002.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(466.9867, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0119],
        [1.0129],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368014.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1426e-02, -5.3435e-04,  2.0806e-02,  ..., -2.3731e-02,
          2.7771e-02, -1.0184e-02],
        [ 0.0000e+00,  0.0000e+00,  2.6864e-06,  ...,  0.0000e+00,
          6.4324e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6864e-06,  ...,  0.0000e+00,
          6.4324e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  2.6864e-06,  ...,  0.0000e+00,
          6.4324e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6864e-06,  ...,  0.0000e+00,
          6.4324e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  2.6864e-06,  ...,  0.0000e+00,
          6.4324e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1144.5435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6946, device='cuda:0')



h[100].sum tensor(91.1487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7853, device='cuda:0')



h[200].sum tensor(10.8327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 6.5298e-02,  ..., 0.0000e+00, 8.7733e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.8171e-02,  ..., 0.0000e+00, 5.2362e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0876e-05,  ..., 0.0000e+00, 2.6040e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 1.0975e-05,  ..., 0.0000e+00, 2.6277e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0977e-05,  ..., 0.0000e+00, 2.6284e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.0978e-05,  ..., 0.0000e+00, 2.6285e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46098.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.3153e-01, 3.9242e-01, 0.0000e+00,  ..., 0.0000e+00, 2.5862e-01,
         0.0000e+00],
        [2.3255e-01, 2.7461e-01, 0.0000e+00,  ..., 0.0000e+00, 1.7467e-01,
         2.4548e-04],
        [1.1910e-01, 1.4082e-01, 1.2475e-03,  ..., 0.0000e+00, 7.9540e-02,
         4.3484e-03],
        ...,
        [1.7208e-03, 3.2649e-03, 1.8393e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.4642e-02],
        [1.7217e-03, 3.2652e-03, 1.8398e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.4654e-02],
        [1.7233e-03, 3.2653e-03, 1.8399e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.4655e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481985.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5809.3135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(126.3530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11373.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-878.7908, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.6408, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1328],
        [ 0.1480],
        [ 0.1650],
        ...,
        [-0.8480],
        [-0.8455],
        [-0.8447]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171908.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0119],
        [1.0129],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368014.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0121],
        [1.0131],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368025.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2974e-02, -6.0005e-04,  2.3622e-02,  ..., -2.6966e-02,
          3.1478e-02, -1.1562e-02],
        [-5.8284e-03, -2.6956e-04,  1.0594e-02,  ..., -1.2114e-02,
          1.4489e-02, -5.1940e-03],
        [-7.4826e-03, -3.4606e-04,  1.3610e-02,  ..., -1.5552e-02,
          1.8422e-02, -6.6681e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.1658e-05,  ...,  0.0000e+00,
          6.3192e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.1658e-05,  ...,  0.0000e+00,
          6.3192e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.1658e-05,  ...,  0.0000e+00,
          6.3192e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1302.0809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.8562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9131, device='cuda:0')



h[100].sum tensor(95.6881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6711, device='cuda:0')



h[200].sum tensor(14.7185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1150, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0685,  ..., 0.0000, 0.0921, 0.0000],
        [0.0000, 0.0000, 0.0688,  ..., 0.0000, 0.0925, 0.0000],
        [0.0000, 0.0000, 0.0220,  ..., 0.0000, 0.0313, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52798.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5058, 0.5956, 0.0000,  ..., 0.0000, 0.4033, 0.0000],
        [0.4094, 0.4809, 0.0000,  ..., 0.0000, 0.3214, 0.0000],
        [0.2668, 0.3125, 0.0000,  ..., 0.0000, 0.2015, 0.0000],
        ...,
        [0.0061, 0.0087, 0.0176,  ..., 0.0000, 0.0000, 0.0437],
        [0.0160, 0.0196, 0.0158,  ..., 0.0000, 0.0007, 0.0405],
        [0.0210, 0.0251, 0.0150,  ..., 0.0000, 0.0015, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516334.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6896.9072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(152.4587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12190.1455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-926.5344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.5015, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2228],
        [ 0.2204],
        [ 0.2160],
        ...,
        [-0.7373],
        [-0.6487],
        [-0.5908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169062.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0121],
        [1.0131],
        ...,
        [1.0031],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368025.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0122],
        [1.0132],
        ...,
        [1.0032],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368036.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -6.2177e-05,  ...,  0.0000e+00,
          6.2193e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.2177e-05,  ...,  0.0000e+00,
          6.2193e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.2177e-05,  ...,  0.0000e+00,
          6.2193e-04,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -6.2177e-05,  ...,  0.0000e+00,
          6.2193e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.2177e-05,  ...,  0.0000e+00,
          6.2193e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -6.2177e-05,  ...,  0.0000e+00,
          6.2193e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1168.8053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4058, device='cuda:0')



h[100].sum tensor(92.3606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1785, device='cuda:0')



h[200].sum tensor(10.8191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9449, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45054.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0727, 0.0867, 0.0077,  ..., 0.0000, 0.0455, 0.0209],
        [0.0225, 0.0279, 0.0145,  ..., 0.0000, 0.0053, 0.0380],
        [0.0057, 0.0088, 0.0173,  ..., 0.0000, 0.0000, 0.0437],
        ...,
        [0.0007, 0.0032, 0.0184,  ..., 0.0000, 0.0000, 0.0458],
        [0.0007, 0.0032, 0.0184,  ..., 0.0000, 0.0000, 0.0458],
        [0.0007, 0.0032, 0.0184,  ..., 0.0000, 0.0000, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(466923.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4850.8584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(120.9308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10748.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-875.1683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.8631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0162],
        [-0.1438],
        [-0.2595],
        ...,
        [-0.8762],
        [-0.8734],
        [-0.8726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201170.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0122],
        [1.0132],
        ...,
        [1.0032],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368036.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0101],
        [1.0123],
        [1.0133],
        ...,
        [1.0032],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368048.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8150e-03, -3.0822e-04,  1.2375e-02,  ..., -1.4185e-02,
          1.6868e-02, -6.0712e-03],
        [-4.9812e-03, -2.2528e-04,  9.0222e-03,  ..., -1.0368e-02,
          1.2497e-02, -4.4375e-03],
        [-3.4736e-03, -1.5710e-04,  6.2662e-03,  ..., -7.2299e-03,
          8.9028e-03, -3.0945e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.3955e-05,  ...,  0.0000e+00,
          6.2232e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.3955e-05,  ...,  0.0000e+00,
          6.2232e-04,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.3955e-05,  ...,  0.0000e+00,
          6.2232e-04,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1102.4839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1225, device='cuda:0')



h[100].sum tensor(90.4192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9159, device='cuda:0')



h[200].sum tensor(8.7973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3391, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0433,  ..., 0.0000, 0.0594, 0.0000],
        [0.0000, 0.0000, 0.0314,  ..., 0.0000, 0.0439, 0.0000],
        [0.0000, 0.0000, 0.0252,  ..., 0.0000, 0.0358, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0025, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41913.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9974e-01, 2.3307e-01, 0.0000e+00,  ..., 0.0000e+00, 1.4511e-01,
         0.0000e+00],
        [2.0083e-01, 2.3290e-01, 0.0000e+00,  ..., 0.0000e+00, 1.4474e-01,
         0.0000e+00],
        [1.8499e-01, 2.1375e-01, 0.0000e+00,  ..., 0.0000e+00, 1.3091e-01,
         6.5345e-05],
        ...,
        [3.2720e-04, 3.2923e-03, 1.8300e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.6129e-02],
        [3.2735e-04, 3.2945e-03, 1.8306e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.6144e-02],
        [3.2785e-04, 3.2958e-03, 1.8306e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.6145e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(453447.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4430.0928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(108.3614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10673.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-861.7064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-272.2058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1399],
        [ 0.1606],
        [ 0.1611],
        ...,
        [-0.8869],
        [-0.8841],
        [-0.8833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194097.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0101],
        [1.0123],
        [1.0133],
        ...,
        [1.0032],
        [1.0022],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368048.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0124],
        [1.0134],
        ...,
        [1.0032],
        [1.0023],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368059.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [-0.0098, -0.0004,  0.0178,  ..., -0.0204,  0.0240, -0.0087],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1389.3225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9696, device='cuda:0')



h[100].sum tensor(97.4576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.8084, device='cuda:0')



h[200].sum tensor(16.2458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.5614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0181,  ..., 0.0000, 0.0262, 0.0000],
        [0.0000, 0.0000, 0.0599,  ..., 0.0000, 0.0809, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56515.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.2024e-02, 6.4166e-02, 9.9463e-03,  ..., 0.0000e+00, 3.0717e-02,
         2.7336e-02],
        [1.8936e-01, 2.2722e-01, 5.4163e-03,  ..., 0.0000e+00, 1.4160e-01,
         1.5211e-02],
        [4.2075e-01, 5.0172e-01, 0.0000e+00,  ..., 0.0000e+00, 3.3859e-01,
         3.1536e-04],
        ...,
        [0.0000e+00, 3.5012e-03, 1.8281e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.6461e-02],
        [0.0000e+00, 3.5011e-03, 1.8285e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.6475e-02],
        [0.0000e+00, 3.5005e-03, 1.8286e-02,  ..., 0.0000e+00, 0.0000e+00,
         4.6475e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546980.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7363.7646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(165.3309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12693.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-951.5942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-277.0017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0026],
        [ 0.0515],
        [ 0.0883],
        ...,
        [-0.8955],
        [-0.8929],
        [-0.8921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189150.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0124],
        [1.0134],
        ...,
        [1.0032],
        [1.0023],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368059.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0125],
        [1.0135],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368071.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0006,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1178.6711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6119, device='cuda:0')



h[100].sum tensor(91.2784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2925, device='cuda:0')



h[200].sum tensor(10.6152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0899, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0026, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45912.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0038, 0.0181,  ..., 0.0000, 0.0000, 0.0462],
        [0.0009, 0.0048, 0.0179,  ..., 0.0000, 0.0000, 0.0459],
        [0.0034, 0.0072, 0.0174,  ..., 0.0000, 0.0000, 0.0452],
        ...,
        [0.0000, 0.0038, 0.0183,  ..., 0.0000, 0.0000, 0.0467],
        [0.0000, 0.0038, 0.0183,  ..., 0.0000, 0.0000, 0.0467],
        [0.0000, 0.0038, 0.0183,  ..., 0.0000, 0.0000, 0.0467]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482758., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5494.9912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(124.1553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11752.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-891.6572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-281.6161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9464],
        [-0.8254],
        [-0.6364],
        ...,
        [-0.9000],
        [-0.8964],
        [-0.8949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168783.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0125],
        [1.0135],
        ...,
        [1.0032],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368071.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0126],
        [1.0136],
        ...,
        [1.0033],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368082.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0116, -0.0005,  0.0212,  ..., -0.0242,  0.0284, -0.0103],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1015.5571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.0310, device='cuda:0')



h[100].sum tensor(86.1252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2063, device='cuda:0')



h[200].sum tensor(6.3315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0600,  ..., 0.0000, 0.0814, 0.0000],
        [0.0000, 0.0000, 0.0388,  ..., 0.0000, 0.0536, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0027, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38784.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3300, 0.3919, 0.0000,  ..., 0.0000, 0.2582, 0.0000],
        [0.2140, 0.2559, 0.0017,  ..., 0.0000, 0.1610, 0.0068],
        [0.0680, 0.0842, 0.0084,  ..., 0.0000, 0.0444, 0.0230],
        ...,
        [0.0000, 0.0041, 0.0184,  ..., 0.0000, 0.0000, 0.0470],
        [0.0000, 0.0041, 0.0184,  ..., 0.0000, 0.0000, 0.0470],
        [0.0000, 0.0041, 0.0184,  ..., 0.0000, 0.0000, 0.0470]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(438548.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3636.1890, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(97.8705, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10500.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-840.6239, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-279.3555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1124],
        [ 0.0448],
        [-0.1264],
        ...,
        [-0.9070],
        [-0.9043],
        [-0.9035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199772.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0126],
        [1.0136],
        ...,
        [1.0033],
        [1.0023],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368082.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0127],
        [1.0137],
        ...,
        [1.0033],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368094.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [-0.0039, -0.0002,  0.0071,  ..., -0.0082,  0.0101, -0.0035],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1816.9126, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.0981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.8074, device='cuda:0')



h[100].sum tensor(105.7071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(21.4608, device='cuda:0')



h[200].sum tensor(27.1031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0468,  ..., 0.0000, 0.0645, 0.0000],
        [0.0000, 0.0000, 0.0058,  ..., 0.0000, 0.0105, 0.0000],
        [0.0000, 0.0000, 0.0453,  ..., 0.0000, 0.0624, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74117.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3150, 0.3723, 0.0000,  ..., 0.0000, 0.2428, 0.0000],
        [0.2799, 0.3318, 0.0000,  ..., 0.0000, 0.2140, 0.0000],
        [0.3968, 0.4705, 0.0000,  ..., 0.0000, 0.3133, 0.0000],
        ...,
        [0.0000, 0.0044, 0.0185,  ..., 0.0000, 0.0000, 0.0472],
        [0.0000, 0.0044, 0.0185,  ..., 0.0000, 0.0000, 0.0473],
        [0.0000, 0.0044, 0.0185,  ..., 0.0000, 0.0000, 0.0473]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643190.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10440.7559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.0148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15180.4336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1061.2463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-285.5095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0871],
        [ 0.0762],
        [ 0.0659],
        ...,
        [-0.9077],
        [-0.9055],
        [-0.9050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163704.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0127],
        [1.0137],
        ...,
        [1.0033],
        [1.0024],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368094.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0128],
        [1.0138],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368105.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0149, -0.0006,  0.0273,  ..., -0.0311,  0.0365, -0.0133],
        [-0.0161, -0.0007,  0.0296,  ..., -0.0337,  0.0395, -0.0144],
        [-0.0150, -0.0006,  0.0276,  ..., -0.0314,  0.0368, -0.0134],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1307.7080, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5200, device='cuda:0')



h[100].sum tensor(92.2758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4537, device='cuda:0')



h[200].sum tensor(14.0834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1044,  ..., 0.0000, 0.1396, 0.0000],
        [0.0000, 0.0000, 0.1205,  ..., 0.0000, 0.1607, 0.0000],
        [0.0000, 0.0000, 0.1255,  ..., 0.0000, 0.1672, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0029, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53215.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6960, 0.8282, 0.0000,  ..., 0.0000, 0.5689, 0.0000],
        [0.7122, 0.8477, 0.0000,  ..., 0.0000, 0.5829, 0.0000],
        [0.6783, 0.8071, 0.0000,  ..., 0.0000, 0.5537, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0186,  ..., 0.0000, 0.0000, 0.0478],
        [0.0000, 0.0045, 0.0186,  ..., 0.0000, 0.0000, 0.0478],
        [0.0000, 0.0045, 0.0186,  ..., 0.0000, 0.0000, 0.0478]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(527138.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6776.3818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(156.6308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12922.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-938.1882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-286.5151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1132],
        [ 0.1171],
        [ 0.1253],
        ...,
        [-0.9187],
        [-0.9161],
        [-0.9152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156038.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0128],
        [1.0138],
        ...,
        [1.0033],
        [1.0024],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368105.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0129],
        [1.0139],
        ...,
        [1.0034],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368116.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0170, -0.0007,  0.0312,  ..., -0.0356,  0.0416, -0.0151],
        [-0.0119, -0.0005,  0.0218,  ..., -0.0248,  0.0293, -0.0106],
        [-0.0040, -0.0002,  0.0072,  ..., -0.0083,  0.0103, -0.0035],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1224.1511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2646, device='cuda:0')



h[100].sum tensor(90.6896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2065, device='cuda:0')



h[200].sum tensor(12.3176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1143,  ..., 0.0000, 0.1526, 0.0000],
        [0.0000, 0.0000, 0.0637,  ..., 0.0000, 0.0867, 0.0000],
        [0.0000, 0.0000, 0.0546,  ..., 0.0000, 0.0748, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48655.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5656, 0.6723, 0.0000,  ..., 0.0000, 0.4574, 0.0000],
        [0.4320, 0.5115, 0.0000,  ..., 0.0000, 0.3418, 0.0000],
        [0.3494, 0.4122, 0.0000,  ..., 0.0000, 0.2703, 0.0000],
        ...,
        [0.0000, 0.0042, 0.0186,  ..., 0.0000, 0.0000, 0.0489],
        [0.0000, 0.0042, 0.0186,  ..., 0.0000, 0.0000, 0.0489],
        [0.0000, 0.0042, 0.0186,  ..., 0.0000, 0.0000, 0.0489]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(498006.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5748.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(141.6355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12190.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-914.2365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-287.5879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1790],
        [ 0.1940],
        [ 0.2048],
        ...,
        [-0.9357],
        [-0.9329],
        [-0.9321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169971.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0129],
        [1.0139],
        ...,
        [1.0034],
        [1.0025],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368116.2188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(533.2886, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0130],
        [1.0141],
        ...,
        [1.0035],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368126.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000],
        [-0.0046, -0.0002,  0.0083,  ..., -0.0096,  0.0118, -0.0041],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1146.3824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0238, device='cuda:0')



h[100].sum tensor(89.4272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9673, device='cuda:0')



h[200].sum tensor(10.6776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6763, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0141, 0.0000],
        [0.0000, 0.0000, 0.0171,  ..., 0.0000, 0.0256, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47398.7852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0333, 0.0436, 0.0123,  ..., 0.0000, 0.0079, 0.0372],
        [0.0659, 0.0811, 0.0072,  ..., 0.0000, 0.0344, 0.0258],
        [0.1149, 0.1372, 0.0036,  ..., 0.0000, 0.0741, 0.0135],
        ...,
        [0.0000, 0.0038, 0.0186,  ..., 0.0000, 0.0000, 0.0499],
        [0.0000, 0.0038, 0.0186,  ..., 0.0000, 0.0000, 0.0499],
        [0.0000, 0.0038, 0.0186,  ..., 0.0000, 0.0000, 0.0499]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(497702.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5560.0918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.0192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11906.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-909.8879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.5912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0608],
        [ 0.0330],
        [ 0.1101],
        ...,
        [-0.9521],
        [-0.9492],
        [-0.9483]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188546.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0130],
        [1.0141],
        ...,
        [1.0035],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368126.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0131],
        [1.0142],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368137.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0128, -0.0005,  0.0235,  ..., -0.0268,  0.0316, -0.0114],
        [-0.0119, -0.0005,  0.0218,  ..., -0.0248,  0.0294, -0.0106],
        [-0.0030, -0.0001,  0.0054,  ..., -0.0063,  0.0080, -0.0027],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0007,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1273.0623, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9839, device='cuda:0')



h[100].sum tensor(93.2446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1573, device='cuda:0')



h[200].sum tensor(14.1930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.4614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1072,  ..., 0.0000, 0.1436, 0.0000],
        [0.0000, 0.0000, 0.0787,  ..., 0.0000, 0.1064, 0.0000],
        [0.0000, 0.0000, 0.1001,  ..., 0.0000, 0.1343, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0030, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50319.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5998, 0.7156, 0.0000,  ..., 0.0000, 0.4893, 0.0000],
        [0.5595, 0.6671, 0.0000,  ..., 0.0000, 0.4543, 0.0000],
        [0.5628, 0.6708, 0.0000,  ..., 0.0000, 0.4569, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0186,  ..., 0.0000, 0.0000, 0.0506],
        [0.0000, 0.0035, 0.0186,  ..., 0.0000, 0.0000, 0.0506],
        [0.0000, 0.0036, 0.0186,  ..., 0.0000, 0.0000, 0.0506]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504592.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5747.9551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(148.0995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11943.2061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-931.0350, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.0635, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0812],
        [ 0.0787],
        [ 0.0787],
        ...,
        [-0.9629],
        [-0.9599],
        [-0.9588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198007.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0131],
        [1.0142],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368137.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0131],
        [1.0143],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368148.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        [-0.0048, -0.0002,  0.0088,  ..., -0.0101,  0.0124, -0.0043],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1209.1045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0769, device='cuda:0')



h[100].sum tensor(91.8245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1027, device='cuda:0')



h[200].sum tensor(12.7440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0089,  ..., 0.0000, 0.0148, 0.0000],
        [0.0000, 0.0000, 0.0072,  ..., 0.0000, 0.0127, 0.0000],
        [0.0000, 0.0000, 0.0372,  ..., 0.0000, 0.0524, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49925.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0558, 0.0696, 0.0085,  ..., 0.0000, 0.0288, 0.0303],
        [0.0862, 0.1044, 0.0053,  ..., 0.0000, 0.0508, 0.0206],
        [0.1470, 0.1747, 0.0000,  ..., 0.0000, 0.1006, 0.0035],
        ...,
        [0.0000, 0.0035, 0.0188,  ..., 0.0000, 0.0000, 0.0511],
        [0.0000, 0.0035, 0.0188,  ..., 0.0000, 0.0000, 0.0511],
        [0.0000, 0.0035, 0.0188,  ..., 0.0000, 0.0000, 0.0511]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(513210.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5877.5601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(144.9143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11933.4248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-926.2706, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.2949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0549],
        [ 0.0711],
        [ 0.1374],
        ...,
        [-0.9763],
        [-0.9734],
        [-0.9724]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205865.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0131],
        [1.0143],
        ...,
        [1.0035],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368148.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0132],
        [1.0144],
        ...,
        [1.0036],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368159.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1274.2175, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7795, device='cuda:0')



h[100].sum tensor(93.3183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.0442, device='cuda:0')



h[200].sum tensor(14.5619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.3177, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0031, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48564.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0066, 0.0183,  ..., 0.0000, 0.0000, 0.0498],
        [0.0007, 0.0049, 0.0185,  ..., 0.0000, 0.0000, 0.0504],
        [0.0031, 0.0079, 0.0179,  ..., 0.0000, 0.0000, 0.0496],
        ...,
        [0.0000, 0.0037, 0.0189,  ..., 0.0000, 0.0000, 0.0513],
        [0.0000, 0.0037, 0.0189,  ..., 0.0000, 0.0000, 0.0513],
        [0.0000, 0.0037, 0.0189,  ..., 0.0000, 0.0000, 0.0513]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(489221.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5151.2593, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.6636, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11493.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-917.8729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.5012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6835],
        [-0.8027],
        [-0.8326],
        ...,
        [-0.9799],
        [-0.9774],
        [-0.9771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208460.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0132],
        [1.0144],
        ...,
        [1.0036],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368159.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0133],
        [1.0145],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368169.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000],
        [-0.0038, -0.0002,  0.0069,  ..., -0.0079,  0.0099, -0.0034],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1150.4253, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5178, device='cuda:0')



h[100].sum tensor(90.0217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6875, device='cuda:0')



h[200].sum tensor(11.4788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0032, 0.0000],
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0125, 0.0000],
        [0.0000, 0.0000, 0.0235,  ..., 0.0000, 0.0342, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0033, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46105.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1312, 0.1590, 0.0038,  ..., 0.0000, 0.0897, 0.0137],
        [0.1568, 0.1882, 0.0025,  ..., 0.0000, 0.1101, 0.0081],
        [0.2008, 0.2390, 0.0009,  ..., 0.0000, 0.1461, 0.0047],
        ...,
        [0.0000, 0.0040, 0.0191,  ..., 0.0000, 0.0000, 0.0513],
        [0.0154, 0.0224, 0.0164,  ..., 0.0000, 0.0026, 0.0457],
        [0.0509, 0.0640, 0.0103,  ..., 0.0000, 0.0267, 0.0328]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487211.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5251.2168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(124.8388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11691.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-903.9075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.8525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0938],
        [ 0.1141],
        [ 0.1323],
        ...,
        [-0.7781],
        [-0.5168],
        [-0.2197]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188203.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0133],
        [1.0145],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368169.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0134],
        [1.0146],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368180.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040, -0.0002,  0.0073,  ..., -0.0084,  0.0105, -0.0035],
        [-0.0038, -0.0001,  0.0069,  ..., -0.0079,  0.0100, -0.0033],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0008,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1208.3127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7176, device='cuda:0')



h[100].sum tensor(91.1202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3510, device='cuda:0')



h[200].sum tensor(12.9236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0387,  ..., 0.0000, 0.0546, 0.0000],
        [0.0000, 0.0000, 0.0191,  ..., 0.0000, 0.0287, 0.0000],
        [0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0126, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47463.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1645, 0.1942, 0.0000,  ..., 0.0000, 0.1135, 0.0026],
        [0.1184, 0.1408, 0.0037,  ..., 0.0000, 0.0757, 0.0135],
        [0.0625, 0.0759, 0.0082,  ..., 0.0000, 0.0328, 0.0286],
        ...,
        [0.0000, 0.0044, 0.0192,  ..., 0.0000, 0.0000, 0.0512],
        [0.0000, 0.0044, 0.0192,  ..., 0.0000, 0.0000, 0.0512],
        [0.0000, 0.0044, 0.0192,  ..., 0.0000, 0.0000, 0.0512]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494620.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5439.4639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(126.4206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11704.4551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-904.1772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.2425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1751],
        [ 0.1236],
        [-0.0022],
        ...,
        [-0.9840],
        [-0.9815],
        [-0.9811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203495.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0134],
        [1.0146],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368180.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0135],
        [1.0148],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368191.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1181.3884, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5698, device='cuda:0')



h[100].sum tensor(90.2653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7162, device='cuda:0')



h[200].sum tensor(12.2528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0034, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45902.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0230, 0.0293, 0.0150,  ..., 0.0000, 0.0029, 0.0433],
        [0.0201, 0.0261, 0.0156,  ..., 0.0000, 0.0013, 0.0443],
        [0.0223, 0.0289, 0.0152,  ..., 0.0000, 0.0005, 0.0434],
        ...,
        [0.0004, 0.0046, 0.0193,  ..., 0.0000, 0.0000, 0.0512],
        [0.0004, 0.0046, 0.0193,  ..., 0.0000, 0.0000, 0.0512],
        [0.0004, 0.0046, 0.0193,  ..., 0.0000, 0.0000, 0.0512]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(484474.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5352.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(116.1206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11704.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-896.3297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.4722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0745],
        [-0.0950],
        [-0.0746],
        ...,
        [-0.9919],
        [-0.9889],
        [-0.9882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191750.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0135],
        [1.0148],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368191.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0136],
        [1.0149],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368202.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1560.5280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.6204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3006, device='cuda:0')



h[100].sum tensor(99.4754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.2035, device='cuda:0')



h[200].sum tensor(21.7229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0035, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62272.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0049, 0.0192,  ..., 0.0000, 0.0000, 0.0505],
        [0.0011, 0.0049, 0.0193,  ..., 0.0000, 0.0000, 0.0506],
        [0.0011, 0.0049, 0.0193,  ..., 0.0000, 0.0000, 0.0506],
        ...,
        [0.0011, 0.0050, 0.0195,  ..., 0.0000, 0.0000, 0.0511],
        [0.0011, 0.0050, 0.0195,  ..., 0.0000, 0.0000, 0.0511],
        [0.0011, 0.0050, 0.0195,  ..., 0.0000, 0.0000, 0.0511]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587240.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8683.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(173.4438, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13803.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-991.5750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.4781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9860],
        [-1.0996],
        [-1.1736],
        ...,
        [-0.9939],
        [-0.9919],
        [-0.9918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188415.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0136],
        [1.0149],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368202.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0110],
        [1.0137],
        [1.0150],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368213.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045, -0.0002,  0.0082,  ..., -0.0094,  0.0118, -0.0040],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1183.6660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.7242, device='cuda:0')



h[100].sum tensor(90.1116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2486, device='cuda:0')



h[200].sum tensor(12.3075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0174,  ..., 0.0000, 0.0266, 0.0000],
        [0.0000, 0.0000, 0.0083,  ..., 0.0000, 0.0146, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0036, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44232.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1247, 0.1477, 0.0043,  ..., 0.0000, 0.0804, 0.0135],
        [0.0661, 0.0799, 0.0094,  ..., 0.0000, 0.0351, 0.0271],
        [0.0200, 0.0265, 0.0164,  ..., 0.0000, 0.0049, 0.0440],
        ...,
        [0.0016, 0.0052, 0.0197,  ..., 0.0000, 0.0000, 0.0512],
        [0.0016, 0.0052, 0.0197,  ..., 0.0000, 0.0000, 0.0512],
        [0.0016, 0.0052, 0.0197,  ..., 0.0000, 0.0000, 0.0512]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474184.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5190.8027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(97.2596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11626.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-882.9521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.6002, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0256],
        [-0.2379],
        [-0.5762],
        ...,
        [-1.0023],
        [-0.9994],
        [-0.9985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191373.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0110],
        [1.0137],
        [1.0150],
        ...,
        [1.0036],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368213.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0138],
        [1.0151],
        ...,
        [1.0037],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368224.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [-0.0081, -0.0003,  0.0151,  ..., -0.0171,  0.0208, -0.0072],
        [-0.0070, -0.0003,  0.0131,  ..., -0.0148,  0.0181, -0.0063],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1268.2637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9204, device='cuda:0')



h[100].sum tensor(93.0432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4631, device='cuda:0')



h[200].sum tensor(14.4364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3068, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0153,  ..., 0.0000, 0.0238, 0.0000],
        [0.0000, 0.0000, 0.0257,  ..., 0.0000, 0.0376, 0.0000],
        [0.0000, 0.0000, 0.0810,  ..., 0.0000, 0.1100, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0037, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46014.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0992, 0.1194, 0.0075,  ..., 0.0000, 0.0619, 0.0214],
        [0.1808, 0.2159, 0.0028,  ..., 0.0000, 0.1299, 0.0094],
        [0.3351, 0.3994, 0.0000,  ..., 0.0000, 0.2607, 0.0000],
        ...,
        [0.0490, 0.0600, 0.0122,  ..., 0.0000, 0.0208, 0.0339],
        [0.0374, 0.0464, 0.0141,  ..., 0.0000, 0.0141, 0.0382],
        [0.0137, 0.0188, 0.0180,  ..., 0.0000, 0.0018, 0.0470]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480763.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5438.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(95.9665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11734.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-893.1049, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.3151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0336],
        [ 0.1033],
        [ 0.1490],
        ...,
        [-0.2866],
        [-0.4408],
        [-0.6630]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196820.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0138],
        [1.0151],
        ...,
        [1.0037],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368224.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(515.1202, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0139],
        [1.0152],
        ...,
        [1.0037],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368234.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0009,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1077.7190, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.7197, device='cuda:0')



h[100].sum tensor(89.0090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0341, device='cuda:0')



h[200].sum tensor(9.7576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0038, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39943.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0099, 0.0136, 0.0185,  ..., 0.0000, 0.0000, 0.0484],
        [0.0107, 0.0148, 0.0185,  ..., 0.0000, 0.0002, 0.0480],
        [0.0207, 0.0266, 0.0169,  ..., 0.0000, 0.0055, 0.0442],
        ...,
        [0.0023, 0.0049, 0.0201,  ..., 0.0000, 0.0000, 0.0517],
        [0.0023, 0.0049, 0.0201,  ..., 0.0000, 0.0000, 0.0517],
        [0.0023, 0.0049, 0.0201,  ..., 0.0000, 0.0000, 0.0517]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456925.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4746.1777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.8111, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11312.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-858.6376, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.8751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2388],
        [-0.2594],
        [-0.2032],
        ...,
        [-1.0247],
        [-1.0217],
        [-1.0208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198754.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0139],
        [1.0152],
        ...,
        [1.0037],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368234.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0140],
        [1.0153],
        ...,
        [1.0037],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368244.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [-0.0038, -0.0001,  0.0070,  ..., -0.0080,  0.0102, -0.0034],
        [-0.0039, -0.0001,  0.0072,  ..., -0.0082,  0.0105, -0.0035],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1323.8110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9802, device='cuda:0')



h[100].sum tensor(95.2565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0492, device='cuda:0')



h[200].sum tensor(15.9670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0071,  ..., 0.0000, 0.0133, 0.0000],
        [0.0000, 0.0000, 0.0237,  ..., 0.0000, 0.0352, 0.0000],
        [0.0000, 0.0000, 0.0568,  ..., 0.0000, 0.0786, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0039, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49761.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0841, 0.0994, 0.0076,  ..., 0.0000, 0.0499, 0.0232],
        [0.1648, 0.1945, 0.0027,  ..., 0.0000, 0.1145, 0.0099],
        [0.2514, 0.2965, 0.0000,  ..., 0.0000, 0.1869, 0.0000],
        ...,
        [0.0024, 0.0049, 0.0202,  ..., 0.0000, 0.0000, 0.0522],
        [0.0024, 0.0049, 0.0202,  ..., 0.0000, 0.0000, 0.0522],
        [0.0024, 0.0049, 0.0202,  ..., 0.0000, 0.0000, 0.0522]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509261.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6464.2593, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.9899, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12383.7441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-917.3597, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.4061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0477],
        [ 0.1296],
        [ 0.1719],
        ...,
        [-1.0354],
        [-1.0324],
        [-1.0315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196122.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0140],
        [1.0153],
        ...,
        [1.0037],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368244.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0141],
        [1.0153],
        ...,
        [1.0037],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368255.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1356.9514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9477, device='cuda:0')



h[100].sum tensor(95.8204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.5842, device='cuda:0')



h[200].sum tensor(16.9580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0040, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0041, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51584.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0051, 0.0081, 0.0195,  ..., 0.0000, 0.0000, 0.0509],
        [0.0024, 0.0049, 0.0199,  ..., 0.0000, 0.0000, 0.0520],
        [0.0024, 0.0049, 0.0200,  ..., 0.0000, 0.0000, 0.0520],
        ...,
        [0.0024, 0.0050, 0.0202,  ..., 0.0000, 0.0000, 0.0525],
        [0.0024, 0.0050, 0.0202,  ..., 0.0000, 0.0000, 0.0525],
        [0.0024, 0.0050, 0.0202,  ..., 0.0000, 0.0000, 0.0525]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531583.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7087.1318, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.1830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12789.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-927.9753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.6823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7694],
        [-0.9991],
        [-1.1431],
        ...,
        [-1.0444],
        [-1.0414],
        [-1.0405]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202042., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0141],
        [1.0153],
        ...,
        [1.0037],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368255.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0142],
        [1.0154],
        ...,
        [1.0037],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368266.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [-0.0075, -0.0003,  0.0140,  ..., -0.0158,  0.0194, -0.0066],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1290.2513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6264, device='cuda:0')



h[100].sum tensor(93.9362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3006, device='cuda:0')



h[200].sum tensor(15.5503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1001, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0141,  ..., 0.0000, 0.0227, 0.0000],
        [0.0000, 0.0000, 0.0227,  ..., 0.0000, 0.0339, 0.0000],
        [0.0000, 0.0000, 0.1084,  ..., 0.0000, 0.1461, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0042, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0042, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47114.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2267, 0.2708, 0.0020,  ..., 0.0000, 0.1689, 0.0064],
        [0.3429, 0.4098, 0.0000,  ..., 0.0000, 0.2679, 0.0000],
        [0.5520, 0.6596, 0.0000,  ..., 0.0000, 0.4458, 0.0000],
        ...,
        [0.0023, 0.0050, 0.0201,  ..., 0.0000, 0.0000, 0.0529],
        [0.0023, 0.0050, 0.0201,  ..., 0.0000, 0.0000, 0.0530],
        [0.0023, 0.0050, 0.0201,  ..., 0.0000, 0.0000, 0.0530]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(494566.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5993.4268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(88.2104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12233.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-906.3995, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.1060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1947],
        [ 0.1977],
        [ 0.2023],
        ...,
        [-1.0548],
        [-1.0518],
        [-1.0506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195616.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0142],
        [1.0154],
        ...,
        [1.0037],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368266.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0143],
        [1.0155],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368277.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0357, -0.0013,  0.0673,  ..., -0.0754,  0.0889, -0.0317],
        [-0.0142, -0.0005,  0.0266,  ..., -0.0299,  0.0359, -0.0126],
        [-0.0074, -0.0003,  0.0139,  ..., -0.0157,  0.0193, -0.0066],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0010,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1354.4968, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0994, device='cuda:0')



h[100].sum tensor(95.1141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1151, device='cuda:0')



h[200].sum tensor(17.3564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1143,  ..., 0.0000, 0.1538, 0.0000],
        [0.0000, 0.0000, 0.1459,  ..., 0.0000, 0.1951, 0.0000],
        [0.0000, 0.0000, 0.0847,  ..., 0.0000, 0.1153, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0043, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51055.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.7274, 0.8706, 0.0000,  ..., 0.0000, 0.5959, 0.0000],
        [0.7417, 0.8874, 0.0000,  ..., 0.0000, 0.6078, 0.0000],
        [0.5631, 0.6714, 0.0000,  ..., 0.0000, 0.4533, 0.0000],
        ...,
        [0.0023, 0.0052, 0.0200,  ..., 0.0000, 0.0000, 0.0532],
        [0.0023, 0.0052, 0.0200,  ..., 0.0000, 0.0000, 0.0532],
        [0.0023, 0.0052, 0.0200,  ..., 0.0000, 0.0000, 0.0532]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(523580., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7017.6392, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(101.6602, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13014.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-932.0417, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-297.3094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2120],
        [ 0.2185],
        [ 0.2251],
        ...,
        [-1.0634],
        [-1.0604],
        [-1.0594]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185137.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0143],
        [1.0155],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368277.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0144],
        [1.0156],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368287.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0201, -0.0007,  0.0378,  ..., -0.0424,  0.0505, -0.0178],
        [-0.0144, -0.0005,  0.0271,  ..., -0.0305,  0.0366, -0.0128],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        [-0.0075, -0.0003,  0.0140,  ..., -0.0158,  0.0195, -0.0066]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1686.8953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9437, device='cuda:0')



h[100].sum tensor(102.5324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5591, device='cuda:0')



h[200].sum tensor(25.7142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1445,  ..., 0.0000, 0.1933, 0.0000],
        [0.0000, 0.0000, 0.0699,  ..., 0.0000, 0.0959, 0.0000],
        [0.0000, 0.0000, 0.0380,  ..., 0.0000, 0.0542, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0144,  ..., 0.0000, 0.0233, 0.0000],
        [0.0000, 0.0000, 0.0259,  ..., 0.0000, 0.0385, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65820.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.7371, 0.8839, 0.0000,  ..., 0.0000, 0.6050, 0.0000],
        [0.4546, 0.5449, 0.0000,  ..., 0.0000, 0.3634, 0.0000],
        [0.2414, 0.2898, 0.0027,  ..., 0.0000, 0.1818, 0.0095],
        ...,
        [0.0381, 0.0475, 0.0139,  ..., 0.0000, 0.0144, 0.0396],
        [0.0978, 0.1182, 0.0076,  ..., 0.0000, 0.0605, 0.0230],
        [0.1762, 0.2111, 0.0030,  ..., 0.0000, 0.1254, 0.0106]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623746.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10259.0771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(156.4065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15236.6826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1023.7183, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.0230, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1650],
        [ 0.1014],
        [-0.0351],
        ...,
        [-0.2592],
        [-0.0770],
        [ 0.0744]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172723.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0144],
        [1.0156],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368287.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0145],
        [1.0157],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368298.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0011,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1556.2688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6207, device='cuda:0')



h[100].sum tensor(98.9294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1685, device='cuda:0')



h[200].sum tensor(22.7427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0193, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0044, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55310.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.0478, 0.0133,  ..., 0.0000, 0.0130, 0.0393],
        [0.0391, 0.0482, 0.0132,  ..., 0.0000, 0.0130, 0.0393],
        [0.0387, 0.0477, 0.0133,  ..., 0.0000, 0.0101, 0.0394],
        ...,
        [0.0022, 0.0056, 0.0197,  ..., 0.0000, 0.0000, 0.0535],
        [0.0022, 0.0056, 0.0197,  ..., 0.0000, 0.0000, 0.0535],
        [0.0022, 0.0056, 0.0197,  ..., 0.0000, 0.0000, 0.0535]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(536043.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7166.6709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(113.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13218.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-954.3745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-305.1035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0467],
        [-0.0017],
        [ 0.0386],
        ...,
        [-1.0803],
        [-1.0772],
        [-1.0761]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195542.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0145],
        [1.0157],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368298.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0147],
        [1.0158],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368309.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -9.3414e-05,  ...,  0.0000e+00,
          1.1003e-03,  0.0000e+00],
        [-9.8856e-03, -3.3794e-04,  1.8653e-02,  ..., -2.0938e-02,
          2.5533e-02, -8.7727e-03],
        [ 0.0000e+00,  0.0000e+00, -9.3414e-05,  ...,  0.0000e+00,
          1.1003e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -9.3414e-05,  ...,  0.0000e+00,
          1.1003e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.3414e-05,  ...,  0.0000e+00,
          1.1003e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -9.3414e-05,  ...,  0.0000e+00,
          1.1003e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1657.4236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.9830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5345, device='cuda:0')



h[100].sum tensor(100.8277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2268, device='cuda:0')



h[200].sum tensor(25.3008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0685,  ..., 0.0000, 0.0942, 0.0000],
        [0.0000, 0.0000, 0.0154,  ..., 0.0000, 0.0247, 0.0000],
        [0.0000, 0.0000, 0.0189,  ..., 0.0000, 0.0293, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63408.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1919, 0.2303, 0.0000,  ..., 0.0000, 0.1395, 0.0000],
        [0.1130, 0.1370, 0.0042,  ..., 0.0000, 0.0731, 0.0153],
        [0.0890, 0.1086, 0.0078,  ..., 0.0000, 0.0530, 0.0239],
        ...,
        [0.0019, 0.0056, 0.0195,  ..., 0.0000, 0.0000, 0.0537],
        [0.0019, 0.0056, 0.0195,  ..., 0.0000, 0.0000, 0.0537],
        [0.0019, 0.0056, 0.0195,  ..., 0.0000, 0.0000, 0.0537]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(605692.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9509.3154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(142.0858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15019.9434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1011.0424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-314.5497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1110],
        [-0.2330],
        [-0.4158],
        ...,
        [-1.0890],
        [-1.0858],
        [-1.0847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167588.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0147],
        [1.0158],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368309.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0147],
        [1.0159],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368321., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2492e-03, -1.4350e-04,  7.9832e-03,  ..., -9.0067e-03,
          1.1622e-02, -3.7702e-03],
        [-8.3916e-03, -2.8341e-04,  1.5850e-02,  ..., -1.7787e-02,
          2.1875e-02, -7.4457e-03],
        [ 0.0000e+00,  0.0000e+00, -8.6099e-05,  ...,  0.0000e+00,
          1.1061e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.6099e-05,  ...,  0.0000e+00,
          1.1061e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.6099e-05,  ...,  0.0000e+00,
          1.1061e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.6099e-05,  ...,  0.0000e+00,
          1.1061e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1211.0912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1304, device='cuda:0')



h[100].sum tensor(89.7446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3673, device='cuda:0')



h[200].sum tensor(14.6000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6414, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0409,  ..., 0.0000, 0.0581, 0.0000],
        [0.0000, 0.0000, 0.0212,  ..., 0.0000, 0.0323, 0.0000],
        [0.0000, 0.0000, 0.0584,  ..., 0.0000, 0.0811, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43192.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2983, 0.3560, 0.0000,  ..., 0.0000, 0.2289, 0.0000],
        [0.2005, 0.2399, 0.0004,  ..., 0.0000, 0.1462, 0.0032],
        [0.1929, 0.2311, 0.0000,  ..., 0.0000, 0.1400, 0.0000],
        ...,
        [0.0012, 0.0054, 0.0195,  ..., 0.0000, 0.0000, 0.0543],
        [0.0012, 0.0054, 0.0195,  ..., 0.0000, 0.0000, 0.0543],
        [0.0012, 0.0054, 0.0195,  ..., 0.0000, 0.0000, 0.0543]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(481665.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5067.0557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(67.6255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12093.4023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-884.1859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-314.2378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1826],
        [ 0.1663],
        [ 0.1329],
        ...,
        [-1.1062],
        [-1.1030],
        [-1.1019]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214480.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0147],
        [1.0159],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368321., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0121],
        [1.0148],
        [1.0160],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368332.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -7.7701e-05,  ...,  0.0000e+00,
          1.1098e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7701e-05,  ...,  0.0000e+00,
          1.1098e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7701e-05,  ...,  0.0000e+00,
          1.1098e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.7701e-05,  ...,  0.0000e+00,
          1.1098e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7701e-05,  ...,  0.0000e+00,
          1.1098e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.7701e-05,  ...,  0.0000e+00,
          1.1098e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1455.6755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9913, device='cuda:0')



h[100].sum tensor(95.1069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1614, device='cuda:0')



h[200].sum tensor(20.4705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.4667, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0045, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55226.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0004, 0.0052, 0.0193,  ..., 0.0000, 0.0000, 0.0542],
        [0.0004, 0.0052, 0.0193,  ..., 0.0000, 0.0000, 0.0543],
        [0.0004, 0.0052, 0.0193,  ..., 0.0000, 0.0000, 0.0543],
        ...,
        [0.0028, 0.0080, 0.0191,  ..., 0.0000, 0.0000, 0.0540],
        [0.0004, 0.0053, 0.0195,  ..., 0.0000, 0.0000, 0.0549],
        [0.0004, 0.0053, 0.0195,  ..., 0.0000, 0.0000, 0.0549]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558535.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7948.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(116.3684, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14453.3975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-975.8957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-326.2012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3486],
        [-1.3418],
        [-1.3139],
        ...,
        [-1.0592],
        [-1.0973],
        [-1.1122]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162959.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0121],
        [1.0148],
        [1.0160],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368332.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(527.4770, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0149],
        [1.0161],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368343.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.0192e-03, -2.6430e-04,  1.5197e-02,  ..., -1.7023e-02,
          2.1015e-02, -7.1129e-03],
        [-8.1823e-03, -2.6968e-04,  1.5508e-02,  ..., -1.7369e-02,
          2.1420e-02, -7.2575e-03],
        [-1.0861e-02, -3.5795e-04,  2.0609e-02,  ..., -2.3055e-02,
          2.8067e-02, -9.6332e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -7.5066e-05,  ...,  0.0000e+00,
          1.1130e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.5066e-05,  ...,  0.0000e+00,
          1.1130e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.5066e-05,  ...,  0.0000e+00,
          1.1130e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1376.1753, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1064, device='cuda:0')



h[100].sum tensor(93.2218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1190, device='cuda:0')



h[200].sum tensor(18.4650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0695,  ..., 0.0000, 0.0955, 0.0000],
        [0.0000, 0.0000, 0.0661,  ..., 0.0000, 0.0911, 0.0000],
        [0.0000, 0.0000, 0.0710,  ..., 0.0000, 0.0973, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49196.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3141, 0.3776, 0.0000,  ..., 0.0000, 0.2454, 0.0000],
        [0.3712, 0.4458, 0.0000,  ..., 0.0000, 0.2941, 0.0000],
        [0.3976, 0.4783, 0.0000,  ..., 0.0000, 0.3176, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0197,  ..., 0.0000, 0.0000, 0.0557],
        [0.0000, 0.0049, 0.0197,  ..., 0.0000, 0.0000, 0.0557],
        [0.0000, 0.0049, 0.0197,  ..., 0.0000, 0.0000, 0.0557]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(514663.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5690.1475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(96.1665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12654.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-926.8563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-320.0237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1610],
        [ 0.1727],
        [ 0.1734],
        ...,
        [-1.1392],
        [-1.1358],
        [-1.1347]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228227.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0149],
        [1.0161],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368343.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0150],
        [1.0162],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368354.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3165e-02, -4.2861e-04,  2.5037e-02,  ..., -2.7966e-02,
          3.3836e-02, -1.1675e-02],
        [ 0.0000e+00,  0.0000e+00, -7.0177e-05,  ...,  0.0000e+00,
          1.1169e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -7.0177e-05,  ...,  0.0000e+00,
          1.1169e-03,  0.0000e+00],
        ...,
        [-5.5895e-03, -1.8198e-04,  1.0590e-02,  ..., -1.1874e-02,
          1.5009e-02, -4.9570e-03],
        [-1.2359e-02, -4.0238e-04,  2.3501e-02,  ..., -2.6255e-02,
          3.1833e-02, -1.0960e-02],
        [-8.7659e-03, -2.8540e-04,  1.6648e-02,  ..., -1.8622e-02,
          2.2903e-02, -7.7739e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1452.3824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1248, device='cuda:0')



h[100].sum tensor(94.8857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2352, device='cuda:0')



h[200].sum tensor(20.1150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.5606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0936,  ..., 0.0000, 0.1268, 0.0000],
        [0.0000, 0.0000, 0.0605,  ..., 0.0000, 0.0837, 0.0000],
        [0.0000, 0.0000, 0.0273,  ..., 0.0000, 0.0404, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0441,  ..., 0.0000, 0.0623, 0.0000],
        [0.0000, 0.0000, 0.0776,  ..., 0.0000, 0.1061, 0.0000],
        [0.0000, 0.0000, 0.0913,  ..., 0.0000, 0.1240, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51912.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6479, 0.7816, 0.0000,  ..., 0.0000, 0.5363, 0.0000],
        [0.4965, 0.5986, 0.0000,  ..., 0.0000, 0.4046, 0.0000],
        [0.4039, 0.4871, 0.0000,  ..., 0.0000, 0.3245, 0.0000],
        ...,
        [0.2491, 0.3013, 0.0010,  ..., 0.0000, 0.1908, 0.0046],
        [0.3509, 0.4220, 0.0000,  ..., 0.0000, 0.2772, 0.0000],
        [0.3594, 0.4329, 0.0000,  ..., 0.0000, 0.2851, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531141.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6007.2271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(108.3362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12992.3818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-950.2648, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-323.2548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0091],
        [0.0205],
        [0.0300],
        ...,
        [0.0308],
        [0.1262],
        [0.1180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239852.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0150],
        [1.0162],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368354.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0150],
        [1.0163],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368365.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7553e-02, -5.6450e-04,  3.3469e-02,  ..., -3.7316e-02,
          4.4814e-02, -1.5564e-02],
        [-1.8959e-02, -6.0974e-04,  3.6156e-02,  ..., -4.0306e-02,
          4.8315e-02, -1.6811e-02],
        [-1.7418e-02, -5.6018e-04,  3.3213e-02,  ..., -3.7030e-02,
          4.4479e-02, -1.5445e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -5.7559e-05,  ...,  0.0000e+00,
          1.1260e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.7559e-05,  ...,  0.0000e+00,
          1.1260e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -5.7559e-05,  ...,  0.0000e+00,
          1.1260e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1332.9817, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4727, device='cuda:0')



h[100].sum tensor(91.5494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2155, device='cuda:0')



h[200].sum tensor(17.2289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1196,  ..., 0.0000, 0.1607, 0.0000],
        [0.0000, 0.0000, 0.1230,  ..., 0.0000, 0.1651, 0.0000],
        [0.0000, 0.0000, 0.1136,  ..., 0.0000, 0.1528, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0046, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47551.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5752, 0.6946, 0.0000,  ..., 0.0000, 0.4739, 0.0000],
        [0.5538, 0.6693, 0.0000,  ..., 0.0000, 0.4557, 0.0000],
        [0.4891, 0.5916, 0.0000,  ..., 0.0000, 0.3998, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0200,  ..., 0.0000, 0.0000, 0.0566],
        [0.0000, 0.0049, 0.0200,  ..., 0.0000, 0.0000, 0.0566],
        [0.0000, 0.0049, 0.0200,  ..., 0.0000, 0.0000, 0.0566]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(509756.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5499.0454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.4627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12910.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-929.6383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-326.7077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1268],
        [ 0.1107],
        [ 0.0725],
        ...,
        [-1.1637],
        [-1.1603],
        [-1.1591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214463.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0150],
        [1.0163],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368365.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0163],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368376.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5281e-03, -1.4384e-04,  8.6175e-03,  ..., -9.6335e-03,
          1.2427e-02, -4.0144e-03],
        [-7.0744e-03, -2.2473e-04,  1.3489e-02,  ..., -1.5051e-02,
          1.8774e-02, -6.2718e-03],
        [-1.5004e-02, -4.7662e-04,  2.8658e-02,  ..., -3.1921e-02,
          3.8539e-02, -1.3302e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.4688e-05,  ...,  0.0000e+00,
          1.1399e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.4688e-05,  ...,  0.0000e+00,
          1.1399e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.4688e-05,  ...,  0.0000e+00,
          1.1399e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1413.5081, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3513, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5086, device='cuda:0')



h[100].sum tensor(92.7934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3414, device='cuda:0')



h[200].sum tensor(19.0096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0314,  ..., 0.0000, 0.0457, 0.0000],
        [0.0000, 0.0000, 0.0721,  ..., 0.0000, 0.0988, 0.0000],
        [0.0000, 0.0000, 0.0915,  ..., 0.0000, 0.1241, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50925.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2374, 0.2849, 0.0011,  ..., 0.0000, 0.1785, 0.0053],
        [0.3878, 0.4656, 0.0000,  ..., 0.0000, 0.3082, 0.0000],
        [0.4927, 0.5920, 0.0000,  ..., 0.0000, 0.3990, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0203,  ..., 0.0000, 0.0000, 0.0568],
        [0.0000, 0.0052, 0.0203,  ..., 0.0000, 0.0000, 0.0568],
        [0.0000, 0.0052, 0.0203,  ..., 0.0000, 0.0000, 0.0568]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531000.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6106.5654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(103.9556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13421.5254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-951.9796, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-326.9272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1580],
        [ 0.1668],
        [ 0.1613],
        ...,
        [-1.1710],
        [-1.1674],
        [-1.1661]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214810.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0163],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368376.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0163],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368376.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8209e-03, -2.1667e-04,  1.3004e-02,  ..., -1.4511e-02,
          1.8142e-02, -6.0470e-03],
        [-1.2711e-02, -4.0377e-04,  2.4271e-02,  ..., -2.7042e-02,
          3.2823e-02, -1.1269e-02],
        [-3.1669e-03, -1.0060e-04,  6.0136e-03,  ..., -6.7377e-03,
          9.0338e-03, -2.8076e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.4688e-05,  ...,  0.0000e+00,
          1.1399e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.4688e-05,  ...,  0.0000e+00,
          1.1399e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.4688e-05,  ...,  0.0000e+00,
          1.1399e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1346.6355, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4594, device='cuda:0')



h[100].sum tensor(91.2268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2082, device='cuda:0')



h[200].sum tensor(17.4336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9827, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1001,  ..., 0.0000, 0.1353, 0.0000],
        [0.0000, 0.0000, 0.0545,  ..., 0.0000, 0.0759, 0.0000],
        [0.0000, 0.0000, 0.0513,  ..., 0.0000, 0.0717, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0047, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48182.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4122, 0.4952, 0.0000,  ..., 0.0000, 0.3297, 0.0000],
        [0.3467, 0.4150, 0.0000,  ..., 0.0000, 0.2717, 0.0000],
        [0.2933, 0.3501, 0.0000,  ..., 0.0000, 0.2249, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0203,  ..., 0.0000, 0.0000, 0.0568],
        [0.0000, 0.0052, 0.0203,  ..., 0.0000, 0.0000, 0.0568],
        [0.0000, 0.0052, 0.0203,  ..., 0.0000, 0.0000, 0.0568]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517796.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5559.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(93.4714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13003.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-932.0164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-325.0437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1848],
        [ 0.1879],
        [ 0.1891],
        ...,
        [-1.1714],
        [-1.1679],
        [-1.1668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224141.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0151],
        [1.0163],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368376.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0151],
        [1.0164],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368387.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6760e-02, -5.2587e-04,  3.2083e-02,  ..., -3.5684e-02,
          4.2996e-02, -1.4856e-02],
        [-1.7366e-02, -5.4488e-04,  3.3245e-02,  ..., -3.6974e-02,
          4.4509e-02, -1.5394e-02],
        [-7.8092e-03, -2.4502e-04,  1.4934e-02,  ..., -1.6626e-02,
          2.0652e-02, -6.9221e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.8451e-05,  ...,  0.0000e+00,
          1.1573e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8451e-05,  ...,  0.0000e+00,
          1.1573e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8451e-05,  ...,  0.0000e+00,
          1.1573e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1417.4381, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1669, device='cuda:0')



h[100].sum tensor(92.0691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1524, device='cuda:0')



h[200].sum tensor(18.8500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1835, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0994,  ..., 0.0000, 0.1344, 0.0000],
        [0.0000, 0.0000, 0.0999,  ..., 0.0000, 0.1350, 0.0000],
        [0.0000, 0.0000, 0.0955,  ..., 0.0000, 0.1293, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50673.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4242, 0.5126, 0.0000,  ..., 0.0000, 0.3422, 0.0000],
        [0.4277, 0.5164, 0.0000,  ..., 0.0000, 0.3449, 0.0000],
        [0.3742, 0.4518, 0.0000,  ..., 0.0000, 0.2984, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0206,  ..., 0.0000, 0.0000, 0.0567],
        [0.0000, 0.0056, 0.0206,  ..., 0.0000, 0.0000, 0.0567],
        [0.0000, 0.0056, 0.0206,  ..., 0.0000, 0.0000, 0.0567]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532718.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6345.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(101.2244, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13774.1006, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-951.9103, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-326.7986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1700],
        [ 0.1873],
        [ 0.1978],
        ...,
        [-1.1734],
        [-1.1655],
        [-1.1462]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-187816.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0151],
        [1.0164],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368387.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0125],
        [1.0152],
        [1.0165],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368398.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8319e-03, -1.1875e-04,  7.3337e-03,  ..., -8.1645e-03,
          1.0752e-02, -3.3960e-03],
        [-3.5827e-03, -1.1102e-04,  6.8555e-03,  ..., -7.6334e-03,
          1.0129e-02, -3.1752e-03],
        [-7.4146e-03, -2.2977e-04,  1.4208e-02,  ..., -1.5798e-02,
          1.9709e-02, -6.5712e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.9116e-05,  ...,  0.0000e+00,
          1.1718e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9116e-05,  ...,  0.0000e+00,
          1.1718e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9116e-05,  ...,  0.0000e+00,
          1.1718e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1240.1143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.5071, device='cuda:0')



h[100].sum tensor(87.7962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0225, device='cuda:0')



h[200].sum tensor(14.3566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2029, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0237,  ..., 0.0000, 0.0357, 0.0000],
        [0.0000, 0.0000, 0.0498,  ..., 0.0000, 0.0698, 0.0000],
        [0.0000, 0.0000, 0.0238,  ..., 0.0000, 0.0359, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43012.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1509, 0.1824, 0.0030,  ..., 0.0000, 0.1050, 0.0109],
        [0.2115, 0.2538, 0.0000,  ..., 0.0000, 0.1560, 0.0000],
        [0.1653, 0.1991, 0.0019,  ..., 0.0000, 0.1168, 0.0065],
        ...,
        [0.0000, 0.0059, 0.0209,  ..., 0.0000, 0.0000, 0.0568],
        [0.0000, 0.0059, 0.0209,  ..., 0.0000, 0.0000, 0.0568],
        [0.0000, 0.0059, 0.0209,  ..., 0.0000, 0.0000, 0.0568]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(490541.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4602.9536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(67.7819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12431.2021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-894.8082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.7025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1386],
        [ 0.1627],
        [ 0.1216],
        ...,
        [-1.1829],
        [-1.1794],
        [-1.1781]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235307.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0125],
        [1.0152],
        [1.0165],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368398.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0153],
        [1.0166],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368409.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.0212e-05,  ...,  0.0000e+00,
          1.1861e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0212e-05,  ...,  0.0000e+00,
          1.1861e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0212e-05,  ...,  0.0000e+00,
          1.1861e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0212e-05,  ...,  0.0000e+00,
          1.1861e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0212e-05,  ...,  0.0000e+00,
          1.1861e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0212e-05,  ...,  0.0000e+00,
          1.1861e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1540.4088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7099, device='cuda:0')



h[100].sum tensor(94.9818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5588, device='cuda:0')



h[200].sum tensor(21.0099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.9721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0048, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54327.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0091, 0.0164, 0.0193,  ..., 0.0000, 0.0000, 0.0529],
        [0.0118, 0.0195, 0.0189,  ..., 0.0000, 0.0000, 0.0520],
        [0.0141, 0.0221, 0.0185,  ..., 0.0000, 0.0007, 0.0512],
        ...,
        [0.0188, 0.0284, 0.0179,  ..., 0.0000, 0.0053, 0.0494],
        [0.0041, 0.0112, 0.0204,  ..., 0.0000, 0.0000, 0.0554],
        [0.0000, 0.0060, 0.0211,  ..., 0.0000, 0.0000, 0.0572]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554500., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7011.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.2956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14152.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-972.9681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-320.5085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7276],
        [-0.6166],
        [-0.5172],
        ...,
        [-0.9119],
        [-1.0708],
        [-1.1502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202120.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0153],
        [1.0166],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368409.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0153],
        [1.0167],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368420.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.6899e-03, -2.3244e-04,  1.4781e-02,  ..., -1.6409e-02,
          2.0485e-02, -6.8130e-03],
        [ 0.0000e+00,  0.0000e+00, -1.8741e-05,  ...,  0.0000e+00,
          1.2010e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8741e-05,  ...,  0.0000e+00,
          1.2010e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8741e-05,  ...,  0.0000e+00,
          1.2010e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8741e-05,  ...,  0.0000e+00,
          1.2010e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8741e-05,  ...,  0.0000e+00,
          1.2010e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1328.4005, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5513, device='cuda:0')



h[100].sum tensor(90.1857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1530, device='cuda:0')



h[200].sum tensor(15.6152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0232,  ..., 0.0000, 0.0351, 0.0000],
        [0.0000, 0.0000, 0.0150,  ..., 0.0000, 0.0244, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46737.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.3056e-01, 2.8116e-01, 2.9764e-03,  ..., 0.0000e+00, 1.7588e-01,
         8.5346e-03],
        [1.6371e-01, 2.0205e-01, 4.6594e-03,  ..., 0.0000e+00, 1.2025e-01,
         1.2661e-02],
        [1.0817e-01, 1.3607e-01, 7.7237e-03,  ..., 0.0000e+00, 7.3354e-02,
         2.1440e-02],
        ...,
        [1.4353e-05, 6.1615e-03, 2.1418e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.7464e-02],
        [1.5348e-05, 6.1651e-03, 2.1423e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.7481e-02],
        [1.5867e-05, 6.1664e-03, 2.1424e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.7479e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(516260.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5475.0493, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(73.7355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12941.8018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-919.1813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-311.9003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0493],
        [ 0.0416],
        [ 0.0402],
        ...,
        [-1.2063],
        [-1.2028],
        [-1.2016]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240817.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0153],
        [1.0167],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368420.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0128],
        [1.0154],
        [1.0168],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368430.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.4452e-03, -1.3269e-04,  8.5522e-03,  ..., -9.4924e-03,
          1.2382e-02, -3.9376e-03],
        [ 0.0000e+00,  0.0000e+00, -1.5635e-05,  ...,  0.0000e+00,
          1.2176e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5635e-05,  ...,  0.0000e+00,
          1.2176e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5635e-05,  ...,  0.0000e+00,
          1.2176e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5635e-05,  ...,  0.0000e+00,
          1.2176e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5635e-05,  ...,  0.0000e+00,
          1.2176e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1465.9333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2625, device='cuda:0')



h[100].sum tensor(93.4669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2053, device='cuda:0')



h[200].sum tensor(18.4472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2508, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0138,  ..., 0.0000, 0.0230, 0.0000],
        [0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54014.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.3140e-02, 1.0323e-01, 7.5659e-03,  ..., 0.0000e+00, 4.7567e-02,
         2.4084e-02],
        [5.3903e-02, 6.9457e-02, 1.2474e-02,  ..., 0.0000e+00, 2.3783e-02,
         3.5574e-02],
        [2.7001e-02, 3.7788e-02, 1.6885e-02,  ..., 0.0000e+00, 2.2301e-03,
         4.6482e-02],
        ...,
        [1.0389e-04, 6.3486e-03, 2.1738e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.7820e-02],
        [1.0421e-04, 6.3502e-03, 2.1743e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.7835e-02],
        [1.0309e-04, 6.3480e-03, 2.1742e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.7833e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574872.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7650.2661, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.3609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14517.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-972.8471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-312.5274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5182],
        [-0.5527],
        [-0.5049],
        ...,
        [-1.2161],
        [-1.2125],
        [-1.2114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207895.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0128],
        [1.0154],
        [1.0168],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368430.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(446.6834, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0155],
        [1.0169],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368441.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.2901e-03, -2.1490e-04,  1.4062e-02,  ..., -1.5579e-02,
          1.9577e-02, -6.4566e-03],
        [-1.2659e-02, -3.7317e-04,  2.4426e-02,  ..., -2.7053e-02,
          3.3082e-02, -1.1212e-02],
        [-4.0888e-03, -1.2053e-04,  7.8826e-03,  ..., -8.7377e-03,
          1.1523e-02, -3.6213e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.0222e-05,  ...,  0.0000e+00,
          1.2378e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.0222e-05,  ...,  0.0000e+00,
          1.2378e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.0222e-05,  ...,  0.0000e+00,
          1.2378e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1470.5620, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0848, device='cuda:0')



h[100].sum tensor(93.3600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1071, device='cuda:0')



h[200].sum tensor(18.2297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1124,  ..., 0.0000, 0.1515, 0.0000],
        [0.0000, 0.0000, 0.0779,  ..., 0.0000, 0.1066, 0.0000],
        [0.0000, 0.0000, 0.0423,  ..., 0.0000, 0.0601, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52979.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.0284e-01, 6.0747e-01, 0.0000e+00,  ..., 0.0000e+00, 4.0846e-01,
         0.0000e+00],
        [4.2037e-01, 5.0777e-01, 0.0000e+00,  ..., 0.0000e+00, 3.3688e-01,
         0.0000e+00],
        [2.8057e-01, 3.4047e-01, 1.9855e-03,  ..., 0.0000e+00, 2.1709e-01,
         5.6097e-03],
        ...,
        [2.1888e-04, 6.6706e-03, 2.2046e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8038e-02],
        [2.1837e-04, 6.6720e-03, 2.2051e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8053e-02],
        [2.1877e-04, 6.6746e-03, 2.2052e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8053e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559188.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7441.3853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(94.7154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14524.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-973.8906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-312.5599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1543],
        [ 0.1468],
        [ 0.1185],
        ...,
        [-1.2241],
        [-1.2206],
        [-1.2194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185080.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0155],
        [1.0169],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368441.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0156],
        [1.0170],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368452.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.2404e-05,  ...,  0.0000e+00,
          1.2572e-03,  0.0000e+00],
        [-6.9007e-03, -2.0088e-04,  1.3317e-02,  ..., -1.4758e-02,
          1.8644e-02, -6.1107e-03],
        [-1.3442e-02, -3.9130e-04,  2.5963e-02,  ..., -2.8747e-02,
          3.5125e-02, -1.1903e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.2404e-05,  ...,  0.0000e+00,
          1.2572e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.2404e-05,  ...,  0.0000e+00,
          1.2572e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.2404e-05,  ...,  0.0000e+00,
          1.2572e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1569.7045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0950, device='cuda:0')



h[100].sum tensor(96.4126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7717, device='cuda:0')



h[200].sum tensor(20.2080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0245,  ..., 0.0000, 0.0371, 0.0000],
        [0.0000, 0.0000, 0.0478,  ..., 0.0000, 0.0675, 0.0000],
        [0.0000, 0.0000, 0.0718,  ..., 0.0000, 0.0989, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56336.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.5952e-01, 1.9683e-01, 5.4652e-03,  ..., 0.0000e+00, 1.1370e-01,
         1.5365e-02],
        [2.9190e-01, 3.5567e-01, 1.0385e-03,  ..., 0.0000e+00, 2.2725e-01,
         3.3188e-03],
        [4.1478e-01, 5.0328e-01, 0.0000e+00,  ..., 0.0000e+00, 3.3278e-01,
         0.0000e+00],
        ...,
        [1.5786e-02, 2.5450e-02, 1.9450e-02,  ..., 0.0000e+00, 2.1517e-03,
         5.2375e-02],
        [6.0249e-05, 6.7120e-03, 2.2087e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8828e-02],
        [6.0290e-05, 6.7119e-03, 2.2088e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.8827e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575712.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7755.9150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(104.1979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14577.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-992.8041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.9519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1419],
        [ 0.1450],
        [ 0.1446],
        ...,
        [-0.8568],
        [-1.0715],
        [-1.1798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198694.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0156],
        [1.0170],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368452.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0157],
        [1.0171],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368463.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.9300e-03, -1.1297e-04,  7.5711e-03,  ..., -8.4110e-03,
          1.1195e-02, -3.4795e-03],
        [-1.0113e-02, -2.9068e-04,  1.9540e-02,  ..., -2.1643e-02,
          2.6798e-02, -8.9535e-03],
        [-1.2358e-02, -3.5522e-04,  2.3886e-02,  ..., -2.6448e-02,
          3.2464e-02, -1.0941e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.6657e-05,  ...,  0.0000e+00,
          1.2775e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.6657e-05,  ...,  0.0000e+00,
          1.2775e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.6657e-05,  ...,  0.0000e+00,
          1.2775e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1621.7338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7103, device='cuda:0')



h[100].sum tensor(98.4227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.6650, device='cuda:0')



h[200].sum tensor(21.0677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.3790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0401,  ..., 0.0000, 0.0577, 0.0000],
        [0.0000, 0.0000, 0.0742,  ..., 0.0000, 0.1021, 0.0000],
        [0.0000, 0.0000, 0.1085,  ..., 0.0000, 0.1469, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57851.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3165, 0.3873, 0.0000,  ..., 0.0000, 0.2490, 0.0000],
        [0.3977, 0.4853, 0.0000,  ..., 0.0000, 0.3190, 0.0000],
        [0.4573, 0.5577, 0.0000,  ..., 0.0000, 0.3708, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0221,  ..., 0.0000, 0.0000, 0.0598],
        [0.0000, 0.0067, 0.0221,  ..., 0.0000, 0.0000, 0.0598],
        [0.0000, 0.0067, 0.0221,  ..., 0.0000, 0.0000, 0.0598]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578846.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7558.6396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(109.7204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14263.6396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1002.3046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.2573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0253],
        [ 0.0349],
        [ 0.0458],
        ...,
        [-1.2546],
        [-1.2508],
        [-1.2497]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229758.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0157],
        [1.0171],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368463.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0157],
        [1.0172],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368474.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -4.2669e-05,  ...,  0.0000e+00,
          1.2936e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.2669e-05,  ...,  0.0000e+00,
          1.2936e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.2669e-05,  ...,  0.0000e+00,
          1.2936e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.2669e-05,  ...,  0.0000e+00,
          1.2936e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.2669e-05,  ...,  0.0000e+00,
          1.2936e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.2669e-05,  ...,  0.0000e+00,
          1.2936e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1307.0857, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1622, device='cuda:0')



h[100].sum tensor(91.8931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9379, device='cuda:0')



h[200].sum tensor(13.7975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45493.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0066, 0.0218,  ..., 0.0000, 0.0000, 0.0596],
        [0.0000, 0.0067, 0.0218,  ..., 0.0000, 0.0000, 0.0597],
        [0.0000, 0.0067, 0.0218,  ..., 0.0000, 0.0000, 0.0597],
        ...,
        [0.0000, 0.0067, 0.0221,  ..., 0.0000, 0.0000, 0.0603],
        [0.0000, 0.0067, 0.0221,  ..., 0.0000, 0.0000, 0.0604],
        [0.0000, 0.0067, 0.0221,  ..., 0.0000, 0.0000, 0.0604]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511305.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5374.2744, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(59.3182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12847.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-933.0757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.2658, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5116],
        [-1.5235],
        [-1.5272],
        ...,
        [-1.2604],
        [-1.2586],
        [-1.2576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230529.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0157],
        [1.0172],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368474.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0158],
        [1.0173],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368485.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -3.2313e-05,  ...,  0.0000e+00,
          1.3174e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.2313e-05,  ...,  0.0000e+00,
          1.3174e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.2313e-05,  ...,  0.0000e+00,
          1.3174e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.2313e-05,  ...,  0.0000e+00,
          1.3174e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.2313e-05,  ...,  0.0000e+00,
          1.3174e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.2313e-05,  ...,  0.0000e+00,
          1.3174e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1299.3148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4978, device='cuda:0')



h[100].sum tensor(91.7383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5704, device='cuda:0')



h[200].sum tensor(13.9891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44661.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[7.6645e-05, 7.0483e-03, 2.1813e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.9397e-02],
        [8.0721e-05, 7.0632e-03, 2.1853e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.9505e-02],
        [1.0415e-04, 7.0741e-03, 2.1866e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.9542e-02],
        ...,
        [8.5784e-05, 7.1519e-03, 2.2098e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0183e-02],
        [8.4974e-05, 7.1545e-03, 2.2103e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0198e-02],
        [8.4182e-05, 7.1546e-03, 2.2103e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0198e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505461.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5118.0659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(51.5378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12605.3145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-921.6246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.5093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3294],
        [-1.4201],
        [-1.4875],
        ...,
        [-1.2709],
        [-1.2674],
        [-1.2660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243705.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0158],
        [1.0173],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368485.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0159],
        [1.0174],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368496.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.3385e-05,  ...,  0.0000e+00,
          1.3475e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3385e-05,  ...,  0.0000e+00,
          1.3475e-03,  0.0000e+00],
        [-4.1518e-03, -1.1488e-04,  8.0614e-03,  ..., -8.9057e-03,
          1.1875e-02, -3.6740e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.3385e-05,  ...,  0.0000e+00,
          1.3475e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3385e-05,  ...,  0.0000e+00,
          1.3475e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3385e-05,  ...,  0.0000e+00,
          1.3475e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1415.2689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4029, device='cuda:0')



h[100].sum tensor(93.9525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1769, device='cuda:0')



h[200].sum tensor(17.2539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0121,  ..., 0.0000, 0.0213, 0.0000],
        [0.0000, 0.0000, 0.0082,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0142, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48750.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0928, 0.1158, 0.0066,  ..., 0.0000, 0.0533, 0.0217],
        [0.0662, 0.0847, 0.0110,  ..., 0.0000, 0.0315, 0.0325],
        [0.0561, 0.0729, 0.0127,  ..., 0.0000, 0.0230, 0.0366],
        ...,
        [0.0006, 0.0077, 0.0221,  ..., 0.0000, 0.0000, 0.0597],
        [0.0006, 0.0077, 0.0221,  ..., 0.0000, 0.0000, 0.0597],
        [0.0006, 0.0077, 0.0221,  ..., 0.0000, 0.0000, 0.0597]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530158.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6131.9248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(62.5486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13398.6670, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-947.0638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.1224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0803],
        [-0.0635],
        [-0.2836],
        ...,
        [-1.2707],
        [-1.2671],
        [-1.2660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228106.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0159],
        [1.0174],
        ...,
        [1.0038],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368496.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0160],
        [1.0175],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368507.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1697e-03, -8.6591e-05,  6.1871e-03,  ..., -6.8043e-03,
          9.4249e-03, -2.8045e-03],
        [-3.1697e-03, -8.6591e-05,  6.1871e-03,  ..., -6.8043e-03,
          9.4249e-03, -2.8045e-03],
        [ 0.0000e+00,  0.0000e+00,  1.1919e-05,  ...,  0.0000e+00,
          1.3746e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  1.1919e-05,  ...,  0.0000e+00,
          1.3746e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.1919e-05,  ...,  0.0000e+00,
          1.3746e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  1.1919e-05,  ...,  0.0000e+00,
          1.3746e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1639.2756, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4339, device='cuda:0')



h[100].sum tensor(98.3915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5121, device='cuda:0')



h[200].sum tensor(23.1673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.1845, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 2.2718e-02,  ..., 0.0000e+00, 3.5121e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.7088e-02,  ..., 0.0000e+00, 2.7793e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.1450e-02,  ..., 0.0000e+00, 2.0453e-02,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.9017e-05,  ..., 0.0000e+00, 5.6529e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.9029e-05,  ..., 0.0000e+00, 5.6542e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.9027e-05,  ..., 0.0000e+00, 5.6540e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59546.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1388, 0.1688, 0.0028,  ..., 0.0000, 0.0905, 0.0091],
        [0.1225, 0.1493, 0.0036,  ..., 0.0000, 0.0766, 0.0118],
        [0.0890, 0.1100, 0.0076,  ..., 0.0000, 0.0502, 0.0229],
        ...,
        [0.0016, 0.0083, 0.0221,  ..., 0.0000, 0.0000, 0.0590],
        [0.0017, 0.0083, 0.0222,  ..., 0.0000, 0.0000, 0.0590],
        [0.0016, 0.0083, 0.0222,  ..., 0.0000, 0.0000, 0.0590]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612275., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8895.6885, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(97.4854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15262.9365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1006.3616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.9970, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1614],
        [ 0.1219],
        [-0.0350],
        ...,
        [-1.2661],
        [-1.2626],
        [-1.2614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209492.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0160],
        [1.0175],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368507.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0160],
        [1.0176],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368518.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 3.6906e-05,  ..., 0.0000e+00, 1.3941e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6906e-05,  ..., 0.0000e+00, 1.3941e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6906e-05,  ..., 0.0000e+00, 1.3941e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 3.6906e-05,  ..., 0.0000e+00, 1.3941e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6906e-05,  ..., 0.0000e+00, 1.3941e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 3.6906e-05,  ..., 0.0000e+00, 1.3941e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1336.4216, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.0550, device='cuda:0')



h[100].sum tensor(91.1412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3255, device='cuda:0')



h[200].sum tensor(17.1353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0001,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46348.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0055, 0.0115, 0.0213,  ..., 0.0000, 0.0000, 0.0565],
        [0.0036, 0.0096, 0.0217,  ..., 0.0000, 0.0000, 0.0573],
        [0.0027, 0.0087, 0.0219,  ..., 0.0000, 0.0000, 0.0576],
        ...,
        [0.0028, 0.0088, 0.0222,  ..., 0.0000, 0.0000, 0.0582],
        [0.0028, 0.0088, 0.0222,  ..., 0.0000, 0.0000, 0.0582],
        [0.0028, 0.0088, 0.0222,  ..., 0.0000, 0.0000, 0.0582]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525432.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6483.2695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.7703, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13962.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-927.4933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.0343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8444],
        [-1.0746],
        [-1.2453],
        ...,
        [-1.2538],
        [-1.2503],
        [-1.2490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191005.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0160],
        [1.0176],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368518.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0161],
        [1.0176],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368528.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 4.4508e-05,  ..., 0.0000e+00, 1.3991e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4508e-05,  ..., 0.0000e+00, 1.3991e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4508e-05,  ..., 0.0000e+00, 1.3991e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.4508e-05,  ..., 0.0000e+00, 1.3991e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4508e-05,  ..., 0.0000e+00, 1.3991e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 4.4508e-05,  ..., 0.0000e+00, 1.3991e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1440.8352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5910, device='cuda:0')



h[100].sum tensor(93.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7280, device='cuda:0')



h[200].sum tensor(20.3473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3719, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0002,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49128.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0055, 0.0111, 0.0215,  ..., 0.0000, 0.0000, 0.0564],
        [0.0102, 0.0162, 0.0207,  ..., 0.0000, 0.0000, 0.0547],
        [0.0232, 0.0311, 0.0185,  ..., 0.0000, 0.0029, 0.0495],
        ...,
        [0.0036, 0.0092, 0.0222,  ..., 0.0000, 0.0000, 0.0579],
        [0.0036, 0.0092, 0.0222,  ..., 0.0000, 0.0000, 0.0579],
        [0.0036, 0.0092, 0.0222,  ..., 0.0000, 0.0000, 0.0579]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541218.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7229.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(39.0890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14605.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-946.1479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.8545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8835],
        [-0.6235],
        [-0.3213],
        ...,
        [-1.2598],
        [-1.2565],
        [-1.2554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-171291.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0161],
        [1.0176],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368528.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0162],
        [1.0177],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368539.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 2.0664e-05,  ..., 0.0000e+00, 1.3883e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.0664e-05,  ..., 0.0000e+00, 1.3883e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.0664e-05,  ..., 0.0000e+00, 1.3883e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 2.0664e-05,  ..., 0.0000e+00, 1.3883e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.0664e-05,  ..., 0.0000e+00, 1.3883e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.0664e-05,  ..., 0.0000e+00, 1.3883e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2091.2703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.1531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.6439, device='cuda:0')



h[100].sum tensor(108.8024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.1583, device='cuda:0')



h[200].sum tensor(35.6843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 0.0000e+00, 8.3715e-05,  ..., 0.0000e+00, 5.6240e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.3884e-05,  ..., 0.0000e+00, 5.6354e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.4040e-05,  ..., 0.0000e+00, 5.6458e-03,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 8.5019e-05,  ..., 0.0000e+00, 5.7116e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.5039e-05,  ..., 0.0000e+00, 5.7130e-03,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 8.5038e-05,  ..., 0.0000e+00, 5.7129e-03,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(77714.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0263, 0.0197,  ..., 0.0000, 0.0046, 0.0513],
        [0.0065, 0.0124, 0.0216,  ..., 0.0000, 0.0000, 0.0565],
        [0.0136, 0.0207, 0.0205,  ..., 0.0000, 0.0000, 0.0535],
        ...,
        [0.0038, 0.0093, 0.0223,  ..., 0.0000, 0.0000, 0.0583],
        [0.0038, 0.0093, 0.0223,  ..., 0.0000, 0.0000, 0.0583],
        [0.0038, 0.0093, 0.0223,  ..., 0.0000, 0.0000, 0.0583]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(732538.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13238.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(141.2011, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18541.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1121.4049, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.1852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3556],
        [-0.6145],
        [-0.7499],
        ...,
        [-1.2687],
        [-1.2656],
        [-1.2645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-163356.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0162],
        [1.0177],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368539.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(535.1457, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0135],
        [1.0163],
        [1.0178],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368549.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.2333e-03, -1.3579e-04,  1.0228e-02,  ..., -1.1268e-02,
          1.4746e-02, -4.6272e-03],
        [ 0.0000e+00,  0.0000e+00, -3.3043e-05,  ...,  0.0000e+00,
          1.3698e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.3043e-05,  ...,  0.0000e+00,
          1.3698e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.3043e-05,  ...,  0.0000e+00,
          1.3698e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.3043e-05,  ...,  0.0000e+00,
          1.3698e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.3043e-05,  ...,  0.0000e+00,
          1.3698e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1662.0387, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5746, device='cuda:0')



h[100].sum tensor(100.5805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0370, device='cuda:0')



h[200].sum tensor(26.4339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.5802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0262,  ..., 0.0000, 0.0398, 0.0000],
        [0.0000, 0.0000, 0.0104,  ..., 0.0000, 0.0191, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59223.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2016, 0.2460, 0.0040,  ..., 0.0000, 0.1455, 0.0105],
        [0.0917, 0.1145, 0.0100,  ..., 0.0000, 0.0550, 0.0262],
        [0.0278, 0.0383, 0.0183,  ..., 0.0000, 0.0097, 0.0481],
        ...,
        [0.0032, 0.0091, 0.0226,  ..., 0.0000, 0.0000, 0.0593],
        [0.0032, 0.0091, 0.0226,  ..., 0.0000, 0.0000, 0.0593],
        [0.0032, 0.0091, 0.0226,  ..., 0.0000, 0.0000, 0.0593]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(610789.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8942.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(68.2974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15521.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1003.6756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.2225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0516],
        [-0.1434],
        [-0.3607],
        ...,
        [-1.2890],
        [-1.2857],
        [-1.2847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213593.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0135],
        [1.0163],
        [1.0178],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368549.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0164],
        [1.0178],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368559.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00],
        [-4.6613e-03, -1.1939e-04,  9.0710e-03,  ..., -1.0044e-02,
          1.3284e-02, -4.1207e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1831.2421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.6266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8654, device='cuda:0')



h[100].sum tensor(105.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.9628, device='cuda:0')



h[200].sum tensor(30.5425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0092,  ..., 0.0000, 0.0176, 0.0000],
        [0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0154, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64559.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0243, 0.0203,  ..., 0.0000, 0.0005, 0.0539],
        [0.0463, 0.0603, 0.0153,  ..., 0.0000, 0.0174, 0.0414],
        [0.0691, 0.0868, 0.0115,  ..., 0.0000, 0.0325, 0.0323],
        ...,
        [0.0025, 0.0086, 0.0228,  ..., 0.0000, 0.0000, 0.0603],
        [0.0025, 0.0086, 0.0228,  ..., 0.0000, 0.0000, 0.0603],
        [0.0025, 0.0086, 0.0228,  ..., 0.0000, 0.0000, 0.0603]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640359.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10090.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(84.8482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16436.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1049.9882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.7754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9279],
        [-0.5656],
        [-0.2049],
        ...,
        [-1.3023],
        [-1.2989],
        [-1.2977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-189071.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0164],
        [1.0178],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368559.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0164],
        [1.0178],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368559.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5842e-03, -1.6864e-04,  1.2847e-02,  ..., -1.4187e-02,
          1.8207e-02, -5.8207e-03],
        [-1.1654e-02, -2.9848e-04,  2.2801e-02,  ..., -2.5111e-02,
          3.1185e-02, -1.0302e-02],
        [-2.1040e-02, -5.3888e-04,  4.1231e-02,  ..., -4.5335e-02,
          5.5213e-02, -1.8600e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -8.1663e-05,  ...,  0.0000e+00,
          1.3517e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1497.3276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6997, device='cuda:0')



h[100].sum tensor(98.1747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8941, device='cuda:0')



h[200].sum tensor(23.0903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8550, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0425,  ..., 0.0000, 0.0611, 0.0000],
        [0.0000, 0.0000, 0.1000,  ..., 0.0000, 0.1363, 0.0000],
        [0.0000, 0.0000, 0.1232,  ..., 0.0000, 0.1666, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51712.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2910, 0.3543, 0.0015,  ..., 0.0000, 0.2221, 0.0042],
        [0.4916, 0.5968, 0.0000,  ..., 0.0000, 0.3939, 0.0000],
        [0.6406, 0.7776, 0.0000,  ..., 0.0000, 0.5220, 0.0000],
        ...,
        [0.0277, 0.0386, 0.0187,  ..., 0.0000, 0.0098, 0.0496],
        [0.0025, 0.0086, 0.0228,  ..., 0.0000, 0.0000, 0.0603],
        [0.0025, 0.0086, 0.0228,  ..., 0.0000, 0.0000, 0.0603]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556866.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7251.5698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(35.9560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14527.6465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-968.6849, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.9398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1748],
        [ 0.1947],
        [ 0.1981],
        ...,
        [-0.5746],
        [-0.9417],
        [-1.1687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213892.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0164],
        [1.0178],
        ...,
        [1.0039],
        [1.0028],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368559.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0164],
        [1.0179],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368569.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058, -0.0001,  0.0113,  ..., -0.0125,  0.0163, -0.0051],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [-0.0058, -0.0001,  0.0113,  ..., -0.0125,  0.0163, -0.0051],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1594.3501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7346, device='cuda:0')



h[100].sum tensor(101.1617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5724, device='cuda:0')



h[200].sum tensor(25.5033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.9894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0178, 0.0000],
        [0.0000, 0.0000, 0.0417,  ..., 0.0000, 0.0604, 0.0000],
        [0.0000, 0.0000, 0.0094,  ..., 0.0000, 0.0178, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56957.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0703, 0.0895, 0.0115,  ..., 0.0000, 0.0347, 0.0318],
        [0.1322, 0.1632, 0.0020,  ..., 0.0000, 0.0862, 0.0069],
        [0.1122, 0.1389, 0.0053,  ..., 0.0000, 0.0691, 0.0158],
        ...,
        [0.0020, 0.0080, 0.0231,  ..., 0.0000, 0.0000, 0.0613],
        [0.0020, 0.0080, 0.0231,  ..., 0.0000, 0.0000, 0.0614],
        [0.0020, 0.0080, 0.0231,  ..., 0.0000, 0.0000, 0.0614]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598011.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8584.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(52.0143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15416.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1012.2280, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.7292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2152],
        [ 0.0256],
        [ 0.1439],
        ...,
        [-1.3285],
        [-1.3249],
        [-1.3238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198498.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0164],
        [1.0179],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368569.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0165],
        [1.0179],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368579.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.5335e-04,  ...,  0.0000e+00,
          1.3412e-03,  0.0000e+00],
        [-3.5737e-03, -8.9177e-05,  6.8859e-03,  ..., -7.7119e-03,
          1.0520e-02, -3.1582e-03],
        [-3.3939e-03, -8.4692e-05,  6.5319e-03,  ..., -7.3241e-03,
          1.0059e-02, -2.9993e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5335e-04,  ...,  0.0000e+00,
          1.3412e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5335e-04,  ...,  0.0000e+00,
          1.3412e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5335e-04,  ...,  0.0000e+00,
          1.3412e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1383.6711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1554, device='cuda:0')



h[100].sum tensor(96.9623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4871, device='cuda:0')



h[200].sum tensor(21.4698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0070,  ..., 0.0000, 0.0147, 0.0000],
        [0.0000, 0.0000, 0.0230,  ..., 0.0000, 0.0358, 0.0000],
        [0.0000, 0.0000, 0.0480,  ..., 0.0000, 0.0689, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48433.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0781, 0.0989, 0.0103,  ..., 0.0000, 0.0444, 0.0296],
        [0.1466, 0.1812, 0.0046,  ..., 0.0000, 0.0983, 0.0133],
        [0.2057, 0.2520, 0.0000,  ..., 0.0000, 0.1477, 0.0007],
        ...,
        [0.0014, 0.0080, 0.0233,  ..., 0.0000, 0.0000, 0.0622],
        [0.0014, 0.0080, 0.0233,  ..., 0.0000, 0.0000, 0.0622],
        [0.0014, 0.0080, 0.0233,  ..., 0.0000, 0.0000, 0.0622]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539154., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6507.8257, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.7983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14160.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-958.3256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.7082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0608],
        [ 0.1070],
        [ 0.1727],
        ...,
        [-1.3414],
        [-1.3379],
        [-1.3367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-218946.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0165],
        [1.0179],
        ...,
        [1.0038],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368579.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0166],
        [1.0180],
        ...,
        [1.0038],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368590.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1600.9360, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.3843, device='cuda:0')



h[100].sum tensor(101.7510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.9317, device='cuda:0')



h[200].sum tensor(27.1768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.4464, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54889.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0011, 0.0078, 0.0231,  ..., 0.0000, 0.0000, 0.0619],
        [0.0011, 0.0078, 0.0232,  ..., 0.0000, 0.0000, 0.0620],
        [0.0011, 0.0078, 0.0232,  ..., 0.0000, 0.0000, 0.0620],
        ...,
        [0.0011, 0.0079, 0.0234,  ..., 0.0000, 0.0000, 0.0627],
        [0.0011, 0.0079, 0.0234,  ..., 0.0000, 0.0000, 0.0627],
        [0.0011, 0.0079, 0.0234,  ..., 0.0000, 0.0000, 0.0627]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(567795.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7382.6772, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(53.5591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14867.7529, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-999.2718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.5325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2100],
        [-1.3981],
        [-1.5211],
        ...,
        [-1.3530],
        [-1.3492],
        [-1.3481]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210721.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0166],
        [1.0180],
        ...,
        [1.0038],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368590.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0167],
        [1.0180],
        ...,
        [1.0038],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368601., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1788.1757, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1833, device='cuda:0')



h[100].sum tensor(105.5344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.5856, device='cuda:0')



h[200].sum tensor(32.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.8216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65437.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0351, 0.0483, 0.0175,  ..., 0.0000, 0.0114, 0.0479],
        [0.0287, 0.0408, 0.0186,  ..., 0.0000, 0.0024, 0.0506],
        [0.0947, 0.1205, 0.0104,  ..., 0.0000, 0.0621, 0.0281],
        ...,
        [0.0010, 0.0079, 0.0234,  ..., 0.0000, 0.0000, 0.0629],
        [0.0010, 0.0079, 0.0234,  ..., 0.0000, 0.0000, 0.0629],
        [0.0010, 0.0079, 0.0234,  ..., 0.0000, 0.0000, 0.0629]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654700.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10243.7676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(97.0690, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17007.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1069.7301, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.3407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0517],
        [ 0.0602],
        [ 0.1224],
        ...,
        [-1.3614],
        [-1.3579],
        [-1.3566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188262.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0167],
        [1.0180],
        ...,
        [1.0038],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368601., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0168],
        [1.0181],
        ...,
        [1.0038],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368611.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0086, -0.0002,  0.0168,  ..., -0.0186,  0.0235, -0.0076],
        [-0.0126, -0.0003,  0.0247,  ..., -0.0272,  0.0338, -0.0111],
        [-0.0086, -0.0002,  0.0168,  ..., -0.0186,  0.0235, -0.0076],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1747.6519, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1278, device='cuda:0')



h[100].sum tensor(104.7384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0019, device='cuda:0')



h[200].sum tensor(32.1346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.0792, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0914,  ..., 0.0000, 0.1257, 0.0000],
        [0.0000, 0.0000, 0.0980,  ..., 0.0000, 0.1343, 0.0000],
        [0.0000, 0.0000, 0.1685,  ..., 0.0000, 0.2262, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62050.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.2658e-01, 6.4233e-01, 0.0000e+00,  ..., 0.0000e+00, 4.2253e-01,
         0.0000e+00],
        [6.1765e-01, 7.5295e-01, 0.0000e+00,  ..., 0.0000e+00, 5.0048e-01,
         0.0000e+00],
        [8.0195e-01, 9.7705e-01, 0.0000e+00,  ..., 0.0000e+00, 6.5849e-01,
         0.0000e+00],
        ...,
        [9.2203e-04, 8.0511e-03, 2.3350e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.2909e-02],
        [9.2480e-04, 8.0546e-03, 2.3357e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.2925e-02],
        [8.7096e-03, 1.7316e-02, 2.2056e-02,  ..., 0.0000e+00, 0.0000e+00,
         5.9738e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615883.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8825.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(81.7094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16186.9082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1046.5066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-308.7053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2560],
        [ 0.2635],
        [ 0.2679],
        ...,
        [-1.3259],
        [-1.2422],
        [-1.0917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207966.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0168],
        [1.0181],
        ...,
        [1.0038],
        [1.0027],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368611.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0169],
        [1.0182],
        ...,
        [1.0037],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368621.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1591.3198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3971, device='cuda:0')



h[100].sum tensor(101.4480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3858, device='cuda:0')



h[200].sum tensor(29.3342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7521, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56505.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0139, 0.0238, 0.0207,  ..., 0.0000, 0.0000, 0.0566],
        [0.0032, 0.0109, 0.0226,  ..., 0.0000, 0.0000, 0.0612],
        [0.0009, 0.0083, 0.0230,  ..., 0.0000, 0.0000, 0.0621],
        ...,
        [0.0009, 0.0084, 0.0232,  ..., 0.0000, 0.0000, 0.0628],
        [0.0009, 0.0084, 0.0232,  ..., 0.0000, 0.0000, 0.0628],
        [0.0009, 0.0084, 0.0232,  ..., 0.0000, 0.0000, 0.0628]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588893.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7964.3115, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.6363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15893.5137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1014.3317, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-317.6324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5939],
        [-0.9288],
        [-1.1847],
        ...,
        [-1.3670],
        [-1.3633],
        [-1.3621]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198839.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0169],
        [1.0182],
        ...,
        [1.0037],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368621.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0170],
        [1.0182],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368631.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0187, -0.0004,  0.0369,  ..., -0.0405,  0.0498, -0.0165],
        [-0.0206, -0.0005,  0.0406,  ..., -0.0446,  0.0546, -0.0182],
        [-0.0131, -0.0003,  0.0258,  ..., -0.0284,  0.0352, -0.0116],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1334.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3713, device='cuda:0')



h[100].sum tensor(95.4811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5005, device='cuda:0')



h[200].sum tensor(24.2990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1735,  ..., 0.0000, 0.2329, 0.0000],
        [0.0000, 0.0000, 0.1450,  ..., 0.0000, 0.1958, 0.0000],
        [0.0000, 0.0000, 0.1102,  ..., 0.0000, 0.1504, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45711.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.8046, 0.9862, 0.0000,  ..., 0.0000, 0.6685, 0.0000],
        [0.7332, 0.8988, 0.0000,  ..., 0.0000, 0.6063, 0.0000],
        [0.6117, 0.7503, 0.0000,  ..., 0.0000, 0.5009, 0.0000],
        ...,
        [0.0010, 0.0089, 0.0231,  ..., 0.0000, 0.0000, 0.0624],
        [0.0010, 0.0089, 0.0231,  ..., 0.0000, 0.0000, 0.0624],
        [0.0010, 0.0089, 0.0231,  ..., 0.0000, 0.0000, 0.0624]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525536.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5691.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(10.4882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14548.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-940.2343, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.6037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1527],
        [ 0.1537],
        [ 0.1598],
        ...,
        [-1.3731],
        [-1.3696],
        [-1.3685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216071.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0170],
        [1.0182],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368631.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(470.0474, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0171],
        [1.0183],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368641.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.8026e-03, -6.4618e-05,  5.3016e-03,  ..., -6.0755e-03,
          8.5899e-03, -2.4742e-03],
        [-2.8026e-03, -6.4618e-05,  5.3016e-03,  ..., -6.0755e-03,
          8.5899e-03, -2.4742e-03],
        [ 0.0000e+00,  0.0000e+00, -2.6871e-04,  ...,  0.0000e+00,
          1.3285e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.6871e-04,  ...,  0.0000e+00,
          1.3285e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6871e-04,  ...,  0.0000e+00,
          1.3285e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6871e-04,  ...,  0.0000e+00,
          1.3285e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1442.4871, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0981, device='cuda:0')



h[100].sum tensor(97.2318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0084, device='cuda:0')



h[200].sum tensor(27.1394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0188, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0188, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0188, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48763.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0644, 0.0834, 0.0112,  ..., 0.0000, 0.0299, 0.0367],
        [0.0704, 0.0904, 0.0102,  ..., 0.0000, 0.0347, 0.0345],
        [0.0653, 0.0843, 0.0111,  ..., 0.0000, 0.0304, 0.0366],
        ...,
        [0.0009, 0.0092, 0.0230,  ..., 0.0000, 0.0000, 0.0622],
        [0.0009, 0.0092, 0.0230,  ..., 0.0000, 0.0000, 0.0622],
        [0.0009, 0.0092, 0.0230,  ..., 0.0000, 0.0000, 0.0622]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542782.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6371.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(22.1514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15455.7861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-965.8844, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-332.8344, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1401],
        [-0.0584],
        [-0.0846],
        ...,
        [-1.3757],
        [-1.3722],
        [-1.3710]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177955.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0171],
        [1.0183],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368641.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0171],
        [1.0184],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368652.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [-0.0066, -0.0002,  0.0129,  ..., -0.0143,  0.0185, -0.0058],
        [-0.0126, -0.0003,  0.0249,  ..., -0.0274,  0.0341, -0.0112],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1312.9740, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7277, device='cuda:0')



h[100].sum tensor(94.0723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1445, device='cuda:0')



h[200].sum tensor(24.7195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3581, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0130,  ..., 0.0000, 0.0228, 0.0000],
        [0.0000, 0.0000, 0.0460,  ..., 0.0000, 0.0661, 0.0000],
        [0.0000, 0.0000, 0.1138,  ..., 0.0000, 0.1553, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44928.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.2597e-01, 1.6226e-01, 8.7592e-03,  ..., 0.0000e+00, 8.7809e-02,
         2.4293e-02],
        [2.9435e-01, 3.6770e-01, 1.8183e-03,  ..., 0.0000e+00, 2.3124e-01,
         5.5149e-03],
        [5.3307e-01, 6.5923e-01, 0.0000e+00,  ..., 0.0000e+00, 4.3751e-01,
         0.0000e+00],
        ...,
        [4.8690e-04, 9.2655e-03, 2.3062e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.2615e-02],
        [4.8782e-04, 9.2706e-03, 2.3070e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.2630e-02],
        [4.8946e-04, 9.2709e-03, 2.3071e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.2633e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525025.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5336.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(11.4948, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14839.9629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-935.7061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.1688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2356],
        [-0.0104],
        [ 0.0784],
        ...,
        [-1.3877],
        [-1.3840],
        [-1.3830]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209312.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0171],
        [1.0184],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368652.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0172],
        [1.0185],
        ...,
        [1.0037],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368662.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045, -0.0001,  0.0086,  ..., -0.0097,  0.0130, -0.0040],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [-0.0073, -0.0002,  0.0143,  ..., -0.0159,  0.0204, -0.0065],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1530.5667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7492, device='cuda:0')



h[100].sum tensor(98.4314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.4745, device='cuda:0')



h[200].sum tensor(29.6505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.5931, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0457,  ..., 0.0000, 0.0662, 0.0000],
        [0.0000, 0.0000, 0.0422,  ..., 0.0000, 0.0620, 0.0000],
        [0.0000, 0.0000, 0.0297,  ..., 0.0000, 0.0449, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0190,  ..., 0.0000, 0.0307, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52085.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3273, 0.4075, 0.0000,  ..., 0.0000, 0.2585, 0.0000],
        [0.2573, 0.3214, 0.0000,  ..., 0.0000, 0.1974, 0.0000],
        [0.2348, 0.2944, 0.0000,  ..., 0.0000, 0.1786, 0.0000],
        ...,
        [0.0059, 0.0168, 0.0221,  ..., 0.0000, 0.0000, 0.0606],
        [0.0335, 0.0503, 0.0174,  ..., 0.0000, 0.0179, 0.0486],
        [0.1148, 0.1500, 0.0083,  ..., 0.0000, 0.0770, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564812.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6436.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(44.7759, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15898.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-988.7564, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-342.1887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1630],
        [ 0.1711],
        [ 0.1729],
        ...,
        [-1.1754],
        [-0.8740],
        [-0.4881]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206858.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0172],
        [1.0185],
        ...,
        [1.0037],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368662.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0173],
        [1.0185],
        ...,
        [1.0036],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368672.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0079, -0.0002,  0.0154,  ..., -0.0172,  0.0219, -0.0070],
        [-0.0069, -0.0002,  0.0134,  ..., -0.0149,  0.0192, -0.0061],
        [-0.0067, -0.0001,  0.0131,  ..., -0.0146,  0.0188, -0.0059],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1510.8108, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1046, device='cuda:0')



h[100].sum tensor(97.5406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1180, device='cuda:0')



h[200].sum tensor(29.2167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0563,  ..., 0.0000, 0.0805, 0.0000],
        [0.0000, 0.0000, 0.0747,  ..., 0.0000, 0.1045, 0.0000],
        [0.0000, 0.0000, 0.0713,  ..., 0.0000, 0.1002, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52836.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3501, 0.4357, 0.0000,  ..., 0.0000, 0.2783, 0.0000],
        [0.4389, 0.5445, 0.0000,  ..., 0.0000, 0.3551, 0.0000],
        [0.4517, 0.5603, 0.0000,  ..., 0.0000, 0.3662, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0237,  ..., 0.0000, 0.0000, 0.0643],
        [0.0000, 0.0090, 0.0237,  ..., 0.0000, 0.0000, 0.0643],
        [0.0000, 0.0090, 0.0237,  ..., 0.0000, 0.0000, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571773.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6539.4033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.5433, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16184.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1006.3697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-346.8229, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1547],
        [ 0.1778],
        [ 0.1839],
        ...,
        [-1.3401],
        [-1.2757],
        [-1.2336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202784.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0173],
        [1.0185],
        ...,
        [1.0036],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368672.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0174],
        [1.0186],
        ...,
        [1.0036],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368682.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.4171e-03, -7.4683e-05,  6.4713e-03,  ..., -7.4303e-03,
          1.0232e-02, -3.0147e-03],
        [-3.4859e-03, -7.6186e-05,  6.6087e-03,  ..., -7.5799e-03,
          1.0412e-02, -3.0753e-03],
        [ 0.0000e+00,  0.0000e+00, -3.5768e-04,  ...,  0.0000e+00,
          1.3275e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.5768e-04,  ...,  0.0000e+00,
          1.3275e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.5768e-04,  ...,  0.0000e+00,
          1.3275e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.5768e-04,  ...,  0.0000e+00,
          1.3275e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1363.4108, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9584, device='cuda:0')



h[100].sum tensor(94.6833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3781, device='cuda:0')



h[200].sum tensor(26.0903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0468,  ..., 0.0000, 0.0683, 0.0000],
        [0.0000, 0.0000, 0.0227,  ..., 0.0000, 0.0359, 0.0000],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0146, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47052.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1986, 0.2520, 0.0000,  ..., 0.0000, 0.1480, 0.0018],
        [0.1434, 0.1849, 0.0037,  ..., 0.0000, 0.1010, 0.0141],
        [0.0820, 0.1103, 0.0083,  ..., 0.0000, 0.0486, 0.0297],
        ...,
        [0.0000, 0.0084, 0.0236,  ..., 0.0000, 0.0000, 0.0655],
        [0.0000, 0.0084, 0.0236,  ..., 0.0000, 0.0000, 0.0655],
        [0.0000, 0.0084, 0.0236,  ..., 0.0000, 0.0000, 0.0655]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(533480.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4823.3755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.4230, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14991.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-966.0286, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-345.6425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1860],
        [ 0.1769],
        [ 0.1481],
        ...,
        [-1.4476],
        [-1.4435],
        [-1.4416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241699.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0174],
        [1.0186],
        ...,
        [1.0036],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368682.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0175],
        [1.0187],
        ...,
        [1.0036],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368692.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1578.8083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8947, device='cuda:0')



h[100].sum tensor(99.4827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6610, device='cuda:0')



h[200].sum tensor(30.6658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1021, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0087,  ..., 0.0000, 0.0177, 0.0000],
        [0.0000, 0.0000, 0.0044,  ..., 0.0000, 0.0116, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56030.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0446, 0.0652, 0.0142,  ..., 0.0000, 0.0167, 0.0471],
        [0.0303, 0.0485, 0.0170,  ..., 0.0000, 0.0081, 0.0527],
        [0.0147, 0.0289, 0.0202,  ..., 0.0000, 0.0012, 0.0591],
        ...,
        [0.0000, 0.0081, 0.0237,  ..., 0.0000, 0.0000, 0.0666],
        [0.0000, 0.0081, 0.0237,  ..., 0.0000, 0.0000, 0.0667],
        [0.0000, 0.0081, 0.0237,  ..., 0.0000, 0.0000, 0.0667]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592645.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6612.6865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(73.6863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16369.8252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1029.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-349.8508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8922],
        [-1.0561],
        [-1.2421],
        ...,
        [-1.4686],
        [-1.4646],
        [-1.4633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230368.7031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0175],
        [1.0187],
        ...,
        [1.0036],
        [1.0025],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368692.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0176],
        [1.0188],
        ...,
        [1.0036],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368702.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1345.7700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1530, device='cuda:0')



h[100].sum tensor(94.9719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4858, device='cuda:0')



h[200].sum tensor(25.7320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0639, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49759.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0077, 0.0236,  ..., 0.0000, 0.0000, 0.0668],
        [0.0000, 0.0077, 0.0236,  ..., 0.0000, 0.0000, 0.0669],
        [0.0000, 0.0077, 0.0236,  ..., 0.0000, 0.0000, 0.0669],
        ...,
        [0.0000, 0.0078, 0.0239,  ..., 0.0000, 0.0000, 0.0677],
        [0.0000, 0.0078, 0.0239,  ..., 0.0000, 0.0000, 0.0677],
        [0.0000, 0.0078, 0.0239,  ..., 0.0000, 0.0000, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559957.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5446.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(52.0851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15698.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-998.8726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-352.9901, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6584],
        [-1.7148],
        [-1.7593],
        ...,
        [-1.4808],
        [-1.4781],
        [-1.4776]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240714.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0176],
        [1.0188],
        ...,
        [1.0036],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368702.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0177],
        [1.0189],
        ...,
        [1.0037],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368712.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1294.0393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0128, device='cuda:0')



h[100].sum tensor(94.1035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8552, device='cuda:0')



h[200].sum tensor(24.6802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0068,  ..., 0.0000, 0.0149, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46598.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0783, 0.1098, 0.0099,  ..., 0.0000, 0.0513, 0.0324],
        [0.0257, 0.0454, 0.0180,  ..., 0.0000, 0.0080, 0.0544],
        [0.0167, 0.0343, 0.0196,  ..., 0.0000, 0.0027, 0.0583],
        ...,
        [0.0000, 0.0081, 0.0241,  ..., 0.0000, 0.0000, 0.0682],
        [0.0000, 0.0081, 0.0241,  ..., 0.0000, 0.0000, 0.0682],
        [0.0000, 0.0081, 0.0241,  ..., 0.0000, 0.0000, 0.0682]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537699.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4418.8374, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(35.9552, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14856.1289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-970.5961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-348.0515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1192],
        [-0.3314],
        [-0.4697],
        ...,
        [-1.5001],
        [-1.4961],
        [-1.4947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271986.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0177],
        [1.0189],
        ...,
        [1.0037],
        [1.0026],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368712.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0178],
        [1.0190],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368722.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1466.3635, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5271, device='cuda:0')



h[100].sum tensor(97.5989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3517, device='cuda:0')



h[200].sum tensor(28.2073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51018.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0082, 0.0240,  ..., 0.0000, 0.0000, 0.0673],
        [0.0000, 0.0082, 0.0240,  ..., 0.0000, 0.0000, 0.0674],
        [0.0000, 0.0083, 0.0240,  ..., 0.0000, 0.0000, 0.0675],
        ...,
        [0.0000, 0.0084, 0.0243,  ..., 0.0000, 0.0000, 0.0683],
        [0.0000, 0.0084, 0.0243,  ..., 0.0000, 0.0000, 0.0683],
        [0.0000, 0.0084, 0.0243,  ..., 0.0000, 0.0000, 0.0683]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555681.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5236.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.7025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15614.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1004.2811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-348.2700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7286],
        [-1.7602],
        [-1.7810],
        ...,
        [-1.5056],
        [-1.5009],
        [-1.4992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236970.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0178],
        [1.0190],
        ...,
        [1.0037],
        [1.0026],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368722.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0148],
        [1.0179],
        [1.0191],
        ...,
        [1.0038],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368732.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [-0.0109, -0.0002,  0.0216,  ..., -0.0239,  0.0301, -0.0097],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1415.3015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5625, device='cuda:0')



h[100].sum tensor(95.9590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2652, device='cuda:0')



h[200].sum tensor(26.9583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0399,  ..., 0.0000, 0.0588, 0.0000],
        [0.0000, 0.0000, 0.0803,  ..., 0.0000, 0.1121, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50143.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0982, 0.1339, 0.0094,  ..., 0.0000, 0.0694, 0.0271],
        [0.2494, 0.3227, 0.0031,  ..., 0.0000, 0.1959, 0.0093],
        [0.4074, 0.5186, 0.0000,  ..., 0.0000, 0.3335, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0244,  ..., 0.0000, 0.0000, 0.0676],
        [0.0000, 0.0090, 0.0244,  ..., 0.0000, 0.0000, 0.0676],
        [0.0000, 0.0090, 0.0244,  ..., 0.0000, 0.0000, 0.0676]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558044.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5290.1450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(40.0217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15645.8623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-992.4087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-342.7634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0140],
        [ 0.0888],
        [ 0.1230],
        ...,
        [-1.5082],
        [-1.5041],
        [-1.5028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242693.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0148],
        [1.0179],
        [1.0191],
        ...,
        [1.0038],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368732.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(499.1071, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0180],
        [1.0192],
        ...,
        [1.0039],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368743.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.0934e-03, -6.2325e-05,  5.8210e-03,  ..., -6.7576e-03,
          9.5336e-03, -2.7263e-03],
        [-3.7753e-03, -7.6063e-05,  7.1957e-03,  ..., -8.2470e-03,
          1.1327e-02, -3.3272e-03],
        [ 0.0000e+00,  0.0000e+00, -4.1578e-04,  ...,  0.0000e+00,
          1.3967e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.1578e-04,  ...,  0.0000e+00,
          1.3967e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.1578e-04,  ...,  0.0000e+00,
          1.3967e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.1578e-04,  ...,  0.0000e+00,
          1.3967e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1658.3501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4206, device='cuda:0')



h[100].sum tensor(100.4165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5048, device='cuda:0')



h[200].sum tensor(31.9009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.1752, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0490,  ..., 0.0000, 0.0718, 0.0000],
        [0.0000, 0.0000, 0.0226,  ..., 0.0000, 0.0362, 0.0000],
        [0.0000, 0.0000, 0.0145,  ..., 0.0000, 0.0257, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61326.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2081, 0.2681, 0.0000,  ..., 0.0000, 0.1560, 0.0000],
        [0.1543, 0.2019, 0.0024,  ..., 0.0000, 0.1098, 0.0103],
        [0.1159, 0.1547, 0.0034,  ..., 0.0000, 0.0768, 0.0149],
        ...,
        [0.0000, 0.0095, 0.0245,  ..., 0.0000, 0.0000, 0.0667],
        [0.0000, 0.0095, 0.0245,  ..., 0.0000, 0.0000, 0.0668],
        [0.0000, 0.0095, 0.0245,  ..., 0.0000, 0.0000, 0.0668]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(650166.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8257.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(77.2768, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17660.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1055.3502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-338.1499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2104],
        [ 0.2057],
        [ 0.1833],
        ...,
        [-1.5052],
        [-1.5013],
        [-1.5000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232354.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0180],
        [1.0192],
        ...,
        [1.0039],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368743.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0182],
        [1.0193],
        ...,
        [1.0039],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368753.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1735.9211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5969, device='cuda:0')



h[100].sum tensor(101.5605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1553, device='cuda:0')



h[200].sum tensor(33.4658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0025, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62270.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0099, 0.0243,  ..., 0.0000, 0.0000, 0.0650],
        [0.0016, 0.0121, 0.0239,  ..., 0.0000, 0.0000, 0.0643],
        [0.0064, 0.0182, 0.0230,  ..., 0.0000, 0.0000, 0.0623],
        ...,
        [0.0000, 0.0101, 0.0246,  ..., 0.0000, 0.0000, 0.0659],
        [0.0050, 0.0171, 0.0236,  ..., 0.0000, 0.0000, 0.0633],
        [0.0239, 0.0406, 0.0201,  ..., 0.0000, 0.0092, 0.0547]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640305., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8098.8062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(73.2814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17590.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1057.9607, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.1893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2362],
        [-1.1344],
        [-0.9775],
        ...,
        [-1.4380],
        [-1.3087],
        [-1.0516]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227694.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0182],
        [1.0193],
        ...,
        [1.0039],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368753.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0183],
        [1.0194],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368763.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -3.9711e-04,  ...,  0.0000e+00,
          1.4065e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.9711e-04,  ...,  0.0000e+00,
          1.4065e-03,  0.0000e+00],
        [-3.9606e-03, -7.7636e-05,  7.6139e-03,  ..., -8.6651e-03,
          1.1858e-02, -3.4893e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.9711e-04,  ...,  0.0000e+00,
          1.4065e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.9711e-04,  ...,  0.0000e+00,
          1.4065e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.9711e-04,  ...,  0.0000e+00,
          1.4065e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1429.3083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8197, device='cuda:0')



h[100].sum tensor(95.0327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8544, device='cuda:0')



h[200].sum tensor(26.9471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0077,  ..., 0.0000, 0.0164, 0.0000],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0144, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51725.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0128, 0.0270, 0.0218,  ..., 0.0000, 0.0000, 0.0586],
        [0.0360, 0.0553, 0.0175,  ..., 0.0000, 0.0123, 0.0487],
        [0.0464, 0.0679, 0.0156,  ..., 0.0000, 0.0164, 0.0443],
        ...,
        [0.0000, 0.0105, 0.0247,  ..., 0.0000, 0.0000, 0.0654],
        [0.0000, 0.0105, 0.0248,  ..., 0.0000, 0.0000, 0.0655],
        [0.0000, 0.0105, 0.0248,  ..., 0.0000, 0.0000, 0.0655]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(581070.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6649.6777, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.1024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16701.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-996.2532, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-332.7025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7241],
        [-0.8836],
        [-0.9724],
        ...,
        [-1.5021],
        [-1.4983],
        [-1.4969]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195328.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0183],
        [1.0194],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368763.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0184],
        [1.0195],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368773.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1771.3323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6297, device='cuda:0')



h[100].sum tensor(102.5845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7264, device='cuda:0')



h[200].sum tensor(33.9916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62419.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0001, 0.0105, 0.0246,  ..., 0.0000, 0.0000, 0.0648],
        [0.0001, 0.0105, 0.0247,  ..., 0.0000, 0.0000, 0.0649],
        [0.0002, 0.0106, 0.0247,  ..., 0.0000, 0.0000, 0.0649],
        ...,
        [0.0497, 0.0727, 0.0158,  ..., 0.0000, 0.0214, 0.0428],
        [0.0472, 0.0696, 0.0163,  ..., 0.0000, 0.0193, 0.0440],
        [0.0356, 0.0549, 0.0185,  ..., 0.0000, 0.0120, 0.0494]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(646546.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8755.0791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.0508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18003.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1059.8822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-325.1536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5859],
        [-1.6421],
        [-1.6658],
        ...,
        [-0.2246],
        [-0.2335],
        [-0.2796]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188216.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0184],
        [1.0195],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368773.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0185],
        [1.0196],
        ...,
        [1.0041],
        [1.0030],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368783.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -4.0726e-04,  ...,  0.0000e+00,
          1.4232e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.0726e-04,  ...,  0.0000e+00,
          1.4232e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.0726e-04,  ...,  0.0000e+00,
          1.4232e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -4.0726e-04,  ...,  0.0000e+00,
          1.4232e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -4.0726e-04,  ...,  0.0000e+00,
          1.4232e-03,  0.0000e+00],
        [-4.9563e-03, -9.4511e-05,  9.6481e-03,  ..., -1.0860e-02,
          1.4546e-02, -4.3650e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1493.8082, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8609, device='cuda:0')



h[100].sum tensor(97.6138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9832, device='cuda:0')



h[200].sum tensor(27.9366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9683, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0194, 0.0000],
        [0.0000, 0.0000, 0.0081,  ..., 0.0000, 0.0169, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53336.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0182, 0.0335, 0.0215,  ..., 0.0000, 0.0026, 0.0573],
        [0.0087, 0.0217, 0.0233,  ..., 0.0000, 0.0000, 0.0617],
        [0.0179, 0.0330, 0.0216,  ..., 0.0000, 0.0017, 0.0578],
        ...,
        [0.0144, 0.0289, 0.0226,  ..., 0.0000, 0.0005, 0.0599],
        [0.0465, 0.0691, 0.0166,  ..., 0.0000, 0.0188, 0.0453],
        [0.0600, 0.0858, 0.0140,  ..., 0.0000, 0.0263, 0.0394]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(583599.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6668.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(23.6178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16396.1309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1003.5419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-316.6234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2388],
        [-0.3469],
        [-0.3683],
        ...,
        [-1.2736],
        [-1.0620],
        [-0.8809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217360.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0185],
        [1.0196],
        ...,
        [1.0041],
        [1.0030],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368783.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0185],
        [1.0196],
        ...,
        [1.0041],
        [1.0030],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368783.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1415.6592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8019, device='cuda:0')



h[100].sum tensor(95.9991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8446, device='cuda:0')



h[200].sum tensor(26.3043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50463.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.6594e-02, 4.3627e-02, 1.9897e-02,  ..., 0.0000e+00, 5.4422e-03,
         5.3813e-02],
        [1.1783e-02, 2.4832e-02, 2.2784e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0702e-02],
        [1.2663e-02, 2.5893e-02, 2.2640e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.0381e-02],
        ...,
        [7.3667e-05, 1.0711e-02, 2.5326e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.6458e-02],
        [7.4409e-05, 1.0714e-02, 2.5333e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.6474e-02],
        [7.4667e-05, 1.0715e-02, 2.5333e-02,  ..., 0.0000e+00, 0.0000e+00,
         6.6473e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568323.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6365.6504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(12.3001, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16324.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-992.5308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.9973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1602],
        [-0.3075],
        [-0.3974],
        ...,
        [-1.5299],
        [-1.5260],
        [-1.5246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-200270.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0185],
        [1.0196],
        ...,
        [1.0041],
        [1.0030],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368783.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0187],
        [1.0196],
        ...,
        [1.0042],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368793.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1276.2166, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7579, device='cuda:0')



h[100].sum tensor(94.0181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1612, device='cuda:0')



h[200].sum tensor(22.9232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3793, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46281.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0104, 0.0253,  ..., 0.0000, 0.0000, 0.0668],
        [0.0000, 0.0104, 0.0254,  ..., 0.0000, 0.0000, 0.0669],
        [0.0000, 0.0104, 0.0254,  ..., 0.0000, 0.0000, 0.0669],
        ...,
        [0.0000, 0.0106, 0.0257,  ..., 0.0000, 0.0000, 0.0677],
        [0.0000, 0.0106, 0.0257,  ..., 0.0000, 0.0000, 0.0677],
        [0.0000, 0.0106, 0.0257,  ..., 0.0000, 0.0000, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546852.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5468.0464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3.7497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15428.3291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-966.2482, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-307.1799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5842],
        [-1.4737],
        [-1.2845],
        ...,
        [-1.5507],
        [-1.5465],
        [-1.5451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228166.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0187],
        [1.0196],
        ...,
        [1.0042],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368793.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0155],
        [1.0188],
        [1.0197],
        ...,
        [1.0042],
        [1.0031],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368803.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0200, -0.0004,  0.0402,  ..., -0.0438,  0.0545, -0.0176],
        [-0.0157, -0.0003,  0.0316,  ..., -0.0345,  0.0432, -0.0138],
        [-0.0134, -0.0002,  0.0268,  ..., -0.0294,  0.0370, -0.0118],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1569.9690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8662, device='cuda:0')



h[100].sum tensor(100.6321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6452, device='cuda:0')



h[200].sum tensor(28.6861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.0820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1198,  ..., 0.0000, 0.1646, 0.0000],
        [0.0000, 0.0000, 0.1507,  ..., 0.0000, 0.2049, 0.0000],
        [0.0000, 0.0000, 0.1593,  ..., 0.0000, 0.2161, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54255.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.7086, 0.9014, 0.0000,  ..., 0.0000, 0.5923, 0.0000],
        [0.8456, 1.0739, 0.0000,  ..., 0.0000, 0.7126, 0.0000],
        [0.9036, 1.1470, 0.0000,  ..., 0.0000, 0.7635, 0.0000],
        ...,
        [0.0000, 0.0104, 0.0260,  ..., 0.0000, 0.0000, 0.0688],
        [0.0000, 0.0104, 0.0260,  ..., 0.0000, 0.0000, 0.0688],
        [0.0000, 0.0104, 0.0260,  ..., 0.0000, 0.0000, 0.0688]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578371., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6209.3940, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(28.6021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15602.6191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1010.1326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-298.6156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1244],
        [ 0.0934],
        [ 0.0650],
        ...,
        [-1.5688],
        [-1.5643],
        [-1.5628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261999.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0155],
        [1.0188],
        [1.0197],
        ...,
        [1.0042],
        [1.0031],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368803.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0157],
        [1.0189],
        [1.0198],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368813.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0161, -0.0003,  0.0324,  ..., -0.0354,  0.0444, -0.0142],
        [-0.0108, -0.0002,  0.0217,  ..., -0.0238,  0.0303, -0.0095],
        [-0.0220, -0.0004,  0.0443,  ..., -0.0482,  0.0599, -0.0193],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1697.4727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4003, device='cuda:0')



h[100].sum tensor(103.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.5996, device='cuda:0')



h[200].sum tensor(31.1291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.5675, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0942,  ..., 0.0000, 0.1311, 0.0000],
        [0.0000, 0.0000, 0.1406,  ..., 0.0000, 0.1917, 0.0000],
        [0.0000, 0.0000, 0.0762,  ..., 0.0000, 0.1077, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60983.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4938, 0.6315, 0.0000,  ..., 0.0000, 0.4028, 0.0000],
        [0.5948, 0.7584, 0.0000,  ..., 0.0000, 0.4909, 0.0000],
        [0.4728, 0.6050, 0.0000,  ..., 0.0000, 0.3842, 0.0000],
        ...,
        [0.0000, 0.0102, 0.0261,  ..., 0.0000, 0.0000, 0.0695],
        [0.0000, 0.0102, 0.0261,  ..., 0.0000, 0.0000, 0.0695],
        [0.0000, 0.0102, 0.0261,  ..., 0.0000, 0.0000, 0.0695]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(626427.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8069.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(52.8104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16975.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1066.2697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-301.5575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2250],
        [ 0.2307],
        [ 0.2080],
        ...,
        [-1.5752],
        [-1.5705],
        [-1.5685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215308.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0157],
        [1.0189],
        [1.0198],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368813.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0190],
        [1.0199],
        ...,
        [1.0043],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368823.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0110, -0.0002,  0.0221,  ..., -0.0242,  0.0308, -0.0097],
        [-0.0173, -0.0003,  0.0349,  ..., -0.0380,  0.0475, -0.0152],
        [-0.0196, -0.0004,  0.0397,  ..., -0.0431,  0.0538, -0.0173],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1406.7603, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1096, device='cuda:0')



h[100].sum tensor(97.5753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5677, device='cuda:0')



h[200].sum tensor(25.3062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1072,  ..., 0.0000, 0.1479, 0.0000],
        [0.0000, 0.0000, 0.1112,  ..., 0.0000, 0.1531, 0.0000],
        [0.0000, 0.0000, 0.1260,  ..., 0.0000, 0.1725, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49928.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6315, 0.8020, 0.0000,  ..., 0.0000, 0.5210, 0.0000],
        [0.6168, 0.7834, 0.0000,  ..., 0.0000, 0.5079, 0.0000],
        [0.5868, 0.7461, 0.0000,  ..., 0.0000, 0.4822, 0.0000],
        ...,
        [0.0000, 0.0101, 0.0260,  ..., 0.0000, 0.0000, 0.0697],
        [0.0000, 0.0101, 0.0260,  ..., 0.0000, 0.0000, 0.0697],
        [0.0000, 0.0101, 0.0260,  ..., 0.0000, 0.0000, 0.0697]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559381.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5876.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(14.3995, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15307.8418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-989.1032, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-300.9210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2266],
        [ 0.2251],
        [ 0.2094],
        ...,
        [-1.5909],
        [-1.5867],
        [-1.5857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247072.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0190],
        [1.0199],
        ...,
        [1.0043],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368823.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(478.3447, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0191],
        [1.0200],
        ...,
        [1.0043],
        [1.0032],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368834.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0004,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1363.2305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6707, device='cuda:0')



h[100].sum tensor(96.3615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7720, device='cuda:0')



h[200].sum tensor(24.9200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49610.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0127, 0.0249,  ..., 0.0000, 0.0000, 0.0675],
        [0.0281, 0.0444, 0.0202,  ..., 0.0000, 0.0090, 0.0559],
        [0.0645, 0.0896, 0.0137,  ..., 0.0000, 0.0348, 0.0391],
        ...,
        [0.0004, 0.0100, 0.0258,  ..., 0.0000, 0.0000, 0.0695],
        [0.0004, 0.0100, 0.0258,  ..., 0.0000, 0.0000, 0.0695],
        [0.0004, 0.0100, 0.0258,  ..., 0.0000, 0.0000, 0.0695]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560323.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6035.7769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(14.0935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15369.8496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-984.1542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-305.2751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8032],
        [-0.4312],
        [-0.1139],
        ...,
        [-1.5959],
        [-1.5914],
        [-1.5866]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250924.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0191],
        [1.0200],
        ...,
        [1.0043],
        [1.0032],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368834.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1272.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1267, device='cuda:0')



h[100].sum tensor(94.3489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3652, device='cuda:0')



h[200].sum tensor(23.6385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6388, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0069,  ..., 0.0000, 0.0152, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0074,  ..., 0.0000, 0.0158, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46630.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0362, 0.0531, 0.0184,  ..., 0.0000, 0.0089, 0.0526],
        [0.0220, 0.0357, 0.0212,  ..., 0.0000, 0.0000, 0.0590],
        [0.0387, 0.0562, 0.0181,  ..., 0.0000, 0.0104, 0.0516],
        ...,
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550277.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5862.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1.5330, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15261.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-963.4376, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-310.9507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1419],
        [-1.2890],
        [-1.3095],
        ...,
        [-1.6064],
        [-1.6021],
        [-1.6006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247766.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1562.4607, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7239, device='cuda:0')



h[100].sum tensor(100.2285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.5665, device='cuda:0')



h[200].sum tensor(29.5959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.9819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55523.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0118, 0.0248,  ..., 0.0000, 0.0000, 0.0674],
        [0.0047, 0.0140, 0.0244,  ..., 0.0000, 0.0000, 0.0668],
        [0.0133, 0.0244, 0.0227,  ..., 0.0000, 0.0000, 0.0631],
        ...,
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593756.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7469.4624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.8280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16483.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1025.8696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-315.0711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2626],
        [-1.0606],
        [-0.8030],
        ...,
        [-1.6037],
        [-1.5994],
        [-1.5979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-216964.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8988e-03, -6.8379e-05,  7.6394e-03,  ..., -8.5830e-03,
          1.1828e-02, -3.4301e-03],
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00],
        [-3.8988e-03, -6.8379e-05,  7.6394e-03,  ..., -8.5830e-03,
          1.1828e-02, -3.4301e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1842.8718, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.2842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7544, device='cuda:0')



h[100].sum tensor(105.9156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.4544, device='cuda:0')



h[200].sum tensor(35.3581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.9265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0143, 0.0000],
        [0.0000, 0.0000, 0.0281,  ..., 0.0000, 0.0443, 0.0000],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0144, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66703.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0692, 0.0937, 0.0122,  ..., 0.0000, 0.0319, 0.0377],
        [0.0940, 0.1238, 0.0074,  ..., 0.0000, 0.0523, 0.0272],
        [0.0708, 0.0950, 0.0116,  ..., 0.0000, 0.0325, 0.0378],
        ...,
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(667135.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9858.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(75.7559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18146.4941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1097.2207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-317.0201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0624],
        [ 0.0568],
        [ 0.0863],
        ...,
        [-1.6064],
        [-1.6021],
        [-1.6006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204790.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00],
        [-4.1032e-03, -7.1963e-05,  8.0581e-03,  ..., -9.0328e-03,
          1.2375e-02, -3.6099e-03],
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -3.4989e-04,  ...,  0.0000e+00,
          1.4020e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1371.8998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7158, device='cuda:0')



h[100].sum tensor(96.3637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7970, device='cuda:0')



h[200].sum tensor(25.6800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4597, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0243,  ..., 0.0000, 0.0388, 0.0000],
        [0.0000, 0.0000, 0.0216,  ..., 0.0000, 0.0349, 0.0000],
        [0.0000, 0.0000, 0.0343,  ..., 0.0000, 0.0524, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49917.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1349, 0.1741, 0.0033,  ..., 0.0000, 0.0870, 0.0121],
        [0.1414, 0.1821, 0.0035,  ..., 0.0000, 0.0925, 0.0129],
        [0.1443, 0.1855, 0.0018,  ..., 0.0000, 0.0947, 0.0082],
        ...,
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692],
        [0.0010, 0.0099, 0.0256,  ..., 0.0000, 0.0000, 0.0692]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(565796.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6532.3384, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(13.4176, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15826.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-988.7397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-313.3839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0910],
        [ 0.1804],
        [ 0.1402],
        ...,
        [-1.6054],
        [-1.6014],
        [-1.5998]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223617.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0193],
        [1.0201],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368844.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0162],
        [1.0194],
        [1.0202],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368855.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0126, -0.0002,  0.0256,  ..., -0.0278,  0.0352, -0.0111],
        [-0.0083, -0.0001,  0.0168,  ..., -0.0184,  0.0237, -0.0073],
        [-0.0097, -0.0002,  0.0197,  ..., -0.0215,  0.0275, -0.0086],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1405.0187, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5617, device='cuda:0')



h[100].sum tensor(97.2522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2648, device='cuda:0')



h[200].sum tensor(26.8232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0719,  ..., 0.0000, 0.1011, 0.0000],
        [0.0000, 0.0000, 0.0829,  ..., 0.0000, 0.1155, 0.0000],
        [0.0000, 0.0000, 0.0751,  ..., 0.0000, 0.1053, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53019.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4146, 0.5198, 0.0000,  ..., 0.0000, 0.3273, 0.0000],
        [0.4141, 0.5194, 0.0000,  ..., 0.0000, 0.3270, 0.0000],
        [0.3814, 0.4788, 0.0000,  ..., 0.0000, 0.2987, 0.0000],
        ...,
        [0.0012, 0.0095, 0.0255,  ..., 0.0000, 0.0000, 0.0692],
        [0.0012, 0.0095, 0.0255,  ..., 0.0000, 0.0000, 0.0692],
        [0.0012, 0.0095, 0.0255,  ..., 0.0000, 0.0000, 0.0692]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595666.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7524.5942, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.3512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16515.0840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1006.9017, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.7639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3101],
        [ 0.3152],
        [ 0.3211],
        ...,
        [-1.6166],
        [-1.6123],
        [-1.6109]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221943.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0162],
        [1.0194],
        [1.0202],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368855.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0195],
        [1.0203],
        ...,
        [1.0044],
        [1.0033],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368865.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1325.2341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3546, device='cuda:0')



h[100].sum tensor(96.0600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0442, device='cuda:0')



h[200].sum tensor(25.5097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0055,  ..., 0.0000, 0.0130, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48218.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0813, 0.1066, 0.0100,  ..., 0.0000, 0.0424, 0.0328],
        [0.0431, 0.0600, 0.0173,  ..., 0.0000, 0.0160, 0.0499],
        [0.0162, 0.0270, 0.0225,  ..., 0.0000, 0.0001, 0.0620],
        ...,
        [0.0015, 0.0092, 0.0256,  ..., 0.0000, 0.0000, 0.0694],
        [0.0015, 0.0092, 0.0256,  ..., 0.0000, 0.0000, 0.0694],
        [0.0015, 0.0092, 0.0256,  ..., 0.0000, 0.0000, 0.0694]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(559814.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6334.0059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2.3323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15721.0557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-976.6484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-321.9088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0192],
        [-0.2358],
        [-0.5671],
        ...,
        [-1.6281],
        [-1.6240],
        [-1.6226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236345.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0195],
        [1.0203],
        ...,
        [1.0044],
        [1.0033],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368865.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0196],
        [1.0204],
        ...,
        [1.0043],
        [1.0033],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368875.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2004.3839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.7776, device='cuda:0')



h[100].sum tensor(110.4875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.6793, device='cuda:0')



h[200].sum tensor(39.3786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7561, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0363,  ..., 0.0000, 0.0534, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70044.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2796, 0.3533, 0.0039,  ..., 0.0000, 0.2141, 0.0111],
        [0.0870, 0.1145, 0.0123,  ..., 0.0000, 0.0589, 0.0335],
        [0.0244, 0.0369, 0.0212,  ..., 0.0000, 0.0063, 0.0583],
        ...,
        [0.0017, 0.0089, 0.0257,  ..., 0.0000, 0.0000, 0.0698],
        [0.0017, 0.0089, 0.0257,  ..., 0.0000, 0.0000, 0.0698],
        [0.0017, 0.0089, 0.0257,  ..., 0.0000, 0.0000, 0.0698]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680017.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10140.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(70.9869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18268.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1111.1218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-325.6494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0831],
        [-0.0303],
        [-0.1423],
        ...,
        [-1.6418],
        [-1.6378],
        [-1.6361]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224450.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0196],
        [1.0204],
        ...,
        [1.0043],
        [1.0033],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368875.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0197],
        [1.0205],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368885.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.9687e-03, -6.5778e-05,  7.8860e-03,  ..., -8.7641e-03,
          1.1934e-02, -3.4891e-03],
        [-9.1569e-03, -1.5177e-04,  1.8586e-02,  ..., -2.0221e-02,
          2.5891e-02, -8.0504e-03],
        [ 0.0000e+00,  0.0000e+00, -2.9930e-04,  ...,  0.0000e+00,
          1.2575e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.9930e-04,  ...,  0.0000e+00,
          1.2575e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.9930e-04,  ...,  0.0000e+00,
          1.2575e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.9930e-04,  ...,  0.0000e+00,
          1.2575e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1482.0308, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2772, device='cuda:0')



h[100].sum tensor(100.5778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2135, device='cuda:0')



h[200].sum tensor(28.8148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0892,  ..., 0.0000, 0.1231, 0.0000],
        [0.0000, 0.0000, 0.0459,  ..., 0.0000, 0.0662, 0.0000],
        [0.0000, 0.0000, 0.0480,  ..., 0.0000, 0.0689, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51821.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4452, 0.5569, 0.0000,  ..., 0.0000, 0.3565, 0.0000],
        [0.3242, 0.4067, 0.0000,  ..., 0.0000, 0.2516, 0.0000],
        [0.2554, 0.3214, 0.0012,  ..., 0.0000, 0.1922, 0.0045],
        ...,
        [0.0018, 0.0086, 0.0257,  ..., 0.0000, 0.0000, 0.0701],
        [0.0018, 0.0086, 0.0257,  ..., 0.0000, 0.0000, 0.0701],
        [0.0018, 0.0086, 0.0257,  ..., 0.0000, 0.0000, 0.0701]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(575961.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6903.2295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.1835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16163.2900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1001.0319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-328.7807, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2309],
        [ 0.2028],
        [ 0.1127],
        ...,
        [-1.6548],
        [-1.6506],
        [-1.6491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-233180.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0197],
        [1.0205],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368885.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0198],
        [1.0206],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368895.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1302.6538, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4929, device='cuda:0')



h[100].sum tensor(97.1901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5677, device='cuda:0')



h[200].sum tensor(25.3709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8963, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0212,  ..., 0.0000, 0.0334, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47833.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0117, 0.0203, 0.0234,  ..., 0.0000, 0.0013, 0.0648],
        [0.0679, 0.0896, 0.0140,  ..., 0.0000, 0.0433, 0.0396],
        [0.1854, 0.2345, 0.0065,  ..., 0.0000, 0.1330, 0.0196],
        ...,
        [0.0019, 0.0082, 0.0257,  ..., 0.0000, 0.0000, 0.0703],
        [0.0019, 0.0082, 0.0257,  ..., 0.0000, 0.0000, 0.0703],
        [0.0019, 0.0082, 0.0257,  ..., 0.0000, 0.0000, 0.0703]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564354.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6481.5317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-22.8346, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15976.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-978.4731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-332.2766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2785],
        [-0.1339],
        [ 0.0323],
        ...,
        [-1.6676],
        [-1.6633],
        [-1.6618]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243517., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0198],
        [1.0206],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368895.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(359.4361, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0199],
        [1.0207],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368905.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1421.1444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1037, device='cuda:0')



h[100].sum tensor(99.4190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0115, device='cuda:0')



h[200].sum tensor(27.8790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0210,  ..., 0.0000, 0.0330, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51487.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0267, 0.0383, 0.0206,  ..., 0.0000, 0.0089, 0.0580],
        [0.0894, 0.1155, 0.0106,  ..., 0.0000, 0.0565, 0.0311],
        [0.1895, 0.2384, 0.0040,  ..., 0.0000, 0.1362, 0.0118],
        ...,
        [0.0021, 0.0081, 0.0256,  ..., 0.0000, 0.0000, 0.0703],
        [0.0021, 0.0081, 0.0256,  ..., 0.0000, 0.0000, 0.0703],
        [0.0021, 0.0081, 0.0256,  ..., 0.0000, 0.0000, 0.0703]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584053.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6956.9287, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-9.7378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16305.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-994.5457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-333.4022, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2612],
        [-0.0609],
        [ 0.0754],
        ...,
        [-1.6360],
        [-1.6476],
        [-1.6558]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256360.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0199],
        [1.0207],
        ...,
        [1.0043],
        [1.0032],
        [1.0015]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368905.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0199],
        [1.0208],
        ...,
        [1.0042],
        [1.0032],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368915.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0012,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1485.8850, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4444, device='cuda:0')



h[100].sum tensor(100.2058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7529, device='cuda:0')



h[200].sum tensor(29.3515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0049, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52946.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0663, 0.0866, 0.0131,  ..., 0.0000, 0.0342, 0.0397],
        [0.0758, 0.0982, 0.0114,  ..., 0.0000, 0.0387, 0.0356],
        [0.0756, 0.0980, 0.0114,  ..., 0.0000, 0.0394, 0.0357],
        ...,
        [0.0023, 0.0081, 0.0254,  ..., 0.0000, 0.0000, 0.0702],
        [0.0023, 0.0081, 0.0254,  ..., 0.0000, 0.0000, 0.0702],
        [0.0023, 0.0081, 0.0254,  ..., 0.0000, 0.0000, 0.0702]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590217.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7152.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-4.7880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16653.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1001.9058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-337.9549, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0629],
        [ 0.0254],
        [ 0.0733],
        ...,
        [-1.6802],
        [-1.6761],
        [-1.6748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249348.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0199],
        [1.0208],
        ...,
        [1.0042],
        [1.0032],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368915.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0200],
        [1.0209],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368925.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8638e-03, -6.0481e-05,  7.7609e-03,  ..., -8.5591e-03,
          1.1670e-02, -3.3945e-03],
        [ 0.0000e+00,  0.0000e+00, -2.5873e-04,  ...,  0.0000e+00,
          1.2125e-03,  0.0000e+00],
        [-3.8638e-03, -6.0481e-05,  7.7609e-03,  ..., -8.5591e-03,
          1.1670e-02, -3.3945e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.5873e-04,  ...,  0.0000e+00,
          1.2125e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5873e-04,  ...,  0.0000e+00,
          1.2125e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5873e-04,  ...,  0.0000e+00,
          1.2125e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1470.5383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9249, device='cuda:0')



h[100].sum tensor(99.2277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4656, device='cuda:0')



h[200].sum tensor(29.1396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0123,  ..., 0.0000, 0.0213, 0.0000],
        [0.0000, 0.0000, 0.0404,  ..., 0.0000, 0.0590, 0.0000],
        [0.0000, 0.0000, 0.0123,  ..., 0.0000, 0.0214, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52455.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0999, 0.1266, 0.0084,  ..., 0.0000, 0.0588, 0.0284],
        [0.1533, 0.1917, 0.0008,  ..., 0.0000, 0.1042, 0.0077],
        [0.1046, 0.1321, 0.0077,  ..., 0.0000, 0.0626, 0.0268],
        ...,
        [0.0024, 0.0082, 0.0253,  ..., 0.0000, 0.0000, 0.0702],
        [0.0024, 0.0082, 0.0253,  ..., 0.0000, 0.0000, 0.0702],
        [0.0024, 0.0082, 0.0253,  ..., 0.0000, 0.0000, 0.0702]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(589816.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7301.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-6.6788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17132.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1004.1997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.7397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1037],
        [ 0.0646],
        [ 0.0283],
        ...,
        [-1.6814],
        [-1.6770],
        [-1.6728]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214945.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0200],
        [1.0209],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368925.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0201],
        [1.0211],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368935.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.5931e-03, -1.3259e-04,  1.7608e-02,  ..., -1.9050e-02,
          2.4510e-02, -7.5481e-03],
        [-3.9488e-03, -6.0928e-05,  7.9537e-03,  ..., -8.7543e-03,
          1.1921e-02, -3.4686e-03],
        [ 0.0000e+00,  0.0000e+00, -2.5491e-04,  ...,  0.0000e+00,
          1.2168e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.5491e-04,  ...,  0.0000e+00,
          1.2168e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5491e-04,  ...,  0.0000e+00,
          1.2168e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5491e-04,  ...,  0.0000e+00,
          1.2168e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1341.3429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8830, device='cuda:0')



h[100].sum tensor(96.4835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7834, device='cuda:0')



h[200].sum tensor(26.6999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0227,  ..., 0.0000, 0.0352, 0.0000],
        [0.0000, 0.0000, 0.0245,  ..., 0.0000, 0.0376, 0.0000],
        [0.0000, 0.0000, 0.0401,  ..., 0.0000, 0.0587, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47725.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.9139e-01, 2.3933e-01, 0.0000e+00,  ..., 0.0000e+00, 1.3803e-01,
         2.1936e-03],
        [1.7912e-01, 2.2374e-01, 0.0000e+00,  ..., 0.0000e+00, 1.2698e-01,
         4.2655e-03],
        [2.0323e-01, 2.5263e-01, 0.0000e+00,  ..., 0.0000e+00, 1.4698e-01,
         1.7467e-04],
        ...,
        [1.9828e-03, 7.9991e-03, 2.5324e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.0879e-02],
        [1.9798e-03, 7.9997e-03, 2.5329e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.0893e-02],
        [1.9792e-03, 7.9991e-03, 2.5329e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.0892e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563427.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6259.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-21.1045, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16478.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-975.0717, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-345.6453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2647],
        [ 0.2726],
        [ 0.2777],
        ...,
        [-1.7036],
        [-1.6992],
        [-1.6979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234535.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0201],
        [1.0211],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368935.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0202],
        [1.0212],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368945.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.5587e-03, -8.4539e-05,  1.1316e-02,  ..., -1.2333e-02,
          1.6323e-02, -4.8818e-03],
        [-7.5089e-03, -1.1420e-04,  1.5376e-02,  ..., -1.6660e-02,
          2.1618e-02, -6.5945e-03],
        [-7.3791e-03, -1.1223e-04,  1.5106e-02,  ..., -1.6372e-02,
          2.1266e-02, -6.4805e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.5687e-04,  ...,  0.0000e+00,
          1.2316e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5687e-04,  ...,  0.0000e+00,
          1.2316e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.5687e-04,  ...,  0.0000e+00,
          1.2316e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1870.9675, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0615, device='cuda:0')



h[100].sum tensor(106.8003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.0713, device='cuda:0')



h[200].sum tensor(37.2606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.4392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0250,  ..., 0.0000, 0.0383, 0.0000],
        [0.0000, 0.0000, 0.0397,  ..., 0.0000, 0.0578, 0.0000],
        [0.0000, 0.0000, 0.0874,  ..., 0.0000, 0.1204, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66801.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2157, 0.2697, 0.0007,  ..., 0.0000, 0.1589, 0.0047],
        [0.3018, 0.3763, 0.0000,  ..., 0.0000, 0.2333, 0.0000],
        [0.4360, 0.5431, 0.0000,  ..., 0.0000, 0.3499, 0.0000],
        ...,
        [0.0013, 0.0078, 0.0254,  ..., 0.0000, 0.0000, 0.0719],
        [0.0013, 0.0078, 0.0254,  ..., 0.0000, 0.0000, 0.0719],
        [0.0013, 0.0078, 0.0254,  ..., 0.0000, 0.0000, 0.0719]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676598.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9508.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.6954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18665.8711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1090.7959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.1846, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2272],
        [ 0.2405],
        [ 0.2312],
        ...,
        [-1.7242],
        [-1.7198],
        [-1.7182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248993.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0202],
        [1.0212],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368945.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0203],
        [1.0213],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368954.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.6011e-04,  ...,  0.0000e+00,
          1.2459e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6011e-04,  ...,  0.0000e+00,
          1.2459e-03,  0.0000e+00],
        [-6.5479e-03, -9.8155e-05,  1.3392e-02,  ..., -1.4539e-02,
          1.9052e-02, -5.7495e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.6011e-04,  ...,  0.0000e+00,
          1.2459e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6011e-04,  ...,  0.0000e+00,
          1.2459e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6011e-04,  ...,  0.0000e+00,
          1.2459e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1344.8324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4770, device='cuda:0')



h[100].sum tensor(96.3951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1119, device='cuda:0')



h[200].sum tensor(26.7164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0137,  ..., 0.0000, 0.0232, 0.0000],
        [0.0000, 0.0000, 0.0111,  ..., 0.0000, 0.0199, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48225.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0252, 0.0376, 0.0206,  ..., 0.0000, 0.0045, 0.0605],
        [0.0766, 0.1008, 0.0130,  ..., 0.0000, 0.0428, 0.0392],
        [0.1394, 0.1781, 0.0075,  ..., 0.0000, 0.0945, 0.0235],
        ...,
        [0.0010, 0.0077, 0.0256,  ..., 0.0000, 0.0000, 0.0728],
        [0.0010, 0.0077, 0.0256,  ..., 0.0000, 0.0000, 0.0728],
        [0.0010, 0.0077, 0.0256,  ..., 0.0000, 0.0000, 0.0728]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568969.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5979.2822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-13.6169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16371.6260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-978.2947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-342.7417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7459],
        [-0.6008],
        [-0.3110],
        ...,
        [-1.7389],
        [-1.7342],
        [-1.7274]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264201.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0203],
        [1.0213],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368954.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0205],
        [1.0214],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368963.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0099, -0.0001,  0.0203,  ..., -0.0219,  0.0281, -0.0087],
        [-0.0228, -0.0003,  0.0473,  ..., -0.0507,  0.0634, -0.0200],
        [-0.0093, -0.0001,  0.0192,  ..., -0.0207,  0.0266, -0.0082],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1505.3083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2841, device='cuda:0')



h[100].sum tensor(99.7598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2173, device='cuda:0')



h[200].sum tensor(30.0607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1009,  ..., 0.0000, 0.1380, 0.0000],
        [0.0000, 0.0000, 0.0793,  ..., 0.0000, 0.1100, 0.0000],
        [0.0000, 0.0000, 0.0812,  ..., 0.0000, 0.1120, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53625.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4443, 0.5565, 0.0000,  ..., 0.0000, 0.3587, 0.0000],
        [0.4355, 0.5456, 0.0000,  ..., 0.0000, 0.3510, 0.0000],
        [0.3893, 0.4888, 0.0000,  ..., 0.0000, 0.3113, 0.0000],
        ...,
        [0.0007, 0.0075, 0.0257,  ..., 0.0000, 0.0000, 0.0737],
        [0.0007, 0.0075, 0.0257,  ..., 0.0000, 0.0000, 0.0738],
        [0.0007, 0.0075, 0.0257,  ..., 0.0000, 0.0000, 0.0738]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596368.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6904.7349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(4.7619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17211.3789, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1020.0417, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-348.9335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2217],
        [ 0.1873],
        [ 0.1022],
        ...,
        [-1.7612],
        [-1.7566],
        [-1.7550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248965.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0205],
        [1.0214],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368963.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0205],
        [1.0214],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368963.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1860e-03, -1.0617e-04,  1.4742e-02,  ..., -1.5969e-02,
          2.0825e-02, -6.3088e-03],
        [-4.1471e-03, -6.1271e-05,  8.3961e-03,  ..., -9.2157e-03,
          1.2548e-02, -3.6408e-03],
        [ 0.0000e+00,  0.0000e+00, -2.6359e-04,  ...,  0.0000e+00,
          1.2526e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.6359e-04,  ...,  0.0000e+00,
          1.2526e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6359e-04,  ...,  0.0000e+00,
          1.2526e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6359e-04,  ...,  0.0000e+00,
          1.2526e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1387.3586, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5891, device='cuda:0')



h[100].sum tensor(97.4452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7269, device='cuda:0')



h[200].sum tensor(27.7134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3706, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0498,  ..., 0.0000, 0.0715, 0.0000],
        [0.0000, 0.0000, 0.0272,  ..., 0.0000, 0.0413, 0.0000],
        [0.0000, 0.0000, 0.0086,  ..., 0.0000, 0.0166, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49957.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2172, 0.2730, 0.0000,  ..., 0.0000, 0.1610, 0.0019],
        [0.1541, 0.1958, 0.0047,  ..., 0.0000, 0.1071, 0.0176],
        [0.0789, 0.1037, 0.0107,  ..., 0.0000, 0.0462, 0.0374],
        ...,
        [0.0007, 0.0075, 0.0257,  ..., 0.0000, 0.0000, 0.0737],
        [0.0007, 0.0075, 0.0257,  ..., 0.0000, 0.0000, 0.0738],
        [0.0007, 0.0075, 0.0257,  ..., 0.0000, 0.0000, 0.0738]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578163.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6222.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.2378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16652.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-993.6747, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-346.4842, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2165],
        [ 0.1078],
        [-0.0726],
        ...,
        [-1.7612],
        [-1.7566],
        [-1.7550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256524.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0205],
        [1.0214],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368963.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0206],
        [1.0215],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368973.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1327.6160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0769, device='cuda:0')



h[100].sum tensor(96.4955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8906, device='cuda:0')



h[200].sum tensor(26.5674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3070, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47548., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0004, 0.0073, 0.0255,  ..., 0.0000, 0.0000, 0.0736],
        [0.0004, 0.0073, 0.0255,  ..., 0.0000, 0.0000, 0.0737],
        [0.0004, 0.0073, 0.0256,  ..., 0.0000, 0.0000, 0.0738],
        ...,
        [0.0004, 0.0074, 0.0259,  ..., 0.0000, 0.0000, 0.0746],
        [0.0004, 0.0074, 0.0259,  ..., 0.0000, 0.0000, 0.0746],
        [0.0004, 0.0074, 0.0259,  ..., 0.0000, 0.0000, 0.0746]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563567., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5633.9697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-19.0088, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16340.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-980.5563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-348.1360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9623],
        [-1.3255],
        [-1.5980],
        ...,
        [-1.7790],
        [-1.7742],
        [-1.7726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264970.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0206],
        [1.0215],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368973.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0206],
        [1.0216],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368983.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1468.2551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7075, device='cuda:0')



h[100].sum tensor(99.2420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8984, device='cuda:0')



h[200].sum tensor(29.2739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8604, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0052, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53429.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.0401e-04, 7.4239e-03, 2.5700e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.4164e-02],
        [1.2693e-03, 8.6421e-03, 2.5491e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.3880e-02],
        [3.9607e-03, 1.1670e-02, 2.4868e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.2880e-02],
        ...,
        [5.1860e-03, 1.4098e-02, 2.4961e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.2959e-02],
        [1.5935e-02, 2.7153e-02, 2.2770e-02,  ..., 0.0000e+00, 1.2725e-05,
         6.8437e-02],
        [2.0888e-02, 3.3678e-02, 2.1673e-02,  ..., 0.0000e+00, 2.6805e-05,
         6.6166e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602836.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6933.8081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-0.9020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17439.5605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1022.0591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.3491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7996],
        [-1.6338],
        [-1.3768],
        ...,
        [-1.5985],
        [-1.4622],
        [-1.3714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-239808.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0206],
        [1.0216],
        ...,
        [1.0042],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368983.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(447.1956, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0176],
        [1.0207],
        [1.0216],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368993.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.7639e-04,  ...,  0.0000e+00,
          1.3001e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7639e-04,  ...,  0.0000e+00,
          1.3001e-03,  0.0000e+00],
        [-5.1162e-03, -7.2354e-05,  1.0455e-02,  ..., -1.1396e-02,
          1.5302e-02, -4.4892e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.7639e-04,  ...,  0.0000e+00,
          1.3001e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7639e-04,  ...,  0.0000e+00,
          1.3001e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7639e-04,  ...,  0.0000e+00,
          1.3001e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1221.8170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.2940, device='cuda:0')



h[100].sum tensor(94.1849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3517, device='cuda:0')



h[200].sum tensor(24.1886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3498, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0053, 0.0000],
        [0.0000, 0.0000, 0.0107,  ..., 0.0000, 0.0196, 0.0000],
        [0.0000, 0.0000, 0.0360,  ..., 0.0000, 0.0530, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0054, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45323.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.8566e-02, 6.8198e-02, 1.6284e-02,  ..., 0.0000e+00, 1.7216e-02,
         5.2230e-02],
        [1.0573e-01, 1.3944e-01, 9.6761e-03,  ..., 0.0000e+00, 6.7650e-02,
         3.2800e-02],
        [2.4505e-01, 3.1330e-01, 3.3746e-03,  ..., 0.0000e+00, 1.8745e-01,
         1.3871e-02],
        ...,
        [1.9845e-04, 7.9451e-03, 2.6305e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.5504e-02],
        [1.9658e-04, 7.9427e-03, 2.6310e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.5519e-02],
        [1.9620e-04, 7.9426e-03, 2.6311e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.5518e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561817.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5333.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-31.6387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16308.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-961.4743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.7639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0146],
        [ 0.0368],
        [ 0.1246],
        ...,
        [-1.7994],
        [-1.7949],
        [-1.7931]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276115.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0176],
        [1.0207],
        [1.0216],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368993.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0208],
        [1.0217],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369003.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0155, -0.0002,  0.0322,  ..., -0.0345,  0.0438, -0.0136],
        [-0.0142, -0.0002,  0.0294,  ..., -0.0316,  0.0401, -0.0124],
        [-0.0133, -0.0002,  0.0276,  ..., -0.0296,  0.0377, -0.0116],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1555.2290, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1164, device='cuda:0')



h[100].sum tensor(100.5372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6776, device='cuda:0')



h[200].sum tensor(30.4711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8514, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0681,  ..., 0.0000, 0.0954, 0.0000],
        [0.0000, 0.0000, 0.0966,  ..., 0.0000, 0.1330, 0.0000],
        [0.0000, 0.0000, 0.1049,  ..., 0.0000, 0.1438, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55089.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.7708e-01, 6.0519e-01, 0.0000e+00,  ..., 0.0000e+00, 3.8910e-01,
         0.0000e+00],
        [5.3519e-01, 6.7782e-01, 0.0000e+00,  ..., 0.0000e+00, 4.3941e-01,
         0.0000e+00],
        [5.5437e-01, 7.0159e-01, 0.0000e+00,  ..., 0.0000e+00, 4.5583e-01,
         0.0000e+00],
        ...,
        [1.5456e-04, 8.4092e-03, 2.6535e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.5878e-02],
        [1.5552e-04, 8.4109e-03, 2.6539e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.5894e-02],
        [1.5532e-04, 8.4099e-03, 2.6538e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.5889e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615573.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7113.6143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-0.7426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17673.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1022.7839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-345.2528, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2068],
        [ 0.2085],
        [ 0.2117],
        ...,
        [-1.8080],
        [-1.8034],
        [-1.8015]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254082.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0208],
        [1.0217],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369003.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0178],
        [1.0209],
        [1.0218],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369013.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.7529e-03, -1.0647e-04,  1.6026e-02,  ..., -1.7296e-02,
          2.2644e-02, -6.8004e-03],
        [-3.3798e-03, -4.6416e-05,  6.8253e-03,  ..., -7.5403e-03,
          1.0637e-02, -2.9646e-03],
        [-3.2787e-03, -4.5027e-05,  6.6126e-03,  ..., -7.3147e-03,
          1.0359e-02, -2.8759e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.8573e-04,  ...,  0.0000e+00,
          1.3564e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8573e-04,  ...,  0.0000e+00,
          1.3564e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8573e-04,  ...,  0.0000e+00,
          1.3564e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1391.1816, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8819, device='cuda:0')



h[100].sum tensor(97.1142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3358, device='cuda:0')



h[200].sum tensor(26.8726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0314,  ..., 0.0000, 0.0480, 0.0000],
        [0.0000, 0.0000, 0.0437,  ..., 0.0000, 0.0641, 0.0000],
        [0.0000, 0.0000, 0.0358,  ..., 0.0000, 0.0538, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49770.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.3450e-01, 3.0019e-01, 0.0000e+00,  ..., 0.0000e+00, 1.7619e-01,
         0.0000e+00],
        [2.4784e-01, 3.1707e-01, 0.0000e+00,  ..., 0.0000e+00, 1.8773e-01,
         0.0000e+00],
        [2.2060e-01, 2.8313e-01, 0.0000e+00,  ..., 0.0000e+00, 1.6413e-01,
         0.0000e+00],
        ...,
        [1.9204e-04, 8.8431e-03, 2.6758e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6032e-02],
        [1.9326e-04, 8.8443e-03, 2.6764e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6047e-02],
        [1.9272e-04, 8.8444e-03, 2.6763e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6044e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585034.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6069.5127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-24.0861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16992.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-985.4744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-341.0621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2791],
        [ 0.2769],
        [ 0.2730],
        ...,
        [-1.8147],
        [-1.8098],
        [-1.8083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258285.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0178],
        [1.0209],
        [1.0218],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369013.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0210],
        [1.0219],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369023.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0947e-02, -1.4814e-04,  2.2781e-02,  ..., -2.4442e-02,
          3.1486e-02, -9.6005e-03],
        [-4.9385e-03, -6.6828e-05,  1.0119e-02,  ..., -1.1026e-02,
          1.4960e-02, -4.3309e-03],
        [-5.4480e-03, -7.3724e-05,  1.1193e-02,  ..., -1.2164e-02,
          1.6361e-02, -4.7778e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.8728e-04,  ...,  0.0000e+00,
          1.3775e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8728e-04,  ...,  0.0000e+00,
          1.3775e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8728e-04,  ...,  0.0000e+00,
          1.3775e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1446.2098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8499, device='cuda:0')



h[100].sum tensor(97.9375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8712, device='cuda:0')



h[200].sum tensor(27.7054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.5540, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0565,  ..., 0.0000, 0.0808, 0.0000],
        [0.0000, 0.0000, 0.0618,  ..., 0.0000, 0.0878, 0.0000],
        [0.0000, 0.0000, 0.0364,  ..., 0.0000, 0.0543, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50970.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4778e-01, 3.1935e-01, 0.0000e+00,  ..., 0.0000e+00, 1.8870e-01,
         0.0000e+00],
        [2.4935e-01, 3.2149e-01, 0.0000e+00,  ..., 0.0000e+00, 1.9011e-01,
         0.0000e+00],
        [2.0623e-01, 2.6742e-01, 0.0000e+00,  ..., 0.0000e+00, 1.5264e-01,
         9.4513e-04],
        ...,
        [2.8227e-04, 9.2710e-03, 2.6992e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6092e-02],
        [2.8015e-04, 9.2717e-03, 2.6996e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6109e-02],
        [2.7920e-04, 9.2709e-03, 2.6995e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6107e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(591644.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6385.2451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-24.9922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17396.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-995.5029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-340.8665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2404],
        [ 0.2078],
        [ 0.1448],
        ...,
        [-1.8199],
        [-1.8149],
        [-1.8134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236995.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0210],
        [1.0219],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369023.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0211],
        [1.0220],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369033.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.7530e-03, -7.6710e-05,  1.1857e-02,  ..., -1.2855e-02,
          1.7237e-02, -5.0444e-03],
        [-3.7276e-03, -4.9704e-05,  7.5825e-03,  ..., -8.3294e-03,
          1.1658e-02, -3.2685e-03],
        [-2.8921e-03, -3.8562e-05,  5.8190e-03,  ..., -6.4623e-03,
          9.3559e-03, -2.5358e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.8508e-04,  ...,  0.0000e+00,
          1.3890e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8508e-04,  ...,  0.0000e+00,
          1.3890e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.8508e-04,  ...,  0.0000e+00,
          1.3890e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1723.8464, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4459, device='cuda:0')



h[100].sum tensor(102.9127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5188, device='cuda:0')



h[200].sum tensor(32.8867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.1930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0413,  ..., 0.0000, 0.0607, 0.0000],
        [0.0000, 0.0000, 0.0351,  ..., 0.0000, 0.0527, 0.0000],
        [0.0000, 0.0000, 0.0624,  ..., 0.0000, 0.0886, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59369.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.7251e-01, 4.7659e-01, 0.0000e+00,  ..., 0.0000e+00, 2.9688e-01,
         0.0000e+00],
        [3.0281e-01, 3.8845e-01, 0.0000e+00,  ..., 0.0000e+00, 2.3584e-01,
         0.0000e+00],
        [3.3193e-01, 4.2482e-01, 0.0000e+00,  ..., 0.0000e+00, 2.6084e-01,
         0.0000e+00],
        ...,
        [3.4846e-04, 9.5044e-03, 2.7204e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6120e-02],
        [3.4810e-04, 9.5045e-03, 2.7211e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6135e-02],
        [3.4758e-04, 9.5041e-03, 2.7209e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.6132e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637078.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7622.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(7.3159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18210.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1039.0328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.9843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2085],
        [ 0.2253],
        [ 0.2356],
        ...,
        [-1.8269],
        [-1.8222],
        [-1.8205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254119.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0211],
        [1.0220],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369033.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0213],
        [1.0221],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369043.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1333.1428, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4923, device='cuda:0')



h[100].sum tensor(95.0046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0144, device='cuda:0')



h[200].sum tensor(25.0700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1926, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0430,  ..., 0.0000, 0.0626, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47069.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2252, 0.2918, 0.0036,  ..., 0.0000, 0.1693, 0.0118],
        [0.0811, 0.1107, 0.0121,  ..., 0.0000, 0.0504, 0.0370],
        [0.0329, 0.0503, 0.0207,  ..., 0.0000, 0.0110, 0.0600],
        ...,
        [0.0005, 0.0097, 0.0274,  ..., 0.0000, 0.0000, 0.0761],
        [0.0005, 0.0097, 0.0274,  ..., 0.0000, 0.0000, 0.0761],
        [0.0005, 0.0097, 0.0274,  ..., 0.0000, 0.0000, 0.0761]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(572227.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5611.7407, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-39.6846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16966.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-962.7714, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-334.1601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1036],
        [-0.1376],
        [-0.4673],
        ...,
        [-1.8301],
        [-1.8243],
        [-1.8209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256828.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0213],
        [1.0221],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369043.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0214],
        [1.0222],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369053.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1731.7543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6440, device='cuda:0')



h[100].sum tensor(102.2695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0753, device='cuda:0')



h[200].sum tensor(32.7522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0215,  ..., 0.0000, 0.0345, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60484.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0244, 0.0403, 0.0225,  ..., 0.0000, 0.0052, 0.0636],
        [0.0483, 0.0696, 0.0181,  ..., 0.0000, 0.0274, 0.0526],
        [0.1390, 0.1829, 0.0081,  ..., 0.0000, 0.0939, 0.0243],
        ...,
        [0.0006, 0.0099, 0.0276,  ..., 0.0000, 0.0000, 0.0760],
        [0.0006, 0.0099, 0.0276,  ..., 0.0000, 0.0000, 0.0760],
        [0.0006, 0.0099, 0.0276,  ..., 0.0000, 0.0000, 0.0760]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654243.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8559.9150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(5.5858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19306.5996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1055.5266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-339.5879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7489],
        [-0.5557],
        [-0.2058],
        ...,
        [-1.8387],
        [-1.8340],
        [-1.8326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193933.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0214],
        [1.0222],
        ...,
        [1.0041],
        [1.0031],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369053.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0183],
        [1.0215],
        [1.0223],
        ...,
        [1.0041],
        [1.0030],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369063.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.2738e-03, -4.1751e-05,  6.6646e-03,  ..., -7.3328e-03,
          1.0480e-02, -2.8690e-03],
        [-3.4272e-03, -4.3706e-05,  6.9897e-03,  ..., -7.6762e-03,
          1.0905e-02, -3.0034e-03],
        [-6.7010e-03, -8.5457e-05,  1.3932e-02,  ..., -1.5009e-02,
          1.9966e-02, -5.8724e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.7781e-04,  ...,  0.0000e+00,
          1.4187e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7781e-04,  ...,  0.0000e+00,
          1.4187e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7781e-04,  ...,  0.0000e+00,
          1.4187e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1845.0564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0909, device='cuda:0')



h[100].sum tensor(104.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.4285, device='cuda:0')



h[200].sum tensor(34.9638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.3499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0234,  ..., 0.0000, 0.0371, 0.0000],
        [0.0000, 0.0000, 0.0477,  ..., 0.0000, 0.0695, 0.0000],
        [0.0000, 0.0000, 0.0383,  ..., 0.0000, 0.0573, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64356.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2168, 0.2809, 0.0000,  ..., 0.0000, 0.1615, 0.0000],
        [0.2319, 0.2992, 0.0000,  ..., 0.0000, 0.1739, 0.0000],
        [0.2425, 0.3125, 0.0000,  ..., 0.0000, 0.1831, 0.0000],
        ...,
        [0.0005, 0.0101, 0.0280,  ..., 0.0000, 0.0000, 0.0767],
        [0.0005, 0.0101, 0.0280,  ..., 0.0000, 0.0000, 0.0767],
        [0.0005, 0.0101, 0.0280,  ..., 0.0000, 0.0000, 0.0767]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684517.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9188.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(16.1349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19592.3809, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1073.3630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.8561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2704],
        [ 0.2821],
        [ 0.2843],
        ...,
        [-1.8507],
        [-1.8450],
        [-1.8426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230644.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0183],
        [1.0215],
        [1.0223],
        ...,
        [1.0041],
        [1.0030],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369063.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0185],
        [1.0216],
        [1.0223],
        ...,
        [1.0041],
        [1.0030],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369072.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.7584e-04,  ...,  0.0000e+00,
          1.4330e-03,  0.0000e+00],
        [-6.4230e-03, -8.0698e-05,  1.3366e-02,  ..., -1.4398e-02,
          1.9239e-02, -5.6277e-03],
        [-1.3876e-02, -1.7434e-04,  2.9196e-02,  ..., -3.1105e-02,
          3.9902e-02, -1.2158e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.7584e-04,  ...,  0.0000e+00,
          1.4330e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7584e-04,  ...,  0.0000e+00,
          1.4330e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.7584e-04,  ...,  0.0000e+00,
          1.4330e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1583.3496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7596, device='cuda:0')



h[100].sum tensor(99.7236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9272, device='cuda:0')



h[200].sum tensor(29.9060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0345,  ..., 0.0000, 0.0515, 0.0000],
        [0.0000, 0.0000, 0.0602,  ..., 0.0000, 0.0856, 0.0000],
        [0.0000, 0.0000, 0.0476,  ..., 0.0000, 0.0690, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53892.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2563, 0.3316, 0.0009,  ..., 0.0000, 0.1963, 0.0039],
        [0.3279, 0.4218, 0.0000,  ..., 0.0000, 0.2586, 0.0000],
        [0.3068, 0.3950, 0.0000,  ..., 0.0000, 0.2400, 0.0000],
        ...,
        [0.0006, 0.0103, 0.0282,  ..., 0.0000, 0.0000, 0.0772],
        [0.0006, 0.0103, 0.0282,  ..., 0.0000, 0.0000, 0.0772],
        [0.0006, 0.0103, 0.0282,  ..., 0.0000, 0.0000, 0.0772]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613298.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7035.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-30.2228, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18153.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1010.2179, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-334.9789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2211],
        [ 0.2219],
        [ 0.2111],
        ...,
        [-1.8648],
        [-1.8602],
        [-1.8586]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227758., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0185],
        [1.0216],
        [1.0223],
        ...,
        [1.0041],
        [1.0030],
        [1.0014]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369072.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0217],
        [1.0224],
        ...,
        [1.0040],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369082.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.6841e-04,  ...,  0.0000e+00,
          1.4386e-03,  0.0000e+00],
        [-6.2890e-03, -7.7841e-05,  1.3111e-02,  ..., -1.4109e-02,
          1.8901e-02, -5.5094e-03],
        [ 0.0000e+00,  0.0000e+00, -2.6841e-04,  ...,  0.0000e+00,
          1.4386e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.6841e-04,  ...,  0.0000e+00,
          1.4386e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6841e-04,  ...,  0.0000e+00,
          1.4386e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.6841e-04,  ...,  0.0000e+00,
          1.4386e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1831.7799, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4669, device='cuda:0')



h[100].sum tensor(104.4503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0834, device='cuda:0')



h[200].sum tensor(34.7549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9111, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0484,  ..., 0.0000, 0.0704, 0.0000],
        [0.0000, 0.0000, 0.0109,  ..., 0.0000, 0.0204, 0.0000],
        [0.0000, 0.0000, 0.0134,  ..., 0.0000, 0.0237, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60941.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1682, 0.2201, 0.0010,  ..., 0.0000, 0.1195, 0.0042],
        [0.1032, 0.1389, 0.0092,  ..., 0.0000, 0.0632, 0.0288],
        [0.1344, 0.1781, 0.0035,  ..., 0.0000, 0.0902, 0.0137],
        ...,
        [0.0006, 0.0102, 0.0285,  ..., 0.0000, 0.0000, 0.0777],
        [0.0006, 0.0102, 0.0285,  ..., 0.0000, 0.0000, 0.0778],
        [0.0006, 0.0102, 0.0285,  ..., 0.0000, 0.0000, 0.0778]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(651849.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8147.4419, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.0849, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18904.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1055.1007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.7618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1741],
        [ 0.1731],
        [ 0.1816],
        ...,
        [-1.8791],
        [-1.8744],
        [-1.8726]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240387.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0217],
        [1.0224],
        ...,
        [1.0040],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369082.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(505.1229, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0187],
        [1.0217],
        [1.0225],
        ...,
        [1.0040],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369092.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1654.7834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0159, device='cuda:0')



h[100].sum tensor(100.8863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6220, device='cuda:0')



h[200].sum tensor(31.3928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7807, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56367.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0099, 0.0282,  ..., 0.0000, 0.0000, 0.0770],
        [0.0006, 0.0099, 0.0283,  ..., 0.0000, 0.0000, 0.0771],
        [0.0007, 0.0100, 0.0283,  ..., 0.0000, 0.0000, 0.0772],
        ...,
        [0.0007, 0.0101, 0.0286,  ..., 0.0000, 0.0000, 0.0781],
        [0.0026, 0.0130, 0.0282,  ..., 0.0000, 0.0000, 0.0771],
        [0.0103, 0.0224, 0.0266,  ..., 0.0000, 0.0000, 0.0737]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(630835.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7504.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-25.6392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18542.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1029.7296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-337.1574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7775],
        [-1.9237],
        [-2.0327],
        ...,
        [-1.8640],
        [-1.8064],
        [-1.7029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237436.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0187],
        [1.0217],
        [1.0225],
        ...,
        [1.0040],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369092.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0188],
        [1.0218],
        [1.0226],
        ...,
        [1.0040],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369101.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.1289e-02, -1.3559e-04,  2.3853e-02,  ..., -2.5365e-02,
          3.2871e-02, -9.8856e-03],
        [-6.7545e-03, -8.1128e-05,  1.4174e-02,  ..., -1.5177e-02,
          2.0241e-02, -5.9150e-03],
        [-3.3178e-03, -3.9850e-05,  6.8381e-03,  ..., -7.4550e-03,
          1.0669e-02, -2.9054e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.4410e-04,  ...,  0.0000e+00,
          1.4271e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.4410e-04,  ...,  0.0000e+00,
          1.4271e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.4410e-04,  ...,  0.0000e+00,
          1.4271e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1433.1902, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5881, device='cuda:0')



h[100].sum tensor(96.4512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6204, device='cuda:0')



h[200].sum tensor(27.1921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9633, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0697,  ..., 0.0000, 0.0980, 0.0000],
        [0.0000, 0.0000, 0.0678,  ..., 0.0000, 0.0956, 0.0000],
        [0.0000, 0.0000, 0.0597,  ..., 0.0000, 0.0851, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48391.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4691, 0.5943, 0.0000,  ..., 0.0000, 0.3806, 0.0000],
        [0.4277, 0.5422, 0.0000,  ..., 0.0000, 0.3443, 0.0000],
        [0.3947, 0.5005, 0.0000,  ..., 0.0000, 0.3153, 0.0000],
        ...,
        [0.0007, 0.0099, 0.0287,  ..., 0.0000, 0.0000, 0.0784],
        [0.0007, 0.0099, 0.0287,  ..., 0.0000, 0.0000, 0.0784],
        [0.0007, 0.0099, 0.0287,  ..., 0.0000, 0.0000, 0.0784]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585182.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5976.8433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.1370, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17522.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-982.0687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-337.8725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2070],
        [ 0.2210],
        [ 0.2294],
        ...,
        [-1.9043],
        [-1.8996],
        [-1.8978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249797.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0188],
        [1.0218],
        [1.0226],
        ...,
        [1.0040],
        [1.0030],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369101.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0189],
        [1.0218],
        [1.0227],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369111.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0091, -0.0001,  0.0192,  ..., -0.0204,  0.0268, -0.0080],
        [-0.0096, -0.0001,  0.0202,  ..., -0.0215,  0.0281, -0.0084],
        [-0.0130, -0.0002,  0.0276,  ..., -0.0292,  0.0377, -0.0114],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1813.1925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1963, device='cuda:0')



h[100].sum tensor(103.3225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3807, device='cuda:0')



h[200].sum tensor(34.4951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1082,  ..., 0.0000, 0.1482, 0.0000],
        [0.0000, 0.0000, 0.0807,  ..., 0.0000, 0.1120, 0.0000],
        [0.0000, 0.0000, 0.0436,  ..., 0.0000, 0.0634, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0176, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60767.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.4011e-01, 8.0719e-01, 0.0000e+00,  ..., 0.0000e+00, 5.2947e-01,
         0.0000e+00],
        [4.8812e-01, 6.1711e-01, 0.0000e+00,  ..., 0.0000e+00, 3.9727e-01,
         0.0000e+00],
        [3.2620e-01, 4.1474e-01, 0.0000e+00,  ..., 0.0000e+00, 2.5651e-01,
         4.9578e-04],
        ...,
        [3.5166e-03, 1.3302e-02, 2.8160e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.7448e-02],
        [1.9473e-02, 3.2939e-02, 2.5096e-02,  ..., 0.0000e+00, 3.8136e-03,
         7.0071e-02],
        [7.2942e-02, 9.9164e-02, 1.4842e-02,  ..., 0.0000e+00, 4.0867e-02,
         4.4963e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656849.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8533.8105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-10.5778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19484.8730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1071.9316, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-346.8071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2059],
        [ 0.2233],
        [ 0.2384],
        ...,
        [-1.6725],
        [-1.3014],
        [-0.7646]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211406.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0189],
        [1.0218],
        [1.0227],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369111.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0189],
        [1.0219],
        [1.0228],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369121.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.5841e-03, -1.1169e-04,  2.0307e-02,  ..., -2.1570e-02,
          2.8201e-02, -8.3898e-03],
        [-5.5650e-03, -6.4853e-05,  1.1698e-02,  ..., -1.2524e-02,
          1.6970e-02, -4.8715e-03],
        [-3.3093e-03, -3.8566e-05,  6.8665e-03,  ..., -7.4478e-03,
          1.0667e-02, -2.8969e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.2196e-04,  ...,  0.0000e+00,
          1.4196e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.2196e-04,  ...,  0.0000e+00,
          1.4196e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.2196e-04,  ...,  0.0000e+00,
          1.4196e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1529.8691, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7280, device='cuda:0')



h[100].sum tensor(98.0858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8037, device='cuda:0')



h[200].sum tensor(29.3151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0603,  ..., 0.0000, 0.0856, 0.0000],
        [0.0000, 0.0000, 0.0490,  ..., 0.0000, 0.0709, 0.0000],
        [0.0000, 0.0000, 0.0280,  ..., 0.0000, 0.0436, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51510.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.5963e-01, 3.2919e-01, 0.0000e+00,  ..., 0.0000e+00, 1.9810e-01,
         0.0000e+00],
        [2.3512e-01, 2.9834e-01, 0.0000e+00,  ..., 0.0000e+00, 1.7662e-01,
         7.2573e-05],
        [1.9537e-01, 2.4832e-01, 0.0000e+00,  ..., 0.0000e+00, 1.4184e-01,
         2.5252e-03],
        ...,
        [6.8628e-04, 9.2561e-03, 2.8949e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.9795e-02],
        [6.8771e-04, 9.2608e-03, 2.8960e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.9820e-02],
        [6.8655e-04, 9.2601e-03, 2.8959e-02,  ..., 0.0000e+00, 0.0000e+00,
         7.9816e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602957.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6541.9307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-41.0591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17933.6191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1009.7499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-341.8182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2973],
        [ 0.3132],
        [ 0.3021],
        ...,
        [-1.9391],
        [-1.9343],
        [-1.9329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258001.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0189],
        [1.0219],
        [1.0228],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369121.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0219],
        [1.0229],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369131.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [-0.0091, -0.0001,  0.0192,  ..., -0.0204,  0.0267, -0.0079],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1601.8145, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5686, device='cuda:0')



h[100].sum tensor(99.5798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8216, device='cuda:0')



h[200].sum tensor(30.8615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7628, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0271,  ..., 0.0000, 0.0418, 0.0000],
        [0.0000, 0.0000, 0.0532,  ..., 0.0000, 0.0760, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53144.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0522, 0.0729, 0.0188,  ..., 0.0000, 0.0307, 0.0547],
        [0.1736, 0.2238, 0.0085,  ..., 0.0000, 0.1245, 0.0253],
        [0.3142, 0.3984, 0.0016,  ..., 0.0000, 0.2460, 0.0059],
        ...,
        [0.0006, 0.0089, 0.0291,  ..., 0.0000, 0.0000, 0.0807],
        [0.0006, 0.0089, 0.0291,  ..., 0.0000, 0.0000, 0.0807],
        [0.0006, 0.0089, 0.0291,  ..., 0.0000, 0.0000, 0.0807]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(613534.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6953.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-36.8010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18276.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1029.9851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-345.8984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9760],
        [-0.5188],
        [-0.1306],
        ...,
        [-1.9598],
        [-1.9546],
        [-1.9527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252918.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0219],
        [1.0229],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369131.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0219],
        [1.0229],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369131.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.6350e-03, -8.7638e-05,  1.6166e-02,  ..., -1.7197e-02,
          2.2784e-02, -6.6824e-03],
        [-6.8240e-03, -7.8328e-05,  1.4426e-02,  ..., -1.5370e-02,
          2.0514e-02, -5.9726e-03],
        [-6.9106e-03, -7.9323e-05,  1.4612e-02,  ..., -1.5565e-02,
          2.0756e-02, -6.0484e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.1553e-04,  ...,  0.0000e+00,
          1.4145e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1553e-04,  ...,  0.0000e+00,
          1.4145e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1553e-04,  ...,  0.0000e+00,
          1.4145e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1748.5669, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9239, device='cuda:0')



h[100].sum tensor(102.3334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6771, device='cuda:0')



h[200].sum tensor(33.6567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1225, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0646,  ..., 0.0000, 0.0912, 0.0000],
        [0.0000, 0.0000, 0.0432,  ..., 0.0000, 0.0632, 0.0000],
        [0.0000, 0.0000, 0.0447,  ..., 0.0000, 0.0653, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60190.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3447, 0.4361, 0.0000,  ..., 0.0000, 0.2724, 0.0000],
        [0.2556, 0.3246, 0.0000,  ..., 0.0000, 0.1948, 0.0000],
        [0.2193, 0.2793, 0.0000,  ..., 0.0000, 0.1632, 0.0008],
        ...,
        [0.0006, 0.0089, 0.0291,  ..., 0.0000, 0.0000, 0.0807],
        [0.0006, 0.0089, 0.0291,  ..., 0.0000, 0.0000, 0.0807],
        [0.0006, 0.0089, 0.0291,  ..., 0.0000, 0.0000, 0.0807]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(662332.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8394.7949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-9.5430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19184.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1070.1045, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.4370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0703],
        [ 0.0371],
        [-0.0124],
        ...,
        [-1.9606],
        [-1.9556],
        [-1.9539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255054.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0219],
        [1.0229],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369131.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0219],
        [1.0230],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369140.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.9845e-03, -4.5046e-05,  8.3514e-03,  ..., -8.9816e-03,
          1.2583e-02, -3.4867e-03],
        [-2.6345e-03, -2.9784e-05,  5.4501e-03,  ..., -5.9385e-03,
          8.7988e-03, -2.3054e-03],
        [-6.6189e-03, -7.4829e-05,  1.4013e-02,  ..., -1.4920e-02,
          1.9969e-02, -5.7920e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.1176e-04,  ...,  0.0000e+00,
          1.4131e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1176e-04,  ...,  0.0000e+00,
          1.4131e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1176e-04,  ...,  0.0000e+00,
          1.4131e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1440.1803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5969, device='cuda:0')



h[100].sum tensor(96.5842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6252, device='cuda:0')



h[200].sum tensor(27.7956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9694, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0236,  ..., 0.0000, 0.0371, 0.0000],
        [0.0000, 0.0000, 0.0601,  ..., 0.0000, 0.0853, 0.0000],
        [0.0000, 0.0000, 0.0352,  ..., 0.0000, 0.0526, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48661.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1740, 0.2234, 0.0051,  ..., 0.0000, 0.1241, 0.0175],
        [0.2630, 0.3334, 0.0000,  ..., 0.0000, 0.2006, 0.0000],
        [0.2293, 0.2915, 0.0017,  ..., 0.0000, 0.1714, 0.0066],
        ...,
        [0.0006, 0.0090, 0.0291,  ..., 0.0000, 0.0000, 0.0811],
        [0.0006, 0.0090, 0.0292,  ..., 0.0000, 0.0000, 0.0811],
        [0.0006, 0.0090, 0.0292,  ..., 0.0000, 0.0000, 0.0811]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592282., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6004.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.0642, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17450.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-995.3224, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-340.9785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0085],
        [ 0.1808],
        [ 0.1958],
        ...,
        [-1.9742],
        [-1.9689],
        [-1.9671]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295918.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0219],
        [1.0230],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369140.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0219],
        [1.0230],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369150.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.8747e-03, -8.7680e-05,  1.6738e-02,  ..., -1.7765e-02,
          2.3527e-02, -6.8897e-03],
        [ 0.0000e+00,  0.0000e+00, -2.1324e-04,  ...,  0.0000e+00,
          1.4148e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1324e-04,  ...,  0.0000e+00,
          1.4148e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.1324e-04,  ...,  0.0000e+00,
          1.4148e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1324e-04,  ...,  0.0000e+00,
          1.4148e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.1324e-04,  ...,  0.0000e+00,
          1.4148e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1894.1406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.9976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5469, device='cuda:0')



h[100].sum tensor(105.1575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1276, device='cuda:0')



h[200].sum tensor(36.3742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9673, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0139,  ..., 0.0000, 0.0242, 0.0000],
        [0.0000, 0.0000, 0.0171,  ..., 0.0000, 0.0283, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63152.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1616, 0.2095, 0.0070,  ..., 0.0000, 0.1136, 0.0216],
        [0.1076, 0.1424, 0.0099,  ..., 0.0000, 0.0669, 0.0310],
        [0.0691, 0.0946, 0.0154,  ..., 0.0000, 0.0336, 0.0470],
        ...,
        [0.0007, 0.0094, 0.0291,  ..., 0.0000, 0.0000, 0.0810],
        [0.0007, 0.0094, 0.0291,  ..., 0.0000, 0.0000, 0.0810],
        [0.0007, 0.0094, 0.0291,  ..., 0.0000, 0.0000, 0.0810]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679975.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8929.7900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.3036, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19462.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1085.2233, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-344.4244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1741],
        [ 0.1311],
        [ 0.0990],
        ...,
        [-1.9821],
        [-1.9769],
        [-1.9750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266108.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0219],
        [1.0230],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369150.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0219],
        [1.0231],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369161.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1659.0793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6476, device='cuda:0')



h[100].sum tensor(100.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8653, device='cuda:0')



h[200].sum tensor(31.6778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.8183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0068,  ..., 0.0000, 0.0149, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55279.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0030, 0.0127, 0.0280,  ..., 0.0000, 0.0000, 0.0781],
        [0.0170, 0.0299, 0.0253,  ..., 0.0000, 0.0026, 0.0716],
        [0.0523, 0.0737, 0.0181,  ..., 0.0000, 0.0245, 0.0550],
        ...,
        [0.0010, 0.0101, 0.0289,  ..., 0.0000, 0.0000, 0.0803],
        [0.0010, 0.0101, 0.0289,  ..., 0.0000, 0.0000, 0.0803],
        [0.0010, 0.0101, 0.0289,  ..., 0.0000, 0.0000, 0.0803]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(628924.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7496.6504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-44.5034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18564.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1035.4965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-347.3145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8670],
        [-1.4959],
        [-0.9844],
        ...,
        [-1.9823],
        [-1.9774],
        [-1.9756]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252105.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0219],
        [1.0231],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369161.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0220],
        [1.0232],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369171.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.0855e-04,  ...,  0.0000e+00,
          1.4068e-03,  0.0000e+00],
        [-6.6104e-03, -7.1386e-05,  1.4067e-02,  ..., -1.4937e-02,
          2.0027e-02, -5.7814e-03],
        [-3.8655e-03, -4.1743e-05,  8.1392e-03,  ..., -8.7344e-03,
          1.2295e-02, -3.3807e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0855e-04,  ...,  0.0000e+00,
          1.4068e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0855e-04,  ...,  0.0000e+00,
          1.4068e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0855e-04,  ...,  0.0000e+00,
          1.4068e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1815.4855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0683, device='cuda:0')



h[100].sum tensor(103.3070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7569, device='cuda:0')



h[200].sum tensor(34.3550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0333,  ..., 0.0000, 0.0503, 0.0000],
        [0.0000, 0.0000, 0.0198,  ..., 0.0000, 0.0324, 0.0000],
        [0.0000, 0.0000, 0.0528,  ..., 0.0000, 0.0757, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60488.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1492, 0.1933, 0.0010,  ..., 0.0000, 0.1015, 0.0108],
        [0.1652, 0.2132, 0.0029,  ..., 0.0000, 0.1152, 0.0124],
        [0.2783, 0.3543, 0.0000,  ..., 0.0000, 0.2130, 0.0000],
        ...,
        [0.0014, 0.0108, 0.0286,  ..., 0.0000, 0.0000, 0.0794],
        [0.0014, 0.0108, 0.0286,  ..., 0.0000, 0.0000, 0.0795],
        [0.0014, 0.0108, 0.0286,  ..., 0.0000, 0.0000, 0.0794]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664597.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8768.0176, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-33.6118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19468.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1064.7880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.7105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2652],
        [ 0.2678],
        [ 0.2721],
        ...,
        [-1.9798],
        [-1.9750],
        [-1.9734]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231731.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0220],
        [1.0232],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369171.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(452.9712, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0220],
        [1.0233],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369182.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1514.9326, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.4387, device='cuda:0')



h[100].sum tensor(97.2908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5377, device='cuda:0')



h[200].sum tensor(28.4223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.8581, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49054.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0160, 0.0269,  ..., 0.0000, 0.0000, 0.0755],
        [0.0024, 0.0118, 0.0278,  ..., 0.0000, 0.0000, 0.0773],
        [0.0025, 0.0118, 0.0278,  ..., 0.0000, 0.0000, 0.0773],
        ...,
        [0.0021, 0.0115, 0.0283,  ..., 0.0000, 0.0000, 0.0785],
        [0.0021, 0.0115, 0.0283,  ..., 0.0000, 0.0000, 0.0785],
        [0.0021, 0.0115, 0.0283,  ..., 0.0000, 0.0000, 0.0785]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596252.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6523.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.8536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17931.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-985.0341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.4672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1676],
        [-1.1495],
        [-1.0344],
        ...,
        [-1.9752],
        [-1.9705],
        [-1.9686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250953.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0220],
        [1.0233],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369182.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0221],
        [1.0234],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369192.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2360.2271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9439, device='cuda:0')



h[100].sum tensor(112.5413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(19.3243, device='cuda:0')



h[200].sum tensor(44.0230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74255.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0116, 0.0275,  ..., 0.0000, 0.0000, 0.0768],
        [0.0024, 0.0116, 0.0276,  ..., 0.0000, 0.0000, 0.0770],
        [0.0024, 0.0117, 0.0276,  ..., 0.0000, 0.0000, 0.0770],
        ...,
        [0.0025, 0.0119, 0.0279,  ..., 0.0000, 0.0000, 0.0779],
        [0.0025, 0.0119, 0.0279,  ..., 0.0000, 0.0000, 0.0779],
        [0.0025, 0.0119, 0.0279,  ..., 0.0000, 0.0000, 0.0779]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(740886.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10984.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(11.8143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20869.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1131.5880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-352.5832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9576],
        [-2.0077],
        [-1.9959],
        ...,
        [-1.9769],
        [-1.9723],
        [-1.9705]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237893.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0221],
        [1.0234],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369192.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0222],
        [1.0235],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369201.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.8688e-04,  ...,  0.0000e+00,
          1.3853e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8688e-04,  ...,  0.0000e+00,
          1.3853e-03,  0.0000e+00],
        [-3.3487e-03, -3.4531e-05,  7.0813e-03,  ..., -7.5850e-03,
          1.0863e-02, -2.9271e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8688e-04,  ...,  0.0000e+00,
          1.3853e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8688e-04,  ...,  0.0000e+00,
          1.3853e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8688e-04,  ...,  0.0000e+00,
          1.3853e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1441.0648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.4438, device='cuda:0')



h[100].sum tensor(95.3669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4345, device='cuda:0')



h[200].sum tensor(26.6668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0072,  ..., 0.0000, 0.0153, 0.0000],
        [0.0000, 0.0000, 0.0227,  ..., 0.0000, 0.0357, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46675.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0324, 0.0487, 0.0212,  ..., 0.0000, 0.0129, 0.0620],
        [0.0933, 0.1240, 0.0105,  ..., 0.0000, 0.0566, 0.0335],
        [0.1750, 0.2253, 0.0037,  ..., 0.0000, 0.1225, 0.0144],
        ...,
        [0.0027, 0.0120, 0.0277,  ..., 0.0000, 0.0000, 0.0779],
        [0.0027, 0.0120, 0.0277,  ..., 0.0000, 0.0000, 0.0780],
        [0.0027, 0.0120, 0.0277,  ..., 0.0000, 0.0000, 0.0779]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586218.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6168.1074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.5940, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17659.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-964.1512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.1569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2940],
        [-0.0648],
        [ 0.1219],
        ...,
        [-1.9867],
        [-1.9819],
        [-1.9803]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251470.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0222],
        [1.0235],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369201.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0223],
        [1.0236],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369211.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0143, -0.0001,  0.0310,  ..., -0.0325,  0.0420, -0.0125],
        [-0.0122, -0.0001,  0.0263,  ..., -0.0276,  0.0359, -0.0106],
        [-0.0117, -0.0001,  0.0252,  ..., -0.0264,  0.0344, -0.0102],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1643.8628, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9225, device='cuda:0')



h[100].sum tensor(99.0574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9113, device='cuda:0')



h[200].sum tensor(30.4196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6050, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0831,  ..., 0.0000, 0.1150, 0.0000],
        [0.0000, 0.0000, 0.1155,  ..., 0.0000, 0.1572, 0.0000],
        [0.0000, 0.0000, 0.0804,  ..., 0.0000, 0.1115, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52864.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3970, 0.5016, 0.0000,  ..., 0.0000, 0.3144, 0.0000],
        [0.4639, 0.5855, 0.0000,  ..., 0.0000, 0.3723, 0.0000],
        [0.3958, 0.5004, 0.0000,  ..., 0.0000, 0.3131, 0.0000],
        ...,
        [0.0026, 0.0119, 0.0276,  ..., 0.0000, 0.0000, 0.0782],
        [0.0026, 0.0119, 0.0276,  ..., 0.0000, 0.0000, 0.0783],
        [0.0026, 0.0119, 0.0276,  ..., 0.0000, 0.0000, 0.0783]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618561.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7286.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.1983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18491.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1008.0878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-359.4981, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2852],
        [ 0.2711],
        [ 0.2536],
        ...,
        [-2.0018],
        [-1.9967],
        [-1.9952]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242288.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0223],
        [1.0236],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369211.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0223],
        [1.0237],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369220.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1907.4473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8808, device='cuda:0')



h[100].sum tensor(104.0741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2063, device='cuda:0')



h[200].sum tensor(35.4301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0173,  ..., 0.0000, 0.0286, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62650.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0266, 0.0411, 0.0219,  ..., 0.0000, 0.0042, 0.0662],
        [0.0653, 0.0892, 0.0141,  ..., 0.0000, 0.0346, 0.0470],
        [0.1391, 0.1803, 0.0060,  ..., 0.0000, 0.0913, 0.0211],
        ...,
        [0.0024, 0.0116, 0.0276,  ..., 0.0000, 0.0000, 0.0790],
        [0.0024, 0.0116, 0.0276,  ..., 0.0000, 0.0000, 0.0790],
        [0.0024, 0.0116, 0.0276,  ..., 0.0000, 0.0000, 0.0790]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(681177.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9531.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-39.0508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20175.2070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1083.1422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.6185, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5304],
        [-0.1854],
        [ 0.0568],
        ...,
        [-2.0215],
        [-2.0165],
        [-2.0142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199872.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0223],
        [1.0237],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369220.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0224],
        [1.0238],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369230.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0518e-02, -1.0354e-04,  2.2775e-02,  ..., -2.3883e-02,
          3.1278e-02, -9.1891e-03],
        [-6.0044e-03, -5.9103e-05,  1.2927e-02,  ..., -1.3633e-02,
          1.8439e-02, -5.2456e-03],
        [-8.5692e-03, -8.4348e-05,  1.8522e-02,  ..., -1.9457e-02,
          2.5734e-02, -7.4862e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.7145e-04,  ...,  0.0000e+00,
          1.3608e-03,  0.0000e+00],
        [-5.2045e-03, -5.1229e-05,  1.1182e-02,  ..., -1.1817e-02,
          1.6164e-02, -4.5468e-03],
        [-5.2045e-03, -5.1229e-05,  1.1182e-02,  ..., -1.1817e-02,
          1.6164e-02, -4.5468e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1650.2456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3261, device='cuda:0')



h[100].sum tensor(99.5638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1345, device='cuda:0')



h[200].sum tensor(30.7465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8889, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0319,  ..., 0.0000, 0.0478, 0.0000],
        [0.0000, 0.0000, 0.0692,  ..., 0.0000, 0.0967, 0.0000],
        [0.0000, 0.0000, 0.0395,  ..., 0.0000, 0.0577, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0210,  ..., 0.0000, 0.0336, 0.0000],
        [0.0000, 0.0000, 0.0211,  ..., 0.0000, 0.0336, 0.0000],
        [0.0000, 0.0000, 0.0211,  ..., 0.0000, 0.0336, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52535.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2233, 0.2852, 0.0000,  ..., 0.0000, 0.1642, 0.0019],
        [0.2711, 0.3448, 0.0000,  ..., 0.0000, 0.2054, 0.0000],
        [0.2227, 0.2845, 0.0000,  ..., 0.0000, 0.1635, 0.0021],
        ...,
        [0.0986, 0.1305, 0.0088,  ..., 0.0000, 0.0563, 0.0318],
        [0.1191, 0.1558, 0.0046,  ..., 0.0000, 0.0739, 0.0216],
        [0.1191, 0.1558, 0.0046,  ..., 0.0000, 0.0739, 0.0216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615236.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6956.2393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.5513, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18210.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1011.8972, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.2071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2246],
        [ 0.2279],
        [ 0.2107],
        ...,
        [-0.9855],
        [-0.7149],
        [-0.7134]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-266604.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0224],
        [1.0238],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369230.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0197],
        [1.0225],
        [1.0239],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369239.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1516.2463, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.5174, device='cuda:0')



h[100].sum tensor(97.2696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5812, device='cuda:0')



h[200].sum tensor(28.3705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9135, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49010.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0105, 0.0275,  ..., 0.0000, 0.0000, 0.0796],
        [0.0024, 0.0114, 0.0273,  ..., 0.0000, 0.0000, 0.0794],
        [0.0130, 0.0241, 0.0250,  ..., 0.0000, 0.0000, 0.0744],
        ...,
        [0.0018, 0.0107, 0.0278,  ..., 0.0000, 0.0000, 0.0808],
        [0.0018, 0.0107, 0.0279,  ..., 0.0000, 0.0000, 0.0808],
        [0.0018, 0.0107, 0.0279,  ..., 0.0000, 0.0000, 0.0808]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599441.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6324.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.7747, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17770.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-993.2960, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.2866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9527],
        [-1.6843],
        [-1.2896],
        ...,
        [-2.0677],
        [-2.0625],
        [-2.0607]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287445.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0197],
        [1.0225],
        [1.0239],
        ...,
        [1.0040],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369239.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0226],
        [1.0239],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369249.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.5851e-04,  ...,  0.0000e+00,
          1.3490e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5851e-04,  ...,  0.0000e+00,
          1.3490e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5851e-04,  ...,  0.0000e+00,
          1.3490e-03,  0.0000e+00],
        ...,
        [-2.6268e-03, -2.5062e-05,  5.5913e-03,  ..., -5.9739e-03,
          8.8449e-03, -2.2940e-03],
        [-4.3421e-03, -4.1427e-05,  9.3459e-03,  ..., -9.8748e-03,
          1.3740e-02, -3.7919e-03],
        [ 0.0000e+00,  0.0000e+00, -1.5851e-04,  ...,  0.0000e+00,
          1.3490e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1639.0950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4074, device='cuda:0')



h[100].sum tensor(99.4916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1794, device='cuda:0')



h[200].sum tensor(30.8002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0531,  ..., 0.0000, 0.0757, 0.0000],
        [0.0000, 0.0000, 0.0186,  ..., 0.0000, 0.0303, 0.0000],
        [0.0000, 0.0000, 0.0097,  ..., 0.0000, 0.0185, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55018.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0403, 0.0581, 0.0195,  ..., 0.0000, 0.0191, 0.0610],
        [0.0090, 0.0195, 0.0260,  ..., 0.0000, 0.0000, 0.0768],
        [0.0015, 0.0103, 0.0276,  ..., 0.0000, 0.0000, 0.0805],
        ...,
        [0.2423, 0.3080, 0.0000,  ..., 0.0000, 0.1796, 0.0010],
        [0.1567, 0.2020, 0.0057,  ..., 0.0000, 0.1063, 0.0206],
        [0.0804, 0.1078, 0.0127,  ..., 0.0000, 0.0449, 0.0419]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643435.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7791.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.8303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18860.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1037.9414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-367.2583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6080],
        [-1.1517],
        [-1.6283],
        ...,
        [ 0.1086],
        [-0.2160],
        [-0.7715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269602.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0226],
        [1.0239],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369249.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0199],
        [1.0227],
        [1.0240],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369259.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1442.9679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.9467, device='cuda:0')



h[100].sum tensor(95.7869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7127, device='cuda:0')



h[200].sum tensor(27.2439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47436.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0100, 0.0277,  ..., 0.0000, 0.0000, 0.0805],
        [0.0015, 0.0100, 0.0277,  ..., 0.0000, 0.0000, 0.0807],
        [0.0016, 0.0101, 0.0277,  ..., 0.0000, 0.0000, 0.0808],
        ...,
        [0.0259, 0.0401, 0.0230,  ..., 0.0000, 0.0031, 0.0698],
        [0.0097, 0.0202, 0.0264,  ..., 0.0000, 0.0000, 0.0778],
        [0.0016, 0.0103, 0.0281,  ..., 0.0000, 0.0000, 0.0817]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593211.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6151.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-90.6064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17818.6895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-992.6277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-369.4227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8723],
        [-1.6742],
        [-1.3592],
        ...,
        [-1.4368],
        [-1.7242],
        [-1.9261]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279195.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0199],
        [1.0227],
        [1.0240],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369259.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0200],
        [1.0228],
        [1.0241],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369268.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.4738e-04,  ...,  0.0000e+00,
          1.3406e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4738e-04,  ...,  0.0000e+00,
          1.3406e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4738e-04,  ...,  0.0000e+00,
          1.3406e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.4738e-04,  ...,  0.0000e+00,
          1.3406e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4738e-04,  ...,  0.0000e+00,
          1.3406e-03,  0.0000e+00],
        [-7.6646e-03, -7.0869e-05,  1.6687e-02,  ..., -1.7459e-02,
          2.3284e-02, -6.6910e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2271.7139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3982, device='cuda:0')



h[100].sum tensor(110.7123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.9165, device='cuda:0')



h[200].sum tensor(42.6231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7859, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0055, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0173,  ..., 0.0000, 0.0284, 0.0000],
        [0.0000, 0.0000, 0.0315,  ..., 0.0000, 0.0470, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74139.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0015, 0.0101, 0.0278,  ..., 0.0000, 0.0000, 0.0808],
        [0.0178, 0.0303, 0.0245,  ..., 0.0000, 0.0031, 0.0729],
        [0.0727, 0.0981, 0.0146,  ..., 0.0000, 0.0434, 0.0454],
        ...,
        [0.0322, 0.0483, 0.0220,  ..., 0.0000, 0.0131, 0.0663],
        [0.1119, 0.1471, 0.0113,  ..., 0.0000, 0.0699, 0.0362],
        [0.2087, 0.2670, 0.0042,  ..., 0.0000, 0.1514, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(754733.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11446.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(3.6492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21563.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1165.3920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-377.8378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5558],
        [-1.0854],
        [-0.5241],
        ...,
        [-1.4764],
        [-0.9414],
        [-0.4012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241636.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0200],
        [1.0228],
        [1.0241],
        ...,
        [1.0040],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369268.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(501.9860, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0201],
        [1.0229],
        [1.0242],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369278.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1431.0693, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3767, device='cuda:0')



h[100].sum tensor(95.2053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3974, device='cuda:0')



h[200].sum tensor(26.9504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4079, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0083,  ..., 0.0000, 0.0167, 0.0000],
        [0.0000, 0.0000, 0.0042,  ..., 0.0000, 0.0111, 0.0000],
        [0.0000, 0.0000, 0.0380,  ..., 0.0000, 0.0554, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46334.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0636, 0.0854, 0.0141,  ..., 0.0000, 0.0273, 0.0516],
        [0.0949, 0.1246, 0.0084,  ..., 0.0000, 0.0541, 0.0349],
        [0.2150, 0.2740, 0.0024,  ..., 0.0000, 0.1572, 0.0128],
        ...,
        [0.0015, 0.0105, 0.0284,  ..., 0.0000, 0.0000, 0.0820],
        [0.0015, 0.0105, 0.0284,  ..., 0.0000, 0.0000, 0.0820],
        [0.0015, 0.0105, 0.0284,  ..., 0.0000, 0.0000, 0.0820]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588543.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5706.6611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.2368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17599.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-980.9269, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.6044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6562],
        [-0.2237],
        [ 0.0839],
        ...,
        [-2.1161],
        [-2.1106],
        [-2.1086]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305888.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0201],
        [1.0229],
        [1.0242],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369278.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0202],
        [1.0230],
        [1.0243],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369288.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0013,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2177.1362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8884, device='cuda:0')



h[100].sum tensor(108.5250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(16.5285, device='cuda:0')



h[200].sum tensor(40.3902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0231,  ..., 0.0000, 0.0359, 0.0000],
        [0.0000, 0.0000, 0.0239,  ..., 0.0000, 0.0370, 0.0000],
        [0.0000, 0.0000, 0.0082,  ..., 0.0000, 0.0164, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68461.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1323, 0.1716, 0.0050,  ..., 0.0000, 0.0861, 0.0211],
        [0.1330, 0.1725, 0.0049,  ..., 0.0000, 0.0866, 0.0208],
        [0.0851, 0.1135, 0.0113,  ..., 0.0000, 0.0459, 0.0397],
        ...,
        [0.0015, 0.0108, 0.0286,  ..., 0.0000, 0.0000, 0.0821],
        [0.0015, 0.0108, 0.0286,  ..., 0.0000, 0.0000, 0.0821],
        [0.0015, 0.0108, 0.0286,  ..., 0.0000, 0.0000, 0.0821]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(720813.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10435.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-24.7123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21188.4746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1136.0914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.4097, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3415],
        [-0.4176],
        [-0.7489],
        ...,
        [-2.1245],
        [-2.1193],
        [-2.1172]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215714.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0202],
        [1.0230],
        [1.0243],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369288.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0231],
        [1.0244],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369298.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.3388e-03, -2.9445e-05,  7.2126e-03,  ..., -7.6240e-03,
          1.0964e-02, -2.9130e-03],
        [ 0.0000e+00,  0.0000e+00, -1.5572e-04,  ...,  0.0000e+00,
          1.3597e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5572e-04,  ...,  0.0000e+00,
          1.3597e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5572e-04,  ...,  0.0000e+00,
          1.3597e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5572e-04,  ...,  0.0000e+00,
          1.3597e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5572e-04,  ...,  0.0000e+00,
          1.3597e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2134.6597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3325, device='cuda:0')



h[100].sum tensor(107.7347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.6681, device='cuda:0')



h[200].sum tensor(39.3164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.9265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0243,  ..., 0.0000, 0.0377, 0.0000],
        [0.0000, 0.0000, 0.0074,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71090.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1560, 0.2015, 0.0048,  ..., 0.0000, 0.1064, 0.0179],
        [0.1006, 0.1330, 0.0085,  ..., 0.0000, 0.0588, 0.0314],
        [0.0793, 0.1070, 0.0124,  ..., 0.0000, 0.0406, 0.0417],
        ...,
        [0.0017, 0.0112, 0.0288,  ..., 0.0000, 0.0000, 0.0821],
        [0.0017, 0.0112, 0.0288,  ..., 0.0000, 0.0000, 0.0821],
        [0.0017, 0.0112, 0.0288,  ..., 0.0000, 0.0000, 0.0821]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(753239.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10978.4131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-17.6368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21370.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1135.0587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.8482, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1711],
        [ 0.0885],
        [ 0.0176],
        ...,
        [-2.1228],
        [-2.1173],
        [-2.1153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261438.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0231],
        [1.0244],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369298.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0232],
        [1.0245],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369307.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.1373e-03, -5.3274e-05,  1.3408e-02,  ..., -1.4026e-02,
          1.9058e-02, -5.3537e-03],
        [ 0.0000e+00,  0.0000e+00, -1.5867e-04,  ...,  0.0000e+00,
          1.3743e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5867e-04,  ...,  0.0000e+00,
          1.3743e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5867e-04,  ...,  0.0000e+00,
          1.3743e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5867e-04,  ...,  0.0000e+00,
          1.3743e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5867e-04,  ...,  0.0000e+00,
          1.3743e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1708.2731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7039, device='cuda:0')



h[100].sum tensor(99.8805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3434, device='cuda:0')



h[200].sum tensor(31.1769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0630,  ..., 0.0000, 0.0883, 0.0000],
        [0.0000, 0.0000, 0.0230,  ..., 0.0000, 0.0360, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0056, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55840.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4093, 0.5184, 0.0000,  ..., 0.0000, 0.3250, 0.0000],
        [0.2871, 0.3660, 0.0014,  ..., 0.0000, 0.2195, 0.0058],
        [0.2027, 0.2611, 0.0015,  ..., 0.0000, 0.1468, 0.0059],
        ...,
        [0.0018, 0.0116, 0.0289,  ..., 0.0000, 0.0000, 0.0822],
        [0.0018, 0.0116, 0.0289,  ..., 0.0000, 0.0000, 0.0822],
        [0.0018, 0.0116, 0.0289,  ..., 0.0000, 0.0000, 0.0822]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649999.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8080.1558, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.8405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19779.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1054.3895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.8386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2178],
        [ 0.2191],
        [ 0.2196],
        ...,
        [-2.1357],
        [-2.1303],
        [-2.1284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228118.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0232],
        [1.0245],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369307.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0233],
        [1.0246],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369317.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2170.7437, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8069, device='cuda:0')



h[100].sum tensor(108.1673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.9304, device='cuda:0')



h[200].sum tensor(39.3679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0057, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72701.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0040, 0.0141, 0.0281,  ..., 0.0000, 0.0000, 0.0802],
        [0.0024, 0.0123, 0.0286,  ..., 0.0000, 0.0000, 0.0812],
        [0.0016, 0.0115, 0.0288,  ..., 0.0000, 0.0000, 0.0816],
        ...,
        [0.0017, 0.0117, 0.0291,  ..., 0.0000, 0.0000, 0.0826],
        [0.0017, 0.0117, 0.0291,  ..., 0.0000, 0.0000, 0.0826],
        [0.0017, 0.0117, 0.0291,  ..., 0.0000, 0.0000, 0.0826]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(777085.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(11665.5693, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-19.8241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21988.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1145.9299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.3210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2155],
        [-1.5401],
        [-1.8314],
        ...,
        [-2.1478],
        [-2.1424],
        [-2.1403]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263169.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0233],
        [1.0246],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369317.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0234],
        [1.0247],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369326.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6906e-03, -2.2624e-05,  5.8022e-03,  ..., -6.1590e-03,
          9.1777e-03, -2.3462e-03],
        [-7.4694e-03, -6.2806e-05,  1.6399e-02,  ..., -1.7098e-02,
          2.2991e-02, -6.5133e-03],
        [ 0.0000e+00,  0.0000e+00, -1.6440e-04,  ...,  0.0000e+00,
          1.4002e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6440e-04,  ...,  0.0000e+00,
          1.4002e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6440e-04,  ...,  0.0000e+00,
          1.4002e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6440e-04,  ...,  0.0000e+00,
          1.4002e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1725.9651, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0943, device='cuda:0')



h[100].sum tensor(100.1462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5593, device='cuda:0')



h[200].sum tensor(30.9312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.4292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0895,  ..., 0.0000, 0.1232, 0.0000],
        [0.0000, 0.0000, 0.0246,  ..., 0.0000, 0.0382, 0.0000],
        [0.0000, 0.0000, 0.0168,  ..., 0.0000, 0.0278, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0058, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54385.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3397, 0.4319, 0.0000,  ..., 0.0000, 0.2653, 0.0000],
        [0.1973, 0.2544, 0.0042,  ..., 0.0000, 0.1424, 0.0152],
        [0.1130, 0.1495, 0.0105,  ..., 0.0000, 0.0715, 0.0334],
        ...,
        [0.0016, 0.0118, 0.0293,  ..., 0.0000, 0.0000, 0.0829],
        [0.0016, 0.0118, 0.0293,  ..., 0.0000, 0.0000, 0.0829],
        [0.0016, 0.0118, 0.0293,  ..., 0.0000, 0.0000, 0.0829]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639634.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7548.0112, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.2869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19459.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1041.1021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.0972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2103],
        [ 0.0254],
        [-0.3278],
        ...,
        [-2.1474],
        [-2.1509],
        [-2.1506]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248996.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0234],
        [1.0247],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369326.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0235],
        [1.0247],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369336.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.6468e-04,  ...,  0.0000e+00,
          1.4109e-03,  0.0000e+00],
        [-4.1308e-03, -3.4184e-05,  9.0106e-03,  ..., -9.4635e-03,
          1.3371e-02, -3.6014e-03],
        [ 0.0000e+00,  0.0000e+00, -1.6468e-04,  ...,  0.0000e+00,
          1.4109e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6468e-04,  ...,  0.0000e+00,
          1.4109e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6468e-04,  ...,  0.0000e+00,
          1.4109e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6468e-04,  ...,  0.0000e+00,
          1.4109e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1967.7399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.5377, device='cuda:0')



h[100].sum tensor(104.5786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.5696, device='cuda:0')



h[200].sum tensor(35.1745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.2576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0333,  ..., 0.0000, 0.0501, 0.0000],
        [0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0157, 0.0000],
        [0.0000, 0.0000, 0.0312,  ..., 0.0000, 0.0469, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59677.2852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1008, 0.1341, 0.0081,  ..., 0.0000, 0.0590, 0.0324],
        [0.1111, 0.1475, 0.0079,  ..., 0.0000, 0.0681, 0.0279],
        [0.2098, 0.2707, 0.0025,  ..., 0.0000, 0.1534, 0.0113],
        ...,
        [0.0016, 0.0118, 0.0294,  ..., 0.0000, 0.0000, 0.0834],
        [0.0016, 0.0118, 0.0295,  ..., 0.0000, 0.0000, 0.0834],
        [0.0016, 0.0118, 0.0295,  ..., 0.0000, 0.0000, 0.0834]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661171.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8149.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.5826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19797.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1070.5421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-375.0226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8484],
        [-0.4672],
        [-0.0988],
        ...,
        [-2.1723],
        [-2.1668],
        [-2.1645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256371.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0235],
        [1.0247],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369336.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0236],
        [1.0248],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369346., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.6619e-03, -2.9822e-05,  7.9871e-03,  ..., -8.3962e-03,
          1.2047e-02, -3.1920e-03],
        [ 0.0000e+00,  0.0000e+00, -1.6015e-04,  ...,  0.0000e+00,
          1.4276e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6015e-04,  ...,  0.0000e+00,
          1.4276e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6015e-04,  ...,  0.0000e+00,
          1.4276e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6015e-04,  ...,  0.0000e+00,
          1.4276e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6015e-04,  ...,  0.0000e+00,
          1.4276e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1671.4568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4955, device='cuda:0')



h[100].sum tensor(99.0055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6752, device='cuda:0')



h[200].sum tensor(29.4740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3047, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0252,  ..., 0.0000, 0.0391, 0.0000],
        [0.0000, 0.0000, 0.0082,  ..., 0.0000, 0.0167, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0059, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53860.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2503, 0.3200, 0.0037,  ..., 0.0000, 0.1880, 0.0118],
        [0.1402, 0.1833, 0.0072,  ..., 0.0000, 0.0972, 0.0224],
        [0.0803, 0.1092, 0.0132,  ..., 0.0000, 0.0479, 0.0414],
        ...,
        [0.0018, 0.0119, 0.0295,  ..., 0.0000, 0.0000, 0.0836],
        [0.0018, 0.0119, 0.0295,  ..., 0.0000, 0.0000, 0.0836],
        [0.0018, 0.0119, 0.0295,  ..., 0.0000, 0.0000, 0.0836]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641782., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7538.3965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.8358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19499.9492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1036.5065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.7823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1108],
        [ 0.0732],
        [ 0.0424],
        ...,
        [-2.1722],
        [-2.1661],
        [-2.1644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255212.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0236],
        [1.0248],
        ...,
        [1.0041],
        [1.0029],
        [1.0013]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369346., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0207],
        [1.0237],
        [1.0250],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369355.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0014,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1845.1132, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0574, device='cuda:0')



h[100].sum tensor(101.8755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6449, device='cuda:0')



h[200].sum tensor(32.4936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.8099, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0142,  ..., 0.0000, 0.0248, 0.0000],
        [0.0000, 0.0000, 0.0042,  ..., 0.0000, 0.0116, 0.0000],
        [0.0000, 0.0000, 0.0078,  ..., 0.0000, 0.0163, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59668.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1021, 0.1342, 0.0088,  ..., 0.0000, 0.0598, 0.0338],
        [0.0870, 0.1158, 0.0108,  ..., 0.0000, 0.0468, 0.0414],
        [0.0958, 0.1268, 0.0092,  ..., 0.0000, 0.0542, 0.0368],
        ...,
        [0.0018, 0.0119, 0.0297,  ..., 0.0000, 0.0000, 0.0840],
        [0.0018, 0.0119, 0.0297,  ..., 0.0000, 0.0000, 0.0840],
        [0.0018, 0.0119, 0.0297,  ..., 0.0000, 0.0000, 0.0840]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680521.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8829.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.4882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20459.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1076.1719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.0797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0616],
        [ 0.0481],
        [ 0.1094],
        ...,
        [-2.1923],
        [-2.1867],
        [-2.1847]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245265.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0207],
        [1.0237],
        [1.0250],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369355.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0237],
        [1.0251],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369364.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.6126e-04,  ...,  0.0000e+00,
          1.4676e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6126e-04,  ...,  0.0000e+00,
          1.4676e-03,  0.0000e+00],
        [-2.7972e-03, -2.2060e-05,  6.0825e-03,  ..., -6.4240e-03,
          9.6062e-03, -2.4373e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6126e-04,  ...,  0.0000e+00,
          1.4676e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6126e-04,  ...,  0.0000e+00,
          1.4676e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6126e-04,  ...,  0.0000e+00,
          1.4676e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1935.5283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8893, device='cuda:0')



h[100].sum tensor(103.5931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6580, device='cuda:0')



h[200].sum tensor(34.2014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.0983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0060, 0.0000],
        [0.0000, 0.0000, 0.0169,  ..., 0.0000, 0.0285, 0.0000],
        [0.0000, 0.0000, 0.0225,  ..., 0.0000, 0.0359, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60738.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1120, 0.1478, 0.0064,  ..., 0.0000, 0.0684, 0.0277],
        [0.1297, 0.1690, 0.0052,  ..., 0.0000, 0.0833, 0.0236],
        [0.1642, 0.2113, 0.0034,  ..., 0.0000, 0.1127, 0.0120],
        ...,
        [0.0017, 0.0119, 0.0298,  ..., 0.0000, 0.0000, 0.0848],
        [0.0017, 0.0119, 0.0298,  ..., 0.0000, 0.0000, 0.0848],
        [0.0017, 0.0119, 0.0298,  ..., 0.0000, 0.0000, 0.0848]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680742.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8748.6221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.2947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20286.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1079.9021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.6737, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2345],
        [ 0.2408],
        [ 0.2533],
        ...,
        [-2.2085],
        [-2.2027],
        [-2.2007]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264547.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0237],
        [1.0251],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369364.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(507.3420, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0238],
        [1.0251],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369374.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4278e-02, -1.1080e-04,  3.1765e-02,  ..., -3.2816e-02,
          4.3093e-02, -1.2438e-02],
        [-9.6131e-03, -7.4600e-05,  2.1336e-02,  ..., -2.2095e-02,
          2.9500e-02, -8.3748e-03],
        [-5.0972e-03, -3.9556e-05,  1.1239e-02,  ..., -1.1716e-02,
          1.6339e-02, -4.4406e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5792e-04,  ...,  0.0000e+00,
          1.4837e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5792e-04,  ...,  0.0000e+00,
          1.4837e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5792e-04,  ...,  0.0000e+00,
          1.4837e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1823.8093, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3371, device='cuda:0')



h[100].sum tensor(101.4810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2466, device='cuda:0')



h[200].sum tensor(32.2490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0981,  ..., 0.0000, 0.1348, 0.0000],
        [0.0000, 0.0000, 0.0966,  ..., 0.0000, 0.1328, 0.0000],
        [0.0000, 0.0000, 0.0476,  ..., 0.0000, 0.0686, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57696.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[4.9454e-01, 6.2331e-01, 0.0000e+00,  ..., 0.0000e+00, 3.9812e-01,
         0.0000e+00],
        [4.6428e-01, 5.8560e-01, 0.0000e+00,  ..., 0.0000e+00, 3.7189e-01,
         0.0000e+00],
        [3.3308e-01, 4.2236e-01, 2.2825e-04,  ..., 0.0000e+00, 2.5856e-01,
         2.6739e-03],
        ...,
        [1.9871e-03, 1.1926e-02, 2.9804e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.5186e-02],
        [1.9816e-03, 1.1921e-02, 2.9806e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.5196e-02],
        [1.9769e-03, 1.1918e-02, 2.9801e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.5185e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666037.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8314.6084, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.2501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20027.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1064.8599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-379.1746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2362],
        [ 0.2252],
        [ 0.1914],
        ...,
        [-2.2189],
        [-2.2129],
        [-2.2111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262640.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0238],
        [1.0251],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369374.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0210],
        [1.0239],
        [1.0252],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369383.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1938.0337, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8157, device='cuda:0')



h[100].sum tensor(103.1164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.6173, device='cuda:0')



h[200].sum tensor(34.1399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.0465, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60078.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0116, 0.0293,  ..., 0.0000, 0.0000, 0.0839],
        [0.0021, 0.0116, 0.0294,  ..., 0.0000, 0.0000, 0.0841],
        [0.0022, 0.0117, 0.0294,  ..., 0.0000, 0.0000, 0.0842],
        ...,
        [0.0023, 0.0119, 0.0298,  ..., 0.0000, 0.0000, 0.0852],
        [0.0023, 0.0119, 0.0298,  ..., 0.0000, 0.0000, 0.0852],
        [0.0023, 0.0119, 0.0298,  ..., 0.0000, 0.0000, 0.0852]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679072.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8750.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.2373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20352.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1075.9276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.3253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3833],
        [-2.2965],
        [-2.1326],
        ...,
        [-2.2266],
        [-2.2209],
        [-2.2189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258312.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0210],
        [1.0239],
        [1.0252],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369383.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0211],
        [1.0240],
        [1.0253],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369393.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1705.8048, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0983, device='cuda:0')



h[100].sum tensor(98.6027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4555, device='cuda:0')



h[200].sum tensor(29.7859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54211.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0280, 0.0429, 0.0240,  ..., 0.0000, 0.0069, 0.0708],
        [0.0048, 0.0145, 0.0288,  ..., 0.0000, 0.0000, 0.0829],
        [0.0025, 0.0116, 0.0294,  ..., 0.0000, 0.0000, 0.0841],
        ...,
        [0.0026, 0.0118, 0.0297,  ..., 0.0000, 0.0000, 0.0851],
        [0.0026, 0.0118, 0.0297,  ..., 0.0000, 0.0000, 0.0851],
        [0.0026, 0.0118, 0.0297,  ..., 0.0000, 0.0000, 0.0851]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648279.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7755.0410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.5473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19763.0117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1038.5924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.6046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1146],
        [-1.5464],
        [-1.7965],
        ...,
        [-2.2316],
        [-2.2257],
        [-2.2235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265170.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0211],
        [1.0240],
        [1.0253],
        ...,
        [1.0041],
        [1.0029],
        [1.0012]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369393.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0240],
        [1.0254],
        ...,
        [1.0041],
        [1.0029],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369403.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.9092e-03, -3.6290e-05,  1.0900e-02,  ..., -1.1312e-02,
          1.5882e-02, -4.2744e-03],
        [-5.7264e-03, -4.2330e-05,  1.2737e-02,  ..., -1.3194e-02,
          1.8275e-02, -4.9859e-03],
        [ 0.0000e+00,  0.0000e+00, -1.3299e-04,  ...,  0.0000e+00,
          1.5061e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.3299e-04,  ...,  0.0000e+00,
          1.5061e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3299e-04,  ...,  0.0000e+00,
          1.5061e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3299e-04,  ...,  0.0000e+00,
          1.5061e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2025.6716, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.8261, device='cuda:0')



h[100].sum tensor(103.8886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.1760, device='cuda:0')



h[200].sum tensor(35.4084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7571, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1122,  ..., 0.0000, 0.1531, 0.0000],
        [0.0000, 0.0000, 0.0391,  ..., 0.0000, 0.0574, 0.0000],
        [0.0000, 0.0000, 0.0130,  ..., 0.0000, 0.0233, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62406.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6978, 0.8710, 0.0000,  ..., 0.0000, 0.5725, 0.0000],
        [0.4056, 0.5090, 0.0021,  ..., 0.0000, 0.3207, 0.0073],
        [0.2077, 0.2640, 0.0058,  ..., 0.0000, 0.1502, 0.0186],
        ...,
        [0.0028, 0.0118, 0.0298,  ..., 0.0000, 0.0000, 0.0853],
        [0.0028, 0.0118, 0.0298,  ..., 0.0000, 0.0000, 0.0853],
        [0.0028, 0.0118, 0.0298,  ..., 0.0000, 0.0000, 0.0853]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692378.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9287.2090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.0627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20945.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1095.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.2782, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0717],
        [ 0.0822],
        [ 0.0716],
        ...,
        [-2.2298],
        [-2.2243],
        [-2.2229]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244598.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0240],
        [1.0254],
        ...,
        [1.0041],
        [1.0029],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369403.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0241],
        [1.0255],
        ...,
        [1.0041],
        [1.0029],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369412.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1835.1465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5373, device='cuda:0')



h[100].sum tensor(100.4202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8043, device='cuda:0')



h[200].sum tensor(32.0345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0112,  ..., 0.0000, 0.0210, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58530.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1138, 0.1473, 0.0102,  ..., 0.0000, 0.0710, 0.0319],
        [0.1586, 0.2023, 0.0066,  ..., 0.0000, 0.1081, 0.0220],
        [0.2135, 0.2690, 0.0038,  ..., 0.0000, 0.1549, 0.0126],
        ...,
        [0.0029, 0.0116, 0.0298,  ..., 0.0000, 0.0000, 0.0855],
        [0.0029, 0.0116, 0.0298,  ..., 0.0000, 0.0000, 0.0855],
        [0.0029, 0.0116, 0.0298,  ..., 0.0000, 0.0000, 0.0855]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(675866.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8819.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.8217, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20759.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1073.8359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.2828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0493],
        [ 0.1061],
        [ 0.1401],
        ...,
        [-2.2530],
        [-2.2473],
        [-2.2450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235850.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0241],
        [1.0255],
        ...,
        [1.0041],
        [1.0029],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369412.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0215],
        [1.0242],
        [1.0256],
        ...,
        [1.0040],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369422.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.2321e-04,  ...,  0.0000e+00,
          1.4929e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2321e-04,  ...,  0.0000e+00,
          1.4929e-03,  0.0000e+00],
        [-9.7765e-03, -6.9952e-05,  2.1924e-02,  ..., -2.2564e-02,
          3.0212e-02, -8.5090e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.2321e-04,  ...,  0.0000e+00,
          1.4929e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2321e-04,  ...,  0.0000e+00,
          1.4929e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2321e-04,  ...,  0.0000e+00,
          1.4929e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1725.0270, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1686, device='cuda:0')



h[100].sum tensor(98.5889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4944, device='cuda:0')



h[200].sum tensor(30.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0225,  ..., 0.0000, 0.0355, 0.0000],
        [0.0000, 0.0000, 0.0183,  ..., 0.0000, 0.0302, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52309.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0344, 0.0501, 0.0232,  ..., 0.0000, 0.0146, 0.0678],
        [0.1112, 0.1445, 0.0128,  ..., 0.0000, 0.0679, 0.0384],
        [0.1558, 0.1992, 0.0067,  ..., 0.0000, 0.1060, 0.0210],
        ...,
        [0.0029, 0.0114, 0.0300,  ..., 0.0000, 0.0000, 0.0859],
        [0.0029, 0.0114, 0.0300,  ..., 0.0000, 0.0000, 0.0859],
        [0.0029, 0.0114, 0.0300,  ..., 0.0000, 0.0000, 0.0859]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(632317.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7271.0068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.6908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19687.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1033.2848, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.3790, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8127],
        [-1.3394],
        [-0.8613],
        ...,
        [-2.2664],
        [-2.2613],
        [-2.2593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258331., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0215],
        [1.0242],
        [1.0256],
        ...,
        [1.0040],
        [1.0028],
        [1.0011]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369422.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0243],
        [1.0257],
        ...,
        [1.0040],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369431.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.1119e-03, -5.7100e-05,  1.8198e-02,  ..., -1.8737e-02,
          2.5360e-02, -7.0589e-03],
        [-6.3688e-03, -4.4830e-05,  1.4261e-02,  ..., -1.4711e-02,
          2.0232e-02, -5.5421e-03],
        [-1.7610e-02, -1.2396e-04,  3.9654e-02,  ..., -4.0678e-02,
          5.3308e-02, -1.5324e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.2499e-04,  ...,  0.0000e+00,
          1.4924e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2499e-04,  ...,  0.0000e+00,
          1.4924e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2499e-04,  ...,  0.0000e+00,
          1.4924e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1952.1438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5240, device='cuda:0')



h[100].sum tensor(102.5977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9030, device='cuda:0')



h[200].sum tensor(34.1214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0417,  ..., 0.0000, 0.0607, 0.0000],
        [0.0000, 0.0000, 0.1129,  ..., 0.0000, 0.1538, 0.0000],
        [0.0000, 0.0000, 0.1060,  ..., 0.0000, 0.1449, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59276.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3017, 0.3785, 0.0026,  ..., 0.0000, 0.2316, 0.0083],
        [0.5153, 0.6412, 0.0000,  ..., 0.0000, 0.4152, 0.0000],
        [0.5981, 0.7427, 0.0000,  ..., 0.0000, 0.4861, 0.0000],
        ...,
        [0.0029, 0.0113, 0.0302,  ..., 0.0000, 0.0000, 0.0864],
        [0.0029, 0.0113, 0.0302,  ..., 0.0000, 0.0000, 0.0864],
        [0.0029, 0.0113, 0.0302,  ..., 0.0000, 0.0000, 0.0864]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676104.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8543.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.9944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20487.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1071.1826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.3275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0415],
        [ 0.1588],
        [ 0.2165],
        ...,
        [-2.2795],
        [-2.2738],
        [-2.2715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273290.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0243],
        [1.0257],
        ...,
        [1.0040],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369431.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0244],
        [1.0258],
        ...,
        [1.0040],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369441.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.7921e-03, -3.3183e-05,  1.0715e-02,  ..., -1.1078e-02,
          1.5616e-02, -4.1693e-03],
        [ 0.0000e+00,  0.0000e+00, -1.2737e-04,  ...,  0.0000e+00,
          1.4934e-03,  0.0000e+00],
        [-2.5097e-03, -1.7379e-05,  5.5509e-03,  ..., -5.8018e-03,
          8.8897e-03, -2.1835e-03],
        ...,
        [-8.3323e-03, -5.7698e-05,  1.8725e-02,  ..., -1.9262e-02,
          2.6050e-02, -7.2493e-03],
        [ 0.0000e+00,  0.0000e+00, -1.2737e-04,  ...,  0.0000e+00,
          1.4934e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.2737e-04,  ...,  0.0000e+00,
          1.4934e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2261.1631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6586, device='cuda:0')



h[100].sum tensor(107.9592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.2954, device='cuda:0')



h[200].sum tensor(39.4702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0399,  ..., 0.0000, 0.0585, 0.0000],
        [0.0000, 0.0000, 0.0302,  ..., 0.0000, 0.0461, 0.0000],
        [0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0273, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0159,  ..., 0.0000, 0.0271, 0.0000],
        [0.0000, 0.0000, 0.0195,  ..., 0.0000, 0.0318, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66887.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3053, 0.3803, 0.0000,  ..., 0.0000, 0.2340, 0.0024],
        [0.2330, 0.2905, 0.0000,  ..., 0.0000, 0.1716, 0.0027],
        [0.1693, 0.2121, 0.0004,  ..., 0.0000, 0.1167, 0.0071],
        ...,
        [0.1192, 0.1537, 0.0097,  ..., 0.0000, 0.0739, 0.0307],
        [0.0934, 0.1221, 0.0142,  ..., 0.0000, 0.0525, 0.0423],
        [0.0313, 0.0460, 0.0246,  ..., 0.0000, 0.0113, 0.0715]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718294.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9947.0605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.5096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21495.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1120.9147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.7756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2657],
        [ 0.2794],
        [ 0.2839],
        ...,
        [-0.9657],
        [-1.3235],
        [-1.7380]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260463.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0244],
        [1.0258],
        ...,
        [1.0040],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369441.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0218],
        [1.0245],
        [1.0259],
        ...,
        [1.0040],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369450.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1993.5211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1399, device='cuda:0')



h[100].sum tensor(103.4021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2436, device='cuda:0')



h[200].sum tensor(34.8590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.5712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62114.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0113, 0.0299,  ..., 0.0000, 0.0000, 0.0856],
        [0.0029, 0.0113, 0.0300,  ..., 0.0000, 0.0000, 0.0858],
        [0.0029, 0.0114, 0.0300,  ..., 0.0000, 0.0000, 0.0858],
        ...,
        [0.0031, 0.0116, 0.0304,  ..., 0.0000, 0.0000, 0.0869],
        [0.0031, 0.0116, 0.0304,  ..., 0.0000, 0.0000, 0.0869],
        [0.0031, 0.0116, 0.0304,  ..., 0.0000, 0.0000, 0.0869]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704425.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9575.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-98.1103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21309.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1094.1107, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.7347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5087],
        [-2.5178],
        [-2.5095],
        ...,
        [-2.2979],
        [-2.2921],
        [-2.2902]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260388.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0218],
        [1.0245],
        [1.0259],
        ...,
        [1.0040],
        [1.0028],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369450.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0220],
        [1.0246],
        [1.0261],
        ...,
        [1.0040],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369458.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2273.6582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7628, device='cuda:0')



h[100].sum tensor(108.5012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.3530, device='cuda:0')



h[200].sum tensor(40.2187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.5258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68594.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0028, 0.0113, 0.0300,  ..., 0.0000, 0.0000, 0.0863],
        [0.0033, 0.0119, 0.0300,  ..., 0.0000, 0.0000, 0.0863],
        [0.0048, 0.0135, 0.0297,  ..., 0.0000, 0.0000, 0.0856],
        ...,
        [0.0031, 0.0116, 0.0305,  ..., 0.0000, 0.0000, 0.0876],
        [0.0031, 0.0116, 0.0305,  ..., 0.0000, 0.0000, 0.0876],
        [0.0031, 0.0116, 0.0305,  ..., 0.0000, 0.0000, 0.0876]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(735106.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10581.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.4732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21889.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1136.1637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.9332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3582],
        [-2.2245],
        [-2.0081],
        ...,
        [-2.3131],
        [-2.3073],
        [-2.3052]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251188.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0220],
        [1.0246],
        [1.0261],
        ...,
        [1.0040],
        [1.0027],
        [1.0010]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369458.5938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(506.4197, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0221],
        [1.0247],
        [1.0262],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369467.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.3943e-04,  ...,  0.0000e+00,
          1.5075e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3943e-04,  ...,  0.0000e+00,
          1.5075e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3943e-04,  ...,  0.0000e+00,
          1.5075e-03,  0.0000e+00],
        ...,
        [-4.3039e-03, -2.8366e-05,  9.6482e-03,  ..., -9.9743e-03,
          1.4258e-02, -3.7423e-03],
        [ 0.0000e+00,  0.0000e+00, -1.3943e-04,  ...,  0.0000e+00,
          1.5075e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3943e-04,  ...,  0.0000e+00,
          1.5075e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2136.9512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0346, device='cuda:0')



h[100].sum tensor(106.2422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.8443, device='cuda:0')



h[200].sum tensor(38.0768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.6071, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0307,  ..., 0.0000, 0.0466, 0.0000],
        [0.0000, 0.0000, 0.0100,  ..., 0.0000, 0.0196, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65689.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0273, 0.0275,  ..., 0.0000, 0.0009, 0.0802],
        [0.0508, 0.0699, 0.0204,  ..., 0.0000, 0.0217, 0.0623],
        [0.0951, 0.1238, 0.0114,  ..., 0.0000, 0.0554, 0.0396],
        ...,
        [0.2493, 0.3137, 0.0033,  ..., 0.0000, 0.1841, 0.0108],
        [0.1288, 0.1656, 0.0114,  ..., 0.0000, 0.0847, 0.0342],
        [0.0416, 0.0588, 0.0228,  ..., 0.0000, 0.0199, 0.0680]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731262., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10539.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.9267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21887.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1122.9419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.2144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7644],
        [-0.3711],
        [-0.0572],
        ...,
        [-0.0580],
        [-0.6096],
        [-1.2900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248315.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0221],
        [1.0247],
        [1.0262],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369467.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0249],
        [1.0263],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369476., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1762.1724, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1347, device='cuda:0')



h[100].sum tensor(99.5527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4756, device='cuda:0')



h[200].sum tensor(31.3599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0077,  ..., 0.0000, 0.0164, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52321.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0646, 0.0862, 0.0175,  ..., 0.0000, 0.0316, 0.0560],
        [0.0208, 0.0331, 0.0266,  ..., 0.0000, 0.0029, 0.0786],
        [0.0084, 0.0181, 0.0292,  ..., 0.0000, 0.0000, 0.0850],
        ...,
        [0.0030, 0.0116, 0.0307,  ..., 0.0000, 0.0000, 0.0889],
        [0.0030, 0.0115, 0.0307,  ..., 0.0000, 0.0000, 0.0889],
        [0.0030, 0.0115, 0.0307,  ..., 0.0000, 0.0000, 0.0889]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(637439.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7352.8501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-139.3426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19667.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1033.8478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.5755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6810],
        [-1.1382],
        [-1.3833],
        ...,
        [-2.3433],
        [-2.3373],
        [-2.3351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281548.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0249],
        [1.0263],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369476., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0224],
        [1.0250],
        [1.0264],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369484.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5067e-03, -2.8734e-05,  1.0152e-02,  ..., -1.0462e-02,
          1.4932e-02, -3.9172e-03],
        [ 0.0000e+00,  0.0000e+00, -1.3267e-04,  ...,  0.0000e+00,
          1.5340e-03,  0.0000e+00],
        [-1.2837e-02, -8.1849e-05,  2.9163e-02,  ..., -2.9800e-02,
          3.9698e-02, -1.1158e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.3267e-04,  ...,  0.0000e+00,
          1.5340e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3267e-04,  ...,  0.0000e+00,
          1.5340e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3267e-04,  ...,  0.0000e+00,
          1.5340e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2015.6396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1257, device='cuda:0')



h[100].sum tensor(103.5504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2357, device='cuda:0')



h[200].sum tensor(35.4647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.5612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0174, 0.0000],
        [0.0000, 0.0000, 0.0572,  ..., 0.0000, 0.0815, 0.0000],
        [0.0000, 0.0000, 0.0285,  ..., 0.0000, 0.0438, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61384.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2120, 0.2669, 0.0010,  ..., 0.0000, 0.1522, 0.0072],
        [0.2873, 0.3590, 0.0000,  ..., 0.0000, 0.2161, 0.0000],
        [0.2822, 0.3528, 0.0010,  ..., 0.0000, 0.2116, 0.0049],
        ...,
        [0.0030, 0.0117, 0.0308,  ..., 0.0000, 0.0000, 0.0892],
        [0.0030, 0.0116, 0.0308,  ..., 0.0000, 0.0000, 0.0892],
        [0.0030, 0.0116, 0.0308,  ..., 0.0000, 0.0000, 0.0892]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692804.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9385.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.8245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21267.7598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1101.3307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.6198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3444],
        [ 0.3516],
        [ 0.3581],
        ...,
        [-2.3532],
        [-2.3474],
        [-2.3451]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238563.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0224],
        [1.0250],
        [1.0264],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369484.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0225],
        [1.0251],
        [1.0265],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369493.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0540e-03, -3.7963e-05,  1.3706e-02,  ..., -1.4065e-02,
          1.9554e-02, -5.2611e-03],
        [-3.8177e-03, -2.3940e-05,  8.5938e-03,  ..., -8.8696e-03,
          1.2895e-02, -3.3176e-03],
        [-2.9803e-03, -1.8689e-05,  6.6798e-03,  ..., -6.9242e-03,
          1.0402e-02, -2.5900e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.3282e-04,  ...,  0.0000e+00,
          1.5286e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3282e-04,  ...,  0.0000e+00,
          1.5286e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3282e-04,  ...,  0.0000e+00,
          1.5286e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1759.8352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1858, device='cuda:0')



h[100].sum tensor(98.9910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9509, device='cuda:0')



h[200].sum tensor(30.3597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3836, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0434,  ..., 0.0000, 0.0635, 0.0000],
        [0.0000, 0.0000, 0.0393,  ..., 0.0000, 0.0581, 0.0000],
        [0.0000, 0.0000, 0.0144,  ..., 0.0000, 0.0254, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52228.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.0171e-01, 2.5280e-01, 4.0261e-05,  ..., 0.0000e+00, 1.4366e-01,
         2.9852e-03],
        [1.6753e-01, 2.1122e-01, 2.7215e-03,  ..., 0.0000e+00, 1.1456e-01,
         1.3117e-02],
        [9.7959e-02, 1.2649e-01, 1.0738e-02,  ..., 0.0000e+00, 5.5325e-02,
         3.9321e-02],
        ...,
        [3.1351e-03, 1.1907e-02, 3.0824e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.9057e-02],
        [3.1330e-03, 1.1906e-02, 3.0825e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.9056e-02],
        [3.1319e-03, 1.1904e-02, 3.0822e-02,  ..., 0.0000e+00, 0.0000e+00,
         8.9047e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640846.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7464.7036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-140.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19960.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1033.9814, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.4246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2192],
        [-0.0773],
        [-0.5908],
        ...,
        [-2.3529],
        [-2.3478],
        [-2.3464]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274107.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0225],
        [1.0251],
        [1.0265],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369493.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0252],
        [1.0266],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369502.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2030.1334, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2603, device='cuda:0')



h[100].sum tensor(103.3710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7571, device='cuda:0')



h[200].sum tensor(34.3373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.9526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        [0.0000, 0.0000, 0.0302,  ..., 0.0000, 0.0460, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60458.8945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0196, 0.0322, 0.0270,  ..., 0.0000, 0.0054, 0.0786],
        [0.0660, 0.0889, 0.0176,  ..., 0.0000, 0.0385, 0.0540],
        [0.2091, 0.2642, 0.0072,  ..., 0.0000, 0.1503, 0.0229],
        ...,
        [0.0033, 0.0123, 0.0309,  ..., 0.0000, 0.0000, 0.0888],
        [0.0033, 0.0123, 0.0309,  ..., 0.0000, 0.0000, 0.0888],
        [0.0033, 0.0123, 0.0309,  ..., 0.0000, 0.0000, 0.0888]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(687289.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9046.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.6099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21200.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1086.3037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.8968, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1242],
        [-0.5616],
        [-0.0918],
        ...,
        [-2.3591],
        [-2.3536],
        [-2.3513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250780.3281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0252],
        [1.0266],
        ...,
        [1.0039],
        [1.0027],
        [1.0009]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369502.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0227],
        [1.0253],
        [1.0267],
        ...,
        [1.0039],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369511.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1673e-03, -1.9210e-05,  7.1264e-03,  ..., -7.3709e-03,
          1.0972e-02, -2.7514e-03],
        [ 0.0000e+00,  0.0000e+00, -1.3683e-04,  ...,  0.0000e+00,
          1.5138e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3683e-04,  ...,  0.0000e+00,
          1.5138e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.3683e-04,  ...,  0.0000e+00,
          1.5138e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3683e-04,  ...,  0.0000e+00,
          1.5138e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3683e-04,  ...,  0.0000e+00,
          1.5138e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1824.5662, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5875, device='cuda:0')



h[100].sum tensor(99.6937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1730, device='cuda:0')



h[200].sum tensor(30.0448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0238,  ..., 0.0000, 0.0377, 0.0000],
        [0.0000, 0.0000, 0.0186,  ..., 0.0000, 0.0308, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55108.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1450, 0.1843, 0.0045,  ..., 0.0000, 0.0963, 0.0178],
        [0.1136, 0.1461, 0.0097,  ..., 0.0000, 0.0694, 0.0335],
        [0.0490, 0.0675, 0.0209,  ..., 0.0000, 0.0212, 0.0645],
        ...,
        [0.0034, 0.0125, 0.0310,  ..., 0.0000, 0.0000, 0.0888],
        [0.0034, 0.0125, 0.0310,  ..., 0.0000, 0.0000, 0.0888],
        [0.0034, 0.0125, 0.0310,  ..., 0.0000, 0.0000, 0.0887]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(667563.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8230.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.8232, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20661.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1043.1243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.6498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0102],
        [-0.1814],
        [-0.5244],
        ...,
        [-2.3631],
        [-2.3574],
        [-2.3552]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276064.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0227],
        [1.0253],
        [1.0267],
        ...,
        [1.0039],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369511.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0254],
        [1.0268],
        ...,
        [1.0039],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369520.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0015,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2102.2573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9312, device='cuda:0')



h[100].sum tensor(104.3213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1281, device='cuda:0')



h[200].sum tensor(34.1864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.4244, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0088,  ..., 0.0000, 0.0179, 0.0000],
        [0.0000, 0.0000, 0.0142,  ..., 0.0000, 0.0250, 0.0000],
        [0.0000, 0.0000, 0.0196,  ..., 0.0000, 0.0321, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61410.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0983, 0.1273, 0.0102,  ..., 0.0000, 0.0570, 0.0395],
        [0.1072, 0.1384, 0.0092,  ..., 0.0000, 0.0644, 0.0343],
        [0.1183, 0.1523, 0.0072,  ..., 0.0000, 0.0737, 0.0279],
        ...,
        [0.0036, 0.0128, 0.0311,  ..., 0.0000, 0.0000, 0.0887],
        [0.0036, 0.0128, 0.0311,  ..., 0.0000, 0.0000, 0.0887],
        [0.0036, 0.0128, 0.0311,  ..., 0.0000, 0.0000, 0.0887]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(698057.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9214.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.9022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21502.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1082.9412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.3562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0792],
        [ 0.0104],
        [-0.1023],
        ...,
        [-2.3667],
        [-2.3610],
        [-2.3589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259062.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0254],
        [1.0268],
        ...,
        [1.0039],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369520.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0255],
        [1.0268],
        ...,
        [1.0039],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369529.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.6869e-03, -2.1624e-05,  8.3443e-03,  ..., -8.5945e-03,
          1.2547e-02, -3.2016e-03],
        [ 0.0000e+00,  0.0000e+00, -1.3766e-04,  ...,  0.0000e+00,
          1.5045e-03,  0.0000e+00],
        [-1.1361e-02, -6.6634e-05,  2.5999e-02,  ..., -2.6484e-02,
          3.5531e-02, -9.8655e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.3766e-04,  ...,  0.0000e+00,
          1.5045e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3766e-04,  ...,  0.0000e+00,
          1.5045e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3766e-04,  ...,  0.0000e+00,
          1.5045e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1938.5292, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9342, device='cuda:0')



h[100].sum tensor(101.3118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9177, device='cuda:0')



h[200].sum tensor(30.7448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6132, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0069,  ..., 0.0000, 0.0154, 0.0000],
        [0.0000, 0.0000, 0.0352,  ..., 0.0000, 0.0523, 0.0000],
        [0.0000, 0.0000, 0.0218,  ..., 0.0000, 0.0347, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55221.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1192, 0.1532, 0.0064,  ..., 0.0000, 0.0751, 0.0267],
        [0.1763, 0.2241, 0.0013,  ..., 0.0000, 0.1242, 0.0093],
        [0.1734, 0.2210, 0.0026,  ..., 0.0000, 0.1216, 0.0106],
        ...,
        [0.0038, 0.0131, 0.0312,  ..., 0.0000, 0.0000, 0.0886],
        [0.0038, 0.0131, 0.0312,  ..., 0.0000, 0.0000, 0.0886],
        [0.0038, 0.0131, 0.0312,  ..., 0.0000, 0.0000, 0.0886]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657250.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7963.2964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-134.1884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20862.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1045.7758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-396.8112, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1759],
        [ 0.1833],
        [ 0.0880],
        ...,
        [-2.3695],
        [-2.3636],
        [-2.3615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254706.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0255],
        [1.0268],
        ...,
        [1.0039],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369529.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0256],
        [1.0269],
        ...,
        [1.0038],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369538.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.5280e-03, -2.0347e-05,  7.9923e-03,  ..., -8.2310e-03,
          1.2091e-02, -3.0630e-03],
        [ 0.0000e+00,  0.0000e+00, -1.3737e-04,  ...,  0.0000e+00,
          1.5083e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3737e-04,  ...,  0.0000e+00,
          1.5083e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.3737e-04,  ...,  0.0000e+00,
          1.5083e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3737e-04,  ...,  0.0000e+00,
          1.5083e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.3737e-04,  ...,  0.0000e+00,
          1.5083e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1955.5708, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9957, device='cuda:0')



h[100].sum tensor(101.2630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9518, device='cuda:0')



h[200].sum tensor(30.3204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0173,  ..., 0.0000, 0.0291, 0.0000],
        [0.0000, 0.0000, 0.0082,  ..., 0.0000, 0.0170, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56473.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1702, 0.2163, 0.0082,  ..., 0.0000, 0.1192, 0.0260],
        [0.0793, 0.1048, 0.0154,  ..., 0.0000, 0.0446, 0.0476],
        [0.0271, 0.0411, 0.0258,  ..., 0.0000, 0.0055, 0.0756],
        ...,
        [0.0039, 0.0134, 0.0313,  ..., 0.0000, 0.0000, 0.0887],
        [0.0039, 0.0134, 0.0313,  ..., 0.0000, 0.0000, 0.0887],
        [0.0039, 0.0134, 0.0312,  ..., 0.0000, 0.0000, 0.0887]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668577.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8371.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-128.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21330.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1054.2756, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.0204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0747],
        [-0.4532],
        [-0.9173],
        ...,
        [-2.3755],
        [-2.3698],
        [-2.3676]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241172.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0256],
        [1.0269],
        ...,
        [1.0038],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369538.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0257],
        [1.0269],
        ...,
        [1.0038],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369547., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.2493e-03, -1.8426e-05,  7.3594e-03,  ..., -7.5871e-03,
          1.1282e-02, -2.8205e-03],
        [ 0.0000e+00,  0.0000e+00, -1.4001e-04,  ...,  0.0000e+00,
          1.5199e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4001e-04,  ...,  0.0000e+00,
          1.5199e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.4001e-04,  ...,  0.0000e+00,
          1.5199e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4001e-04,  ...,  0.0000e+00,
          1.5199e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4001e-04,  ...,  0.0000e+00,
          1.5199e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1881.6639, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3256, device='cuda:0')



h[100].sum tensor(99.8004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0282, device='cuda:0')



h[200].sum tensor(28.4864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0183,  ..., 0.0000, 0.0304, 0.0000],
        [0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0162, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0062, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0063, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54625.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1338, 0.1715, 0.0078,  ..., 0.0000, 0.0880, 0.0255],
        [0.0818, 0.1083, 0.0144,  ..., 0.0000, 0.0433, 0.0469],
        [0.0509, 0.0707, 0.0210,  ..., 0.0000, 0.0167, 0.0631],
        ...,
        [0.0036, 0.0134, 0.0314,  ..., 0.0000, 0.0000, 0.0893],
        [0.0036, 0.0134, 0.0314,  ..., 0.0000, 0.0000, 0.0893],
        [0.0036, 0.0133, 0.0314,  ..., 0.0000, 0.0000, 0.0893]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661296.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8155.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-133.1311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21403.1895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1048.1365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.9502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0310],
        [-0.1018],
        [-0.1860],
        ...,
        [-2.3888],
        [-2.3835],
        [-2.3818]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235857., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0257],
        [1.0269],
        ...,
        [1.0038],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369547., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(504.7466, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0258],
        [1.0270],
        ...,
        [1.0038],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369555.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3075e-02, -7.2904e-05,  3.0083e-02,  ..., -3.0556e-02,
          4.0872e-02, -1.1347e-02],
        [ 0.0000e+00,  0.0000e+00, -1.4283e-04,  ...,  0.0000e+00,
          1.5291e-03,  0.0000e+00],
        [-8.1174e-03, -4.5261e-05,  1.8622e-02,  ..., -1.8970e-02,
          2.5954e-02, -7.0448e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.4283e-04,  ...,  0.0000e+00,
          1.5291e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4283e-04,  ...,  0.0000e+00,
          1.5291e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4283e-04,  ...,  0.0000e+00,
          1.5291e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2248.7561, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1937, device='cuda:0')



h[100].sum tensor(106.0266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.3793, device='cuda:0')



h[200].sum tensor(34.6175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0156, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0841,  ..., 0.0000, 0.1162, 0.0000],
        [0.0000, 0.0000, 0.0997,  ..., 0.0000, 0.1368, 0.0000],
        [0.0000, 0.0000, 0.0230,  ..., 0.0000, 0.0366, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63976.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6190, 0.7713, 0.0000,  ..., 0.0000, 0.5076, 0.0000],
        [0.4955, 0.6185, 0.0000,  ..., 0.0000, 0.4008, 0.0000],
        [0.2843, 0.3574, 0.0000,  ..., 0.0000, 0.2182, 0.0000],
        ...,
        [0.0031, 0.0130, 0.0317,  ..., 0.0000, 0.0000, 0.0903],
        [0.0031, 0.0130, 0.0317,  ..., 0.0000, 0.0000, 0.0903],
        [0.0031, 0.0130, 0.0317,  ..., 0.0000, 0.0000, 0.0903]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(710124.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9436.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.0555, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22213.6133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1103.2688, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.1743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0593],
        [ 0.1099],
        [ 0.1556],
        ...,
        [-2.4163],
        [-2.4106],
        [-2.4084]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262381.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0258],
        [1.0270],
        ...,
        [1.0038],
        [1.0026],
        [1.0008]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369555.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0259],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369564., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.4478e-04,  ...,  0.0000e+00,
          1.5465e-03,  0.0000e+00],
        [-7.4840e-03, -4.1028e-05,  1.7184e-02,  ..., -1.7504e-02,
          2.4102e-02, -6.4937e-03],
        [-7.4723e-03, -4.0964e-05,  1.7157e-02,  ..., -1.7477e-02,
          2.4067e-02, -6.4836e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.4478e-04,  ...,  0.0000e+00,
          1.5465e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4478e-04,  ...,  0.0000e+00,
          1.5465e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.4478e-04,  ...,  0.0000e+00,
          1.5465e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2040.9677, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9305, device='cuda:0')



h[100].sum tensor(102.2587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0217, device='cuda:0')



h[200].sum tensor(30.6550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0173, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0176,  ..., 0.0000, 0.0294, 0.0000],
        [0.0000, 0.0000, 0.0320,  ..., 0.0000, 0.0483, 0.0000],
        [0.0000, 0.0000, 0.0989,  ..., 0.0000, 0.1359, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57163.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1172, 0.1534, 0.0126,  ..., 0.0000, 0.0747, 0.0383],
        [0.2169, 0.2759, 0.0054,  ..., 0.0000, 0.1605, 0.0175],
        [0.3888, 0.4876, 0.0000,  ..., 0.0000, 0.3087, 0.0000],
        ...,
        [0.0028, 0.0129, 0.0318,  ..., 0.0000, 0.0000, 0.0910],
        [0.0028, 0.0129, 0.0318,  ..., 0.0000, 0.0000, 0.0910],
        [0.0028, 0.0129, 0.0318,  ..., 0.0000, 0.0000, 0.0910]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672839.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8287.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.1326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21621.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1067.7919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.4744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5705],
        [-0.2479],
        [ 0.0407],
        ...,
        [-2.4330],
        [-2.4268],
        [-2.4246]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248651.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0259],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369564., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0235],
        [1.0259],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369572.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8049e-03, -2.5898e-05,  1.0992e-02,  ..., -1.1248e-02,
          1.6069e-02, -4.1684e-03],
        [-1.5898e-02, -8.5690e-05,  3.6720e-02,  ..., -3.7216e-02,
          4.9557e-02, -1.3792e-02],
        [-7.4188e-03, -3.9987e-05,  1.7054e-02,  ..., -1.7366e-02,
          2.3960e-02, -6.4360e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5212e-04,  ...,  0.0000e+00,
          1.5637e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5212e-04,  ...,  0.0000e+00,
          1.5637e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5212e-04,  ...,  0.0000e+00,
          1.5637e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1829.8942, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6050, device='cuda:0')



h[100].sum tensor(98.5460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6297, device='cuda:0')



h[200].sum tensor(26.6967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1116,  ..., 0.0000, 0.1525, 0.0000],
        [0.0000, 0.0000, 0.0831,  ..., 0.0000, 0.1154, 0.0000],
        [0.0000, 0.0000, 0.0848,  ..., 0.0000, 0.1176, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53130.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4281, 0.5363, 0.0000,  ..., 0.0000, 0.3426, 0.0000],
        [0.4459, 0.5578, 0.0000,  ..., 0.0000, 0.3577, 0.0000],
        [0.4425, 0.5536, 0.0000,  ..., 0.0000, 0.3547, 0.0000],
        ...,
        [0.0024, 0.0130, 0.0319,  ..., 0.0000, 0.0000, 0.0917],
        [0.0024, 0.0129, 0.0319,  ..., 0.0000, 0.0000, 0.0917],
        [0.0024, 0.0129, 0.0319,  ..., 0.0000, 0.0000, 0.0917]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655199.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7453.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-124.4414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20998.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1040.0251, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.3984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1562],
        [ 0.1666],
        [ 0.1527],
        ...,
        [-2.4517],
        [-2.4460],
        [-2.4435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276006.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0235],
        [1.0259],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369572.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0235],
        [1.0259],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369572.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2366.3225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7200, device='cuda:0')



h[100].sum tensor(107.5635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.7764, device='cuda:0')



h[200].sum tensor(35.8619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7924, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0064, 0.0000],
        [0.0000, 0.0000, 0.0307,  ..., 0.0000, 0.0468, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0065, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68013.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1197, 0.1556, 0.0069,  ..., 0.0000, 0.0767, 0.0274],
        [0.1496, 0.1931, 0.0069,  ..., 0.0000, 0.1026, 0.0229],
        [0.3274, 0.4125, 0.0036,  ..., 0.0000, 0.2558, 0.0116],
        ...,
        [0.0024, 0.0130, 0.0319,  ..., 0.0000, 0.0000, 0.0917],
        [0.0024, 0.0129, 0.0319,  ..., 0.0000, 0.0000, 0.0917],
        [0.0024, 0.0129, 0.0319,  ..., 0.0000, 0.0000, 0.0917]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(745237.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10353.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.6177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22954.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1131.9781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.5973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1642],
        [ 0.1262],
        [ 0.0842],
        ...,
        [-2.4517],
        [-2.4460],
        [-2.4435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268675.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0235],
        [1.0259],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369572.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0236],
        [1.0260],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369581.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.6475e-03, -2.9926e-05,  1.2955e-02,  ..., -1.3231e-02,
          1.8657e-02, -4.8983e-03],
        [-2.7430e-03, -1.4535e-05,  6.2085e-03,  ..., -6.4263e-03,
          9.8747e-03, -2.3791e-03],
        [-8.9344e-03, -4.7343e-05,  2.0590e-02,  ..., -2.0932e-02,
          2.8596e-02, -7.7493e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6298e-04,  ...,  0.0000e+00,
          1.5805e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6298e-04,  ...,  0.0000e+00,
          1.5805e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6298e-04,  ...,  0.0000e+00,
          1.5805e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2098.6931, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2909, device='cuda:0')



h[100].sum tensor(102.9494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.7740, device='cuda:0')



h[200].sum tensor(31.0161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.9740, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0431,  ..., 0.0000, 0.0634, 0.0000],
        [0.0000, 0.0000, 0.0559,  ..., 0.0000, 0.0801, 0.0000],
        [0.0000, 0.0000, 0.0404,  ..., 0.0000, 0.0599, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0066, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59154.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2847, 0.3583, 0.0000,  ..., 0.0000, 0.2184, 0.0000],
        [0.3010, 0.3787, 0.0000,  ..., 0.0000, 0.2324, 0.0000],
        [0.2885, 0.3631, 0.0000,  ..., 0.0000, 0.2214, 0.0000],
        ...,
        [0.0020, 0.0131, 0.0319,  ..., 0.0000, 0.0000, 0.0923],
        [0.0020, 0.0131, 0.0319,  ..., 0.0000, 0.0000, 0.0923],
        [0.0020, 0.0131, 0.0319,  ..., 0.0000, 0.0000, 0.0923]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678747.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8140.4043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.1744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21504.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1079.6342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.9489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2745],
        [ 0.2478],
        [ 0.2084],
        ...,
        [-2.4674],
        [-2.4619],
        [-2.4593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-277862., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0236],
        [1.0260],
        [1.0271],
        ...,
        [1.0038],
        [1.0025],
        [1.0007]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369581.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0260],
        [1.0271],
        ...,
        [1.0037],
        [1.0025],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369589.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.9506e-03, -4.1417e-05,  1.8325e-02,  ..., -1.8642e-02,
          2.5680e-02, -6.8946e-03],
        [-1.1322e-02, -5.8980e-05,  2.6169e-02,  ..., -2.6548e-02,
          3.5892e-02, -9.8182e-03],
        [-9.9337e-03, -5.1748e-05,  2.2939e-02,  ..., -2.3293e-02,
          3.1687e-02, -8.6144e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.7238e-04,  ...,  0.0000e+00,
          1.5986e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.7238e-04,  ...,  0.0000e+00,
          1.5986e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.7238e-04,  ...,  0.0000e+00,
          1.5986e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2181.0928, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0337, device='cuda:0')



h[100].sum tensor(104.2476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.7378, device='cuda:0')



h[200].sum tensor(32.1787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1998, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0741,  ..., 0.0000, 0.1040, 0.0000],
        [0.0000, 0.0000, 0.0896,  ..., 0.0000, 0.1241, 0.0000],
        [0.0000, 0.0000, 0.0938,  ..., 0.0000, 0.1295, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60245.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.3674, 0.4631, 0.0000,  ..., 0.0000, 0.2897, 0.0000],
        [0.4408, 0.5539, 0.0000,  ..., 0.0000, 0.3528, 0.0000],
        [0.4637, 0.5824, 0.0000,  ..., 0.0000, 0.3724, 0.0000],
        ...,
        [0.0080, 0.0208, 0.0306,  ..., 0.0000, 0.0000, 0.0896],
        [0.0111, 0.0246, 0.0300,  ..., 0.0000, 0.0000, 0.0880],
        [0.0080, 0.0208, 0.0306,  ..., 0.0000, 0.0000, 0.0896]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682605.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8178.3379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.9250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21520.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1086.5281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.2477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2125],
        [ 0.2387],
        [ 0.2384],
        ...,
        [-2.2014],
        [-2.1446],
        [-2.1422]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285455.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0260],
        [1.0271],
        ...,
        [1.0037],
        [1.0025],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369589.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0261],
        [1.0271],
        ...,
        [1.0037],
        [1.0025],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369597.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.6263e-02, -8.3282e-05,  3.7717e-02,  ..., -3.8165e-02,
          5.0958e-02, -1.4100e-02],
        [-6.2958e-03, -3.2241e-05,  1.4492e-02,  ..., -1.4775e-02,
          2.0718e-02, -5.4586e-03],
        [-8.7025e-03, -4.4566e-05,  2.0100e-02,  ..., -2.0423e-02,
          2.8020e-02, -7.5452e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.7918e-04,  ...,  0.0000e+00,
          1.6171e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.7918e-04,  ...,  0.0000e+00,
          1.6171e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.7918e-04,  ...,  0.0000e+00,
          1.6171e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2023.0861, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5756, device='cuda:0')



h[100].sum tensor(101.3790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8254, device='cuda:0')



h[200].sum tensor(29.0966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0736,  ..., 0.0000, 0.1034, 0.0000],
        [0.0000, 0.0000, 0.0907,  ..., 0.0000, 0.1257, 0.0000],
        [0.0000, 0.0000, 0.0823,  ..., 0.0000, 0.1148, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57566.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4390, 0.5541, 0.0000,  ..., 0.0000, 0.3512, 0.0000],
        [0.4329, 0.5462, 0.0000,  ..., 0.0000, 0.3457, 0.0000],
        [0.3954, 0.4990, 0.0000,  ..., 0.0000, 0.3131, 0.0000],
        ...,
        [0.0018, 0.0136, 0.0320,  ..., 0.0000, 0.0000, 0.0930],
        [0.0018, 0.0136, 0.0320,  ..., 0.0000, 0.0000, 0.0930],
        [0.0018, 0.0136, 0.0319,  ..., 0.0000, 0.0000, 0.0930]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(673963.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7873.8130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.2926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21342.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1070.9374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.7661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2633],
        [ 0.2795],
        [ 0.2906],
        ...,
        [-2.4907],
        [-2.4848],
        [-2.4825]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278464.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0261],
        [1.0271],
        ...,
        [1.0037],
        [1.0025],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369597.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0261],
        [1.0272],
        ...,
        [1.0037],
        [1.0024],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369606.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.8740e-03, -4.4672e-05,  2.0525e-02,  ..., -2.0843e-02,
          2.8597e-02, -7.6924e-03],
        [-7.3186e-03, -3.6842e-05,  1.6895e-02,  ..., -1.7190e-02,
          2.3870e-02, -6.3441e-03],
        [-8.7608e-03, -4.4103e-05,  2.0261e-02,  ..., -2.0577e-02,
          2.8253e-02, -7.5943e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8701e-04,  ...,  0.0000e+00,
          1.6287e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8701e-04,  ...,  0.0000e+00,
          1.6287e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8701e-04,  ...,  0.0000e+00,
          1.6287e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1888.6825, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0413, device='cuda:0')



h[100].sum tensor(99.2460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4240, device='cuda:0')



h[200].sum tensor(26.7612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9853, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0622,  ..., 0.0000, 0.0886, 0.0000],
        [0.0000, 0.0000, 0.0811,  ..., 0.0000, 0.1133, 0.0000],
        [0.0000, 0.0000, 0.0912,  ..., 0.0000, 0.1265, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53695.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.4841, 0.6102, 0.0000,  ..., 0.0000, 0.3895, 0.0000],
        [0.5749, 0.7232, 0.0000,  ..., 0.0000, 0.4677, 0.0000],
        [0.5931, 0.7461, 0.0000,  ..., 0.0000, 0.4834, 0.0000],
        ...,
        [0.0016, 0.0136, 0.0320,  ..., 0.0000, 0.0000, 0.0936],
        [0.0016, 0.0136, 0.0320,  ..., 0.0000, 0.0000, 0.0936],
        [0.0016, 0.0136, 0.0320,  ..., 0.0000, 0.0000, 0.0936]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652635.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7180.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-118.3994, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20933.8457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1053.1641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.6089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1453],
        [ 0.1391],
        [ 0.1381],
        ...,
        [-2.5109],
        [-2.5048],
        [-2.5023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274027.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0261],
        [1.0272],
        ...,
        [1.0037],
        [1.0024],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369606.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0261],
        [1.0272],
        ...,
        [1.0037],
        [1.0024],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369613.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.2332e-03, -1.5999e-05,  7.3557e-03,  ..., -7.6004e-03,
          1.1480e-02, -2.8022e-03],
        [ 0.0000e+00,  0.0000e+00, -2.0258e-04,  ...,  0.0000e+00,
          1.6375e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0258e-04,  ...,  0.0000e+00,
          1.6375e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0258e-04,  ...,  0.0000e+00,
          1.6375e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0258e-04,  ...,  0.0000e+00,
          1.6375e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0258e-04,  ...,  0.0000e+00,
          1.6375e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2537.8345, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.5327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9367, device='cuda:0')



h[100].sum tensor(110.5287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.1082, device='cuda:0')



h[200].sum tensor(38.1045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0167,  ..., 0.0000, 0.0290, 0.0000],
        [0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0168, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76891.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1186, 0.1578, 0.0092,  ..., 0.0000, 0.0753, 0.0336],
        [0.0639, 0.0906, 0.0180,  ..., 0.0000, 0.0327, 0.0594],
        [0.0255, 0.0429, 0.0264,  ..., 0.0000, 0.0050, 0.0804],
        ...,
        [0.0013, 0.0134, 0.0322,  ..., 0.0000, 0.0000, 0.0945],
        [0.0082, 0.0223, 0.0307,  ..., 0.0000, 0.0000, 0.0904],
        [0.0321, 0.0518, 0.0257,  ..., 0.0000, 0.0135, 0.0767]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(811519., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(12051.4482, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-31.1110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24061.1992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1196.3569, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.6854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1770],
        [-0.6614],
        [-1.2110],
        ...,
        [-2.4454],
        [-2.2616],
        [-1.9231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273081.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0261],
        [1.0272],
        ...,
        [1.0037],
        [1.0024],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369613.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0239],
        [1.0261],
        [1.0272],
        ...,
        [1.0036],
        [1.0024],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369621.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0016,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1878.0564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2848, device='cuda:0')



h[100].sum tensor(100.1816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5586, device='cuda:0')



h[200].sum tensor(27.5036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53989.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0010, 0.0131, 0.0318,  ..., 0.0000, 0.0000, 0.0937],
        [0.0010, 0.0131, 0.0319,  ..., 0.0000, 0.0000, 0.0938],
        [0.0011, 0.0131, 0.0319,  ..., 0.0000, 0.0000, 0.0939],
        ...,
        [0.0011, 0.0134, 0.0323,  ..., 0.0000, 0.0000, 0.0951],
        [0.0011, 0.0134, 0.0323,  ..., 0.0000, 0.0000, 0.0951],
        [0.0011, 0.0134, 0.0323,  ..., 0.0000, 0.0000, 0.0951]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656513.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7056.4497, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-117.5890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20642.0293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1058.1725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.3156, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7704],
        [-2.7974],
        [-2.8079],
        ...,
        [-2.5558],
        [-2.5497],
        [-2.5472]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-306321., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0239],
        [1.0261],
        [1.0272],
        ...,
        [1.0036],
        [1.0024],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369621.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(435.4192, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0261],
        [1.0273],
        ...,
        [1.0036],
        [1.0023],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369630.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.3339e-04,  ...,  0.0000e+00,
          1.6496e-03,  0.0000e+00],
        [-2.5323e-03, -1.2106e-05,  5.7050e-03,  ..., -5.9627e-03,
          9.3840e-03, -2.1938e-03],
        [-2.5323e-03, -1.2106e-05,  5.7050e-03,  ..., -5.9627e-03,
          9.3840e-03, -2.1938e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.3339e-04,  ...,  0.0000e+00,
          1.6496e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3339e-04,  ...,  0.0000e+00,
          1.6496e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3339e-04,  ...,  0.0000e+00,
          1.6496e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1869.9236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1146, device='cuda:0')



h[100].sum tensor(100.4140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4645, device='cuda:0')



h[200].sum tensor(27.6097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0105,  ..., 0.0000, 0.0211, 0.0000],
        [0.0000, 0.0000, 0.0106,  ..., 0.0000, 0.0212, 0.0000],
        [0.0000, 0.0000, 0.0106,  ..., 0.0000, 0.0212, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54481.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0737, 0.1037, 0.0157,  ..., 0.0000, 0.0367, 0.0545],
        [0.0626, 0.0898, 0.0180,  ..., 0.0000, 0.0272, 0.0616],
        [0.0594, 0.0858, 0.0187,  ..., 0.0000, 0.0243, 0.0635],
        ...,
        [0.0012, 0.0137, 0.0323,  ..., 0.0000, 0.0000, 0.0951],
        [0.0012, 0.0137, 0.0323,  ..., 0.0000, 0.0000, 0.0951],
        [0.0012, 0.0137, 0.0323,  ..., 0.0000, 0.0000, 0.0951]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(665797.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7231.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.0145, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20627.8535, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1056.9276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-405.5868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8551],
        [-1.2982],
        [-1.7105],
        ...,
        [-2.5680],
        [-2.5617],
        [-2.5588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-328549.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0261],
        [1.0273],
        ...,
        [1.0036],
        [1.0023],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369630.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0241],
        [1.0262],
        [1.0273],
        ...,
        [1.0035],
        [1.0023],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369638.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2146.7905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0000, device='cuda:0')



h[100].sum tensor(104.5720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1662, device='cuda:0')



h[200].sum tensor(31.9363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.4728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62767.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0723, 0.1022, 0.0164,  ..., 0.0000, 0.0407, 0.0512],
        [0.0658, 0.0939, 0.0179,  ..., 0.0000, 0.0367, 0.0551],
        [0.0543, 0.0796, 0.0205,  ..., 0.0000, 0.0303, 0.0617],
        ...,
        [0.0018, 0.0144, 0.0322,  ..., 0.0000, 0.0000, 0.0939],
        [0.0018, 0.0144, 0.0322,  ..., 0.0000, 0.0000, 0.0939],
        [0.0018, 0.0144, 0.0322,  ..., 0.0000, 0.0000, 0.0939]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718435.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9158.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.2585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21948.5840, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1104.9000, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-405.9341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1405],
        [-0.1883],
        [-0.2899],
        ...,
        [-2.2819],
        [-2.2578],
        [-2.1653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-295055.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0241],
        [1.0262],
        [1.0273],
        ...,
        [1.0035],
        [1.0023],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369638.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0242],
        [1.0262],
        [1.0274],
        ...,
        [1.0035],
        [1.0022],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369647.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.4018e-03, -1.1092e-05,  5.4467e-03,  ..., -5.6650e-03,
          9.0417e-03, -2.0799e-03],
        [-3.5603e-03, -1.6442e-05,  8.1737e-03,  ..., -8.3978e-03,
          1.2592e-02, -3.0832e-03],
        [ 0.0000e+00,  0.0000e+00, -2.0618e-04,  ...,  0.0000e+00,
          1.6821e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0618e-04,  ...,  0.0000e+00,
          1.6821e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0618e-04,  ...,  0.0000e+00,
          1.6821e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0618e-04,  ...,  0.0000e+00,
          1.6821e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2382.0554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8678, device='cuda:0')



h[100].sum tensor(107.8221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.3051, device='cuda:0')



h[200].sum tensor(35.3409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.1930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0466,  ..., 0.0000, 0.0686, 0.0000],
        [0.0000, 0.0000, 0.0171,  ..., 0.0000, 0.0297, 0.0000],
        [0.0000, 0.0000, 0.0084,  ..., 0.0000, 0.0181, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68972.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2215, 0.2849, 0.0005,  ..., 0.0000, 0.1616, 0.0031],
        [0.1435, 0.1887, 0.0080,  ..., 0.0000, 0.0949, 0.0251],
        [0.0726, 0.1012, 0.0164,  ..., 0.0000, 0.0388, 0.0510],
        ...,
        [0.0090, 0.0228, 0.0307,  ..., 0.0000, 0.0000, 0.0891],
        [0.0121, 0.0265, 0.0300,  ..., 0.0000, 0.0000, 0.0874],
        [0.0090, 0.0227, 0.0307,  ..., 0.0000, 0.0000, 0.0891]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(753315.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10571.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.1946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23028.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1145.1777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.7801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1873],
        [-0.1032],
        [-0.5759],
        ...,
        [-2.2491],
        [-2.1991],
        [-2.2402]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260369.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0242],
        [1.0262],
        [1.0274],
        ...,
        [1.0035],
        [1.0022],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369647.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0243],
        [1.0263],
        [1.0275],
        ...,
        [1.0034],
        [1.0021],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369656.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.9140e-04,  ...,  0.0000e+00,
          1.6932e-03,  0.0000e+00],
        [-4.3991e-03, -1.9966e-05,  1.0181e-02,  ..., -1.0385e-02,
          1.5195e-02, -3.8089e-03],
        [-7.7582e-03, -3.5211e-05,  1.8102e-02,  ..., -1.8315e-02,
          2.5504e-02, -6.7172e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.9140e-04,  ...,  0.0000e+00,
          1.6932e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9140e-04,  ...,  0.0000e+00,
          1.6932e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9140e-04,  ...,  0.0000e+00,
          1.6932e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2077.1929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3858, device='cuda:0')



h[100].sum tensor(102.3521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7205, device='cuda:0')



h[200].sum tensor(29.8242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0189,  ..., 0.0000, 0.0320, 0.0000],
        [0.0000, 0.0000, 0.0334,  ..., 0.0000, 0.0512, 0.0000],
        [0.0000, 0.0000, 0.0580,  ..., 0.0000, 0.0834, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57616.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1423, 0.1862, 0.0101,  ..., 0.0000, 0.0931, 0.0302],
        [0.2486, 0.3176, 0.0034,  ..., 0.0000, 0.1842, 0.0107],
        [0.3602, 0.4557, 0.0000,  ..., 0.0000, 0.2797, 0.0000],
        ...,
        [0.0041, 0.0159, 0.0318,  ..., 0.0000, 0.0000, 0.0911],
        [0.0041, 0.0159, 0.0318,  ..., 0.0000, 0.0000, 0.0911],
        [0.0041, 0.0159, 0.0318,  ..., 0.0000, 0.0000, 0.0911]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679399.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8185.3877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-129.0175, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21342.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1062.9093, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.1511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3872],
        [ 0.0422],
        [ 0.2555],
        ...,
        [-2.5261],
        [-2.5201],
        [-2.5177]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278523.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0243],
        [1.0263],
        [1.0275],
        ...,
        [1.0034],
        [1.0021],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369656.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0263],
        [1.0276],
        ...,
        [1.0033],
        [1.0021],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369664.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2886e-03, -2.8049e-05,  1.4674e-02,  ..., -1.4858e-02,
          2.1032e-02, -5.4437e-03],
        [ 0.0000e+00,  0.0000e+00, -1.8059e-04,  ...,  0.0000e+00,
          1.7008e-03,  0.0000e+00],
        [-4.9049e-03, -2.1877e-05,  1.1405e-02,  ..., -1.1589e-02,
          1.6779e-02, -4.2459e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8059e-04,  ...,  0.0000e+00,
          1.7008e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8059e-04,  ...,  0.0000e+00,
          1.7008e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8059e-04,  ...,  0.0000e+00,
          1.7008e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1849.9001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.1447, device='cuda:0')



h[100].sum tensor(98.4077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8221, device='cuda:0')



h[200].sum tensor(25.8191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0234,  ..., 0.0000, 0.0381, 0.0000],
        [0.0000, 0.0000, 0.0513,  ..., 0.0000, 0.0747, 0.0000],
        [0.0000, 0.0000, 0.0726,  ..., 0.0000, 0.1022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52136.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2710, 0.3434, 0.0000,  ..., 0.0000, 0.2026, 0.0000],
        [0.4160, 0.5237, 0.0000,  ..., 0.0000, 0.3271, 0.0000],
        [0.5828, 0.7320, 0.0000,  ..., 0.0000, 0.4705, 0.0000],
        ...,
        [0.0050, 0.0164, 0.0317,  ..., 0.0000, 0.0000, 0.0902],
        [0.0050, 0.0164, 0.0317,  ..., 0.0000, 0.0000, 0.0902],
        [0.0050, 0.0164, 0.0317,  ..., 0.0000, 0.0000, 0.0902]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655881.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7642.5649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-156.5746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21058.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1027.7723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.7096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2901],
        [ 0.2748],
        [ 0.2423],
        ...,
        [-2.5151],
        [-2.5095],
        [-2.5074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260980.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0263],
        [1.0276],
        ...,
        [1.0033],
        [1.0021],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369664.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0264],
        [1.0276],
        ...,
        [1.0033],
        [1.0020],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369672.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.8180e-04,  ...,  0.0000e+00,
          1.6954e-03,  0.0000e+00],
        [-4.5166e-03, -1.9797e-05,  1.0505e-02,  ..., -1.0680e-02,
          1.5602e-02, -3.9090e-03],
        [-3.0531e-03, -1.3382e-05,  7.0420e-03,  ..., -7.2197e-03,
          1.1096e-02, -2.6424e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8180e-04,  ...,  0.0000e+00,
          1.6954e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8180e-04,  ...,  0.0000e+00,
          1.6954e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8180e-04,  ...,  0.0000e+00,
          1.6954e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1975.1600, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4996, device='cuda:0')



h[100].sum tensor(100.8528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1244, device='cuda:0')



h[200].sum tensor(28.0883, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6043, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0107,  ..., 0.0000, 0.0212, 0.0000],
        [0.0000, 0.0000, 0.0160,  ..., 0.0000, 0.0282, 0.0000],
        [0.0000, 0.0000, 0.0636,  ..., 0.0000, 0.0907, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53223.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0981, 0.1308, 0.0124,  ..., 0.0000, 0.0577, 0.0365],
        [0.1734, 0.2236, 0.0065,  ..., 0.0000, 0.1189, 0.0195],
        [0.3052, 0.3867, 0.0000,  ..., 0.0000, 0.2318, 0.0000],
        ...,
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897],
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897],
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(657867.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7786.0928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-165.4024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21175.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1035.0531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.9337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0198],
        [ 0.1374],
        [ 0.2268],
        ...,
        [-2.5133],
        [-2.5076],
        [-2.5022]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247602.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0264],
        [1.0276],
        ...,
        [1.0033],
        [1.0020],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369672.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0264],
        [1.0276],
        ...,
        [1.0033],
        [1.0020],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369672.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1958.6364, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9305, device='cuda:0')



h[100].sum tensor(100.5820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8097, device='cuda:0')



h[200].sum tensor(27.8131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56655.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0052, 0.0163, 0.0313,  ..., 0.0000, 0.0000, 0.0885],
        [0.0086, 0.0206, 0.0306,  ..., 0.0000, 0.0000, 0.0866],
        [0.0337, 0.0516, 0.0252,  ..., 0.0000, 0.0119, 0.0715],
        ...,
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897],
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897],
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(692506.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9076.3887, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.3838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22189.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1064.0497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-406.4319, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6147],
        [-1.3198],
        [-0.8584],
        ...,
        [-2.5122],
        [-2.5071],
        [-2.5051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-224917.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0264],
        [1.0276],
        ...,
        [1.0033],
        [1.0020],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369672.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0264],
        [1.0276],
        ...,
        [1.0033],
        [1.0020],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369672.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2104.8269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3122, device='cuda:0')



h[100].sum tensor(102.9776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6798, device='cuda:0')



h[200].sum tensor(30.2478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5824, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59108.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0052, 0.0163, 0.0313,  ..., 0.0000, 0.0000, 0.0885],
        [0.0052, 0.0164, 0.0314,  ..., 0.0000, 0.0000, 0.0886],
        [0.0053, 0.0165, 0.0314,  ..., 0.0000, 0.0000, 0.0886],
        ...,
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897],
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897],
        [0.0055, 0.0168, 0.0318,  ..., 0.0000, 0.0000, 0.0897]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(699861.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9262.7139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-144.7885, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22245.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1075.9868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.5793, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5486],
        [-2.5145],
        [-2.4397],
        ...,
        [-2.5132],
        [-2.5075],
        [-2.5051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-230623.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0264],
        [1.0276],
        ...,
        [1.0033],
        [1.0020],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369672.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0264],
        [1.0277],
        ...,
        [1.0032],
        [1.0020],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369680.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1989.0876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5477, device='cuda:0')



h[100].sum tensor(101.9102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1510, device='cuda:0')



h[200].sum tensor(28.8608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55726.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0053, 0.0164, 0.0315,  ..., 0.0000, 0.0000, 0.0884],
        [0.0053, 0.0165, 0.0316,  ..., 0.0000, 0.0000, 0.0885],
        [0.0054, 0.0166, 0.0316,  ..., 0.0000, 0.0000, 0.0885],
        ...,
        [0.0057, 0.0169, 0.0320,  ..., 0.0000, 0.0000, 0.0897],
        [0.0057, 0.0169, 0.0320,  ..., 0.0000, 0.0000, 0.0897],
        [0.0057, 0.0169, 0.0320,  ..., 0.0000, 0.0000, 0.0897]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(682097.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8809.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-171.4167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22040.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1059.1914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.4392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3859],
        [-2.5058],
        [-2.6064],
        ...,
        [-2.5180],
        [-2.5121],
        [-2.5095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-219801.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0264],
        [1.0277],
        ...,
        [1.0032],
        [1.0020],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369680.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0249],
        [1.0265],
        [1.0278],
        ...,
        [1.0032],
        [1.0019],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369687.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2042.7056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7220, device='cuda:0')



h[100].sum tensor(103.5712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8004, device='cuda:0')



h[200].sum tensor(30.5047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57939.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0051, 0.0162, 0.0317,  ..., 0.0000, 0.0000, 0.0887],
        [0.0051, 0.0163, 0.0318,  ..., 0.0000, 0.0000, 0.0888],
        [0.0052, 0.0163, 0.0318,  ..., 0.0000, 0.0000, 0.0888],
        ...,
        [0.0055, 0.0167, 0.0322,  ..., 0.0000, 0.0000, 0.0899],
        [0.0055, 0.0167, 0.0322,  ..., 0.0000, 0.0000, 0.0899],
        [0.0055, 0.0167, 0.0322,  ..., 0.0000, 0.0000, 0.0899]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702552.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9234.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-169.7594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22216.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1068.5603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.4810, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4344],
        [-2.5789],
        [-2.6662],
        ...,
        [-2.5384],
        [-2.5327],
        [-2.5305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241038.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0249],
        [1.0265],
        [1.0278],
        ...,
        [1.0032],
        [1.0019],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369687.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0249],
        [1.0265],
        [1.0278],
        ...,
        [1.0032],
        [1.0019],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369687.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.8916e-04,  ...,  0.0000e+00,
          1.6620e-03,  0.0000e+00],
        [-3.3485e-03, -1.4172e-05,  7.7600e-03,  ..., -7.9317e-03,
          1.2005e-02, -2.8969e-03],
        [-2.4939e-03, -1.0555e-05,  5.7314e-03,  ..., -5.9075e-03,
          9.3654e-03, -2.1576e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8916e-04,  ...,  0.0000e+00,
          1.6620e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8916e-04,  ...,  0.0000e+00,
          1.6620e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8916e-04,  ...,  0.0000e+00,
          1.6620e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1889.3960, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4621, device='cuda:0')



h[100].sum tensor(101.0685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9977, device='cuda:0')



h[200].sum tensor(27.9612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1713, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0079,  ..., 0.0000, 0.0174, 0.0000],
        [0.0000, 0.0000, 0.0173,  ..., 0.0000, 0.0298, 0.0000],
        [0.0000, 0.0000, 0.0601,  ..., 0.0000, 0.0860, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54207.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0835, 0.1128, 0.0150,  ..., 0.0000, 0.0461, 0.0432],
        [0.1661, 0.2153, 0.0079,  ..., 0.0000, 0.1132, 0.0230],
        [0.3066, 0.3899, 0.0000,  ..., 0.0000, 0.2340, 0.0000],
        ...,
        [0.0055, 0.0167, 0.0322,  ..., 0.0000, 0.0000, 0.0899],
        [0.0055, 0.0167, 0.0322,  ..., 0.0000, 0.0000, 0.0899],
        [0.0055, 0.0167, 0.0322,  ..., 0.0000, 0.0000, 0.0899]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(680251.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8383.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-179.6788, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21547.9082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1038.2042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.1920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1503],
        [ 0.0514],
        [ 0.1864],
        ...,
        [-2.5380],
        [-2.5322],
        [-2.5298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272643.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0249],
        [1.0265],
        [1.0278],
        ...,
        [1.0032],
        [1.0019],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369687.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0250],
        [1.0266],
        [1.0279],
        ...,
        [1.0031],
        [1.0019],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369695.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.2432e-03, -1.3488e-05,  7.5196e-03,  ..., -7.6889e-03,
          1.1681e-02, -2.8053e-03],
        [ 0.0000e+00,  0.0000e+00, -1.9250e-04,  ...,  0.0000e+00,
          1.6470e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9250e-04,  ...,  0.0000e+00,
          1.6470e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.9250e-04,  ...,  0.0000e+00,
          1.6470e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9250e-04,  ...,  0.0000e+00,
          1.6470e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9250e-04,  ...,  0.0000e+00,
          1.6470e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2312.3242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4219, device='cuda:0')



h[100].sum tensor(108.5800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.9525, device='cuda:0')



h[200].sum tensor(35.7417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.4728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0233,  ..., 0.0000, 0.0376, 0.0000],
        [0.0000, 0.0000, 0.0419,  ..., 0.0000, 0.0621, 0.0000],
        [0.0000, 0.0000, 0.0349,  ..., 0.0000, 0.0527, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63659.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2284, 0.2933, 0.0000,  ..., 0.0000, 0.1674, 0.0000],
        [0.2895, 0.3696, 0.0000,  ..., 0.0000, 0.2200, 0.0000],
        [0.3010, 0.3841, 0.0000,  ..., 0.0000, 0.2298, 0.0000],
        ...,
        [0.0051, 0.0163, 0.0324,  ..., 0.0000, 0.0000, 0.0905],
        [0.0051, 0.0163, 0.0324,  ..., 0.0000, 0.0000, 0.0905],
        [0.0051, 0.0163, 0.0324,  ..., 0.0000, 0.0000, 0.0905]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(733357.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10126.5332, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-152.7287, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22821.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1105.0267, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.2505, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2168],
        [ 0.1992],
        [ 0.1820],
        ...,
        [-2.5600],
        [-2.5543],
        [-2.5519]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248985.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0250],
        [1.0266],
        [1.0279],
        ...,
        [1.0031],
        [1.0019],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369695.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0252],
        [1.0267],
        [1.0279],
        ...,
        [1.0031],
        [1.0019],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369702.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.4136e-03, -3.8465e-05,  2.2240e-02,  ..., -2.2337e-02,
          3.0824e-02, -8.1409e-03],
        [-1.3425e-02, -5.4856e-05,  3.1796e-02,  ..., -3.1854e-02,
          4.3255e-02, -1.1610e-02],
        [-9.6552e-03, -3.9453e-05,  2.2816e-02,  ..., -2.2910e-02,
          3.1573e-02, -8.3498e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8394e-04,  ...,  0.0000e+00,
          1.6505e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8394e-04,  ...,  0.0000e+00,
          1.6505e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8394e-04,  ...,  0.0000e+00,
          1.6505e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2330.7559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6534, device='cuda:0')



h[100].sum tensor(109.0777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.0805, device='cuda:0')



h[200].sum tensor(36.0517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1209,  ..., 0.0000, 0.1650, 0.0000],
        [0.0000, 0.0000, 0.1053,  ..., 0.0000, 0.1447, 0.0000],
        [0.0000, 0.0000, 0.0873,  ..., 0.0000, 0.1214, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65805.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.6771, 0.8515, 0.0000,  ..., 0.0000, 0.5546, 0.0000],
        [0.6340, 0.7977, 0.0000,  ..., 0.0000, 0.5174, 0.0000],
        [0.5700, 0.7176, 0.0000,  ..., 0.0000, 0.4619, 0.0000],
        ...,
        [0.0049, 0.0160, 0.0326,  ..., 0.0000, 0.0000, 0.0912],
        [0.0049, 0.0160, 0.0326,  ..., 0.0000, 0.0000, 0.0912],
        [0.0049, 0.0160, 0.0326,  ..., 0.0000, 0.0000, 0.0912]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(740989.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10409.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-139.6072, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23098.6387, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1121.2980, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.0516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0220],
        [ 0.0198],
        [ 0.0230],
        ...,
        [-2.3690],
        [-2.3010],
        [-2.2985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251122.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0252],
        [1.0267],
        [1.0279],
        ...,
        [1.0031],
        [1.0019],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369702.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0253],
        [1.0268],
        [1.0280],
        ...,
        [1.0031],
        [1.0018],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369709.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2265.6353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4703, device='cuda:0')



h[100].sum tensor(107.8563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.4262, device='cuda:0')



h[200].sum tensor(34.1961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8035, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63249.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0936, 0.1247, 0.0132,  ..., 0.0000, 0.0514, 0.0380],
        [0.0994, 0.1319, 0.0125,  ..., 0.0000, 0.0563, 0.0359],
        [0.1104, 0.1454, 0.0108,  ..., 0.0000, 0.0655, 0.0309],
        ...,
        [0.0049, 0.0157, 0.0330,  ..., 0.0000, 0.0000, 0.0921],
        [0.0049, 0.0157, 0.0330,  ..., 0.0000, 0.0000, 0.0921],
        [0.0049, 0.0157, 0.0329,  ..., 0.0000, 0.0000, 0.0921]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(722003.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10045.0137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-140.2940, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23066.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1117.7368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.7687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0162],
        [-0.0050],
        [ 0.0185],
        ...,
        [-2.6012],
        [-2.5955],
        [-2.5929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222855.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0253],
        [1.0268],
        [1.0280],
        ...,
        [1.0031],
        [1.0018],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369709.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0254],
        [1.0269],
        [1.0280],
        ...,
        [1.0030],
        [1.0018],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369716.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1815.0676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.8774, device='cuda:0')



h[100].sum tensor(100.2646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6743, device='cuda:0')



h[200].sum tensor(25.5743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0091,  ..., 0.0000, 0.0190, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50366.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0074, 0.0185, 0.0323,  ..., 0.0000, 0.0000, 0.0900],
        [0.0254, 0.0404, 0.0284,  ..., 0.0000, 0.0057, 0.0797],
        [0.0782, 0.1045, 0.0171,  ..., 0.0000, 0.0419, 0.0491],
        ...,
        [0.0047, 0.0153, 0.0334,  ..., 0.0000, 0.0000, 0.0931],
        [0.0047, 0.0153, 0.0334,  ..., 0.0000, 0.0000, 0.0931],
        [0.0047, 0.0153, 0.0334,  ..., 0.0000, 0.0000, 0.0931]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647817.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7156.0913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-163.7930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20866.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1023.2729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.5237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0614],
        [-1.5221],
        [-0.8717],
        ...,
        [-2.6243],
        [-2.6184],
        [-2.6162]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-302178.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0254],
        [1.0269],
        [1.0280],
        ...,
        [1.0030],
        [1.0018],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369716.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0255],
        [1.0270],
        [1.0280],
        ...,
        [1.0030],
        [1.0018],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369723.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.1211e-04,  ...,  0.0000e+00,
          1.7148e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.1211e-04,  ...,  0.0000e+00,
          1.7148e-03,  0.0000e+00],
        [-9.0201e-03, -3.4953e-05,  2.1498e-02,  ..., -2.1458e-02,
          2.9813e-02, -7.7959e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.1211e-04,  ...,  0.0000e+00,
          1.7148e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.1211e-04,  ...,  0.0000e+00,
          1.7148e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.1211e-04,  ...,  0.0000e+00,
          1.7148e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2038.3735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6329, device='cuda:0')



h[100].sum tensor(103.5014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3042, device='cuda:0')



h[200].sum tensor(27.8059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.1047, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0221,  ..., 0.0000, 0.0359, 0.0000],
        [0.0000, 0.0000, 0.0180,  ..., 0.0000, 0.0306, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0072, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56942.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0359, 0.0528, 0.0266,  ..., 0.0000, 0.0150, 0.0738],
        [0.1064, 0.1384, 0.0149,  ..., 0.0000, 0.0629, 0.0409],
        [0.1369, 0.1749, 0.0094,  ..., 0.0000, 0.0887, 0.0259],
        ...,
        [0.0047, 0.0149, 0.0339,  ..., 0.0000, 0.0000, 0.0942],
        [0.0047, 0.0149, 0.0339,  ..., 0.0000, 0.0000, 0.0942],
        [0.0047, 0.0149, 0.0339,  ..., 0.0000, 0.0000, 0.0942]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684953.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8600.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-126.0440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22059.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1079.5730, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.8021, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8797],
        [-1.2116],
        [-0.5787],
        ...,
        [-2.6420],
        [-2.6357],
        [-2.6339]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275249.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0255],
        [1.0270],
        [1.0280],
        ...,
        [1.0030],
        [1.0018],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369723.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0255],
        [1.0271],
        [1.0280],
        ...,
        [1.0030],
        [1.0017],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369730.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.0274e-04,  ...,  0.0000e+00,
          1.7140e-03,  0.0000e+00],
        [-3.0837e-03, -1.1739e-05,  7.2981e-03,  ..., -7.3420e-03,
          1.1335e-02, -2.6646e-03],
        [-2.7756e-03, -1.0566e-05,  6.5587e-03,  ..., -6.6085e-03,
          1.0374e-02, -2.3984e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.0274e-04,  ...,  0.0000e+00,
          1.7140e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.0274e-04,  ...,  0.0000e+00,
          1.7140e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.0274e-04,  ...,  0.0000e+00,
          1.7140e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1922.0027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.4557, device='cuda:0')



h[100].sum tensor(101.6460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1002, device='cuda:0')



h[200].sum tensor(25.7706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5735, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0075,  ..., 0.0000, 0.0169, 0.0000],
        [0.0000, 0.0000, 0.0128,  ..., 0.0000, 0.0240, 0.0000],
        [0.0000, 0.0000, 0.0328,  ..., 0.0000, 0.0503, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0072, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0072, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55117.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1026, 0.1322, 0.0126,  ..., 0.0000, 0.0626, 0.0383],
        [0.1501, 0.1890, 0.0063,  ..., 0.0000, 0.1005, 0.0211],
        [0.1990, 0.2476, 0.0015,  ..., 0.0000, 0.1419, 0.0078],
        ...,
        [0.0042, 0.0142, 0.0342,  ..., 0.0000, 0.0000, 0.0953],
        [0.0042, 0.0142, 0.0342,  ..., 0.0000, 0.0000, 0.0953],
        [0.0042, 0.0142, 0.0342,  ..., 0.0000, 0.0000, 0.0953]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(678198.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8237.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.0471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21844.5117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1069.6462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.4228, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0970],
        [ 0.1269],
        [ 0.2533],
        ...,
        [-2.6711],
        [-2.6648],
        [-2.6626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293644.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0255],
        [1.0271],
        [1.0280],
        ...,
        [1.0030],
        [1.0017],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369730.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0256],
        [1.0272],
        [1.0280],
        ...,
        [1.0029],
        [1.0017],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369737.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2519e-02, -4.6819e-05,  2.9989e-02,  ..., -2.9833e-02,
          4.0824e-02, -1.0816e-02],
        [-1.6900e-02, -6.3201e-05,  4.0519e-02,  ..., -4.0272e-02,
          5.4512e-02, -1.4600e-02],
        [-1.3014e-02, -4.8669e-05,  3.1179e-02,  ..., -3.1012e-02,
          4.2369e-02, -1.1243e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.0561e-04,  ...,  0.0000e+00,
          1.7013e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.0561e-04,  ...,  0.0000e+00,
          1.7013e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.0561e-04,  ...,  0.0000e+00,
          1.7013e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2176.4551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5100, device='cuda:0')



h[100].sum tensor(105.9252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8952, device='cuda:0')



h[200].sum tensor(30.3803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.1282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1392,  ..., 0.0000, 0.1885, 0.0000],
        [0.0000, 0.0000, 0.1567,  ..., 0.0000, 0.2112, 0.0000],
        [0.0000, 0.0000, 0.1540,  ..., 0.0000, 0.2078, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62514.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.7536, 0.9276, 0.0000,  ..., 0.0000, 0.6168, 0.0000],
        [0.8372, 1.0301, 0.0000,  ..., 0.0000, 0.6881, 0.0000],
        [0.8242, 1.0142, 0.0000,  ..., 0.0000, 0.6770, 0.0000],
        ...,
        [0.0034, 0.0135, 0.0343,  ..., 0.0000, 0.0000, 0.0963],
        [0.0033, 0.0135, 0.0343,  ..., 0.0000, 0.0000, 0.0963],
        [0.0033, 0.0135, 0.0343,  ..., 0.0000, 0.0000, 0.0962]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(723383.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9596.2803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.5135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22888.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1121.3062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-420.1216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1430],
        [ 0.1289],
        [ 0.1144],
        ...,
        [-2.6977],
        [-2.6910],
        [-2.6885]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286043.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0256],
        [1.0272],
        [1.0280],
        ...,
        [1.0029],
        [1.0017],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369737.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0273],
        [1.0281],
        ...,
        [1.0028],
        [1.0016],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369744.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1956.3157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3892, device='cuda:0')



h[100].sum tensor(102.5610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6164, device='cuda:0')



h[200].sum tensor(27.2589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0104,  ..., 0.0000, 0.0207, 0.0000],
        [0.0000, 0.0000, 0.0052,  ..., 0.0000, 0.0138, 0.0000],
        [0.0000, 0.0000, 0.0076,  ..., 0.0000, 0.0170, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55702.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0706, 0.0938, 0.0187,  ..., 0.0000, 0.0349, 0.0597],
        [0.0603, 0.0815, 0.0210,  ..., 0.0000, 0.0261, 0.0651],
        [0.0716, 0.0949, 0.0186,  ..., 0.0000, 0.0356, 0.0589],
        ...,
        [0.0026, 0.0129, 0.0343,  ..., 0.0000, 0.0000, 0.0970],
        [0.0026, 0.0129, 0.0343,  ..., 0.0000, 0.0000, 0.0970],
        [0.0026, 0.0129, 0.0343,  ..., 0.0000, 0.0000, 0.0970]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679490., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8021.8584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-121.5708, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21949.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1083.8473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.9923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8409],
        [-0.6954],
        [-0.5250],
        ...,
        [-2.7170],
        [-2.7107],
        [-2.7082]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-305145.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0273],
        [1.0281],
        ...,
        [1.0028],
        [1.0016],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369744.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0273],
        [1.0281],
        ...,
        [1.0028],
        [1.0016],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369744.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4145e-03, -1.9891e-05,  1.2924e-02,  ..., -1.2914e-02,
          1.8633e-02, -4.6769e-03],
        [-3.7975e-03, -1.3950e-05,  9.0309e-03,  ..., -9.0570e-03,
          1.3572e-02, -3.2801e-03],
        [ 0.0000e+00,  0.0000e+00, -1.1242e-04,  ...,  0.0000e+00,
          1.6863e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.1242e-04,  ...,  0.0000e+00,
          1.6863e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.1242e-04,  ...,  0.0000e+00,
          1.6863e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.1242e-04,  ...,  0.0000e+00,
          1.6863e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2045.1033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9475, device='cuda:0')



h[100].sum tensor(103.9870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4781, device='cuda:0')



h[200].sum tensor(28.7093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0522,  ..., 0.0000, 0.0753, 0.0000],
        [0.0000, 0.0000, 0.0208,  ..., 0.0000, 0.0343, 0.0000],
        [0.0000, 0.0000, 0.0093,  ..., 0.0000, 0.0191, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57164.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.4349e-01, 3.0505e-01, 0.0000e+00,  ..., 0.0000e+00, 1.8239e-01,
         2.1407e-04],
        [1.6728e-01, 2.1209e-01, 5.0008e-03,  ..., 0.0000e+00, 1.1722e-01,
         1.7109e-02],
        [1.0817e-01, 1.4014e-01, 1.1174e-02,  ..., 0.0000e+00, 6.6610e-02,
         3.5770e-02],
        ...,
        [2.6153e-03, 1.2932e-02, 3.4309e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.6972e-02],
        [2.6133e-03, 1.2933e-02, 3.4312e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.6977e-02],
        [2.6061e-03, 1.2926e-02, 3.4307e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.6968e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(683552.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8180.6025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-116.0847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22074.6992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1092.0392, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.6047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2101],
        [ 0.0893],
        [-0.1479],
        ...,
        [-2.7189],
        [-2.7125],
        [-2.7099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-297130.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0273],
        [1.0281],
        ...,
        [1.0028],
        [1.0016],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369744.8438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(756.9498, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0274],
        [1.0281],
        ...,
        [1.0028],
        [1.0016],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369752.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1936.2036, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5342, device='cuda:0')



h[100].sum tensor(102.4879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1436, device='cuda:0')



h[200].sum tensor(27.4379, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54453.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0125, 0.0336,  ..., 0.0000, 0.0000, 0.0956],
        [0.0021, 0.0125, 0.0336,  ..., 0.0000, 0.0000, 0.0957],
        [0.0055, 0.0166, 0.0329,  ..., 0.0000, 0.0000, 0.0938],
        ...,
        [0.0022, 0.0128, 0.0341,  ..., 0.0000, 0.0000, 0.0970],
        [0.0022, 0.0128, 0.0341,  ..., 0.0000, 0.0000, 0.0970],
        [0.0022, 0.0128, 0.0341,  ..., 0.0000, 0.0000, 0.0970]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(671480.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7528.4619, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.7548, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21672.4395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1071.0312, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-421.1649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7778],
        [-2.6209],
        [-2.3429],
        ...,
        [-2.4729],
        [-2.6481],
        [-2.6962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-319058.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0274],
        [1.0281],
        ...,
        [1.0028],
        [1.0016],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369752.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0259],
        [1.0275],
        [1.0282],
        ...,
        [1.0027],
        [1.0015],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369760.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0001,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2317.9763, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.9053, device='cuda:0')



h[100].sum tensor(108.3366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.2198, device='cuda:0')



h[200].sum tensor(33.5804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.8128, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64735.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0566, 0.0792, 0.0208,  ..., 0.0000, 0.0270, 0.0639],
        [0.0311, 0.0478, 0.0267,  ..., 0.0000, 0.0062, 0.0791],
        [0.0220, 0.0366, 0.0288,  ..., 0.0000, 0.0024, 0.0843],
        ...,
        [0.0023, 0.0133, 0.0337,  ..., 0.0000, 0.0000, 0.0963],
        [0.0023, 0.0133, 0.0337,  ..., 0.0000, 0.0000, 0.0963],
        [0.0023, 0.0133, 0.0337,  ..., 0.0000, 0.0000, 0.0963]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(733130.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9703.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-105.7037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23460.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1140.1659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.7791, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4189],
        [-0.7927],
        [-1.1083],
        ...,
        [-2.7219],
        [-2.7080],
        [-2.6688]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265078.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0259],
        [1.0275],
        [1.0282],
        ...,
        [1.0027],
        [1.0015],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369760.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0276],
        [1.0283],
        ...,
        [1.0027],
        [1.0015],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369768., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2136e-03, -2.1632e-05,  1.4874e-02,  ..., -1.4858e-02,
          2.1216e-02, -5.3639e-03],
        [-3.7192e-03, -1.2948e-05,  8.8405e-03,  ..., -8.8933e-03,
          1.3372e-02, -3.2106e-03],
        [-8.2799e-03, -2.8825e-05,  1.9873e-02,  ..., -1.9799e-02,
          2.7715e-02, -7.1476e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.5650e-04,  ...,  0.0000e+00,
          1.6749e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5650e-04,  ...,  0.0000e+00,
          1.6749e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.5650e-04,  ...,  0.0000e+00,
          1.6749e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2450.8086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.7647, device='cuda:0')



h[100].sum tensor(110.1668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2481, device='cuda:0')



h[200].sum tensor(35.8112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.1205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0365,  ..., 0.0000, 0.0552, 0.0000],
        [0.0000, 0.0000, 0.0495,  ..., 0.0000, 0.0718, 0.0000],
        [0.0000, 0.0000, 0.0258,  ..., 0.0000, 0.0408, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66947.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2245, 0.2868, 0.0000,  ..., 0.0000, 0.1676, 0.0000],
        [0.2487, 0.3174, 0.0000,  ..., 0.0000, 0.1883, 0.0000],
        [0.2099, 0.2697, 0.0000,  ..., 0.0000, 0.1546, 0.0000],
        ...,
        [0.0022, 0.0138, 0.0335,  ..., 0.0000, 0.0000, 0.0960],
        [0.0022, 0.0138, 0.0335,  ..., 0.0000, 0.0000, 0.0960],
        [0.0022, 0.0138, 0.0335,  ..., 0.0000, 0.0000, 0.0960]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(741222.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9777.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.7525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23528.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1149.0035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.6531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3019],
        [ 0.3370],
        [ 0.3329],
        ...,
        [-2.6957],
        [-2.6787],
        [-2.6715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-278264.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0276],
        [1.0283],
        ...,
        [1.0027],
        [1.0015],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369768., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0277],
        [1.0284],
        ...,
        [1.0026],
        [1.0014],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369775.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.9356e-03, -1.0038e-05,  6.9434e-03,  ..., -7.0256e-03,
          1.0933e-02, -2.5336e-03],
        [ 0.0000e+00,  0.0000e+00, -1.6939e-04,  ...,  0.0000e+00,
          1.6848e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6939e-04,  ...,  0.0000e+00,
          1.6848e-03,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.6939e-04,  ...,  0.0000e+00,
          1.6848e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6939e-04,  ...,  0.0000e+00,
          1.6848e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.6939e-04,  ...,  0.0000e+00,
          1.6848e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1932.2679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6260, device='cuda:0')



h[100].sum tensor(101.5044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6413, device='cuda:0')



h[200].sum tensor(27.3999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9899, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0243,  ..., 0.0000, 0.0389, 0.0000],
        [0.0000, 0.0000, 0.0071,  ..., 0.0000, 0.0164, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54395.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1977, 0.2561, 0.0047,  ..., 0.0000, 0.1440, 0.0144],
        [0.1064, 0.1429, 0.0118,  ..., 0.0000, 0.0693, 0.0356],
        [0.0432, 0.0646, 0.0234,  ..., 0.0000, 0.0170, 0.0702],
        ...,
        [0.0022, 0.0142, 0.0333,  ..., 0.0000, 0.0000, 0.0957],
        [0.0022, 0.0142, 0.0333,  ..., 0.0000, 0.0000, 0.0957],
        [0.0022, 0.0142, 0.0333,  ..., 0.0000, 0.0000, 0.0957]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(676075.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7784.9028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.9082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22363.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1074.4292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.1218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1107],
        [-0.1427],
        [-0.5248],
        ...,
        [-2.7185],
        [-2.7124],
        [-2.7096]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245315.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0277],
        [1.0284],
        ...,
        [1.0026],
        [1.0014],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369775.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0263],
        [1.0278],
        [1.0285],
        ...,
        [1.0026],
        [1.0014],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369782.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.8570e-04,  ...,  0.0000e+00,
          1.6914e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8570e-04,  ...,  0.0000e+00,
          1.6914e-03,  0.0000e+00],
        [-5.7083e-03, -1.9170e-05,  1.3668e-02,  ..., -1.3673e-02,
          1.9706e-02, -4.9257e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8570e-04,  ...,  0.0000e+00,
          1.6914e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8570e-04,  ...,  0.0000e+00,
          1.6914e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8570e-04,  ...,  0.0000e+00,
          1.6914e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1872.9357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4251, device='cuda:0')



h[100].sum tensor(100.3946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9772, device='cuda:0')



h[200].sum tensor(26.6007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0197,  ..., 0.0000, 0.0330, 0.0000],
        [0.0000, 0.0000, 0.0182,  ..., 0.0000, 0.0314, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52447.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0651, 0.0926, 0.0178,  ..., 0.0000, 0.0293, 0.0559],
        [0.1064, 0.1438, 0.0099,  ..., 0.0000, 0.0651, 0.0336],
        [0.1391, 0.1840, 0.0056,  ..., 0.0000, 0.0933, 0.0204],
        ...,
        [0.0021, 0.0144, 0.0331,  ..., 0.0000, 0.0000, 0.0958],
        [0.0021, 0.0144, 0.0331,  ..., 0.0000, 0.0000, 0.0958],
        [0.0021, 0.0144, 0.0331,  ..., 0.0000, 0.0000, 0.0958]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666355.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7106.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-160.3290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21720.2520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1049.1514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.9979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1012],
        [-0.0306],
        [ 0.1047],
        ...,
        [-2.7063],
        [-2.7115],
        [-2.7108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294941., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0263],
        [1.0278],
        [1.0285],
        ...,
        [1.0026],
        [1.0014],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369782.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0264],
        [1.0279],
        [1.0286],
        ...,
        [1.0025],
        [1.0013],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369789.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2058.9167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1601, device='cuda:0')



h[100].sum tensor(103.4399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0427, device='cuda:0')



h[200].sum tensor(30.0157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.7722, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56352.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0233, 0.0308,  ..., 0.0000, 0.0000, 0.0903],
        [0.0097, 0.0238, 0.0307,  ..., 0.0000, 0.0000, 0.0904],
        [0.0142, 0.0294, 0.0297,  ..., 0.0000, 0.0000, 0.0879],
        ...,
        [0.0067, 0.0207, 0.0319,  ..., 0.0000, 0.0000, 0.0930],
        [0.0019, 0.0145, 0.0330,  ..., 0.0000, 0.0000, 0.0961],
        [0.0019, 0.0145, 0.0330,  ..., 0.0000, 0.0000, 0.0961]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(686253.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7631.8379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-149.2476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22089.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1071.0227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.3393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5492],
        [-1.5241],
        [-1.2996],
        ...,
        [-2.5140],
        [-2.6563],
        [-2.7106]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294623.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0264],
        [1.0279],
        [1.0286],
        ...,
        [1.0025],
        [1.0013],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369789.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0266],
        [1.0281],
        [1.0287],
        ...,
        [1.0025],
        [1.0013],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369795.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2161.5503, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1095, device='cuda:0')



h[100].sum tensor(105.1026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.1207, device='cuda:0')



h[200].sum tensor(32.2132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.1432, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59815.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0059, 0.0193, 0.0315,  ..., 0.0000, 0.0000, 0.0929],
        [0.0029, 0.0158, 0.0322,  ..., 0.0000, 0.0000, 0.0947],
        [0.0016, 0.0142, 0.0326,  ..., 0.0000, 0.0000, 0.0955],
        ...,
        [0.0017, 0.0145, 0.0330,  ..., 0.0000, 0.0000, 0.0967],
        [0.0017, 0.0145, 0.0330,  ..., 0.0000, 0.0000, 0.0967],
        [0.0017, 0.0145, 0.0330,  ..., 0.0000, 0.0000, 0.0967]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(702335.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8190.2686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-139.1619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22555.2129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1098.3281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.0967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6987],
        [-2.1187],
        [-2.3960],
        ...,
        [-2.7533],
        [-2.7470],
        [-2.7444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279210.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0266],
        [1.0281],
        [1.0287],
        ...,
        [1.0025],
        [1.0013],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369795.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0267],
        [1.0282],
        [1.0289],
        ...,
        [1.0025],
        [1.0012],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369801.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1996.6592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0861, device='cuda:0')



h[100].sum tensor(102.6719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4488, device='cuda:0')



h[200].sum tensor(30.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0168, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55156.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0139, 0.0325,  ..., 0.0000, 0.0000, 0.0960],
        [0.0013, 0.0139, 0.0325,  ..., 0.0000, 0.0000, 0.0961],
        [0.0014, 0.0140, 0.0326,  ..., 0.0000, 0.0000, 0.0961],
        ...,
        [0.0014, 0.0143, 0.0330,  ..., 0.0000, 0.0000, 0.0974],
        [0.0014, 0.0143, 0.0330,  ..., 0.0000, 0.0000, 0.0974],
        [0.0014, 0.0143, 0.0330,  ..., 0.0000, 0.0000, 0.0974]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679221.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7405.0093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-156.6118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22051.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1072.5840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.8704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8889],
        [-2.7975],
        [-2.6369],
        ...,
        [-2.7701],
        [-2.7639],
        [-2.7612]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283876.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0267],
        [1.0282],
        [1.0289],
        ...,
        [1.0025],
        [1.0012],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369801.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0269],
        [1.0283],
        [1.0290],
        ...,
        [1.0024],
        [1.0012],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369807.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -2.4404e-04,  ...,  0.0000e+00,
          1.6930e-03,  0.0000e+00],
        [-5.9668e-03, -1.8636e-05,  1.4333e-02,  ..., -1.4342e-02,
          2.0656e-02, -5.1446e-03],
        [-6.3387e-03, -1.9797e-05,  1.5242e-02,  ..., -1.5236e-02,
          2.1838e-02, -5.4652e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.4404e-04,  ...,  0.0000e+00,
          1.6930e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.4404e-04,  ...,  0.0000e+00,
          1.6930e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.4404e-04,  ...,  0.0000e+00,
          1.6930e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2164.4941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5133, device='cuda:0')



h[100].sum tensor(105.5861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3440, device='cuda:0')



h[200].sum tensor(33.4942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.4272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0147,  ..., 0.0000, 0.0264, 0.0000],
        [0.0000, 0.0000, 0.0276,  ..., 0.0000, 0.0436, 0.0000],
        [0.0000, 0.0000, 0.0828,  ..., 0.0000, 0.1160, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60022.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0930, 0.1313, 0.0145,  ..., 0.0000, 0.0560, 0.0462],
        [0.1773, 0.2378, 0.0063,  ..., 0.0000, 0.1263, 0.0220],
        [0.3236, 0.4227, 0.0000,  ..., 0.0000, 0.2527, 0.0000],
        ...,
        [0.0010, 0.0140, 0.0330,  ..., 0.0000, 0.0000, 0.0981],
        [0.0010, 0.0140, 0.0330,  ..., 0.0000, 0.0000, 0.0981],
        [0.0010, 0.0140, 0.0330,  ..., 0.0000, 0.0000, 0.0981]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(712231.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8356.5518, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-139.1656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22702.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1104.9833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.0248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9308],
        [-0.5076],
        [-0.0765],
        ...,
        [-2.7906],
        [-2.7840],
        [-2.7817]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-281061.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0269],
        [1.0283],
        [1.0290],
        ...,
        [1.0024],
        [1.0012],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369807.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0270],
        [1.0285],
        [1.0291],
        ...,
        [1.0024],
        [1.0012],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369814.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2415.2920, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4558, device='cuda:0')



h[100].sum tensor(109.9171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.0773, device='cuda:0')



h[200].sum tensor(38.3248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68561.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0133, 0.0326,  ..., 0.0000, 0.0000, 0.0972],
        [0.0027, 0.0168, 0.0320,  ..., 0.0000, 0.0000, 0.0957],
        [0.0146, 0.0318, 0.0291,  ..., 0.0000, 0.0000, 0.0888],
        ...,
        [0.0007, 0.0137, 0.0331,  ..., 0.0000, 0.0000, 0.0987],
        [0.0007, 0.0137, 0.0331,  ..., 0.0000, 0.0000, 0.0987],
        [0.0007, 0.0137, 0.0331,  ..., 0.0000, 0.0000, 0.0986]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(760417.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(9850.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-114.3434, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23918.8320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1169.3982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.7887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8628],
        [-1.6837],
        [-1.2853],
        ...,
        [-2.7985],
        [-2.7971],
        [-2.7959]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260863.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0270],
        [1.0285],
        [1.0291],
        ...,
        [1.0024],
        [1.0012],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369814.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(494.3684, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0272],
        [1.0286],
        [1.0292],
        ...,
        [1.0024],
        [1.0011],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369821.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2107.0806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4413, device='cuda:0')



h[100].sum tensor(105.5604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7512, device='cuda:0')



h[200].sum tensor(33.9701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61108.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0004, 0.0129, 0.0327,  ..., 0.0000, 0.0000, 0.0975],
        [0.0004, 0.0129, 0.0328,  ..., 0.0000, 0.0000, 0.0976],
        [0.0005, 0.0130, 0.0328,  ..., 0.0000, 0.0000, 0.0976],
        ...,
        [0.0005, 0.0133, 0.0332,  ..., 0.0000, 0.0000, 0.0989],
        [0.0005, 0.0133, 0.0332,  ..., 0.0000, 0.0000, 0.0989],
        [0.0005, 0.0133, 0.0332,  ..., 0.0000, 0.0000, 0.0989]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(731366.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(8420.3184, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-138.5096, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22705.0508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1106.6328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.8365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4106],
        [-2.5553],
        [-2.6430],
        ...,
        [-2.8271],
        [-2.8203],
        [-2.8175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-334265.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0272],
        [1.0286],
        [1.0292],
        ...,
        [1.0024],
        [1.0011],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369821.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0273],
        [1.0288],
        [1.0292],
        ...,
        [1.0023],
        [1.0011],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369829.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000],
        [ 0.0000,  0.0000, -0.0003,  ...,  0.0000,  0.0016,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2145.1631, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7808, device='cuda:0')



h[100].sum tensor(106.3973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.9389, device='cuda:0')



h[200].sum tensor(35.0134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0067, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59254.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0149, 0.0323,  ..., 0.0000, 0.0000, 0.0963],
        [0.0106, 0.0264, 0.0301,  ..., 0.0000, 0.0000, 0.0909],
        [0.0346, 0.0567, 0.0242,  ..., 0.0000, 0.0141, 0.0763],
        ...,
        [0.0005, 0.0131, 0.0332,  ..., 0.0000, 0.0000, 0.0987],
        [0.0005, 0.0131, 0.0332,  ..., 0.0000, 0.0000, 0.0988],
        [0.0005, 0.0131, 0.0332,  ..., 0.0000, 0.0000, 0.0987]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(704682.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7919.1504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-157.6103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22886.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1114.9739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.1638, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1277],
        [-1.6902],
        [-1.1715],
        ...,
        [-2.8372],
        [-2.8307],
        [-2.8281]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280821.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0273],
        [1.0288],
        [1.0292],
        ...,
        [1.0023],
        [1.0011],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369829.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0275],
        [1.0289],
        [1.0293],
        ...,
        [1.0023],
        [1.0011],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369836.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3461e-03, -1.8422e-05,  1.5370e-02,  ..., -1.5307e-02,
          2.1918e-02, -5.4672e-03],
        [-1.1656e-02, -3.3836e-05,  2.8431e-02,  ..., -2.8114e-02,
          3.8897e-02, -1.0042e-02],
        [-1.4098e-02, -4.0926e-05,  3.4439e-02,  ..., -3.4004e-02,
          4.6706e-02, -1.2146e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.3983e-04,  ...,  0.0000e+00,
          1.6246e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3983e-04,  ...,  0.0000e+00,
          1.6246e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.3983e-04,  ...,  0.0000e+00,
          1.6246e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2035.8157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5103, device='cuda:0')



h[100].sum tensor(104.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6833, device='cuda:0')



h[200].sum tensor(33.0057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.3151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1020,  ..., 0.0000, 0.1405, 0.0000],
        [0.0000, 0.0000, 0.1355,  ..., 0.0000, 0.1842, 0.0000],
        [0.0000, 0.0000, 0.1351,  ..., 0.0000, 0.1837, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0068, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57194.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[6.0666e-01, 7.8233e-01, 0.0000e+00,  ..., 0.0000e+00, 5.0536e-01,
         0.0000e+00],
        [6.9801e-01, 8.9831e-01, 0.0000e+00,  ..., 0.0000e+00, 5.8518e-01,
         0.0000e+00],
        [7.2977e-01, 9.3855e-01, 0.0000e+00,  ..., 0.0000e+00, 6.1285e-01,
         0.0000e+00],
        ...,
        [6.2438e-04, 1.3134e-02, 3.3062e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.8132e-02],
        [6.2170e-04, 1.3133e-02, 3.3067e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.8145e-02],
        [6.1929e-04, 1.3123e-02, 3.3062e-02,  ..., 0.0000e+00, 0.0000e+00,
         9.8126e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(701568.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7714.5728, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-165.9055, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22794.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1097.2433, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.9101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2348],
        [ 0.2283],
        [ 0.2288],
        ...,
        [-2.8367],
        [-2.8301],
        [-2.8270]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286711., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0275],
        [1.0289],
        [1.0293],
        ...,
        [1.0023],
        [1.0011],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369836.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0277],
        [1.0291],
        [1.0293],
        ...,
        [1.0023],
        [1.0011],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369844.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.4961e-03, -1.5664e-05,  1.3320e-02,  ..., -1.3268e-02,
          1.9241e-02, -4.7339e-03],
        [-4.4612e-03, -1.2715e-05,  1.0770e-02,  ..., -1.0770e-02,
          1.5927e-02, -3.8426e-03],
        [-1.0139e-02, -2.8897e-05,  2.4762e-02,  ..., -2.4476e-02,
          3.4110e-02, -8.7329e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.2382e-04,  ...,  0.0000e+00,
          1.6390e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.2382e-04,  ...,  0.0000e+00,
          1.6390e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.2382e-04,  ...,  0.0000e+00,
          1.6390e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1739.6882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.5091, device='cuda:0')



h[100].sum tensor(98.8560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3646, device='cuda:0')



h[200].sum tensor(27.5093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0945, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0221,  ..., 0.0000, 0.0361, 0.0000],
        [0.0000, 0.0000, 0.0706,  ..., 0.0000, 0.0997, 0.0000],
        [0.0000, 0.0000, 0.0734,  ..., 0.0000, 0.1034, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48195.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1472, 0.1992, 0.0077,  ..., 0.0000, 0.1033, 0.0256],
        [0.2709, 0.3554, 0.0000,  ..., 0.0000, 0.2114, 0.0000],
        [0.3185, 0.4155, 0.0000,  ..., 0.0000, 0.2529, 0.0000],
        ...,
        [0.0009, 0.0133, 0.0329,  ..., 0.0000, 0.0000, 0.0972],
        [0.0009, 0.0133, 0.0329,  ..., 0.0000, 0.0000, 0.0973],
        [0.0009, 0.0133, 0.0329,  ..., 0.0000, 0.0000, 0.0972]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648299.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5771.2808, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-195.4898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21388.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1030.6575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-425.3511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7984],
        [-0.1936],
        [ 0.1435],
        ...,
        [-2.8311],
        [-2.8247],
        [-2.8225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-323144.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0277],
        [1.0291],
        [1.0293],
        ...,
        [1.0023],
        [1.0011],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369844.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0278],
        [1.0292],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369851.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.8657e-03, -8.0183e-06,  6.8662e-03,  ..., -6.9241e-03,
          1.0850e-02, -2.4678e-03],
        [ 0.0000e+00,  0.0000e+00, -2.0859e-04,  ...,  0.0000e+00,
          1.6577e-03,  0.0000e+00],
        [-2.8657e-03, -8.0183e-06,  6.8662e-03,  ..., -6.9241e-03,
          1.0850e-02, -2.4678e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -2.0859e-04,  ...,  0.0000e+00,
          1.6577e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0859e-04,  ...,  0.0000e+00,
          1.6577e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -2.0859e-04,  ...,  0.0000e+00,
          1.6577e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2009.9302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.3381, device='cuda:0')



h[100].sum tensor(102.2022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0351, device='cuda:0')



h[200].sum tensor(31.1216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4907, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0057,  ..., 0.0000, 0.0145, 0.0000],
        [0.0000, 0.0000, 0.0256,  ..., 0.0000, 0.0411, 0.0000],
        [0.0000, 0.0000, 0.0057,  ..., 0.0000, 0.0146, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54458.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0406, 0.0640, 0.0221,  ..., 0.0000, 0.0120, 0.0708],
        [0.0701, 0.1009, 0.0148,  ..., 0.0000, 0.0362, 0.0530],
        [0.0409, 0.0643, 0.0221,  ..., 0.0000, 0.0121, 0.0708],
        ...,
        [0.0011, 0.0135, 0.0327,  ..., 0.0000, 0.0000, 0.0965],
        [0.0011, 0.0135, 0.0327,  ..., 0.0000, 0.0000, 0.0966],
        [0.0011, 0.0134, 0.0327,  ..., 0.0000, 0.0000, 0.0965]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(679323., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6890.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-174.5547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22420.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1077.5465, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.1895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8504],
        [-1.9128],
        [-2.1407],
        ...,
        [-2.8273],
        [-2.8211],
        [-2.8185]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289751.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0278],
        [1.0292],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369851.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0280],
        [1.0293],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369859.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  0.0000e+00, -1.9645e-04,  ...,  0.0000e+00,
          1.6696e-03,  0.0000e+00],
        [-3.9350e-03, -1.0809e-05,  9.5357e-03,  ..., -9.5160e-03,
          1.4312e-02, -3.3879e-03],
        [-1.4659e-02, -4.0266e-05,  3.6059e-02,  ..., -3.5450e-02,
          4.8765e-02, -1.2621e-02],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.9645e-04,  ...,  0.0000e+00,
          1.6696e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9645e-04,  ...,  0.0000e+00,
          1.6696e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.9645e-04,  ...,  0.0000e+00,
          1.6696e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3266.9465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.4990, device='cuda:0')



h[100].sum tensor(121.1578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(22.3963, device='cuda:0')



h[200].sum tensor(50.7019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0260,  ..., 0.0000, 0.0411, 0.0000],
        [0.0000, 0.0000, 0.0770,  ..., 0.0000, 0.1077, 0.0000],
        [0.0000, 0.0000, 0.1031,  ..., 0.0000, 0.1419, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89822.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.2968, 0.3860, 0.0045,  ..., 0.0000, 0.2341, 0.0143],
        [0.5448, 0.6985, 0.0000,  ..., 0.0000, 0.4508, 0.0000],
        [0.7466, 0.9529, 0.0000,  ..., 0.0000, 0.6272, 0.0000],
        ...,
        [0.0013, 0.0136, 0.0327,  ..., 0.0000, 0.0000, 0.0960],
        [0.0013, 0.0136, 0.0327,  ..., 0.0000, 0.0000, 0.0960],
        [0.0013, 0.0136, 0.0327,  ..., 0.0000, 0.0000, 0.0960]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(898599.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13771.7715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-47.6518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(27317.3496, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1298.6340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.3056, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0153],
        [ 0.0456],
        [ 0.0222],
        ...,
        [-2.8245],
        [-2.8181],
        [-2.8158]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255616.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0280],
        [1.0293],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369859.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0281],
        [1.0294],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369866.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1860.7678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.0675, device='cuda:0')



h[100].sum tensor(98.8484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2264, device='cuda:0')



h[200].sum tensor(28.1105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50855.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0131, 0.0322,  ..., 0.0000, 0.0000, 0.0945],
        [0.0012, 0.0131, 0.0323,  ..., 0.0000, 0.0000, 0.0946],
        [0.0058, 0.0195, 0.0311,  ..., 0.0000, 0.0000, 0.0914],
        ...,
        [0.0013, 0.0135, 0.0327,  ..., 0.0000, 0.0000, 0.0959],
        [0.0013, 0.0135, 0.0327,  ..., 0.0000, 0.0000, 0.0959],
        [0.0013, 0.0135, 0.0327,  ..., 0.0000, 0.0000, 0.0958]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(664232.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(6191.2881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-182.1971, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21982.8223, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1048.2529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.3126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7481],
        [-2.5470],
        [-2.1704],
        ...,
        [-2.8290],
        [-2.8229],
        [-2.8204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310057.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0281],
        [1.0294],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369866.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0283],
        [1.0295],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369872.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        ...,
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000],
        [ 0.0000,  0.0000, -0.0002,  ...,  0.0000,  0.0017,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2129.9502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2823, device='cuda:0')



h[100].sum tensor(103.0204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1102, device='cuda:0')



h[200].sum tensor(32.4689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.8581, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0070, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57269.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0012, 0.0129, 0.0323,  ..., 0.0000, 0.0000, 0.0949],
        [0.0012, 0.0130, 0.0324,  ..., 0.0000, 0.0000, 0.0950],
        [0.0013, 0.0130, 0.0324,  ..., 0.0000, 0.0000, 0.0951],
        ...,
        [0.0013, 0.0133, 0.0328,  ..., 0.0000, 0.0000, 0.0962],
        [0.0013, 0.0133, 0.0328,  ..., 0.0000, 0.0000, 0.0963],
        [0.0013, 0.0133, 0.0328,  ..., 0.0000, 0.0000, 0.0962]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(697836.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(7566.0342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-163.9978, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(23212.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1103.3788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.0485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7799],
        [-2.8245],
        [-2.8154],
        ...,
        [-2.8428],
        [-2.8365],
        [-2.8343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259245.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0283],
        [1.0295],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369872.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0284],
        [1.0296],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369878.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.5229e-02, -3.9568e-05,  3.7685e-02,  ..., -3.6926e-02,
          5.0865e-02, -1.3104e-02],
        [-1.0059e-02, -2.6135e-05,  2.4829e-02,  ..., -2.4389e-02,
          3.4170e-02, -8.6551e-03],
        [-5.2373e-03, -1.3607e-05,  1.2840e-02,  ..., -1.2699e-02,
          1.8601e-02, -4.5064e-03],
        ...,
        [ 0.0000e+00,  0.0000e+00, -1.8248e-04,  ...,  0.0000e+00,
          1.6903e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8248e-04,  ...,  0.0000e+00,
          1.6903e-03,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00, -1.8248e-04,  ...,  0.0000e+00,
          1.6903e-03,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2620.3882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1612, device='cuda:0')



h[100].sum tensor(111.0454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(15.0204, device='cuda:0')



h[200].sum tensor(40.5607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.1027, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.1145,  ..., 0.0000, 0.1566, 0.0000],
        [0.0000, 0.0000, 0.0835,  ..., 0.0000, 0.1164, 0.0000],
        [0.0000, 0.0000, 0.0466,  ..., 0.0000, 0.0682, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0071, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71369.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.5765, 0.7347, 0.0000,  ..., 0.0000, 0.4781, 0.0000],
        [0.4926, 0.6291, 0.0000,  ..., 0.0000, 0.4049, 0.0000],
        [0.3855, 0.4944, 0.0000,  ..., 0.0000, 0.3113, 0.0000],
        ...,
        [0.0012, 0.0126, 0.0329,  ..., 0.0000, 0.0000, 0.0970],
        [0.0012, 0.0126, 0.0329,  ..., 0.0000, 0.0000, 0.0971],
        [0.0012, 0.0126, 0.0329,  ..., 0.0000, 0.0000, 0.0970]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(786214.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(10490.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.3609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(25401.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-1189.4087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-450.1443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2815],
        [ 0.2969],
        [ 0.3129],
        ...,
        [-2.8656],
        [-2.8593],
        [-2.8570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240497.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0284],
        [1.0296],
        [1.0293],
        ...,
        [1.0023],
        [1.0010],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369878.8750, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:15.733656
evaluation loss: 502.90875244140625
epoch: 0 mean loss: 500.1690673828125
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./Training.py", line 117, in <module>
    writer.add_scalars(f'loss for {modelname}, Batchsize {BatchSize}, {TraEvN} training events, {evalbatchnum * BatchSize} evaluation events', \
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 397, in add_scalars
    fw = FileWriter(fw_tag, self.max_queue, self.flush_secs,
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 60, in __init__
    self.event_writer = EventFileWriter(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/summary/writer/event_file_writer.py", line 72, in __init__
    tf.io.gfile.makedirs(logdir)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 900, in makedirs
    return get_filesystem(path).makedirs(path)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py", line 201, in makedirs
    os.makedirs(path, exist_ok=True)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/os.py", line 225, in makedirs
    mkdir(name, mode)
OSError: [Errno 122] Disk quota exceeded: 'runs/Jul18_15-58-54_cmsgpu001.ihep.ac.cn/loss for DGLpppipiGcnReNewestweight7N2, Batchsize 5, 1998 training events, 120 evaluation events_training (Lr and weight_decay=0.0001,5e-05)'

real	2m9.192s
user	0m30.146s
sys	0m13.780s
