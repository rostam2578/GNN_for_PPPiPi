0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 02:43:43 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   32C    P0    44W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b931c240910> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m4.927s
user	0m2.549s
sys	0m0.960s
[02:43:50] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.0700],
        [ 0.1951],
        [-0.7071],
        ...,
        [ 2.4011],
        [ 0.9116],
        [ 0.5768]], device='cuda:0', requires_grad=True) 
node features sum: tensor(260.3264, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.1470, -0.1220, -0.1121, -0.0352, -0.1029,  0.0066, -0.0015,  0.0642,
         -0.1425,  0.0328, -0.0818, -0.0364, -0.1380,  0.0206, -0.1089, -0.0254,
         -0.0228, -0.0322, -0.1102,  0.1035,  0.0030, -0.0389, -0.0469,  0.0771,
         -0.1523,  0.1396, -0.0549, -0.0323,  0.1101, -0.1416,  0.1192, -0.0608,
         -0.0977,  0.0566, -0.1306, -0.1415,  0.1447,  0.0233,  0.1175,  0.1430,
         -0.1427, -0.0508, -0.0631,  0.0616, -0.1409, -0.0892, -0.0778,  0.1490,
         -0.0260, -0.0373, -0.1139, -0.0457,  0.0613,  0.0240, -0.0713,  0.0565,
          0.0286,  0.1072, -0.0991, -0.1378, -0.1448,  0.1179, -0.1237,  0.0945,
          0.0707, -0.1483,  0.0894, -0.1054, -0.0639,  0.0639, -0.0198,  0.0775,
         -0.1100,  0.0007,  0.0765, -0.1056, -0.0547,  0.1453,  0.0361,  0.0241,
         -0.1320,  0.0724,  0.1311, -0.0030, -0.0942, -0.1159, -0.1260, -0.0458,
         -0.0452,  0.0590,  0.0695,  0.0781, -0.1484, -0.0377,  0.0234,  0.0659,
         -0.1421, -0.0327, -0.0645,  0.0587,  0.0610, -0.0714, -0.1335,  0.0014,
         -0.1492,  0.0474,  0.0941,  0.0542,  0.1021, -0.0456,  0.1456,  0.0910,
         -0.1032, -0.0041, -0.0389, -0.0030,  0.0101, -0.1497,  0.0174,  0.0541,
          0.1439,  0.0657, -0.0430,  0.1193, -0.0637, -0.0089, -0.0896,  0.0829,
         -0.1360,  0.0412, -0.0231, -0.1055, -0.1350, -0.1470,  0.1052,  0.0716,
          0.0101,  0.1379,  0.1219,  0.0150, -0.0599,  0.0517, -0.0914,  0.0721,
          0.0555, -0.1364, -0.0704, -0.0307, -0.0837, -0.1289, -0.0787,  0.0076,
         -0.0641,  0.0367,  0.0481,  0.0891, -0.1212,  0.1086, -0.1490, -0.0760,
         -0.0261, -0.0861,  0.0660,  0.0994, -0.0011,  0.0922,  0.0448, -0.0371,
          0.0433, -0.0882,  0.0057,  0.1291,  0.1149, -0.0883,  0.1458,  0.1408,
         -0.0731, -0.0228, -0.0356,  0.1192,  0.1372,  0.1349,  0.0830,  0.0281,
          0.0302,  0.0223,  0.1451, -0.1072, -0.0626, -0.0258,  0.1111,  0.0425,
          0.1332,  0.1037, -0.0052,  0.0974,  0.0066,  0.0707, -0.1407, -0.0516,
          0.1059, -0.1102,  0.0965,  0.1121, -0.0080,  0.0219,  0.0490,  0.0933,
          0.1269,  0.0115, -0.1455, -0.1522, -0.1188,  0.1262, -0.0375,  0.0156,
          0.0108,  0.0229, -0.0171, -0.0579,  0.0169,  0.0687, -0.0854,  0.1185,
         -0.1483, -0.0266,  0.1327, -0.1012,  0.1173,  0.1156, -0.0375,  0.0488,
          0.0488, -0.1317,  0.0124, -0.1369,  0.1028,  0.1052, -0.1343,  0.0207,
          0.0973,  0.0648, -0.1473, -0.0736,  0.0476, -0.0906,  0.0420,  0.0434,
          0.0240,  0.0680,  0.0947, -0.0238,  0.0766,  0.0608,  0.1485, -0.0670]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1470, -0.1220, -0.1121, -0.0352, -0.1029,  0.0066, -0.0015,  0.0642,
         -0.1425,  0.0328, -0.0818, -0.0364, -0.1380,  0.0206, -0.1089, -0.0254,
         -0.0228, -0.0322, -0.1102,  0.1035,  0.0030, -0.0389, -0.0469,  0.0771,
         -0.1523,  0.1396, -0.0549, -0.0323,  0.1101, -0.1416,  0.1192, -0.0608,
         -0.0977,  0.0566, -0.1306, -0.1415,  0.1447,  0.0233,  0.1175,  0.1430,
         -0.1427, -0.0508, -0.0631,  0.0616, -0.1409, -0.0892, -0.0778,  0.1490,
         -0.0260, -0.0373, -0.1139, -0.0457,  0.0613,  0.0240, -0.0713,  0.0565,
          0.0286,  0.1072, -0.0991, -0.1378, -0.1448,  0.1179, -0.1237,  0.0945,
          0.0707, -0.1483,  0.0894, -0.1054, -0.0639,  0.0639, -0.0198,  0.0775,
         -0.1100,  0.0007,  0.0765, -0.1056, -0.0547,  0.1453,  0.0361,  0.0241,
         -0.1320,  0.0724,  0.1311, -0.0030, -0.0942, -0.1159, -0.1260, -0.0458,
         -0.0452,  0.0590,  0.0695,  0.0781, -0.1484, -0.0377,  0.0234,  0.0659,
         -0.1421, -0.0327, -0.0645,  0.0587,  0.0610, -0.0714, -0.1335,  0.0014,
         -0.1492,  0.0474,  0.0941,  0.0542,  0.1021, -0.0456,  0.1456,  0.0910,
         -0.1032, -0.0041, -0.0389, -0.0030,  0.0101, -0.1497,  0.0174,  0.0541,
          0.1439,  0.0657, -0.0430,  0.1193, -0.0637, -0.0089, -0.0896,  0.0829,
         -0.1360,  0.0412, -0.0231, -0.1055, -0.1350, -0.1470,  0.1052,  0.0716,
          0.0101,  0.1379,  0.1219,  0.0150, -0.0599,  0.0517, -0.0914,  0.0721,
          0.0555, -0.1364, -0.0704, -0.0307, -0.0837, -0.1289, -0.0787,  0.0076,
         -0.0641,  0.0367,  0.0481,  0.0891, -0.1212,  0.1086, -0.1490, -0.0760,
         -0.0261, -0.0861,  0.0660,  0.0994, -0.0011,  0.0922,  0.0448, -0.0371,
          0.0433, -0.0882,  0.0057,  0.1291,  0.1149, -0.0883,  0.1458,  0.1408,
         -0.0731, -0.0228, -0.0356,  0.1192,  0.1372,  0.1349,  0.0830,  0.0281,
          0.0302,  0.0223,  0.1451, -0.1072, -0.0626, -0.0258,  0.1111,  0.0425,
          0.1332,  0.1037, -0.0052,  0.0974,  0.0066,  0.0707, -0.1407, -0.0516,
          0.1059, -0.1102,  0.0965,  0.1121, -0.0080,  0.0219,  0.0490,  0.0933,
          0.1269,  0.0115, -0.1455, -0.1522, -0.1188,  0.1262, -0.0375,  0.0156,
          0.0108,  0.0229, -0.0171, -0.0579,  0.0169,  0.0687, -0.0854,  0.1185,
         -0.1483, -0.0266,  0.1327, -0.1012,  0.1173,  0.1156, -0.0375,  0.0488,
          0.0488, -0.1317,  0.0124, -0.1369,  0.1028,  0.1052, -0.1343,  0.0207,
          0.0973,  0.0648, -0.1473, -0.0736,  0.0476, -0.0906,  0.0420,  0.0434,
          0.0240,  0.0680,  0.0947, -0.0238,  0.0766,  0.0608,  0.1485, -0.0670]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.1161,  0.0640,  0.0333,  ...,  0.0067, -0.1190, -0.0008],
        [ 0.0987, -0.0564,  0.0820,  ..., -0.0124,  0.1068,  0.0936],
        [ 0.0948, -0.1125, -0.0279,  ...,  0.0445, -0.0946,  0.0758],
        ...,
        [ 0.1241, -0.1237, -0.0056,  ..., -0.0685, -0.0312,  0.0496],
        [-0.0588, -0.1038,  0.0799,  ...,  0.0094, -0.1024, -0.0272],
        [ 0.0430, -0.0580,  0.0411,  ...,  0.0668, -0.1077, -0.0350]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1161,  0.0640,  0.0333,  ...,  0.0067, -0.1190, -0.0008],
        [ 0.0987, -0.0564,  0.0820,  ..., -0.0124,  0.1068,  0.0936],
        [ 0.0948, -0.1125, -0.0279,  ...,  0.0445, -0.0946,  0.0758],
        ...,
        [ 0.1241, -0.1237, -0.0056,  ..., -0.0685, -0.0312,  0.0496],
        [-0.0588, -0.1038,  0.0799,  ...,  0.0094, -0.1024, -0.0272],
        [ 0.0430, -0.0580,  0.0411,  ...,  0.0668, -0.1077, -0.0350]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.0897, -0.0278, -0.1546,  ...,  0.1162,  0.0003,  0.0856],
        [ 0.0506,  0.0729,  0.1394,  ..., -0.1676, -0.1220,  0.0150],
        [-0.1758, -0.1650,  0.0289,  ...,  0.0040, -0.0079,  0.1348],
        ...,
        [ 0.1066, -0.0064, -0.0160,  ...,  0.0880, -0.1194,  0.0862],
        [-0.0554, -0.1511, -0.0029,  ..., -0.0040,  0.0833, -0.0481],
        [-0.1475,  0.0538, -0.0477,  ..., -0.0054,  0.1152, -0.0599]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0897, -0.0278, -0.1546,  ...,  0.1162,  0.0003,  0.0856],
        [ 0.0506,  0.0729,  0.1394,  ..., -0.1676, -0.1220,  0.0150],
        [-0.1758, -0.1650,  0.0289,  ...,  0.0040, -0.0079,  0.1348],
        ...,
        [ 0.1066, -0.0064, -0.0160,  ...,  0.0880, -0.1194,  0.0862],
        [-0.0554, -0.1511, -0.0029,  ..., -0.0040,  0.0833, -0.0481],
        [-0.1475,  0.0538, -0.0477,  ..., -0.0054,  0.1152, -0.0599]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0650,  0.0458,  0.2343,  ...,  0.1472, -0.1732, -0.0219],
        [ 0.2266,  0.2335, -0.0359,  ..., -0.2293, -0.2435,  0.0732],
        [ 0.2071, -0.0296, -0.1107,  ..., -0.0798,  0.1597, -0.0394],
        ...,
        [-0.2152, -0.2194,  0.0669,  ..., -0.0976, -0.0440,  0.0123],
        [-0.2254, -0.2187, -0.2303,  ..., -0.0226, -0.1213, -0.2250],
        [ 0.0122,  0.1575,  0.1913,  ..., -0.1815,  0.2020, -0.2475]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0650,  0.0458,  0.2343,  ...,  0.1472, -0.1732, -0.0219],
        [ 0.2266,  0.2335, -0.0359,  ..., -0.2293, -0.2435,  0.0732],
        [ 0.2071, -0.0296, -0.1107,  ..., -0.0798,  0.1597, -0.0394],
        ...,
        [-0.2152, -0.2194,  0.0669,  ..., -0.0976, -0.0440,  0.0123],
        [-0.2254, -0.2187, -0.2303,  ..., -0.0226, -0.1213, -0.2250],
        [ 0.0122,  0.1575,  0.1913,  ..., -0.1815,  0.2020, -0.2475]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2864],
        [-0.2684],
        [-0.2901],
        [-0.2239],
        [-0.0064],
        [-0.2274],
        [ 0.3406],
        [-0.1745],
        [-0.3463],
        [ 0.3960],
        [-0.3286],
        [-0.0443],
        [-0.2170],
        [-0.3701],
        [-0.3852],
        [-0.4094],
        [-0.2166],
        [ 0.0111],
        [-0.0521],
        [-0.1143],
        [ 0.0558],
        [-0.0639],
        [-0.1049],
        [ 0.3444],
        [ 0.2954],
        [-0.4196],
        [-0.3117],
        [-0.0429],
        [ 0.2544],
        [-0.0997],
        [-0.3041],
        [-0.1807]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2864],
        [-0.2684],
        [-0.2901],
        [-0.2239],
        [-0.0064],
        [-0.2274],
        [ 0.3406],
        [-0.1745],
        [-0.3463],
        [ 0.3960],
        [-0.3286],
        [-0.0443],
        [-0.2170],
        [-0.3701],
        [-0.3852],
        [-0.4094],
        [-0.2166],
        [ 0.0111],
        [-0.0521],
        [-0.1143],
        [ 0.0558],
        [-0.0639],
        [-0.1049],
        [ 0.3444],
        [ 0.2954],
        [-0.4196],
        [-0.3117],
        [-0.0429],
        [ 0.2544],
        [-0.0997],
        [-0.3041],
        [-0.1807]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1073, -0.0737,  0.0962,  0.0966,  0.0669, -0.1321, -0.0130, -0.0890,
          0.0759,  0.1428,  0.0087,  0.1237,  0.1427,  0.1508,  0.1062,  0.0337,
         -0.1217, -0.0716,  0.0191, -0.0538,  0.0792,  0.1204, -0.0338, -0.0901,
          0.0920,  0.0786,  0.0471, -0.0703, -0.1447,  0.1200,  0.1174, -0.0995,
         -0.0625,  0.1510, -0.1058,  0.0134,  0.0715, -0.0131, -0.0445,  0.0405,
          0.0812, -0.0169, -0.1351, -0.0740, -0.1384, -0.0432, -0.1481, -0.1088,
          0.0716,  0.1415,  0.0792,  0.0427, -0.1159,  0.0408, -0.0417, -0.0357,
         -0.0783, -0.1012,  0.0346,  0.0347, -0.1058, -0.0167, -0.0545, -0.0458,
          0.0817, -0.1087,  0.0414, -0.1141,  0.0081,  0.1399,  0.1328, -0.1079,
          0.0946, -0.0861, -0.0836,  0.0443, -0.0327,  0.0982, -0.0137,  0.0467,
         -0.0331, -0.0910,  0.1475, -0.1173, -0.0304,  0.0442,  0.0195,  0.0414,
         -0.1093,  0.1278,  0.0589, -0.0825, -0.0276, -0.0049, -0.1127, -0.0933,
         -0.1232, -0.0347, -0.0615, -0.1124, -0.0200,  0.0132,  0.0631, -0.1009,
          0.0170,  0.0177, -0.0974, -0.1043,  0.0896, -0.1092, -0.0372,  0.0772,
          0.0201, -0.0437, -0.0505,  0.1350,  0.1096, -0.0397,  0.1073, -0.0964,
         -0.0857,  0.0213, -0.0129,  0.1316,  0.1140,  0.0064,  0.0213,  0.1326,
          0.1087, -0.0970,  0.1158,  0.1525,  0.0572,  0.0292,  0.1113,  0.0079,
          0.1114,  0.0274, -0.0665, -0.0232,  0.0445, -0.1120,  0.1047,  0.0271,
         -0.1252,  0.0189,  0.0233, -0.1465,  0.0008, -0.0654, -0.0424,  0.1427,
          0.0006,  0.0427, -0.0479, -0.1052,  0.0410, -0.1318,  0.0848, -0.1204,
          0.0875, -0.0125,  0.1016, -0.0491, -0.0702, -0.1226,  0.0772, -0.0267,
         -0.0326,  0.1228,  0.0462,  0.0647, -0.0676, -0.0357,  0.0157,  0.0935,
          0.0780,  0.0904, -0.0947, -0.0003, -0.0202,  0.0698,  0.1205, -0.0963,
         -0.0603,  0.0395,  0.1368, -0.1255, -0.0682, -0.0252, -0.1378,  0.1295,
          0.0826,  0.1346,  0.0289, -0.0755, -0.1471,  0.1357,  0.1452,  0.0879,
          0.0627, -0.1232, -0.0813, -0.0876,  0.0127,  0.0346, -0.1411,  0.0829,
         -0.0100,  0.0607, -0.0516, -0.0918,  0.1152, -0.0141,  0.1427, -0.1036,
          0.0861,  0.0168, -0.1483, -0.0587, -0.0192, -0.0181, -0.0430,  0.0928,
         -0.1189,  0.0036,  0.0943, -0.1366, -0.0326, -0.0068, -0.0877, -0.0868,
          0.0994, -0.1072, -0.0953,  0.0583,  0.0095, -0.0642, -0.0311,  0.0809,
         -0.0295, -0.0654,  0.0949, -0.0406, -0.1389,  0.0149,  0.0778, -0.1092,
         -0.0660, -0.1145, -0.1014, -0.0402, -0.0523,  0.1186,  0.0645,  0.1356]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1073, -0.0737,  0.0962,  0.0966,  0.0669, -0.1321, -0.0130, -0.0890,
          0.0759,  0.1428,  0.0087,  0.1237,  0.1427,  0.1508,  0.1062,  0.0337,
         -0.1217, -0.0716,  0.0191, -0.0538,  0.0792,  0.1204, -0.0338, -0.0901,
          0.0920,  0.0786,  0.0471, -0.0703, -0.1447,  0.1200,  0.1174, -0.0995,
         -0.0625,  0.1510, -0.1058,  0.0134,  0.0715, -0.0131, -0.0445,  0.0405,
          0.0812, -0.0169, -0.1351, -0.0740, -0.1384, -0.0432, -0.1481, -0.1088,
          0.0716,  0.1415,  0.0792,  0.0427, -0.1159,  0.0408, -0.0417, -0.0357,
         -0.0783, -0.1012,  0.0346,  0.0347, -0.1058, -0.0167, -0.0545, -0.0458,
          0.0817, -0.1087,  0.0414, -0.1141,  0.0081,  0.1399,  0.1328, -0.1079,
          0.0946, -0.0861, -0.0836,  0.0443, -0.0327,  0.0982, -0.0137,  0.0467,
         -0.0331, -0.0910,  0.1475, -0.1173, -0.0304,  0.0442,  0.0195,  0.0414,
         -0.1093,  0.1278,  0.0589, -0.0825, -0.0276, -0.0049, -0.1127, -0.0933,
         -0.1232, -0.0347, -0.0615, -0.1124, -0.0200,  0.0132,  0.0631, -0.1009,
          0.0170,  0.0177, -0.0974, -0.1043,  0.0896, -0.1092, -0.0372,  0.0772,
          0.0201, -0.0437, -0.0505,  0.1350,  0.1096, -0.0397,  0.1073, -0.0964,
         -0.0857,  0.0213, -0.0129,  0.1316,  0.1140,  0.0064,  0.0213,  0.1326,
          0.1087, -0.0970,  0.1158,  0.1525,  0.0572,  0.0292,  0.1113,  0.0079,
          0.1114,  0.0274, -0.0665, -0.0232,  0.0445, -0.1120,  0.1047,  0.0271,
         -0.1252,  0.0189,  0.0233, -0.1465,  0.0008, -0.0654, -0.0424,  0.1427,
          0.0006,  0.0427, -0.0479, -0.1052,  0.0410, -0.1318,  0.0848, -0.1204,
          0.0875, -0.0125,  0.1016, -0.0491, -0.0702, -0.1226,  0.0772, -0.0267,
         -0.0326,  0.1228,  0.0462,  0.0647, -0.0676, -0.0357,  0.0157,  0.0935,
          0.0780,  0.0904, -0.0947, -0.0003, -0.0202,  0.0698,  0.1205, -0.0963,
         -0.0603,  0.0395,  0.1368, -0.1255, -0.0682, -0.0252, -0.1378,  0.1295,
          0.0826,  0.1346,  0.0289, -0.0755, -0.1471,  0.1357,  0.1452,  0.0879,
          0.0627, -0.1232, -0.0813, -0.0876,  0.0127,  0.0346, -0.1411,  0.0829,
         -0.0100,  0.0607, -0.0516, -0.0918,  0.1152, -0.0141,  0.1427, -0.1036,
          0.0861,  0.0168, -0.1483, -0.0587, -0.0192, -0.0181, -0.0430,  0.0928,
         -0.1189,  0.0036,  0.0943, -0.1366, -0.0326, -0.0068, -0.0877, -0.0868,
          0.0994, -0.1072, -0.0953,  0.0583,  0.0095, -0.0642, -0.0311,  0.0809,
         -0.0295, -0.0654,  0.0949, -0.0406, -0.1389,  0.0149,  0.0778, -0.1092,
         -0.0660, -0.1145, -0.1014, -0.0402, -0.0523,  0.1186,  0.0645,  0.1356]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0711, -0.0260, -0.0666,  ...,  0.0323,  0.0279,  0.0487],
        [ 0.0977, -0.0581, -0.0634,  ...,  0.0016,  0.0471,  0.1178],
        [ 0.1175, -0.0141,  0.0408,  ...,  0.0110,  0.1155,  0.1202],
        ...,
        [-0.0250, -0.0293,  0.0474,  ..., -0.0944,  0.0120, -0.0061],
        [-0.0592, -0.0422, -0.0944,  ..., -0.0037, -0.0598,  0.0440],
        [ 0.1038, -0.0756,  0.1002,  ..., -0.0801, -0.0014,  0.0487]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0711, -0.0260, -0.0666,  ...,  0.0323,  0.0279,  0.0487],
        [ 0.0977, -0.0581, -0.0634,  ...,  0.0016,  0.0471,  0.1178],
        [ 0.1175, -0.0141,  0.0408,  ...,  0.0110,  0.1155,  0.1202],
        ...,
        [-0.0250, -0.0293,  0.0474,  ..., -0.0944,  0.0120, -0.0061],
        [-0.0592, -0.0422, -0.0944,  ..., -0.0037, -0.0598,  0.0440],
        [ 0.1038, -0.0756,  0.1002,  ..., -0.0801, -0.0014,  0.0487]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.1066,  0.1014, -0.0210,  ..., -0.1691,  0.1699,  0.0284],
        [ 0.1387,  0.0149,  0.1317,  ...,  0.0895,  0.0747,  0.0784],
        [ 0.1723,  0.0733, -0.1664,  ...,  0.1396,  0.0387, -0.0754],
        ...,
        [-0.0064,  0.0186, -0.1554,  ...,  0.1065, -0.1538,  0.0854],
        [-0.1425,  0.0666,  0.1138,  ..., -0.1625, -0.1239, -0.1039],
        [ 0.1645, -0.0394, -0.0818,  ...,  0.1418,  0.0928,  0.0430]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1066,  0.1014, -0.0210,  ..., -0.1691,  0.1699,  0.0284],
        [ 0.1387,  0.0149,  0.1317,  ...,  0.0895,  0.0747,  0.0784],
        [ 0.1723,  0.0733, -0.1664,  ...,  0.1396,  0.0387, -0.0754],
        ...,
        [-0.0064,  0.0186, -0.1554,  ...,  0.1065, -0.1538,  0.0854],
        [-0.1425,  0.0666,  0.1138,  ..., -0.1625, -0.1239, -0.1039],
        [ 0.1645, -0.0394, -0.0818,  ...,  0.1418,  0.0928,  0.0430]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0367,  0.1257,  0.1380,  ..., -0.2243,  0.0242, -0.1167],
        [ 0.0460,  0.1870, -0.2434,  ...,  0.0275,  0.0345, -0.1678],
        [-0.1721,  0.1097,  0.1956,  ...,  0.2014, -0.0269, -0.0962],
        ...,
        [ 0.0553, -0.0041, -0.2202,  ..., -0.0330,  0.1422, -0.1545],
        [-0.0449,  0.0981, -0.1651,  ...,  0.0601,  0.1891, -0.0846],
        [-0.1315, -0.0154,  0.0846,  ..., -0.2497,  0.1477,  0.1694]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0367,  0.1257,  0.1380,  ..., -0.2243,  0.0242, -0.1167],
        [ 0.0460,  0.1870, -0.2434,  ...,  0.0275,  0.0345, -0.1678],
        [-0.1721,  0.1097,  0.1956,  ...,  0.2014, -0.0269, -0.0962],
        ...,
        [ 0.0553, -0.0041, -0.2202,  ..., -0.0330,  0.1422, -0.1545],
        [-0.0449,  0.0981, -0.1651,  ...,  0.0601,  0.1891, -0.0846],
        [-0.1315, -0.0154,  0.0846,  ..., -0.2497,  0.1477,  0.1694]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3033],
        [-0.0783],
        [-0.3570],
        [-0.1194],
        [-0.1043],
        [-0.2579],
        [ 0.2734],
        [ 0.3564],
        [ 0.3477],
        [ 0.2634],
        [-0.0298],
        [-0.3158],
        [-0.0527],
        [ 0.3846],
        [-0.1505],
        [-0.2811],
        [-0.1624],
        [-0.3721],
        [-0.3493],
        [ 0.1939],
        [ 0.2620],
        [-0.3297],
        [-0.0550],
        [ 0.2946],
        [-0.0672],
        [-0.3539],
        [ 0.0952],
        [-0.3648],
        [-0.2058],
        [-0.1791],
        [-0.3870],
        [ 0.4048]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3033],
        [-0.0783],
        [-0.3570],
        [-0.1194],
        [-0.1043],
        [-0.2579],
        [ 0.2734],
        [ 0.3564],
        [ 0.3477],
        [ 0.2634],
        [-0.0298],
        [-0.3158],
        [-0.0527],
        [ 0.3846],
        [-0.1505],
        [-0.2811],
        [-0.1624],
        [-0.3721],
        [-0.3493],
        [ 0.1939],
        [ 0.2620],
        [-0.3297],
        [-0.0550],
        [ 0.2946],
        [-0.0672],
        [-0.3539],
        [ 0.0952],
        [-0.3648],
        [-0.2058],
        [-0.1791],
        [-0.3870],
        [ 0.4048]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(48.2305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.5290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(2.5956, device='cuda:0')



h[100].sum tensor(-3.6364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.7321, device='cuda:0')



h[200].sum tensor(-1.1236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-1.1531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3235.7085, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0004,  ..., 0.0007, 0.0000, 0.0010],
        [0.0000, 0.0071, 0.0022,  ..., 0.0037, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(15496.8857, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-24.2667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(309.2167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(24.7365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.4565, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0074],
        [0.0091],
        [0.0131],
        ...,
        [0.0021],
        [0.0021],
        [0.0017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(186.5933, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0074],
        [0.0091],
        [0.0131],
        ...,
        [0.0021],
        [0.0021],
        [0.0017]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(262.6512, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(2.5665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(2.5457, device='cuda:0')



h[100].sum tensor(17.3470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(17.2064, device='cuda:0')



h[200].sum tensor(0.4604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(0.4566, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15452.8486, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0267, 0.0000, 0.0281,  ..., 0.0174, 0.0000, 0.0000],
        [0.0056, 0.0000, 0.0059,  ..., 0.0036, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(80252.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2161.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(152.0410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1293.7904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.0242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2774.6880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(195.2122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0356],
        [-0.0218],
        [-0.0133],
        ...,
        [ 0.0000],
        [ 0.0000],
        [ 0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-2089.2686, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0074],
        [0.0091],
        [0.0131],
        ...,
        [0.0021],
        [0.0021],
        [0.0017]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 1 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0774,  0.1443,  0.1491, -0.0232, -0.0112, -0.0098, -0.0485,  0.0144,
          0.0618,  0.1036, -0.0794, -0.0220, -0.1222, -0.0218, -0.1338, -0.0238,
         -0.0725, -0.1043,  0.1258,  0.0893,  0.1123, -0.1409,  0.0038,  0.0916,
         -0.0402,  0.0658,  0.1527, -0.0484, -0.1245,  0.0707,  0.0154,  0.0450,
         -0.0046, -0.1127, -0.0961, -0.0820, -0.1001, -0.0552, -0.1197,  0.0953,
          0.0201,  0.0601,  0.0761,  0.0811,  0.0159, -0.0524,  0.0696,  0.0047,
          0.1287,  0.0523, -0.0855,  0.1147,  0.1231,  0.0692,  0.1274, -0.0063,
         -0.1102,  0.0012,  0.0373, -0.0210, -0.0378, -0.0034, -0.0498,  0.1114,
          0.0857,  0.0476, -0.1156,  0.1145,  0.1225,  0.0937, -0.0741, -0.0485,
          0.0249,  0.0178,  0.0472, -0.0891, -0.0415,  0.0478,  0.1426, -0.1468,
          0.1518,  0.0636, -0.0384,  0.0320, -0.0637,  0.0406, -0.1283, -0.0531,
         -0.0206, -0.1460, -0.1219, -0.1126,  0.0255,  0.0838, -0.1385,  0.0188,
          0.1463,  0.0217, -0.0516,  0.1442,  0.1324,  0.0163,  0.1442, -0.0598,
          0.1223,  0.1458,  0.1388, -0.0326, -0.1360, -0.1341,  0.0075,  0.1309,
          0.0653,  0.0236, -0.0782,  0.0985,  0.0750, -0.1522,  0.0067,  0.0663,
          0.0744,  0.1433, -0.0381,  0.1003, -0.0831,  0.0549, -0.0668,  0.0336,
          0.1465,  0.0468,  0.1423, -0.1321,  0.0566,  0.0224, -0.1380,  0.0658,
         -0.1340,  0.0741,  0.0457,  0.0538, -0.0160, -0.1030, -0.0006, -0.0832,
          0.0323, -0.1044,  0.0251,  0.1320, -0.0240, -0.0410, -0.0606, -0.0360,
         -0.0251, -0.0235, -0.0052, -0.0546,  0.1184, -0.1216,  0.0197, -0.0980,
          0.0700,  0.1262, -0.0934, -0.0454,  0.0532, -0.0754,  0.0323,  0.0841,
          0.1228, -0.0329, -0.1203,  0.0559, -0.1020,  0.1009, -0.0895, -0.0601,
         -0.0115,  0.0464,  0.0798,  0.0354, -0.1110,  0.1182,  0.1189, -0.0776,
          0.1356,  0.0541, -0.0204,  0.0340, -0.1298, -0.1207,  0.0347,  0.0385,
         -0.0635, -0.0320,  0.0909,  0.0619, -0.1152, -0.1419,  0.0616, -0.0032,
         -0.0301,  0.0101, -0.0767,  0.0978, -0.0535, -0.0892, -0.0990, -0.0361,
         -0.0044, -0.1390,  0.0689, -0.0726, -0.0149,  0.0677, -0.1042, -0.0632,
         -0.1282,  0.0162,  0.0880,  0.0098,  0.1080, -0.0028, -0.1011,  0.0345,
          0.1325, -0.0359,  0.0598, -0.0108, -0.1450, -0.0302,  0.0344, -0.0058,
         -0.1395, -0.0107,  0.0699,  0.1239, -0.1456,  0.1346, -0.0981,  0.0829,
         -0.1291,  0.1317,  0.1094,  0.0545, -0.0290, -0.0644, -0.0812, -0.1148,
          0.0171,  0.0904, -0.0729, -0.0547, -0.0067,  0.0068,  0.0704,  0.0329]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0582, -0.0639,  0.0200,  ...,  0.0342, -0.1150,  0.0426],
        [ 0.1167,  0.0765,  0.1188,  ...,  0.0061, -0.1010,  0.0432],
        [-0.0241, -0.0109,  0.0397,  ...,  0.0953, -0.0128,  0.0609],
        ...,
        [-0.0689, -0.0685, -0.0612,  ..., -0.0331,  0.1185,  0.1127],
        [-0.1119, -0.0660, -0.1110,  ..., -0.0902, -0.0946, -0.0687],
        [ 0.0346,  0.0428, -0.0197,  ..., -0.0769,  0.0546,  0.0680]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0545, -0.1648, -0.1179,  ..., -0.0992,  0.1106, -0.1729],
        [ 0.0808,  0.0931,  0.0190,  ..., -0.0781,  0.0702, -0.1311],
        [-0.1653,  0.1143, -0.0042,  ..., -0.0736, -0.1426,  0.0721],
        ...,
        [-0.1488, -0.0790, -0.0840,  ..., -0.0634,  0.1608,  0.1498],
        [ 0.0288,  0.1261,  0.1663,  ...,  0.0701, -0.0021,  0.0083],
        [-0.0999, -0.1553,  0.0173,  ..., -0.1089, -0.0108,  0.0125]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0046,  0.1295,  0.2435,  ..., -0.0853,  0.0964, -0.0804],
        [-0.1259,  0.0028,  0.0576,  ...,  0.1763,  0.0277,  0.1418],
        [-0.0751, -0.0983,  0.1953,  ...,  0.0122,  0.1555,  0.2160],
        ...,
        [ 0.2172, -0.0397,  0.1538,  ..., -0.1958, -0.1656,  0.1826],
        [ 0.1637, -0.2262,  0.0830,  ...,  0.0479, -0.0444,  0.1343],
        [ 0.0654,  0.1707, -0.2076,  ...,  0.1457,  0.0016,  0.1313]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.3964],
        [ 0.3735],
        [-0.1815],
        [ 0.1698],
        [ 0.2010],
        [ 0.2712],
        [ 0.4118],
        [-0.1170],
        [ 0.2244],
        [-0.3674],
        [-0.0824],
        [-0.0755],
        [ 0.1231],
        [-0.1225],
        [-0.3035],
        [-0.3318],
        [-0.0968],
        [-0.3880],
        [ 0.4163],
        [-0.2649],
        [-0.1603],
        [-0.2071],
        [ 0.4004],
        [ 0.3157],
        [ 0.3984],
        [-0.2874],
        [ 0.2831],
        [-0.1952],
        [ 0.0617],
        [ 0.1035],
        [ 0.1722],
        [ 0.1445]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[-0.0774,  0.1443,  0.1491, -0.0232, -0.0112, -0.0098, -0.0485,  0.0144,
          0.0618,  0.1036, -0.0794, -0.0220, -0.1222, -0.0218, -0.1338, -0.0238,
         -0.0725, -0.1043,  0.1258,  0.0893,  0.1123, -0.1409,  0.0038,  0.0916,
         -0.0402,  0.0658,  0.1527, -0.0484, -0.1245,  0.0707,  0.0154,  0.0450,
         -0.0046, -0.1127, -0.0961, -0.0820, -0.1001, -0.0552, -0.1197,  0.0953,
          0.0201,  0.0601,  0.0761,  0.0811,  0.0159, -0.0524,  0.0696,  0.0047,
          0.1287,  0.0523, -0.0855,  0.1147,  0.1231,  0.0692,  0.1274, -0.0063,
         -0.1102,  0.0012,  0.0373, -0.0210, -0.0378, -0.0034, -0.0498,  0.1114,
          0.0857,  0.0476, -0.1156,  0.1145,  0.1225,  0.0937, -0.0741, -0.0485,
          0.0249,  0.0178,  0.0472, -0.0891, -0.0415,  0.0478,  0.1426, -0.1468,
          0.1518,  0.0636, -0.0384,  0.0320, -0.0637,  0.0406, -0.1283, -0.0531,
         -0.0206, -0.1460, -0.1219, -0.1126,  0.0255,  0.0838, -0.1385,  0.0188,
          0.1463,  0.0217, -0.0516,  0.1442,  0.1324,  0.0163,  0.1442, -0.0598,
          0.1223,  0.1458,  0.1388, -0.0326, -0.1360, -0.1341,  0.0075,  0.1309,
          0.0653,  0.0236, -0.0782,  0.0985,  0.0750, -0.1522,  0.0067,  0.0663,
          0.0744,  0.1433, -0.0381,  0.1003, -0.0831,  0.0549, -0.0668,  0.0336,
          0.1465,  0.0468,  0.1423, -0.1321,  0.0566,  0.0224, -0.1380,  0.0658,
         -0.1340,  0.0741,  0.0457,  0.0538, -0.0160, -0.1030, -0.0006, -0.0832,
          0.0323, -0.1044,  0.0251,  0.1320, -0.0240, -0.0410, -0.0606, -0.0360,
         -0.0251, -0.0235, -0.0052, -0.0546,  0.1184, -0.1216,  0.0197, -0.0980,
          0.0700,  0.1262, -0.0934, -0.0454,  0.0532, -0.0754,  0.0323,  0.0841,
          0.1228, -0.0329, -0.1203,  0.0559, -0.1020,  0.1009, -0.0895, -0.0601,
         -0.0115,  0.0464,  0.0798,  0.0354, -0.1110,  0.1182,  0.1189, -0.0776,
          0.1356,  0.0541, -0.0204,  0.0340, -0.1298, -0.1207,  0.0347,  0.0385,
         -0.0635, -0.0320,  0.0909,  0.0619, -0.1152, -0.1419,  0.0616, -0.0032,
         -0.0301,  0.0101, -0.0767,  0.0978, -0.0535, -0.0892, -0.0990, -0.0361,
         -0.0044, -0.1390,  0.0689, -0.0726, -0.0149,  0.0677, -0.1042, -0.0632,
         -0.1282,  0.0162,  0.0880,  0.0098,  0.1080, -0.0028, -0.1011,  0.0345,
          0.1325, -0.0359,  0.0598, -0.0108, -0.1450, -0.0302,  0.0344, -0.0058,
         -0.1395, -0.0107,  0.0699,  0.1239, -0.1456,  0.1346, -0.0981,  0.0829,
         -0.1291,  0.1317,  0.1094,  0.0545, -0.0290, -0.0644, -0.0812, -0.1148,
          0.0171,  0.0904, -0.0729, -0.0547, -0.0067,  0.0068,  0.0704,  0.0329]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0582, -0.0639,  0.0200,  ...,  0.0342, -0.1150,  0.0426],
        [ 0.1167,  0.0765,  0.1188,  ...,  0.0061, -0.1010,  0.0432],
        [-0.0241, -0.0109,  0.0397,  ...,  0.0953, -0.0128,  0.0609],
        ...,
        [-0.0689, -0.0685, -0.0612,  ..., -0.0331,  0.1185,  0.1127],
        [-0.1119, -0.0660, -0.1110,  ..., -0.0902, -0.0946, -0.0687],
        [ 0.0346,  0.0428, -0.0197,  ..., -0.0769,  0.0546,  0.0680]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0545, -0.1648, -0.1179,  ..., -0.0992,  0.1106, -0.1729],
        [ 0.0808,  0.0931,  0.0190,  ..., -0.0781,  0.0702, -0.1311],
        [-0.1653,  0.1143, -0.0042,  ..., -0.0736, -0.1426,  0.0721],
        ...,
        [-0.1488, -0.0790, -0.0840,  ..., -0.0634,  0.1608,  0.1498],
        [ 0.0288,  0.1261,  0.1663,  ...,  0.0701, -0.0021,  0.0083],
        [-0.0999, -0.1553,  0.0173,  ..., -0.1089, -0.0108,  0.0125]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0046,  0.1295,  0.2435,  ..., -0.0853,  0.0964, -0.0804],
        [-0.1259,  0.0028,  0.0576,  ...,  0.1763,  0.0277,  0.1418],
        [-0.0751, -0.0983,  0.1953,  ...,  0.0122,  0.1555,  0.2160],
        ...,
        [ 0.2172, -0.0397,  0.1538,  ..., -0.1958, -0.1656,  0.1826],
        [ 0.1637, -0.2262,  0.0830,  ...,  0.0479, -0.0444,  0.1343],
        [ 0.0654,  0.1707, -0.2076,  ...,  0.1457,  0.0016,  0.1313]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.3964],
        [ 0.3735],
        [-0.1815],
        [ 0.1698],
        [ 0.2010],
        [ 0.2712],
        [ 0.4118],
        [-0.1170],
        [ 0.2244],
        [-0.3674],
        [-0.0824],
        [-0.0755],
        [ 0.1231],
        [-0.1225],
        [-0.3035],
        [-0.3318],
        [-0.0968],
        [-0.3880],
        [ 0.4163],
        [-0.2649],
        [-0.1603],
        [-0.2071],
        [ 0.4004],
        [ 0.3157],
        [ 0.3984],
        [-0.2874],
        [ 0.2831],
        [-0.1952],
        [ 0.0617],
        [ 0.1035],
        [ 0.1722],
        [ 0.1445]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0114,  0.0212,  0.0219,  ...,  0.0010,  0.0103,  0.0048],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(183.7097, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.6995, device='cuda:0')



h[100].sum tensor(38.7366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.6812, device='cuda:0')



h[200].sum tensor(-8.8080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0800, 0.0827,  ..., 0.0038, 0.0390, 0.0182],
        [0.0000, 0.0658, 0.0680,  ..., 0.0031, 0.0321, 0.0150],
        [0.0000, 0.0154, 0.0159,  ..., 0.0007, 0.0075, 0.0035],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29532.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0639, 0.0000, 0.0000,  ..., 0.0976, 0.0000, 0.3840],
        [0.0548, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.3290],
        [0.0440, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.2642],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(141682.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(890.2242, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(131.6548, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1818.8986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-368.1106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-95.7756, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[5.2836e-01],
        [5.7171e-01],
        [6.3086e-01],
        ...,
        [6.3474e-07],
        [8.3397e-08],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(9847.1338, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(69.3796, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0001],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365922.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
         -1.0000e-04,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
         -1.0000e-04,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
         -1.0000e-04,  1.0000e-04],
        ...,
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
         -1.0000e-04,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
         -1.0000e-04,  1.0000e-04],
        [ 0.0000e+00, -1.0000e-04,  1.0000e-04,  ..., -1.0000e-04,
         -1.0000e-04,  1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(165.9712, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.8622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.9619, device='cuda:0')



h[100].sum tensor(39.1551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.5056, device='cuda:0')



h[200].sum tensor(-8.0976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0000, 0.0004,  ..., 0.0000, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30068.9180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[1.6087e-03, 0.0000e+00, 5.8717e-04,  ..., 4.9588e-04, 0.0000e+00,
         0.0000e+00],
        [1.6086e-03, 0.0000e+00, 5.8715e-04,  ..., 4.9585e-04, 0.0000e+00,
         0.0000e+00],
        [1.6729e-03, 0.0000e+00, 5.4873e-04,  ..., 6.0458e-04, 0.0000e+00,
         5.0976e-05],
        ...,
        [1.5986e-03, 0.0000e+00, 5.8235e-04,  ..., 4.8860e-04, 0.0000e+00,
         0.0000e+00],
        [1.5986e-03, 0.0000e+00, 5.8235e-04,  ..., 4.8860e-04, 0.0000e+00,
         0.0000e+00],
        [1.5986e-03, 0.0000e+00, 5.8235e-04,  ..., 4.8860e-04, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155805.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1284.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(115.5084, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2445.8708, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-354.6022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-108.8381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0178],
        [-0.0196],
        [-0.0212],
        ...,
        [-0.0104],
        [-0.0104],
        [-0.0104]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-6405.9033, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0001],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365922.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0002],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365925.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000, -0.0002,  0.0002,  ..., -0.0002, -0.0001,  0.0002],
        [ 0.0000, -0.0002,  0.0002,  ..., -0.0002, -0.0001,  0.0002],
        [ 0.0000, -0.0002,  0.0002,  ..., -0.0002, -0.0001,  0.0002],
        ...,
        [ 0.0000, -0.0002,  0.0002,  ..., -0.0002, -0.0001,  0.0002],
        [ 0.0000, -0.0002,  0.0002,  ..., -0.0002, -0.0001,  0.0002],
        [ 0.0000, -0.0002,  0.0002,  ..., -0.0002, -0.0001,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(89.7520, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1858, device='cuda:0')



h[100].sum tensor(27.1705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.5976, device='cuda:0')



h[200].sum tensor(-5.7376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.3873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0705, 0.0744,  ..., 0.0027, 0.0342, 0.0169],
        [0.0000, 0.0468, 0.0496,  ..., 0.0018, 0.0227, 0.0115],
        [0.0000, 0.0387, 0.0412,  ..., 0.0014, 0.0187, 0.0096],
        ...,
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0000, 0.0007,  ..., 0.0000, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23372.0723, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0605, 0.0000, 0.0000,  ..., 0.0807, 0.0000, 0.3133],
        [0.0491, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.2510],
        [0.0380, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.1902],
        ...,
        [0.0030, 0.0000, 0.0005,  ..., 0.0008, 0.0000, 0.0000],
        [0.0030, 0.0000, 0.0005,  ..., 0.0008, 0.0000, 0.0000],
        [0.0030, 0.0000, 0.0005,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(130070.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1447.6418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.4942, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2361.4424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.2212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-95.7343, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0182],
        [-0.0204],
        [-0.0242],
        ...,
        [-0.0210],
        [-0.0209],
        [-0.0208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-14014.5234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0002],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365925.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0002],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365932.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.2005e-03,  7.6931e-03,  8.3825e-03,  ...,  8.6281e-05,
          3.7209e-03,  2.0302e-03],
        [-1.0183e-02,  1.8868e-02,  1.9969e-02,  ...,  5.9074e-04,
          9.1613e-03,  4.6026e-03],
        [-7.4508e-03,  1.3765e-02,  1.4677e-02,  ...,  3.6037e-04,
          6.6768e-03,  3.4279e-03],
        ...,
        [ 0.0000e+00, -1.5332e-04,  2.4738e-04,  ..., -2.6792e-04,
         -9.8955e-05,  2.2395e-04],
        [ 0.0000e+00, -1.5332e-04,  2.4738e-04,  ..., -2.6792e-04,
         -9.8955e-05,  2.2395e-04],
        [ 0.0000e+00, -1.5332e-04,  2.4738e-04,  ..., -2.6792e-04,
         -9.8955e-05,  2.2395e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(131.0944, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5952, device='cuda:0')



h[100].sum tensor(31.7320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5512, device='cuda:0')



h[200].sum tensor(-6.2788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0673, 0.0714,  ..., 0.0020, 0.0327, 0.0165],
        [0.0000, 0.0432, 0.0464,  ..., 0.0009, 0.0209, 0.0110],
        [0.0000, 0.0363, 0.0391,  ..., 0.0009, 0.0176, 0.0094],
        ...,
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0010,  ..., 0.0000, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24415.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0423, 0.0000, 0.0000,  ..., 0.0525, 0.0000, 0.2121],
        [0.0381, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.1894],
        [0.0332, 0.0000, 0.0000,  ..., 0.0405, 0.0000, 0.1629],
        ...,
        [0.0031, 0.0000, 0.0002,  ..., 0.0004, 0.0000, 0.0006],
        [0.0031, 0.0000, 0.0002,  ..., 0.0004, 0.0000, 0.0006],
        [0.0031, 0.0000, 0.0002,  ..., 0.0004, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(132033.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1389.5488, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(80.2648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2184.8145, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.8810, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-117.4250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0076],
        [ 0.0064],
        [ 0.0040],
        ...,
        [-0.0228],
        [-0.0227],
        [-0.0227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-9271.2090, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0002],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365932.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365945.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -9.1206e-05,  3.2360e-04,  ..., -3.3113e-04,
         -3.2784e-05,  2.2666e-04],
        [ 0.0000e+00, -9.1206e-05,  3.2360e-04,  ..., -3.3113e-04,
         -3.2784e-05,  2.2666e-04],
        [ 0.0000e+00, -9.1206e-05,  3.2360e-04,  ..., -3.3113e-04,
         -3.2784e-05,  2.2666e-04],
        ...,
        [ 0.0000e+00, -9.1206e-05,  3.2360e-04,  ..., -3.3113e-04,
         -3.2784e-05,  2.2666e-04],
        [ 0.0000e+00, -9.1206e-05,  3.2360e-04,  ..., -3.3113e-04,
         -3.2784e-05,  2.2666e-04],
        [ 0.0000e+00, -9.1206e-05,  3.2360e-04,  ..., -3.3113e-04,
         -3.2784e-05,  2.2666e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(149.9037, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.5210, device='cuda:0')



h[100].sum tensor(32.3230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8244, device='cuda:0')



h[200].sum tensor(-5.8062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0000, 0.0013,  ..., 0.0000, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23300.4316, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.7164e-03, 0.0000e+00, 1.3278e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.5797e-03],
        [3.1264e-03, 0.0000e+00, 1.1876e-04,  ..., 4.5102e-04, 0.0000e+00,
         4.8364e-03],
        [4.8402e-03, 0.0000e+00, 6.6368e-05,  ..., 2.5849e-03, 0.0000e+00,
         1.4324e-02],
        ...,
        [2.7039e-03, 0.0000e+00, 1.2778e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.5606e-03],
        [2.7039e-03, 0.0000e+00, 1.2779e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.5606e-03],
        [2.7040e-03, 0.0000e+00, 1.2779e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.5606e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(124494.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1106.1437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(87.6221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1828.2954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-221.9647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-125.9539, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0158],
        [ 0.0214],
        [ 0.0310],
        ...,
        [-0.0141],
        [-0.0170],
        [-0.0179]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-976.6222, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [0.9997],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365945.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0003],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365958.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1311e-03,  1.3320e-02,  1.4261e-02,  ...,  2.1248e-04,
          6.5343e-03,  3.2776e-03],
        [ 0.0000e+00, -3.1587e-05,  4.0774e-04,  ..., -3.7875e-04,
          3.7312e-05,  1.9249e-04],
        [ 0.0000e+00, -3.1587e-05,  4.0774e-04,  ..., -3.7875e-04,
          3.7312e-05,  1.9249e-04],
        ...,
        [ 0.0000e+00, -3.1587e-05,  4.0774e-04,  ..., -3.7875e-04,
          3.7312e-05,  1.9249e-04],
        [ 0.0000e+00, -3.1587e-05,  4.0774e-04,  ..., -3.7875e-04,
          3.7312e-05,  1.9249e-04],
        [ 0.0000e+00, -3.1587e-05,  4.0774e-04,  ..., -3.7875e-04,
          3.7312e-05,  1.9249e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(208.1436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.8083, device='cuda:0')



h[100].sum tensor(46.5631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.0782, device='cuda:0')



h[200].sum tensor(-8.4109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0498, 0.0534,  ..., 0.0014, 0.0244, 0.0123],
        [0.0000, 0.0133, 0.0155,  ..., 0.0002, 0.0066, 0.0039],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0001, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0001, 0.0008],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0001, 0.0008],
        [0.0000, 0.0000, 0.0016,  ..., 0.0000, 0.0001, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34933.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[3.7627e-02, 0.0000e+00, 0.0000e+00,  ..., 4.3079e-02, 0.0000e+00,
         1.9436e-01],
        [1.8529e-02, 0.0000e+00, 0.0000e+00,  ..., 1.9048e-02, 0.0000e+00,
         8.9024e-02],
        [7.3913e-03, 0.0000e+00, 1.7412e-04,  ..., 5.1756e-03, 3.2055e-05,
         2.7028e-02],
        ...,
        [3.1095e-03, 0.0000e+00, 3.3706e-04,  ..., 0.0000e+00, 6.3120e-05,
         3.2420e-03],
        [3.1095e-03, 0.0000e+00, 3.3706e-04,  ..., 0.0000e+00, 6.3121e-05,
         3.2420e-03],
        [3.1095e-03, 0.0000e+00, 3.3706e-04,  ..., 0.0000e+00, 6.3121e-05,
         3.2420e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194118.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1564.3733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(135.7559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2636.0220, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-345.0485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.7743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0952],
        [ 0.0709],
        [ 0.0442],
        ...,
        [-0.0249],
        [-0.0248],
        [-0.0248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(3706.4365, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0003],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365958.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [0.9997],
        ...,
        [0.9998],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365968.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.4917e-06,  4.9606e-04,  ..., -4.2963e-04,
          4.2371e-05,  1.4004e-04],
        [ 0.0000e+00,  3.4917e-06,  4.9606e-04,  ..., -4.2963e-04,
          4.2371e-05,  1.4004e-04],
        [ 0.0000e+00,  3.4917e-06,  4.9606e-04,  ..., -4.2963e-04,
          4.2371e-05,  1.4004e-04],
        ...,
        [ 0.0000e+00,  3.4917e-06,  4.9606e-04,  ..., -4.2963e-04,
          4.2371e-05,  1.4004e-04],
        [ 0.0000e+00,  3.4917e-06,  4.9606e-04,  ..., -4.2963e-04,
          4.2371e-05,  1.4004e-04],
        [ 0.0000e+00,  3.4917e-06,  4.9606e-04,  ..., -4.2963e-04,
          4.2371e-05,  1.4004e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(157.9096, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.3642, device='cuda:0')



h[100].sum tensor(45.1322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.4246, device='cuda:0')



h[200].sum tensor(-7.4592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2687, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.3972e-05, 1.9849e-03,  ..., 0.0000e+00, 1.6955e-04,
         5.6038e-04],
        [0.0000e+00, 1.3972e-05, 1.9850e-03,  ..., 0.0000e+00, 1.6955e-04,
         5.6040e-04],
        [0.0000e+00, 1.3970e-05, 1.9846e-03,  ..., 0.0000e+00, 1.6952e-04,
         5.6029e-04],
        ...,
        [0.0000e+00, 1.3967e-05, 1.9842e-03,  ..., 0.0000e+00, 1.6948e-04,
         5.6017e-04],
        [0.0000e+00, 1.3967e-05, 1.9842e-03,  ..., 0.0000e+00, 1.6948e-04,
         5.6017e-04],
        [0.0000e+00, 1.3967e-05, 1.9842e-03,  ..., 0.0000e+00, 1.6948e-04,
         5.6017e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31591.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0101, 0.0000, 0.0003,  ..., 0.0078, 0.0000, 0.0353],
        [0.0055, 0.0000, 0.0005,  ..., 0.0021, 0.0000, 0.0095],
        [0.0043, 0.0000, 0.0009,  ..., 0.0006, 0.0000, 0.0027],
        ...,
        [0.0043, 0.0000, 0.0009,  ..., 0.0006, 0.0000, 0.0027],
        [0.0043, 0.0000, 0.0009,  ..., 0.0006, 0.0000, 0.0027],
        [0.0043, 0.0000, 0.0009,  ..., 0.0006, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173723.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1661.3259, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(92.1679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2286.0632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-272.9146, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.6947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0433],
        [ 0.0153],
        [-0.0061],
        ...,
        [-0.0416],
        [-0.0414],
        [-0.0414]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-11941.4668, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [0.9997],
        ...,
        [0.9998],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365968.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365979.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3083e-02,  2.4543e-02,  2.6066e-02,  ...,  5.9268e-04,
          1.1978e-02,  5.7818e-03],
        [ 0.0000e+00, -4.4480e-06,  5.7997e-04,  ..., -4.7475e-04,
          3.8638e-05,  8.9655e-05],
        [ 0.0000e+00, -4.4480e-06,  5.7997e-04,  ..., -4.7475e-04,
          3.8638e-05,  8.9655e-05],
        ...,
        [ 0.0000e+00, -4.4480e-06,  5.7997e-04,  ..., -4.7475e-04,
          3.8638e-05,  8.9655e-05],
        [ 0.0000e+00, -4.4480e-06,  5.7997e-04,  ..., -4.7475e-04,
          3.8638e-05,  8.9655e-05],
        [ 0.0000e+00, -4.4480e-06,  5.7997e-04,  ..., -4.7475e-04,
          3.8638e-05,  8.9655e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96.0225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1723, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.5794, device='cuda:0')



h[100].sum tensor(40.4338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8639, device='cuda:0')



h[200].sum tensor(-5.8172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0441, 0.0481,  ..., 0.0010, 0.0216, 0.0106],
        [0.0000, 0.0246, 0.0278,  ..., 0.0006, 0.0121, 0.0061],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0002, 0.0004],
        ...,
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0002, 0.0004],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0002, 0.0004],
        [0.0000, 0.0000, 0.0023,  ..., 0.0000, 0.0002, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26162.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0553, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.2695],
        [0.0303, 0.0000, 0.0000,  ..., 0.0312, 0.0000, 0.1359],
        [0.0134, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0463],
        ...,
        [0.0054, 0.0000, 0.0010,  ..., 0.0017, 0.0000, 0.0024],
        [0.0054, 0.0000, 0.0010,  ..., 0.0017, 0.0000, 0.0024],
        [0.0054, 0.0000, 0.0010,  ..., 0.0017, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(152441.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1689.5364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(44.7853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1976.5219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-176.7479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.8086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0525],
        [ 0.0480],
        [ 0.0492],
        ...,
        [-0.0573],
        [-0.0570],
        [-0.0569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-19727.5957, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365979.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365993.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0377e-06,  6.5951e-04,  ..., -5.1146e-04,
          4.8873e-05,  5.5641e-05],
        [ 0.0000e+00,  1.0377e-06,  6.5951e-04,  ..., -5.1146e-04,
          4.8873e-05,  5.5641e-05],
        [ 0.0000e+00,  1.0377e-06,  6.5951e-04,  ..., -5.1146e-04,
          4.8873e-05,  5.5641e-05],
        ...,
        [ 0.0000e+00,  1.0377e-06,  6.5951e-04,  ..., -5.1146e-04,
          4.8873e-05,  5.5641e-05],
        [ 0.0000e+00,  1.0377e-06,  6.5951e-04,  ..., -5.1146e-04,
          4.8873e-05,  5.5641e-05],
        [ 0.0000e+00,  1.0377e-06,  6.5951e-04,  ..., -5.1146e-04,
          4.8873e-05,  5.5641e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55.1846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0313, device='cuda:0')



h[100].sum tensor(37.6992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.1400, device='cuda:0')



h[200].sum tensor(-4.7288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.4173, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.4829e-02, 3.8802e-02,  ..., 4.7950e-04, 1.7128e-02,
         8.3122e-03],
        [0.0000e+00, 1.7418e-02, 2.0723e-02,  ..., 2.3977e-04, 8.6630e-03,
         4.2679e-03],
        [0.0000e+00, 3.0304e-02, 3.4104e-02,  ..., 7.9628e-04, 1.4929e-02,
         7.2612e-03],
        ...,
        [0.0000e+00, 4.1512e-06, 2.6383e-03,  ..., 0.0000e+00, 1.9551e-04,
         2.2259e-04],
        [0.0000e+00, 4.1512e-06, 2.6383e-03,  ..., 0.0000e+00, 1.9551e-04,
         2.2259e-04],
        [0.0000e+00, 4.1512e-06, 2.6383e-03,  ..., 0.0000e+00, 1.9551e-04,
         2.2259e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22875.3418, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0281, 0.0000, 0.0000,  ..., 0.0282, 0.0000, 0.1239],
        [0.0260, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.1119],
        [0.0308, 0.0000, 0.0000,  ..., 0.0310, 0.0000, 0.1358],
        ...,
        [0.0059, 0.0000, 0.0005,  ..., 0.0023, 0.0000, 0.0024],
        [0.0059, 0.0000, 0.0005,  ..., 0.0023, 0.0000, 0.0024],
        [0.0059, 0.0000, 0.0005,  ..., 0.0023, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(141117.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1685.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(13.9727, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1782.8110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-118.9366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.7458, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0317],
        [ 0.0362],
        [ 0.0396],
        ...,
        [-0.0700],
        [-0.0697],
        [-0.0696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-29337.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365993.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [0.9997],
        ...,
        [0.9998],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366007.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -8.4824e-06,  7.3394e-04,  ..., -5.4088e-04,
          7.3949e-05,  3.9433e-05],
        [-5.1980e-03,  9.7674e-03,  1.0887e-02,  ..., -1.2163e-04,
          4.8262e-03,  2.3143e-03],
        [ 0.0000e+00, -8.4824e-06,  7.3394e-04,  ..., -5.4088e-04,
          7.3949e-05,  3.9433e-05],
        ...,
        [ 0.0000e+00, -8.4824e-06,  7.3394e-04,  ..., -5.4088e-04,
          7.3949e-05,  3.9433e-05],
        [ 0.0000e+00, -8.4824e-06,  7.3394e-04,  ..., -5.4088e-04,
          7.3949e-05,  3.9433e-05],
        [ 0.0000e+00, -8.4824e-06,  7.3394e-04,  ..., -5.4088e-04,
          7.3949e-05,  3.9433e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34.2187, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.8018, device='cuda:0')



h[100].sum tensor(37.0615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.3081, device='cuda:0')



h[200].sum tensor(-4.2724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(4.8637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 9.7721e-03, 1.3095e-02,  ..., 0.0000e+00, 5.0505e-03,
         2.4338e-03],
        [0.0000e+00, 1.3826e-02, 1.7306e-02,  ..., 5.2100e-05, 7.0211e-03,
         3.3771e-03],
        [0.0000e+00, 4.7192e-02, 5.1985e-02,  ..., 1.0420e-04, 2.3253e-02,
         1.1148e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 2.9363e-03,  ..., 0.0000e+00, 2.9585e-04,
         1.5776e-04],
        [0.0000e+00, 0.0000e+00, 2.9362e-03,  ..., 0.0000e+00, 2.9585e-04,
         1.5776e-04],
        [0.0000e+00, 0.0000e+00, 2.9363e-03,  ..., 0.0000e+00, 2.9585e-04,
         1.5776e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22311.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0145, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0528],
        [0.0187, 0.0000, 0.0000,  ..., 0.0172, 0.0000, 0.0772],
        [0.0258, 0.0000, 0.0000,  ..., 0.0254, 0.0000, 0.1183],
        ...,
        [0.0059, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0026],
        [0.0059, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0026],
        [0.0059, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(144466.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1639.2150, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(0.2487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1901.3296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-102.2139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.2070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0515],
        [ 0.0707],
        [ 0.0859],
        ...,
        [-0.0795],
        [-0.0792],
        [-0.0788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-30861.8887, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [0.9997],
        ...,
        [0.9998],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366007.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366023.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.7187e-06,  8.0410e-04,  ..., -5.6325e-04,
          1.1386e-04,  5.0435e-05],
        [-1.7156e-02,  3.2302e-02,  3.4359e-02,  ...,  8.1530e-04,
          1.5815e-02,  7.5823e-03],
        [-1.7380e-02,  3.2724e-02,  3.4796e-02,  ...,  8.3329e-04,
          1.6020e-02,  7.6806e-03],
        ...,
        [ 0.0000e+00, -2.7187e-06,  8.0410e-04,  ..., -5.6325e-04,
          1.1386e-04,  5.0435e-05],
        [ 0.0000e+00, -2.7187e-06,  8.0410e-04,  ..., -5.6325e-04,
          1.1386e-04,  5.0435e-05],
        [ 0.0000e+00, -2.7187e-06,  8.0410e-04,  ..., -5.6325e-04,
          1.1386e-04,  5.0435e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79.0886, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5648, device='cuda:0')



h[100].sum tensor(46.2601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5306, device='cuda:0')



h[200].sum tensor(-6.1523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0083, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0425, 0.0474,  ..., 0.0008, 0.0211, 0.0101],
        [0.0000, 0.0591, 0.0646,  ..., 0.0010, 0.0292, 0.0140],
        [0.0000, 0.1238, 0.1318,  ..., 0.0030, 0.0606, 0.0291],
        ...,
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0005, 0.0002],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0005, 0.0002],
        [0.0000, 0.0000, 0.0032,  ..., 0.0000, 0.0005, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28723.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.1863],
        [0.0585, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.2897],
        [0.0867, 0.0000, 0.0000,  ..., 0.0903, 0.0000, 0.4378],
        ...,
        [0.0055, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0035],
        [0.0055, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0035],
        [0.0055, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175937., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1672.1759, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(25.4517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2456.2134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-182.6851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.6844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1226],
        [ 0.1199],
        [ 0.1181],
        ...,
        [-0.0848],
        [-0.0844],
        [-0.0843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-28141.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366023.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366023.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0634e-03,  1.1415e-02,  1.2663e-02,  ..., -7.6030e-05,
          5.6631e-03,  2.7124e-03],
        [-7.3414e-03,  1.3821e-02,  1.5163e-02,  ...,  2.6661e-05,
          6.8327e-03,  3.2735e-03],
        [-1.2080e-02,  2.2744e-02,  2.4431e-02,  ...,  4.0744e-04,
          1.1170e-02,  5.3539e-03],
        ...,
        [ 0.0000e+00, -2.7187e-06,  8.0410e-04,  ..., -5.6325e-04,
          1.1386e-04,  5.0435e-05],
        [ 0.0000e+00, -2.7187e-06,  8.0410e-04,  ..., -5.6325e-04,
          1.1386e-04,  5.0435e-05],
        [ 0.0000e+00, -2.7187e-06,  8.0410e-04,  ..., -5.6325e-04,
          1.1386e-04,  5.0435e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(144.5297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.8747, device='cuda:0')



h[100].sum tensor(57.8442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.7997, device='cuda:0')



h[200].sum tensor(-8.6877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8494, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.9566e-02, 6.5100e-02,  ..., 3.6974e-04, 2.9412e-02,
         1.4092e-02],
        [0.0000e+00, 6.4129e-02, 6.9840e-02,  ..., 7.2416e-04, 3.1630e-02,
         1.5156e-02],
        [0.0000e+00, 4.4388e-02, 4.9334e-02,  ..., 2.6681e-05, 2.2035e-02,
         1.0553e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 3.2172e-03,  ..., 0.0000e+00, 4.5557e-04,
         2.0179e-04],
        [0.0000e+00, 0.0000e+00, 3.2172e-03,  ..., 0.0000e+00, 4.5556e-04,
         2.0179e-04],
        [0.0000e+00, 0.0000e+00, 3.2172e-03,  ..., 0.0000e+00, 4.5557e-04,
         2.0179e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36233.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0334, 0.0000, 0.0000,  ..., 0.0326, 0.0000, 0.1609],
        [0.0415, 0.0000, 0.0000,  ..., 0.0415, 0.0000, 0.2054],
        [0.0499, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.2513],
        ...,
        [0.0055, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0035],
        [0.0055, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0035],
        [0.0055, 0.0000, 0.0000,  ..., 0.0017, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(204193.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1883.8456, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(58.9744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2867.1753, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-275.8855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.7917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0894],
        [ 0.0989],
        [ 0.1069],
        ...,
        [-0.0848],
        [-0.0844],
        [-0.0843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-27385.8379, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [0.9997],
        ...,
        [0.9999],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366023.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0008],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366038.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.1191e-06,  8.7260e-04,  ..., -5.8064e-04,
          1.5567e-04,  6.3686e-05],
        [ 0.0000e+00,  8.1191e-06,  8.7260e-04,  ..., -5.8064e-04,
          1.5567e-04,  6.3686e-05],
        [ 0.0000e+00,  8.1191e-06,  8.7260e-04,  ..., -5.8064e-04,
          1.5567e-04,  6.3686e-05],
        ...,
        [ 0.0000e+00,  8.1191e-06,  8.7260e-04,  ..., -5.8064e-04,
          1.5567e-04,  6.3686e-05],
        [ 0.0000e+00,  8.1191e-06,  8.7260e-04,  ..., -5.8064e-04,
          1.5567e-04,  6.3686e-05],
        [ 0.0000e+00,  8.1191e-06,  8.7260e-04,  ..., -5.8064e-04,
          1.5567e-04,  6.3686e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(232.3612, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-31.3658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.4711, device='cuda:0')



h[100].sum tensor(73.1676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-20.6157, device='cuda:0')



h[200].sum tensor(-11.9258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.7200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.2496e-05, 3.4925e-03,  ..., 0.0000e+00, 6.2304e-04,
         2.5490e-04],
        [0.0000e+00, 3.2502e-05, 3.4931e-03,  ..., 0.0000e+00, 6.2315e-04,
         2.5494e-04],
        [0.0000e+00, 1.5867e-02, 1.9941e-02,  ..., 9.2434e-05, 8.3181e-03,
         3.9534e-03],
        ...,
        [0.0000e+00, 3.2488e-05, 3.4916e-03,  ..., 0.0000e+00, 6.2288e-04,
         2.5483e-04],
        [0.0000e+00, 3.2487e-05, 3.4915e-03,  ..., 0.0000e+00, 6.2288e-04,
         2.5483e-04],
        [0.0000e+00, 3.2488e-05, 3.4916e-03,  ..., 0.0000e+00, 6.2288e-04,
         2.5483e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47279.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0056, 0.0000, 0.0000,  ..., 0.0016, 0.0000, 0.0076],
        [0.0079, 0.0000, 0.0000,  ..., 0.0042, 0.0000, 0.0212],
        [0.0146, 0.0000, 0.0000,  ..., 0.0114, 0.0000, 0.0598],
        ...,
        [0.0049, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0042],
        [0.0049, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0042],
        [0.0049, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(262375.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2088.6909, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(104.9097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3813.5737, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-412.6832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-317.1761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0316],
        [ 0.0144],
        [ 0.0618],
        ...,
        [-0.0881],
        [-0.0878],
        [-0.0877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-23079.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0008],
        [0.9998],
        ...,
        [0.9999],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366038.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0009],
        [1.0009],
        [0.9998],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366051.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.7195e-02,  3.2480e-02,  3.4640e-02,  ...,  7.8350e-04,
          1.5931e-02,  7.6644e-03],
        [-2.5000e-02,  4.7212e-02,  4.9944e-02,  ...,  1.4081e-03,
          2.3087e-02,  1.1110e-02],
        [-1.7871e-02,  3.3756e-02,  3.5966e-02,  ...,  8.3761e-04,
          1.6551e-02,  7.9629e-03],
        ...,
        [ 0.0000e+00,  1.9634e-05,  9.2346e-04,  ..., -5.9273e-04,
          1.6422e-04,  7.2825e-05],
        [ 0.0000e+00,  1.9634e-05,  9.2346e-04,  ..., -5.9273e-04,
          1.6422e-04,  7.2825e-05],
        [ 0.0000e+00,  1.9634e-05,  9.2346e-04,  ..., -5.9273e-04,
          1.6422e-04,  7.2825e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(156.4764, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.8845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.2875, device='cuda:0')



h[100].sum tensor(58.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.0790, device='cuda:0')



h[200].sum tensor(-8.6828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4563e-01, 1.5488e-01,  ..., 3.7985e-03, 7.1356e-02,
         3.4332e-02],
        [0.0000e+00, 1.7010e-01, 1.8030e-01,  ..., 4.8354e-03, 8.3241e-02,
         4.0054e-02],
        [0.0000e+00, 1.4100e-01, 1.5007e-01,  ..., 3.6019e-03, 6.9106e-02,
         3.3248e-02],
        ...,
        [0.0000e+00, 7.8568e-05, 3.6954e-03,  ..., 0.0000e+00, 6.5716e-04,
         2.9143e-04],
        [0.0000e+00, 7.8567e-05, 3.6954e-03,  ..., 0.0000e+00, 6.5714e-04,
         2.9142e-04],
        [0.0000e+00, 7.8568e-05, 3.6954e-03,  ..., 0.0000e+00, 6.5715e-04,
         2.9143e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38318.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0934, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.4697],
        [0.1131, 0.0000, 0.0000,  ..., 0.1131, 0.0000, 0.5710],
        [0.1090, 0.0000, 0.0000,  ..., 0.1089, 0.0000, 0.5516],
        ...,
        [0.0047, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0044],
        [0.0047, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0044],
        [0.0047, 0.0000, 0.0000,  ..., 0.0011, 0.0000, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(227146.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1757.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(58.3699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3351.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.2841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.1381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0726],
        [ 0.0734],
        [ 0.0749],
        ...,
        [-0.0943],
        [-0.0939],
        [-0.0938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-29619.2598, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0009],
        [1.0009],
        [0.9998],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366051.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0010],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366064.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.8119e-05,  9.6306e-04,  ..., -6.0137e-04,
          1.6125e-04,  8.3632e-05],
        [ 0.0000e+00,  2.8119e-05,  9.6306e-04,  ..., -6.0137e-04,
          1.6125e-04,  8.3632e-05],
        [ 0.0000e+00,  2.8119e-05,  9.6306e-04,  ..., -6.0137e-04,
          1.6125e-04,  8.3632e-05],
        ...,
        [ 0.0000e+00,  2.8119e-05,  9.6306e-04,  ..., -6.0137e-04,
          1.6125e-04,  8.3632e-05],
        [ 0.0000e+00,  2.8119e-05,  9.6306e-04,  ..., -6.0137e-04,
          1.6125e-04,  8.3632e-05],
        [ 0.0000e+00,  2.8119e-05,  9.6306e-04,  ..., -6.0137e-04,
          1.6125e-04,  8.3632e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(160.1037, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.0376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.1369, device='cuda:0')



h[100].sum tensor(57.0252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.3005, device='cuda:0')



h[200].sum tensor(-8.3438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0039,  ..., 0.0000, 0.0006, 0.0003],
        [0.0000, 0.0001, 0.0039,  ..., 0.0000, 0.0006, 0.0003],
        [0.0000, 0.0001, 0.0039,  ..., 0.0000, 0.0006, 0.0003],
        ...,
        [0.0000, 0.0001, 0.0039,  ..., 0.0000, 0.0006, 0.0003],
        [0.0000, 0.0001, 0.0039,  ..., 0.0000, 0.0006, 0.0003],
        [0.0000, 0.0001, 0.0039,  ..., 0.0000, 0.0006, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38338.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0047, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0045],
        [0.0047, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0045],
        [0.0047, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0046],
        ...,
        [0.0047, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0045],
        [0.0047, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0045],
        [0.0047, 0.0000, 0.0000,  ..., 0.0015, 0.0000, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(237919.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1791.1038, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(51.0398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3519.6416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.9671, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-307.6012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1229],
        [-0.1124],
        [-0.0887],
        ...,
        [-0.0761],
        [-0.0938],
        [-0.0986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33215.4766, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0010],
        [0.9998],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366064.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0011],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366077.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.5351e-03,  1.4289e-02,  1.5809e-02,  ..., -6.7285e-06,
          7.0769e-03,  3.4315e-03],
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05],
        [-1.2820e-02,  2.4291e-02,  2.6196e-02,  ...,  4.1589e-04,
          1.1930e-02,  5.7756e-03],
        ...,
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05],
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05],
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(148.5134, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.1790, device='cuda:0')



h[100].sum tensor(53.0510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.9759, device='cuda:0')



h[200].sum tensor(-7.4576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.1765e-02, 1.6101e-02,  ..., 0.0000e+00, 6.2831e-03,
         3.0881e-03],
        [0.0000e+00, 6.1985e-02, 6.8255e-02,  ..., 4.1629e-04, 3.0652e-02,
         1.4858e-02],
        [0.0000e+00, 3.4696e-02, 3.9916e-02,  ..., 1.5794e-05, 1.7410e-02,
         8.4627e-03],
        ...,
        [0.0000e+00, 1.1266e-04, 4.0009e-03,  ..., 0.0000e+00, 6.2928e-04,
         3.5727e-04],
        [0.0000e+00, 1.1266e-04, 4.0008e-03,  ..., 0.0000e+00, 6.2928e-04,
         3.5726e-04],
        [0.0000e+00, 1.1266e-04, 4.0009e-03,  ..., 0.0000e+00, 6.2928e-04,
         3.5727e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36254.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0178, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0757],
        [0.0313, 0.0000, 0.0000,  ..., 0.0282, 0.0000, 0.1500],
        [0.0313, 0.0000, 0.0000,  ..., 0.0281, 0.0000, 0.1532],
        ...,
        [0.0049, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0045],
        [0.0049, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0045],
        [0.0049, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230472.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1725.2241, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.8260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3419.9111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.6117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-319.5767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0317],
        [ 0.0607],
        [ 0.0793],
        ...,
        [-0.1079],
        [-0.1075],
        [-0.1074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-32328.2461, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0011],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366077.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0010],
        [1.0011],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366077.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6721e-03,  1.2656e-02,  1.4113e-02,  ..., -7.5743e-05,
          6.2844e-03,  3.0487e-03],
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05],
        [-6.6721e-03,  1.2656e-02,  1.4113e-02,  ..., -7.5743e-05,
          6.2844e-03,  3.0487e-03],
        ...,
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05],
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05],
        [ 0.0000e+00,  2.8147e-05,  9.9962e-04,  ..., -6.0930e-04,
          1.5723e-04,  8.9262e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(100.6358, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7869, device='cuda:0')



h[100].sum tensor(45.1043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.0043, device='cuda:0')



h[200].sum tensor(-5.7418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0104, 0.0147,  ..., 0.0000, 0.0056, 0.0028],
        [0.0000, 0.0460, 0.0517,  ..., 0.0000, 0.0229, 0.0111],
        [0.0000, 0.0104, 0.0147,  ..., 0.0000, 0.0056, 0.0028],
        ...,
        [0.0000, 0.0001, 0.0040,  ..., 0.0000, 0.0006, 0.0004],
        [0.0000, 0.0001, 0.0040,  ..., 0.0000, 0.0006, 0.0004],
        [0.0000, 0.0001, 0.0040,  ..., 0.0000, 0.0006, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29216.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0134, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0524],
        [0.0195, 0.0000, 0.0000,  ..., 0.0167, 0.0000, 0.0869],
        [0.0134, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0524],
        ...,
        [0.0049, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0045],
        [0.0049, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0045],
        [0.0049, 0.0000, 0.0000,  ..., 0.0023, 0.0000, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(195483.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1507.8959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1.3954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2816.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-180.9846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-282.4562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0761],
        [-0.0605],
        [-0.0617],
        ...,
        [-0.1079],
        [-0.1075],
        [-0.1074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-42330.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0010],
        [1.0011],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366077.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0011],
        [1.0012],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366090., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4448e-02,  2.7404e-02,  2.9464e-02,  ...,  5.3865e-04,
          1.3437e-02,  6.5192e-03],
        [-1.3358e-02,  2.5338e-02,  2.7318e-02,  ...,  4.5146e-04,
          1.2434e-02,  6.0344e-03],
        [-2.0577e-02,  3.9019e-02,  4.1524e-02,  ...,  1.0287e-03,
          1.9070e-02,  9.2439e-03],
        ...,
        [ 0.0000e+00,  2.4579e-05,  1.0349e-03,  ..., -6.1654e-04,
          1.5736e-04,  9.6290e-05],
        [ 0.0000e+00,  2.4579e-05,  1.0349e-03,  ..., -6.1654e-04,
          1.5736e-04,  9.6290e-05],
        [ 0.0000e+00,  2.4579e-05,  1.0349e-03,  ..., -6.1654e-04,
          1.5736e-04,  9.6290e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(143.9734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.7709, device='cuda:0')



h[100].sum tensor(50.3153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.0232, device='cuda:0')



h[200].sum tensor(-6.8503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.0925e-01, 1.1747e-01,  ..., 2.3794e-03, 5.3567e-02,
         2.5990e-02],
        [0.0000e+00, 1.4167e-01, 1.5114e-01,  ..., 3.5044e-03, 6.9294e-02,
         3.3597e-02],
        [0.0000e+00, 1.7083e-01, 1.8141e-01,  ..., 4.7347e-03, 8.3433e-02,
         4.0436e-02],
        ...,
        [0.0000e+00, 9.8386e-05, 4.1424e-03,  ..., 0.0000e+00, 6.2989e-04,
         3.8543e-04],
        [0.0000e+00, 9.8385e-05, 4.1423e-03,  ..., 0.0000e+00, 6.2988e-04,
         3.8543e-04],
        [0.0000e+00, 9.8386e-05, 4.1424e-03,  ..., 0.0000e+00, 6.2989e-04,
         3.8543e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32398.4277, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0919, 0.0000, 0.0000,  ..., 0.0862, 0.0000, 0.4374],
        [0.1102, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.5240],
        [0.1311, 0.0000, 0.0000,  ..., 0.1239, 0.0000, 0.6228],
        ...,
        [0.0051, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0045],
        [0.0051, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0045],
        [0.0051, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209402.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1631.6083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(8.5701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2988.5151, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-215.3908, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-306.4407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0575],
        [-0.0698],
        [-0.0801],
        ...,
        [-0.1157],
        [-0.1153],
        [-0.1151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44879.5078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0011],
        [1.0012],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366090., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366103.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.0812e-03,  9.6616e-03,  1.1083e-02,  ..., -2.1560e-04,
          4.8420e-03,  2.3803e-03],
        [-4.7480e-03,  9.0293e-03,  1.0427e-02,  ..., -2.4225e-04,
          4.5355e-03,  2.2318e-03],
        [ 0.0000e+00,  1.9806e-05,  1.0732e-03,  ..., -6.2196e-04,
          1.6753e-04,  1.1603e-04],
        ...,
        [ 0.0000e+00,  1.9806e-05,  1.0732e-03,  ..., -6.2196e-04,
          1.6753e-04,  1.1603e-04],
        [ 0.0000e+00,  1.9806e-05,  1.0732e-03,  ..., -6.2196e-04,
          1.6753e-04,  1.1603e-04],
        [ 0.0000e+00,  1.9806e-05,  1.0732e-03,  ..., -6.2196e-04,
          1.6753e-04,  1.1603e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(268.5758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-28.6220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.7498, device='cuda:0')



h[100].sum tensor(68.4694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.7746, device='cuda:0')



h[200].sum tensor(-10.7676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 7.4752e-02, 8.1821e-02,  ..., 1.2119e-03, 3.6873e-02,
         1.8000e-02],
        [0.0000e+00, 1.7097e-02, 2.1965e-02,  ..., 0.0000e+00, 8.9213e-03,
         4.4610e-03],
        [0.0000e+00, 3.6952e-02, 4.2578e-02,  ..., 2.2354e-04, 1.8547e-02,
         9.1236e-03],
        ...,
        [0.0000e+00, 7.9289e-05, 4.2964e-03,  ..., 0.0000e+00, 6.7064e-04,
         4.6449e-04],
        [0.0000e+00, 7.9288e-05, 4.2963e-03,  ..., 0.0000e+00, 6.7063e-04,
         4.6448e-04],
        [0.0000e+00, 7.9289e-05, 4.2964e-03,  ..., 0.0000e+00, 6.7064e-04,
         4.6449e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49004.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0526, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.2472],
        [0.0320, 0.0000, 0.0000,  ..., 0.0283, 0.0000, 0.1474],
        [0.0347, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.1618],
        ...,
        [0.0052, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0048],
        [0.0052, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0048],
        [0.0052, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(303528.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2326.4658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(81.8786, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4422.4639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-416.5293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.6522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0119],
        [ 0.0294],
        [ 0.0390],
        ...,
        [-0.1094],
        [-0.1088],
        [-0.1108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-40611.9258, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [0.9999],
        ...,
        [0.9999],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366103.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0000],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366117.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.1719e-05,  1.1126e-03,  ..., -6.2393e-04,
          1.8238e-04,  1.3923e-04],
        [ 0.0000e+00,  1.1719e-05,  1.1126e-03,  ..., -6.2393e-04,
          1.8238e-04,  1.3923e-04],
        [ 0.0000e+00,  1.1719e-05,  1.1126e-03,  ..., -6.2393e-04,
          1.8238e-04,  1.3923e-04],
        ...,
        [ 0.0000e+00,  1.1719e-05,  1.1126e-03,  ..., -6.2393e-04,
          1.8238e-04,  1.3923e-04],
        [ 0.0000e+00,  1.1719e-05,  1.1126e-03,  ..., -6.2393e-04,
          1.8238e-04,  1.3923e-04],
        [ 0.0000e+00,  1.1719e-05,  1.1126e-03,  ..., -6.2393e-04,
          1.8238e-04,  1.3923e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(201.7579, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.8908, device='cuda:0')



h[100].sum tensor(55.8629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.1340, device='cuda:0')



h[200].sum tensor(-8.0785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.6915e-05, 4.4542e-03,  ..., 0.0000e+00, 7.3012e-04,
         5.5736e-04],
        [0.0000e+00, 4.6932e-05, 4.4558e-03,  ..., 0.0000e+00, 7.3039e-04,
         5.5756e-04],
        [0.0000e+00, 4.6928e-05, 4.4554e-03,  ..., 0.0000e+00, 7.3033e-04,
         5.5752e-04],
        ...,
        [0.0000e+00, 4.6919e-05, 4.4546e-03,  ..., 0.0000e+00, 7.3018e-04,
         5.5741e-04],
        [0.0000e+00, 4.6918e-05, 4.4545e-03,  ..., 0.0000e+00, 7.3017e-04,
         5.5740e-04],
        [0.0000e+00, 4.6919e-05, 4.4546e-03,  ..., 0.0000e+00, 7.3018e-04,
         5.5741e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38236.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0054, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0052],
        [0.0054, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0052],
        [0.0054, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0052],
        ...,
        [0.0054, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0051],
        [0.0054, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0051],
        [0.0054, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0051]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(249804.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1910.1528, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.8615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3637.1948, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-282.1330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.3122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1786],
        [-0.1811],
        [-0.1793],
        ...,
        [-0.1262],
        [-0.1230],
        [-0.1169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-37193.5820, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0012],
        [1.0013],
        [1.0000],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366117.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0000],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366131.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3326e-03,  1.2050e-02,  1.3656e-02,  ..., -1.1839e-04,
          6.0394e-03,  3.0028e-03],
        [-6.6372e-03,  1.2629e-02,  1.4257e-02,  ..., -9.3971e-05,
          6.3202e-03,  3.1392e-03],
        [-6.3326e-03,  1.2050e-02,  1.3656e-02,  ..., -1.1839e-04,
          6.0394e-03,  3.0028e-03],
        ...,
        [ 0.0000e+00,  9.6111e-07,  1.1508e-03,  ..., -6.2595e-04,
          2.0163e-04,  1.6735e-04],
        [ 0.0000e+00,  9.6111e-07,  1.1508e-03,  ..., -6.2595e-04,
          2.0163e-04,  1.6735e-04],
        [ 0.0000e+00,  9.6111e-07,  1.1508e-03,  ..., -6.2595e-04,
          2.0163e-04,  1.6735e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(255.1323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.1765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6550, device='cuda:0')



h[100].sum tensor(62.1132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.6808, device='cuda:0')



h[200].sum tensor(-9.4308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.5767e-02, 6.2481e-02,  ..., 2.2301e-04, 2.7825e-02,
         1.3793e-02],
        [0.0000e+00, 5.4155e-02, 6.0810e-02,  ..., 2.2312e-04, 2.7045e-02,
         1.3414e-02],
        [0.0000e+00, 2.2501e-02, 2.7957e-02,  ..., 0.0000e+00, 1.1708e-02,
         5.9644e-03],
        ...,
        [0.0000e+00, 3.8483e-06, 4.6080e-03,  ..., 0.0000e+00, 8.0735e-04,
         6.7007e-04],
        [0.0000e+00, 3.8483e-06, 4.6079e-03,  ..., 0.0000e+00, 8.0734e-04,
         6.7006e-04],
        [0.0000e+00, 3.8484e-06, 4.6080e-03,  ..., 0.0000e+00, 8.0736e-04,
         6.7007e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44726.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.1425],
        [0.0305, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.1440],
        [0.0244, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.1128],
        ...,
        [0.0056, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0056],
        [0.0056, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0056],
        [0.0056, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287361.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2226.2002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.9062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4174.2417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-365.5171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.3881, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0628],
        [ 0.0720],
        [ 0.0782],
        ...,
        [-0.1269],
        [-0.1258],
        [-0.1262]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-40841.2422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0000],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366131.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(1014.3227, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0000],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366144.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.7847e-03,  9.1027e-03,  1.0646e-02,  ..., -2.4108e-04,
          4.6396e-03,  2.3419e-03],
        [ 0.0000e+00, -1.3414e-05,  1.1866e-03,  ..., -6.2528e-04,
          2.2361e-04,  1.9445e-04],
        [ 0.0000e+00, -1.3414e-05,  1.1866e-03,  ..., -6.2528e-04,
          2.2361e-04,  1.9445e-04],
        ...,
        [ 0.0000e+00, -1.3414e-05,  1.1866e-03,  ..., -6.2528e-04,
          2.2361e-04,  1.9445e-04],
        [ 0.0000e+00, -1.3414e-05,  1.1866e-03,  ..., -6.2528e-04,
          2.2361e-04,  1.9445e-04],
        [ 0.0000e+00, -1.3414e-05,  1.1866e-03,  ..., -6.2528e-04,
          2.2361e-04,  1.9445e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(179.0487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.1398, device='cuda:0')



h[100].sum tensor(48.4795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5962, device='cuda:0')



h[200].sum tensor(-6.4879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7174, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0351, 0.0412,  ..., 0.0005, 0.0179, 0.0091],
        [0.0000, 0.0091, 0.0142,  ..., 0.0000, 0.0053, 0.0029],
        [0.0000, 0.0000, 0.0048,  ..., 0.0000, 0.0009, 0.0008],
        ...,
        [0.0000, 0.0000, 0.0048,  ..., 0.0000, 0.0009, 0.0008],
        [0.0000, 0.0000, 0.0048,  ..., 0.0000, 0.0009, 0.0008],
        [0.0000, 0.0000, 0.0048,  ..., 0.0000, 0.0009, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33073.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0375, 0.0000, 0.0000,  ..., 0.0318, 0.0000, 0.1713],
        [0.0214, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0891],
        [0.0108, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0326],
        ...,
        [0.0058, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0060],
        [0.0058, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0060],
        [0.0058, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(227898.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1812.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(9.1267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3282.7822, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-224.1543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.9534, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.6423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0277],
        [ 0.0181],
        [-0.0088],
        ...,
        [-0.1338],
        [-0.1333],
        [-0.1331]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-44607.3789, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0014],
        [1.0000],
        ...,
        [1.0000],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366144.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0000],
        ...,
        [1.0001],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366158.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.0954e-05,  1.2229e-03,  ..., -6.2524e-04,
          2.4826e-04,  2.2260e-04],
        [ 0.0000e+00, -2.0954e-05,  1.2229e-03,  ..., -6.2524e-04,
          2.4826e-04,  2.2260e-04],
        [ 0.0000e+00, -2.0954e-05,  1.2229e-03,  ..., -6.2524e-04,
          2.4826e-04,  2.2260e-04],
        ...,
        [ 0.0000e+00, -2.0954e-05,  1.2229e-03,  ..., -6.2524e-04,
          2.4826e-04,  2.2260e-04],
        [ 0.0000e+00, -2.0954e-05,  1.2229e-03,  ..., -6.2524e-04,
          2.4826e-04,  2.2260e-04],
        [ 0.0000e+00, -2.0954e-05,  1.2229e-03,  ..., -6.2524e-04,
          2.4826e-04,  2.2260e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(163.5541, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6316, device='cuda:0')



h[100].sum tensor(44.3489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8993, device='cuda:0')



h[200].sum tensor(-5.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0049,  ..., 0.0000, 0.0010, 0.0009],
        [0.0000, 0.0000, 0.0049,  ..., 0.0000, 0.0010, 0.0009],
        [0.0000, 0.0219, 0.0277,  ..., 0.0003, 0.0116, 0.0061],
        ...,
        [0.0000, 0.0000, 0.0049,  ..., 0.0000, 0.0010, 0.0009],
        [0.0000, 0.0000, 0.0049,  ..., 0.0000, 0.0010, 0.0009],
        [0.0000, 0.0000, 0.0049,  ..., 0.0000, 0.0010, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29488.9980, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0073, 0.0000, 0.0000,  ..., 0.0044, 0.0000, 0.0144],
        [0.0110, 0.0000, 0.0000,  ..., 0.0077, 0.0000, 0.0323],
        [0.0220, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0874],
        ...,
        [0.0060, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0064],
        [0.0060, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0064],
        [0.0060, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(210473.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1747.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.5448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3003.3979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-183.4035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.3589, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-337.7341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0341],
        [ 0.0263],
        [ 0.0191],
        ...,
        [-0.1356],
        [-0.1350],
        [-0.1349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-50854.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0013],
        [1.0015],
        [1.0000],
        ...,
        [1.0001],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366158.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0000],
        ...,
        [1.0001],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366172.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.4871e-05,  1.2640e-03,  ..., -6.2609e-04,
          2.7232e-04,  2.4644e-04],
        [ 0.0000e+00, -2.4871e-05,  1.2640e-03,  ..., -6.2609e-04,
          2.7232e-04,  2.4644e-04],
        [ 0.0000e+00, -2.4871e-05,  1.2640e-03,  ..., -6.2609e-04,
          2.7232e-04,  2.4644e-04],
        ...,
        [ 0.0000e+00, -2.4871e-05,  1.2640e-03,  ..., -6.2609e-04,
          2.7232e-04,  2.4644e-04],
        [ 0.0000e+00, -2.4871e-05,  1.2640e-03,  ..., -6.2609e-04,
          2.7232e-04,  2.4644e-04],
        [ 0.0000e+00, -2.4871e-05,  1.2640e-03,  ..., -6.2609e-04,
          2.7232e-04,  2.4644e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(256.8394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.7835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2668, device='cuda:0')



h[100].sum tensor(56.9330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.3884, device='cuda:0')



h[200].sum tensor(-8.1069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0011, 0.0010],
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0011, 0.0010],
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0011, 0.0010],
        ...,
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0011, 0.0010],
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0011, 0.0010],
        [0.0000, 0.0000, 0.0051,  ..., 0.0000, 0.0011, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40736.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0137, 0.0000, 0.0000,  ..., 0.0098, 0.0000, 0.0468],
        [0.0083, 0.0000, 0.0000,  ..., 0.0051, 0.0000, 0.0175],
        [0.0067, 0.0000, 0.0000,  ..., 0.0036, 0.0000, 0.0091],
        ...,
        [0.0063, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0068],
        [0.0063, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0068],
        [0.0063, 0.0000, 0.0000,  ..., 0.0034, 0.0000, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(273399.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2163.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(47.5957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4114.5820, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-319.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.2516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.9935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0015],
        [-0.0096],
        [ 0.0032],
        ...,
        [-0.1383],
        [-0.1377],
        [-0.1375]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-33653.2070, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0000],
        ...,
        [1.0001],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366172.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366185.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.1012e-03,  7.8022e-03,  9.4533e-03,  ..., -2.9756e-04,
          4.0712e-03,  2.0867e-03],
        [-5.2203e-03,  9.9433e-03,  1.1674e-02,  ..., -2.0730e-04,
          5.1080e-03,  2.5923e-03],
        [ 0.0000e+00, -4.3860e-05,  1.3166e-03,  ..., -6.2830e-04,
          2.7170e-04,  2.3387e-04],
        ...,
        [-6.7329e-03,  1.2837e-02,  1.4674e-02,  ..., -8.5325e-05,
          6.5093e-03,  3.2757e-03],
        [ 0.0000e+00, -4.3860e-05,  1.3166e-03,  ..., -6.2830e-04,
          2.7170e-04,  2.3387e-04],
        [ 0.0000e+00, -4.3860e-05,  1.3166e-03,  ..., -6.2830e-04,
          2.7170e-04,  2.3387e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(240.0385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.3967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.0988, device='cuda:0')



h[100].sum tensor(54.0044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.9217, device='cuda:0')



h[200].sum tensor(-7.2029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5995, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0742, 0.0824,  ..., 0.0008, 0.0371, 0.0185],
        [0.0000, 0.0298, 0.0362,  ..., 0.0003, 0.0155, 0.0080],
        [0.0000, 0.0100, 0.0156,  ..., 0.0000, 0.0059, 0.0033],
        ...,
        [0.0000, 0.0932, 0.1021,  ..., 0.0020, 0.0463, 0.0230],
        [0.0000, 0.0825, 0.0910,  ..., 0.0017, 0.0411, 0.0205],
        [0.0000, 0.0593, 0.0668,  ..., 0.0012, 0.0298, 0.0149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35935.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0477, 0.0000, 0.0000,  ..., 0.0398, 0.0000, 0.2268],
        [0.0335, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.1528],
        [0.0200, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0805],
        ...,
        [0.1126, 0.0000, 0.0000,  ..., 0.0968, 0.0000, 0.5250],
        [0.0908, 0.0000, 0.0000,  ..., 0.0778, 0.0000, 0.4222],
        [0.0646, 0.0000, 0.0000,  ..., 0.0549, 0.0000, 0.2952]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(248251.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2142.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(12.8785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3555.6079, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-257.6442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(10.2430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.1896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0866],
        [ 0.0755],
        [ 0.0515],
        ...,
        [-0.1077],
        [-0.0774],
        [-0.0489]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-54451.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366185.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0002],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366198.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.7904e-05,  1.3690e-03,  ..., -6.3141e-04,
          2.7085e-04,  2.2127e-04],
        [-7.9445e-03,  1.5161e-02,  1.7149e-02,  ...,  9.8648e-06,
          7.6405e-03,  3.8170e-03],
        [-7.9445e-03,  1.5161e-02,  1.7149e-02,  ...,  9.8648e-06,
          7.6405e-03,  3.8170e-03],
        ...,
        [ 0.0000e+00, -5.7904e-05,  1.3690e-03,  ..., -6.3141e-04,
          2.7085e-04,  2.2127e-04],
        [ 0.0000e+00, -5.7904e-05,  1.3690e-03,  ..., -6.3141e-04,
          2.7085e-04,  2.2127e-04],
        [ 0.0000e+00, -5.7904e-05,  1.3690e-03,  ..., -6.3141e-04,
          2.7085e-04,  2.2127e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(219.6115, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4468, device='cuda:0')



h[100].sum tensor(50.6024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1274, device='cuda:0')



h[200].sum tensor(-6.1962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4054, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.7554e-02, 3.4171e-02,  ..., 9.8734e-06, 1.4483e-02,
         7.4231e-03],
        [0.0000e+00, 3.9866e-02, 4.7000e-02,  ..., 9.8782e-06, 2.0474e-02,
         1.0346e-02],
        [0.0000e+00, 5.8068e-02, 6.5872e-02,  ..., 6.6858e-04, 2.9287e-02,
         1.4647e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 5.4845e-03,  ..., 0.0000e+00, 1.0851e-03,
         8.8645e-04],
        [0.0000e+00, 0.0000e+00, 5.4845e-03,  ..., 0.0000e+00, 1.0851e-03,
         8.8644e-04],
        [0.0000e+00, 0.0000e+00, 5.4846e-03,  ..., 0.0000e+00, 1.0851e-03,
         8.8645e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33221.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0400, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.1783],
        [0.0552, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.2530],
        [0.0747, 0.0000, 0.0000,  ..., 0.0637, 0.0000, 0.3454],
        ...,
        [0.0073, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0065],
        [0.0073, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0065],
        [0.0073, 0.0000, 0.0000,  ..., 0.0056, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(238829.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2188.5903, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-11.7481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3239.5674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-220.8692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59.1832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.0284, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0755],
        [-0.1039],
        [-0.1304],
        ...,
        [-0.1662],
        [-0.1655],
        [-0.1653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69775.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0002],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366198.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366211.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.8145e-05,  1.4020e-03,  ..., -6.3476e-04,
          2.7663e-04,  2.1829e-04],
        [ 0.0000e+00, -5.8145e-05,  1.4020e-03,  ..., -6.3476e-04,
          2.7663e-04,  2.1829e-04],
        [-4.9547e-03,  9.4469e-03,  1.1255e-02,  ..., -2.3451e-04,
          4.8791e-03,  2.4651e-03],
        ...,
        [ 0.0000e+00, -5.8145e-05,  1.4020e-03,  ..., -6.3476e-04,
          2.7663e-04,  2.1829e-04],
        [ 0.0000e+00, -5.8145e-05,  1.4020e-03,  ..., -6.3476e-04,
          2.7663e-04,  2.1829e-04],
        [ 0.0000e+00, -5.8145e-05,  1.4020e-03,  ..., -6.3476e-04,
          2.7663e-04,  2.1829e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(243.6137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.2710, device='cuda:0')



h[100].sum tensor(52.9084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.6850, device='cuda:0')



h[200].sum tensor(-6.4093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7765, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0011, 0.0009],
        [0.0000, 0.0095, 0.0155,  ..., 0.0000, 0.0057, 0.0031],
        [0.0000, 0.0217, 0.0282,  ..., 0.0000, 0.0117, 0.0060],
        ...,
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0011, 0.0009],
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0011, 0.0009],
        [0.0000, 0.0000, 0.0056,  ..., 0.0000, 0.0011, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35729.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0104, 0.0000, 0.0000,  ..., 0.0085, 0.0000, 0.0264],
        [0.0187, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0723],
        [0.0304, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.1344],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0064],
        [0.0077, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0064],
        [0.0077, 0.0000, 0.0000,  ..., 0.0063, 0.0000, 0.0064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(257075.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2349.6614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.2669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3566.2437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-245.0567, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(107.4339, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.2786, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0185],
        [ 0.0364],
        [ 0.0417],
        ...,
        [-0.1750],
        [-0.1742],
        [-0.1740]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-62381.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366211.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366224.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3825e-02,  2.6516e-02,  2.8937e-02,  ...,  4.8026e-04,
          1.3151e-02,  6.5118e-03],
        [-1.4314e-02,  2.7456e-02,  2.9911e-02,  ...,  5.1983e-04,
          1.3607e-02,  6.7342e-03],
        [-1.2085e-02,  2.3174e-02,  2.5473e-02,  ...,  3.3959e-04,
          1.1533e-02,  5.7213e-03],
        ...,
        [ 0.0000e+00, -4.2595e-05,  1.4147e-03,  ..., -6.3759e-04,
          2.9206e-04,  2.2967e-04],
        [ 0.0000e+00, -4.2595e-05,  1.4147e-03,  ..., -6.3759e-04,
          2.9206e-04,  2.2967e-04],
        [ 0.0000e+00, -4.2595e-05,  1.4147e-03,  ..., -6.3759e-04,
          2.9206e-04,  2.2967e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(234.2100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.9190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7137, device='cuda:0')



h[100].sum tensor(49.4244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.9548, device='cuda:0')



h[200].sum tensor(-5.5037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0999, 0.1094,  ..., 0.0017, 0.0496, 0.0246],
        [0.0000, 0.1076, 0.1174,  ..., 0.0020, 0.0534, 0.0264],
        [0.0000, 0.1328, 0.1435,  ..., 0.0030, 0.0656, 0.0324],
        ...,
        [0.0000, 0.0000, 0.0057,  ..., 0.0000, 0.0012, 0.0009],
        [0.0000, 0.0000, 0.0057,  ..., 0.0000, 0.0012, 0.0009],
        [0.0000, 0.0000, 0.0057,  ..., 0.0000, 0.0012, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31126.6914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0765, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.3539],
        [0.0830, 0.0000, 0.0000,  ..., 0.0700, 0.0000, 0.3856],
        [0.0902, 0.0000, 0.0000,  ..., 0.0761, 0.0000, 0.4194],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0065],
        [0.0078, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0065],
        [0.0078, 0.0000, 0.0000,  ..., 0.0066, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232675.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2200.5732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-27.9826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3136.8110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-189.7282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(149.5696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.2999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0354],
        [ 0.0344],
        [ 0.0369],
        ...,
        [-0.1813],
        [-0.1805],
        [-0.1803]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72801.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0001],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366224.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366236.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.1178e-05,  1.4126e-03,  ..., -6.4079e-04,
          3.1130e-04,  2.4579e-04],
        [-5.3116e-03,  1.0197e-02,  1.1998e-02,  ..., -2.1094e-04,
          5.2589e-03,  2.6645e-03],
        [ 0.0000e+00, -2.1178e-05,  1.4126e-03,  ..., -6.4079e-04,
          3.1130e-04,  2.4579e-04],
        ...,
        [ 0.0000e+00, -2.1178e-05,  1.4126e-03,  ..., -6.4079e-04,
          3.1130e-04,  2.4579e-04],
        [ 0.0000e+00, -2.1178e-05,  1.4126e-03,  ..., -6.4079e-04,
          3.1130e-04,  2.4579e-04],
        [ 0.0000e+00, -2.1178e-05,  1.4126e-03,  ..., -6.4079e-04,
          3.1130e-04,  2.4579e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(287.4330, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9760, device='cuda:0')



h[100].sum tensor(54.1364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.4854, device='cuda:0')



h[200].sum tensor(-6.3304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.4295e-02, 5.1629e-02,  ..., 1.3765e-05, 2.2734e-02,
         1.1489e-02],
        [0.0000e+00, 8.3340e-03, 1.4314e-02,  ..., 0.0000e+00, 5.2923e-03,
         2.9622e-03],
        [0.0000e+00, 1.0213e-02, 1.6260e-02,  ..., 0.0000e+00, 6.2020e-03,
         3.4069e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 5.6614e-03,  ..., 0.0000e+00, 1.2476e-03,
         9.8504e-04],
        [0.0000e+00, 0.0000e+00, 5.6613e-03,  ..., 0.0000e+00, 1.2476e-03,
         9.8502e-04],
        [0.0000e+00, 0.0000e+00, 5.6614e-03,  ..., 0.0000e+00, 1.2476e-03,
         9.8503e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34820.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0323, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.1457],
        [0.0223, 0.0000, 0.0000,  ..., 0.0183, 0.0000, 0.0904],
        [0.0159, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0557],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0065],
        [0.0078, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0065],
        [0.0078, 0.0000, 0.0000,  ..., 0.0067, 0.0000, 0.0065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(251958.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2311.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.5345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3536.1768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-235.3396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(165.9690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.4438, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0716],
        [ 0.0675],
        [ 0.0579],
        ...,
        [-0.1840],
        [-0.1834],
        [-0.1832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70063.4609, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366236.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366248.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  9.8720e-06,  1.4006e-03,  ..., -6.4420e-04,
          3.4175e-04,  2.7011e-04],
        [ 0.0000e+00,  9.8720e-06,  1.4006e-03,  ..., -6.4420e-04,
          3.4175e-04,  2.7011e-04],
        [ 0.0000e+00,  9.8720e-06,  1.4006e-03,  ..., -6.4420e-04,
          3.4175e-04,  2.7011e-04],
        ...,
        [ 0.0000e+00,  9.8720e-06,  1.4006e-03,  ..., -6.4420e-04,
          3.4175e-04,  2.7011e-04],
        [ 0.0000e+00,  9.8720e-06,  1.4006e-03,  ..., -6.4420e-04,
          3.4175e-04,  2.7011e-04],
        [ 0.0000e+00,  9.8720e-06,  1.4006e-03,  ..., -6.4420e-04,
          3.4175e-04,  2.7011e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(355.4107, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1297, device='cuda:0')



h[100].sum tensor(60.0135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.6191, device='cuda:0')



h[200].sum tensor(-7.4780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0637, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.9522e-05, 5.6073e-03,  ..., 0.0000e+00, 1.3682e-03,
         1.0814e-03],
        [0.0000e+00, 3.9543e-05, 5.6102e-03,  ..., 0.0000e+00, 1.3689e-03,
         1.0820e-03],
        [0.0000e+00, 1.7915e-02, 2.4121e-02,  ..., 0.0000e+00, 1.0024e-02,
         5.3163e-03],
        ...,
        [0.0000e+00, 3.9569e-05, 5.6139e-03,  ..., 0.0000e+00, 1.3698e-03,
         1.0827e-03],
        [0.0000e+00, 3.9568e-05, 5.6138e-03,  ..., 0.0000e+00, 1.3698e-03,
         1.0826e-03],
        [0.0000e+00, 1.2877e-02, 1.8907e-02,  ..., 0.0000e+00, 7.5850e-03,
         4.1235e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39347.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0073, 0.0000, 0.0140],
        [0.0151, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0483],
        [0.0353, 0.0000, 0.0000,  ..., 0.0289, 0.0000, 0.1520],
        ...,
        [0.0081, 0.0000, 0.0000,  ..., 0.0069, 0.0000, 0.0095],
        [0.0133, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0370],
        [0.0242, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0957]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(274866.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2402.8770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.3967, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4022.7009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-294.0443, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(171.0867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.2390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0608],
        [-0.0599],
        [-0.0756],
        ...,
        [-0.1064],
        [-0.0490],
        [-0.0013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-58253.8555, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366248.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366260.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  4.8479e-05,  1.3799e-03,  ..., -6.4850e-04,
          3.7554e-04,  2.9904e-04],
        [ 0.0000e+00,  4.8479e-05,  1.3799e-03,  ..., -6.4850e-04,
          3.7554e-04,  2.9904e-04],
        [ 0.0000e+00,  4.8479e-05,  1.3799e-03,  ..., -6.4850e-04,
          3.7554e-04,  2.9904e-04],
        ...,
        [ 0.0000e+00,  4.8479e-05,  1.3799e-03,  ..., -6.4850e-04,
          3.7554e-04,  2.9904e-04],
        [ 0.0000e+00,  4.8479e-05,  1.3799e-03,  ..., -6.4850e-04,
          3.7554e-04,  2.9904e-04],
        [ 0.0000e+00,  4.8479e-05,  1.3799e-03,  ..., -6.4850e-04,
          3.7554e-04,  2.9904e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(303.4302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6999, device='cuda:0')



h[100].sum tensor(48.4881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.2689, device='cuda:0')



h[200].sum tensor(-5.0867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.1685, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0055,  ..., 0.0000, 0.0015, 0.0012],
        [0.0000, 0.0002, 0.0055,  ..., 0.0000, 0.0015, 0.0012],
        [0.0000, 0.0002, 0.0055,  ..., 0.0000, 0.0015, 0.0012],
        ...,
        [0.0000, 0.0002, 0.0055,  ..., 0.0000, 0.0015, 0.0012],
        [0.0000, 0.0002, 0.0055,  ..., 0.0000, 0.0015, 0.0012],
        [0.0000, 0.0002, 0.0055,  ..., 0.0000, 0.0015, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30254.5098, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0064, 0.0000, 0.0095],
        [0.0074, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0072],
        [0.0074, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0072],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0072],
        [0.0074, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0072],
        [0.0074, 0.0000, 0.0000,  ..., 0.0061, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231284.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2057.7620, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-21.1884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3358.8286, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-189.4037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(228.9689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.1705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0843],
        [-0.1421],
        [-0.1920],
        ...,
        [-0.1786],
        [-0.1778],
        [-0.1776]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-68864.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366260.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(545.0649, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366272.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.5545e-05,  1.3674e-03,  ..., -6.5393e-04,
          4.0575e-04,  3.2258e-04],
        [ 0.0000e+00,  7.5545e-05,  1.3674e-03,  ..., -6.5393e-04,
          4.0575e-04,  3.2258e-04],
        [ 0.0000e+00,  7.5545e-05,  1.3674e-03,  ..., -6.5393e-04,
          4.0575e-04,  3.2258e-04],
        ...,
        [ 0.0000e+00,  7.5545e-05,  1.3674e-03,  ..., -6.5393e-04,
          4.0575e-04,  3.2258e-04],
        [ 0.0000e+00,  7.5545e-05,  1.3674e-03,  ..., -6.5393e-04,
          4.0575e-04,  3.2258e-04],
        [ 0.0000e+00,  7.5545e-05,  1.3674e-03,  ..., -6.5393e-04,
          4.0575e-04,  3.2258e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(377.5271, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.2303, device='cuda:0')



h[100].sum tensor(54.6925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.6574, device='cuda:0')



h[200].sum tensor(-6.3399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0055,  ..., 0.0000, 0.0016, 0.0013],
        [0.0000, 0.0003, 0.0055,  ..., 0.0000, 0.0016, 0.0013],
        [0.0000, 0.0003, 0.0055,  ..., 0.0000, 0.0016, 0.0013],
        ...,
        [0.0000, 0.0003, 0.0055,  ..., 0.0000, 0.0016, 0.0013],
        [0.0000, 0.0003, 0.0055,  ..., 0.0000, 0.0016, 0.0013],
        [0.0000, 0.0003, 0.0055,  ..., 0.0000, 0.0016, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35694.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0072, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0096],
        [0.0072, 0.0000, 0.0000,  ..., 0.0059, 0.0000, 0.0082],
        [0.0072, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0076],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0076],
        [0.0072, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0076],
        [0.0072, 0.0000, 0.0000,  ..., 0.0060, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259973.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2161.4858, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(9.6171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3935.0872, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-258.2862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(230.8319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.4904, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0123],
        [-0.0496],
        [-0.0860],
        ...,
        [-0.1706],
        [-0.1696],
        [-0.1691]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-55522.5898, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366272.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366285.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.7404e-05,  1.3688e-03,  ..., -6.5950e-04,
          4.2706e-04,  3.3842e-04],
        [ 0.0000e+00,  8.7404e-05,  1.3688e-03,  ..., -6.5950e-04,
          4.2706e-04,  3.3842e-04],
        [-5.7741e-03,  1.1261e-02,  1.2926e-02,  ..., -1.9143e-04,
          5.8368e-03,  2.9908e-03],
        ...,
        [ 0.0000e+00,  8.7404e-05,  1.3688e-03,  ..., -6.5950e-04,
          4.2706e-04,  3.3842e-04],
        [ 0.0000e+00,  8.7404e-05,  1.3688e-03,  ..., -6.5950e-04,
          4.2706e-04,  3.3842e-04],
        [ 0.0000e+00,  8.7404e-05,  1.3688e-03,  ..., -6.5950e-04,
          4.2706e-04,  3.3842e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(433.7821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.1595, device='cuda:0')



h[100].sum tensor(59.1737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.9627, device='cuda:0')



h[200].sum tensor(-7.2061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0055,  ..., 0.0000, 0.0017, 0.0014],
        [0.0000, 0.0201, 0.0260,  ..., 0.0000, 0.0113, 0.0061],
        [0.0000, 0.0499, 0.0567,  ..., 0.0007, 0.0257, 0.0131],
        ...,
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0017, 0.0014],
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0017, 0.0014],
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0017, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40062.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0152, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0562],
        [0.0302, 0.0000, 0.0000,  ..., 0.0238, 0.0000, 0.1358],
        [0.0531, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.2492],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0078],
        [0.0071, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0078],
        [0.0071, 0.0000, 0.0000,  ..., 0.0062, 0.0000, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287541.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2342.0122, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.0074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4365.0713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-315.9953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(291.2592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.7560, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0579],
        [ 0.0792],
        [ 0.0847],
        ...,
        [-0.1504],
        [-0.1647],
        [-0.1686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-56178.1836, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366285.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366297.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  9.1234e-05,  1.3762e-03,  ..., -6.6598e-04,
          4.4104e-04,  3.4905e-04],
        [ 0.0000e+00,  9.1234e-05,  1.3762e-03,  ..., -6.6598e-04,
          4.4104e-04,  3.4905e-04],
        [ 0.0000e+00,  9.1234e-05,  1.3762e-03,  ..., -6.6598e-04,
          4.4104e-04,  3.4905e-04],
        ...,
        [ 0.0000e+00,  9.1234e-05,  1.3762e-03,  ..., -6.6598e-04,
          4.4104e-04,  3.4905e-04],
        [ 0.0000e+00,  9.1234e-05,  1.3762e-03,  ..., -6.6598e-04,
          4.4104e-04,  3.4905e-04],
        [ 0.0000e+00,  9.1234e-05,  1.3762e-03,  ..., -6.6598e-04,
          4.4104e-04,  3.4905e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(376.2258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.8351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6279, device='cuda:0')



h[100].sum tensor(48.7732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.2202, device='cuda:0')



h[200].sum tensor(-5.0361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.1361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0018, 0.0014],
        ...,
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0004, 0.0055,  ..., 0.0000, 0.0018, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31127.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0072, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0078],
        [0.0072, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0078],
        [0.0072, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0078],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0078],
        [0.0071, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0078],
        [0.0071, 0.0000, 0.0000,  ..., 0.0065, 0.0000, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(242018.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1971.0457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.9946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3673.1726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-207.5320, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(369.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.5828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2320],
        [-0.2496],
        [-0.2628],
        ...,
        [-0.1748],
        [-0.1739],
        [-0.1736]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-59426.9023, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366297.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366310.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.1273e-02,  4.1362e-02,  4.4064e-02,  ...,  1.0504e-03,
          2.0430e-02,  1.0162e-02],
        [-2.2702e-02,  4.4137e-02,  4.6932e-02,  ...,  1.1663e-03,
          2.1774e-02,  1.0822e-02],
        [-1.8132e-02,  3.5268e-02,  3.7765e-02,  ...,  7.9594e-04,
          1.7480e-02,  8.7141e-03],
        ...,
        [ 0.0000e+00,  8.0628e-05,  1.3901e-03,  ..., -6.7338e-04,
          4.4167e-04,  3.5196e-04],
        [ 0.0000e+00,  8.0628e-05,  1.3901e-03,  ..., -6.7338e-04,
          4.4167e-04,  3.5196e-04],
        [ 0.0000e+00,  8.0628e-05,  1.3901e-03,  ..., -6.7338e-04,
          4.4167e-04,  3.5196e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(547.6611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-24.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.2129, device='cuda:0')



h[100].sum tensor(68.6483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.3816, device='cuda:0')



h[200].sum tensor(-8.9928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1725, 0.1835,  ..., 0.0045, 0.0851, 0.0423],
        [0.0000, 0.1778, 0.1890,  ..., 0.0047, 0.0877, 0.0436],
        [0.0000, 0.1947, 0.2065,  ..., 0.0054, 0.0959, 0.0476],
        ...,
        [0.0000, 0.0003, 0.0056,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0003, 0.0056,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0003, 0.0056,  ..., 0.0000, 0.0018, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48618.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1411, 0.0000, 0.0000,  ..., 0.1139, 0.0000, 0.6553],
        [0.1539, 0.0000, 0.0000,  ..., 0.1243, 0.0000, 0.7136],
        [0.1621, 0.0000, 0.0000,  ..., 0.1309, 0.0000, 0.7502],
        ...,
        [0.0073, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0075],
        [0.0073, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0075],
        [0.0073, 0.0000, 0.0000,  ..., 0.0071, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341897.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2786.4749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(70.5999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5191.8877, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-424.7362, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(449.3444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-454.2982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0220],
        [ 0.0153],
        [ 0.0140],
        ...,
        [-0.1846],
        [-0.1838],
        [-0.1835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-65442.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0001],
        [0.9999],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366310.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366322.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.6109e-03,  1.0968e-02,  1.2677e-02,  ..., -2.3042e-04,
          5.7224e-03,  2.9426e-03],
        [ 0.0000e+00,  6.3728e-05,  1.4091e-03,  ..., -6.8435e-04,
          4.4194e-04,  3.5006e-04],
        [ 0.0000e+00,  6.3728e-05,  1.4091e-03,  ..., -6.8435e-04,
          4.4194e-04,  3.5006e-04],
        ...,
        [ 0.0000e+00,  6.3728e-05,  1.4091e-03,  ..., -6.8435e-04,
          4.4194e-04,  3.5006e-04],
        [ 0.0000e+00,  6.3728e-05,  1.4091e-03,  ..., -6.8435e-04,
          4.4194e-04,  3.5006e-04],
        [ 0.0000e+00,  6.3728e-05,  1.4091e-03,  ..., -6.8435e-04,
          4.4194e-04,  3.5006e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(419.2110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0406, device='cuda:0')



h[100].sum tensor(49.8282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.4994, device='cuda:0')



h[200].sum tensor(-5.1036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.3220, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0148,  ..., 0.0000, 0.0061, 0.0035],
        [0.0000, 0.0112, 0.0169,  ..., 0.0000, 0.0071, 0.0040],
        [0.0000, 0.0003, 0.0056,  ..., 0.0000, 0.0018, 0.0014],
        ...,
        [0.0000, 0.0003, 0.0057,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0003, 0.0057,  ..., 0.0000, 0.0018, 0.0014],
        [0.0000, 0.0003, 0.0057,  ..., 0.0000, 0.0018, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31994.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0145, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0486],
        [0.0130, 0.0000, 0.0000,  ..., 0.0118, 0.0000, 0.0395],
        [0.0093, 0.0000, 0.0000,  ..., 0.0091, 0.0000, 0.0174],
        ...,
        [0.0075, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0073],
        [0.0075, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0073],
        [0.0075, 0.0000, 0.0000,  ..., 0.0080, 0.0000, 0.0073]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(250857.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2115.1372, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-8.2690, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3781.9639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.9430, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(542.8036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.5654, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1265],
        [-0.1344],
        [-0.1325],
        ...,
        [-0.1960],
        [-0.1952],
        [-0.1949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69669.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0014],
        [1.0015],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366322.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366336.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0250e-02,  1.9989e-02,  2.2040e-02,  ...,  1.3243e-04,
          1.0122e-02,  5.0883e-03],
        [-1.1718e-02,  2.2847e-02,  2.4992e-02,  ...,  2.5104e-04,
          1.1507e-02,  5.7681e-03],
        [-3.2816e-02,  6.3907e-02,  6.7409e-02,  ...,  1.9551e-03,
          3.1394e-02,  1.5535e-02],
        ...,
        [ 0.0000e+00,  4.1512e-05,  1.4326e-03,  ..., -6.9544e-04,
          4.6087e-04,  3.4342e-04],
        [ 0.0000e+00,  4.1512e-05,  1.4326e-03,  ..., -6.9544e-04,
          4.6087e-04,  3.4342e-04],
        [ 0.0000e+00,  4.1512e-05,  1.4326e-03,  ..., -6.9544e-04,
          4.6087e-04,  3.4342e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(539.8323, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.9366, device='cuda:0')



h[100].sum tensor(63.1917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.1650, device='cuda:0')



h[200].sum tensor(-7.7273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.5602e-01, 1.6674e-01,  ..., 3.6839e-03, 7.7331e-02,
         3.8446e-02],
        [0.0000e+00, 1.7694e-01, 1.8836e-01,  ..., 4.5508e-03, 8.7467e-02,
         4.3425e-02],
        [0.0000e+00, 1.0774e-01, 1.1687e-01,  ..., 1.6788e-03, 5.3948e-02,
         2.6963e-02],
        ...,
        [0.0000e+00, 1.6653e-04, 5.7470e-03,  ..., 0.0000e+00, 1.8488e-03,
         1.3776e-03],
        [0.0000e+00, 1.6653e-04, 5.7469e-03,  ..., 0.0000e+00, 1.8488e-03,
         1.3776e-03],
        [0.0000e+00, 1.6653e-04, 5.7469e-03,  ..., 0.0000e+00, 1.8488e-03,
         1.3776e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44312.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1291, 0.0000, 0.0000,  ..., 0.1040, 0.0000, 0.6008],
        [0.1384, 0.0000, 0.0000,  ..., 0.1114, 0.0000, 0.6438],
        [0.1150, 0.0000, 0.0000,  ..., 0.0927, 0.0000, 0.5351],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0087, 0.0000, 0.0071],
        [0.0076, 0.0000, 0.0000,  ..., 0.0087, 0.0000, 0.0071],
        [0.0076, 0.0000, 0.0000,  ..., 0.0087, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(319698.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2641.3735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(46.2892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4841.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-372.3745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(553.8284, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.4077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0672],
        [ 0.0632],
        [ 0.0606],
        ...,
        [-0.2040],
        [-0.2031],
        [-0.2028]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69485.8359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366336.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366349.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04],
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04],
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04],
        ...,
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04],
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04],
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(543.3690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.2943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.3796, device='cuda:0')



h[100].sum tensor(61.4374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.7882, device='cuda:0')



h[200].sum tensor(-7.3208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1762, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.6118e-05, 5.8409e-03,  ..., 0.0000e+00, 1.9423e-03,
         1.3388e-03],
        [0.0000e+00, 6.6157e-05, 5.8444e-03,  ..., 0.0000e+00, 1.9434e-03,
         1.3396e-03],
        [0.0000e+00, 2.7205e-02, 3.3873e-02,  ..., 1.2005e-04, 1.5091e-02,
         7.7975e-03],
        ...,
        [0.0000e+00, 6.6258e-05, 5.8533e-03,  ..., 0.0000e+00, 1.9464e-03,
         1.3416e-03],
        [0.0000e+00, 6.6257e-05, 5.8532e-03,  ..., 0.0000e+00, 1.9464e-03,
         1.3416e-03],
        [0.0000e+00, 6.6258e-05, 5.8533e-03,  ..., 0.0000e+00, 1.9464e-03,
         1.3416e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41927.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0102, 0.0000, 0.0000,  ..., 0.0113, 0.0000, 0.0210],
        [0.0124, 0.0000, 0.0000,  ..., 0.0127, 0.0000, 0.0353],
        [0.0229, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0954],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0071],
        [0.0076, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0071],
        [0.0076, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(306843.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2500.2913, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(34.3783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4682.7090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-343.2313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(551.1438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-442.5994, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0037],
        [ 0.0388],
        [ 0.0841],
        ...,
        [-0.2099],
        [-0.2090],
        [-0.2087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-66039.6953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366349.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366349.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.7584e-03,  1.1239e-02,  1.3049e-02,  ..., -2.4290e-04,
          5.9218e-03,  3.0049e-03],
        [-3.7050e-03,  7.2372e-03,  8.9162e-03,  ..., -4.0846e-04,
          3.9831e-03,  2.0526e-03],
        [-9.4634e-03,  1.8460e-02,  2.0507e-02,  ...,  5.5827e-05,
          9.4197e-03,  4.7231e-03],
        ...,
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04],
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04],
        [ 0.0000e+00,  1.6515e-05,  1.4589e-03,  ..., -7.0719e-04,
          4.8514e-04,  3.3441e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(453.4047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9709, device='cuda:0')



h[100].sum tensor(50.1088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.4522, device='cuda:0')



h[200].sum tensor(-5.0548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.2365e-02, 2.8871e-02,  ..., 0.0000e+00, 1.2745e-02,
         6.6450e-03],
        [0.0000e+00, 6.1367e-02, 6.9154e-02,  ..., 5.5905e-05, 3.1640e-02,
         1.5927e-02],
        [0.0000e+00, 3.7054e-02, 4.4043e-02,  ..., 0.0000e+00, 1.9861e-02,
         1.0141e-02],
        ...,
        [0.0000e+00, 6.6258e-05, 5.8533e-03,  ..., 0.0000e+00, 1.9464e-03,
         1.3416e-03],
        [0.0000e+00, 6.6257e-05, 5.8532e-03,  ..., 0.0000e+00, 1.9464e-03,
         1.3416e-03],
        [0.0000e+00, 6.6258e-05, 5.8533e-03,  ..., 0.0000e+00, 1.9464e-03,
         1.3416e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32694.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0284, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.1235],
        [0.0391, 0.0000, 0.0000,  ..., 0.0323, 0.0000, 0.1806],
        [0.0357, 0.0000, 0.0000,  ..., 0.0297, 0.0000, 0.1637],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0071],
        [0.0076, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0071],
        [0.0076, 0.0000, 0.0000,  ..., 0.0095, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259171.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2190.4678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-11.5834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3869.7803, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-232.0007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(631.1235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-396.5211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1038],
        [ 0.1135],
        [ 0.1094],
        ...,
        [-0.1891],
        [-0.1746],
        [-0.1636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-82715.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366349.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366362.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.9375e-06,  1.4787e-03,  ..., -7.1767e-04,
          5.1392e-04,  3.2599e-04],
        [ 0.0000e+00, -2.9375e-06,  1.4787e-03,  ..., -7.1767e-04,
          5.1392e-04,  3.2599e-04],
        [ 0.0000e+00, -2.9375e-06,  1.4787e-03,  ..., -7.1767e-04,
          5.1392e-04,  3.2599e-04],
        ...,
        [ 0.0000e+00, -2.9375e-06,  1.4787e-03,  ..., -7.1767e-04,
          5.1392e-04,  3.2599e-04],
        [ 0.0000e+00, -2.9375e-06,  1.4787e-03,  ..., -7.1767e-04,
          5.1392e-04,  3.2599e-04],
        [ 0.0000e+00, -2.9375e-06,  1.4787e-03,  ..., -7.1767e-04,
          5.1392e-04,  3.2599e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(671.7174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.2919, device='cuda:0')



h[100].sum tensor(74.6714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.4648, device='cuda:0')



h[200].sum tensor(-9.9432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0059,  ..., 0.0000, 0.0021, 0.0013],
        [0.0000, 0.0000, 0.0059,  ..., 0.0000, 0.0021, 0.0013],
        [0.0000, 0.0000, 0.0059,  ..., 0.0000, 0.0021, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0059,  ..., 0.0000, 0.0021, 0.0013],
        [0.0000, 0.0000, 0.0059,  ..., 0.0000, 0.0021, 0.0013],
        [0.0000, 0.0059, 0.0120,  ..., 0.0000, 0.0049, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50296.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0071],
        [0.0076, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0071],
        [0.0083, 0.0000, 0.0000,  ..., 0.0106, 0.0000, 0.0107],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0130],
        [0.0095, 0.0000, 0.0000,  ..., 0.0111, 0.0000, 0.0216],
        [0.0113, 0.0000, 0.0000,  ..., 0.0120, 0.0000, 0.0356]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341043.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2758.7925, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(69.6987, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5206.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-450.2187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(571.0264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-465.9953, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2493],
        [-0.1937],
        [-0.1137],
        ...,
        [-0.1490],
        [-0.0971],
        [-0.0502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-72202.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0016],
        [1.0001],
        ...,
        [1.0002],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366362.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0001],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366375.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.1765e-05,  1.4977e-03,  ..., -7.2632e-04,
          5.3284e-04,  3.1667e-04],
        [ 0.0000e+00, -2.1765e-05,  1.4977e-03,  ..., -7.2632e-04,
          5.3284e-04,  3.1667e-04],
        [ 0.0000e+00, -2.1765e-05,  1.4977e-03,  ..., -7.2632e-04,
          5.3284e-04,  3.1667e-04],
        ...,
        [ 0.0000e+00, -2.1765e-05,  1.4977e-03,  ..., -7.2632e-04,
          5.3284e-04,  3.1667e-04],
        [ 0.0000e+00, -2.1765e-05,  1.4977e-03,  ..., -7.2632e-04,
          5.3284e-04,  3.1667e-04],
        [ 0.0000e+00, -2.1765e-05,  1.4977e-03,  ..., -7.2632e-04,
          5.3284e-04,  3.1667e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(562.6336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7226, device='cuda:0')



h[100].sum tensor(57.9962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.6671, device='cuda:0')



h[200].sum tensor(-6.6221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0219, 0.0286,  ..., 0.0002, 0.0127, 0.0065],
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0021, 0.0013],
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0021, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0021, 0.0013],
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0021, 0.0013],
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0021, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38689.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0273, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.1091],
        [0.0182, 0.0000, 0.0000,  ..., 0.0187, 0.0000, 0.0615],
        [0.0134, 0.0000, 0.0000,  ..., 0.0150, 0.0000, 0.0379],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0071],
        [0.0077, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0071],
        [0.0077, 0.0000, 0.0000,  ..., 0.0109, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(289925.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2381.1960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(14.7232, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4466.1235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-306.6282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(605.2191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-425.4291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0917],
        [ 0.0867],
        [ 0.0865],
        ...,
        [-0.2194],
        [-0.2184],
        [-0.2181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79290.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0015],
        [1.0017],
        [1.0001],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366375.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(505.5971, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0017],
        [1.0001],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366388., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8755e-03,  7.5455e-03,  9.3351e-03,  ..., -4.2087e-04,
          4.2224e-03,  2.1127e-03],
        [-3.8755e-03,  7.5455e-03,  9.3351e-03,  ..., -4.2087e-04,
          4.2224e-03,  2.1127e-03],
        [ 0.0000e+00, -3.8748e-05,  1.5065e-03,  ..., -7.3265e-04,
          5.4722e-04,  3.0680e-04],
        ...,
        [ 0.0000e+00, -3.8748e-05,  1.5065e-03,  ..., -7.3265e-04,
          5.4722e-04,  3.0680e-04],
        [ 0.0000e+00, -3.8748e-05,  1.5065e-03,  ..., -7.3265e-04,
          5.4722e-04,  3.0680e-04],
        [ 0.0000e+00, -3.8748e-05,  1.5065e-03,  ..., -7.3265e-04,
          5.4722e-04,  3.0680e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(564.1216, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7092, device='cuda:0')



h[100].sum tensor(54.4533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3049, device='cuda:0')



h[200].sum tensor(-5.9616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0137, 0.0203,  ..., 0.0000, 0.0089, 0.0045],
        [0.0000, 0.0137, 0.0203,  ..., 0.0000, 0.0089, 0.0045],
        [0.0000, 0.0137, 0.0203,  ..., 0.0000, 0.0089, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0022, 0.0012],
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0022, 0.0012],
        [0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0022, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36252.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0183, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0770],
        [0.0156, 0.0000, 0.0000,  ..., 0.0157, 0.0000, 0.0604],
        [0.0137, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0466],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0070],
        [0.0079, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0070],
        [0.0079, 0.0000, 0.0000,  ..., 0.0116, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(277128.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2345.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1.8335, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4373.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-276.4062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(578.6584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.1841, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0874],
        [ 0.0297],
        [-0.0579],
        ...,
        [-0.2252],
        [-0.2242],
        [-0.2239]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-79264.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0017],
        [1.0001],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366388., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0017],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366400.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.2031e-05,  1.5131e-03,  ..., -7.3804e-04,
          5.6430e-04,  2.9908e-04],
        [-1.7515e-02,  3.4271e-02,  3.6938e-02,  ...,  6.7130e-04,
          1.7197e-02,  8.4729e-03],
        [ 0.0000e+00, -5.2031e-05,  1.5131e-03,  ..., -7.3804e-04,
          5.6430e-04,  2.9908e-04],
        ...,
        [ 0.0000e+00, -5.2031e-05,  1.5131e-03,  ..., -7.3804e-04,
          5.6430e-04,  2.9908e-04],
        [ 0.0000e+00, -5.2031e-05,  1.5131e-03,  ..., -7.3804e-04,
          5.6430e-04,  2.9908e-04],
        [ 0.0000e+00, -5.2031e-05,  1.5131e-03,  ..., -7.3804e-04,
          5.6430e-04,  2.9908e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(689.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.1070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8064, device='cuda:0')



h[100].sum tensor(65.9293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.4301, device='cuda:0')



h[200].sum tensor(-8.2596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2689, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0538, 0.0617,  ..., 0.0007, 0.0284, 0.0140],
        [0.0000, 0.0671, 0.0755,  ..., 0.0013, 0.0348, 0.0172],
        [0.0000, 0.1443, 0.1552,  ..., 0.0030, 0.0723, 0.0356],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0023, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0023, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0023, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46325.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0462, 0.0000, 0.0000,  ..., 0.0410, 0.0000, 0.1959],
        [0.0592, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.2592],
        [0.0714, 0.0000, 0.0000,  ..., 0.0606, 0.0000, 0.3171],
        ...,
        [0.0081, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0069],
        [0.0081, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0069],
        [0.0081, 0.0000, 0.0000,  ..., 0.0121, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(331088.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2777.5049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(49.4166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5322.3252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-400.9475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(559.5836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-452.8747, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0185],
        [ 0.0219],
        [ 0.0156],
        ...,
        [-0.2297],
        [-0.2287],
        [-0.2284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-78777.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0017],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366400.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366412.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.8545e-02,  5.5950e-02,  5.9320e-02,  ...,  1.5581e-03,
          2.7730e-02,  1.3637e-02],
        [-2.6988e-02,  5.2895e-02,  5.6167e-02,  ...,  1.4327e-03,
          2.6250e-02,  1.2909e-02],
        [-2.7304e-02,  5.3516e-02,  5.6807e-02,  ...,  1.4582e-03,
          2.6550e-02,  1.3057e-02],
        ...,
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04],
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04],
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(665.7390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.5558, device='cuda:0')



h[100].sum tensor(59.0813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.2308, device='cuda:0')



h[200].sum tensor(-6.9515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.2115, 0.2246,  ..., 0.0057, 0.1050, 0.0516],
        [0.0000, 0.2305, 0.2442,  ..., 0.0065, 0.1142, 0.0561],
        [0.0000, 0.2260, 0.2395,  ..., 0.0063, 0.1120, 0.0551],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0024, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0024, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0024, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41021.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1797, 0.0000, 0.0000,  ..., 0.1460, 0.0000, 0.8261],
        [0.1858, 0.0000, 0.0000,  ..., 0.1508, 0.0000, 0.8537],
        [0.1759, 0.0000, 0.0000,  ..., 0.1429, 0.0000, 0.8086],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0070],
        [0.0082, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0070],
        [0.0082, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(304911.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2586.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(27.9451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5073.4409, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-336.5901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(530.8947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.0102, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0474],
        [ 0.0485],
        [ 0.0529],
        ...,
        [-0.2321],
        [-0.2311],
        [-0.2308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76036.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366412.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366412.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04],
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04],
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04],
        ...,
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04],
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04],
        [ 0.0000e+00, -6.1335e-05,  1.5154e-03,  ..., -7.4100e-04,
          5.8630e-04,  2.9579e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(706.6555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.5211, device='cuda:0')



h[100].sum tensor(63.9567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.2370, device='cuda:0')



h[200].sum tensor(-7.9092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.1404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0023, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0023, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0023, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0024, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0024, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0024, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44280.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0127, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0337],
        [0.0088, 0.0000, 0.0000,  ..., 0.0125, 0.0000, 0.0125],
        [0.0084, 0.0000, 0.0000,  ..., 0.0123, 0.0000, 0.0095],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0070],
        [0.0082, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0070],
        [0.0082, 0.0000, 0.0000,  ..., 0.0124, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(321314.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2773.1631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(40.0359, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5258.5034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-379.2532, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(570.0520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.1492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0235],
        [-0.0402],
        [-0.1102],
        ...,
        [-0.2321],
        [-0.2311],
        [-0.2308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-85889.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0016],
        [1.0018],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366412.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0018],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366425.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.7807e-05,  1.5148e-03,  ..., -7.4132e-04,
          6.1122e-04,  2.9649e-04],
        [ 0.0000e+00, -6.7807e-05,  1.5148e-03,  ..., -7.4132e-04,
          6.1122e-04,  2.9649e-04],
        [-7.6464e-03,  1.4956e-02,  1.7019e-02,  ..., -1.2435e-04,
          7.8919e-03,  3.8756e-03],
        ...,
        [ 0.0000e+00, -6.7807e-05,  1.5148e-03,  ..., -7.4132e-04,
          6.1122e-04,  2.9649e-04],
        [ 0.0000e+00, -6.7807e-05,  1.5148e-03,  ..., -7.4132e-04,
          6.1122e-04,  2.9649e-04],
        [ 0.0000e+00, -6.7807e-05,  1.5148e-03,  ..., -7.4132e-04,
          6.1122e-04,  2.9649e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(628.2731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2327, device='cuda:0')



h[100].sum tensor(50.8091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3060, device='cuda:0')



h[200].sum tensor(-5.3803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0261, 0.0331,  ..., 0.0003, 0.0151, 0.0074],
        [0.0000, 0.0150, 0.0216,  ..., 0.0000, 0.0097, 0.0048],
        [0.0000, 0.0440, 0.0517,  ..., 0.0006, 0.0239, 0.0117],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0025, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0025, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0025, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34897.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0379, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.1577],
        [0.0359, 0.0000, 0.0000,  ..., 0.0330, 0.0000, 0.1484],
        [0.0476, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.2048],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0072],
        [0.0083, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0072],
        [0.0083, 0.0000, 0.0000,  ..., 0.0126, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(277909.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2433.1321, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(1.8531, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4726.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.3597, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(542.3179, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.3207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0709],
        [ 0.0688],
        [ 0.0634],
        ...,
        [-0.2321],
        [-0.2311],
        [-0.2308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-84698.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0018],
        [1.0002],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366425.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0019],
        [1.0003],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366438.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.4473e-03,  1.8514e-02,  2.0694e-02,  ...,  2.2795e-05,
          9.6433e-03,  4.7262e-03],
        [-4.5781e-03,  8.9341e-03,  1.0808e-02,  ..., -3.7088e-04,
          5.0009e-03,  2.4436e-03],
        [ 0.0000e+00, -7.3168e-05,  1.5132e-03,  ..., -7.4102e-04,
          6.3609e-04,  2.9740e-04],
        ...,
        [ 0.0000e+00, -7.3168e-05,  1.5132e-03,  ..., -7.4102e-04,
          6.3609e-04,  2.9740e-04],
        [ 0.0000e+00, -7.3168e-05,  1.5132e-03,  ..., -7.4102e-04,
          6.3609e-04,  2.9740e-04],
        [ 0.0000e+00, -7.3168e-05,  1.5132e-03,  ..., -7.4102e-04,
          6.3609e-04,  2.9740e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(795.3209, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.9625, device='cuda:0')



h[100].sum tensor(66.7361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.2122, device='cuda:0')



h[200].sum tensor(-8.5145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0793, 0.0882,  ..., 0.0008, 0.0411, 0.0202],
        [0.0000, 0.0476, 0.0553,  ..., 0.0005, 0.0257, 0.0126],
        [0.0000, 0.0090, 0.0154,  ..., 0.0000, 0.0069, 0.0033],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47141.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0659, 0.0000, 0.0000,  ..., 0.0557, 0.0000, 0.3006],
        [0.0507, 0.0000, 0.0000,  ..., 0.0442, 0.0000, 0.2240],
        [0.0340, 0.0000, 0.0000,  ..., 0.0314, 0.0000, 0.1425],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0075],
        [0.0083, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0075],
        [0.0083, 0.0000, 0.0000,  ..., 0.0128, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(333697.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2807.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(66.3370, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5790.6611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-417.2068, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(450.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-451.7516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0537],
        [ 0.0621],
        [ 0.0692],
        ...,
        [-0.2301],
        [-0.2291],
        [-0.2288]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-70184.9453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0019],
        [1.0003],
        ...,
        [1.0003],
        [1.0000],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366438.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0017],
        [1.0019],
        [1.0003],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366451.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.5379e-05,  1.5104e-03,  ..., -7.4096e-04,
          6.5683e-04,  3.0044e-04],
        [ 0.0000e+00, -7.5379e-05,  1.5104e-03,  ..., -7.4096e-04,
          6.5683e-04,  3.0044e-04],
        [ 0.0000e+00, -7.5379e-05,  1.5104e-03,  ..., -7.4096e-04,
          6.5683e-04,  3.0044e-04],
        ...,
        [ 0.0000e+00, -7.5379e-05,  1.5104e-03,  ..., -7.4096e-04,
          6.5683e-04,  3.0044e-04],
        [ 0.0000e+00, -7.5379e-05,  1.5104e-03,  ..., -7.4096e-04,
          6.5683e-04,  3.0044e-04],
        [ 0.0000e+00, -7.5379e-05,  1.5104e-03,  ..., -7.4096e-04,
          6.5683e-04,  3.0044e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(689.7860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4592, device='cuda:0')



h[100].sum tensor(50.9377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.4592, device='cuda:0')



h[200].sum tensor(-5.4391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9607, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0060,  ..., 0.0000, 0.0026, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0026, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34719.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0293, 0.0000, 0.0000,  ..., 0.0288, 0.0000, 0.1116],
        [0.0137, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0358],
        [0.0090, 0.0000, 0.0000,  ..., 0.0132, 0.0000, 0.0137],
        ...,
        [0.0083, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0077],
        [0.0083, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0077],
        [0.0083, 0.0000, 0.0000,  ..., 0.0129, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(271442.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2333.6775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(11.2327, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4880.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-266.5203, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(465.9568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-398.9286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0696],
        [ 0.0725],
        [ 0.0824],
        ...,
        [-0.2281],
        [-0.2273],
        [-0.2272]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-76746.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0017],
        [1.0019],
        [1.0003],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366451.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0020],
        [1.0004],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366464.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.8244e-05,  1.5153e-03,  ..., -7.4139e-04,
          6.7340e-04,  2.9973e-04],
        [ 0.0000e+00, -7.8244e-05,  1.5153e-03,  ..., -7.4139e-04,
          6.7340e-04,  2.9973e-04],
        [-5.3373e-03,  1.0450e-02,  1.2380e-02,  ..., -3.0832e-04,
          5.7756e-03,  2.8093e-03],
        ...,
        [ 0.0000e+00, -7.8244e-05,  1.5153e-03,  ..., -7.4139e-04,
          6.7340e-04,  2.9973e-04],
        [ 0.0000e+00, -7.8244e-05,  1.5153e-03,  ..., -7.4139e-04,
          6.7340e-04,  2.9973e-04],
        [ 0.0000e+00, -7.8244e-05,  1.5153e-03,  ..., -7.4139e-04,
          6.7340e-04,  2.9973e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(757.8099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.3711, device='cuda:0')



h[100].sum tensor(55.9771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.4293, device='cuda:0')



h[200].sum tensor(-6.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2718, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.0297e-02, 2.7092e-02,  ..., 9.5913e-05, 1.2570e-02,
         6.0567e-03],
        [0.0000e+00, 1.0468e-02, 1.6955e-02,  ..., 0.0000e+00, 7.8095e-03,
         3.7150e-03],
        [0.0000e+00, 2.3105e-02, 3.0076e-02,  ..., 0.0000e+00, 1.3971e-02,
         6.7458e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 6.0865e-03,  ..., 0.0000e+00, 2.7048e-03,
         1.2039e-03],
        [0.0000e+00, 0.0000e+00, 6.0863e-03,  ..., 0.0000e+00, 2.7047e-03,
         1.2039e-03],
        [0.0000e+00, 0.0000e+00, 6.0864e-03,  ..., 0.0000e+00, 2.7047e-03,
         1.2039e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38436.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0234, 0.0000, 0.0000,  ..., 0.0235, 0.0000, 0.0885],
        [0.0233, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0914],
        [0.0303, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.1297],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0077],
        [0.0084, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0077],
        [0.0084, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(290074.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2528.9092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.0215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5238.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-315.0364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(434.5194, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.3516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1247],
        [ 0.1304],
        [ 0.1359],
        ...,
        [-0.2326],
        [-0.2316],
        [-0.2313]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77847.1797, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0020],
        [1.0004],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366464.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0020],
        [1.0004],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366478.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.9125e-05,  1.5222e-03,  ..., -7.4206e-04,
          6.8425e-04,  2.9882e-04],
        [ 0.0000e+00, -7.9125e-05,  1.5222e-03,  ..., -7.4206e-04,
          6.8425e-04,  2.9882e-04],
        [ 0.0000e+00, -7.9125e-05,  1.5222e-03,  ..., -7.4206e-04,
          6.8425e-04,  2.9882e-04],
        ...,
        [ 0.0000e+00, -7.9125e-05,  1.5222e-03,  ..., -7.4206e-04,
          6.8425e-04,  2.9882e-04],
        [ 0.0000e+00, -7.9125e-05,  1.5222e-03,  ..., -7.4206e-04,
          6.8425e-04,  2.9882e-04],
        [ 0.0000e+00, -7.9125e-05,  1.5222e-03,  ..., -7.4206e-04,
          6.8425e-04,  2.9882e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(748.9568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3536, device='cuda:0')



h[100].sum tensor(52.6714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.0643, device='cuda:0')



h[200].sum tensor(-5.6842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3634, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0158, 0.0225,  ..., 0.0000, 0.0105, 0.0050],
        ...,
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0061,  ..., 0.0000, 0.0027, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36050.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0098, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0156],
        [0.0124, 0.0000, 0.0000,  ..., 0.0159, 0.0000, 0.0276],
        [0.0208, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0706],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0077],
        [0.0087, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0077],
        [0.0087, 0.0000, 0.0000,  ..., 0.0134, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(281064.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2508.9019, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.7702, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5129.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-288.1454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(426.3576, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.8967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0089],
        [-0.0143],
        [-0.0192],
        ...,
        [-0.2403],
        [-0.2393],
        [-0.2390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-74746.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0020],
        [1.0004],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366478.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0004],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366491.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.9829e-05,  1.5378e-03,  ..., -7.4272e-04,
          6.8405e-04,  2.9327e-04],
        [-1.0708e-02,  2.1101e-02,  2.3391e-02,  ...,  1.2882e-04,
          1.0948e-02,  5.3429e-03],
        [ 0.0000e+00, -7.9829e-05,  1.5378e-03,  ..., -7.4272e-04,
          6.8405e-04,  2.9327e-04],
        ...,
        [ 0.0000e+00, -7.9829e-05,  1.5378e-03,  ..., -7.4272e-04,
          6.8405e-04,  2.9327e-04],
        [ 0.0000e+00, -7.9829e-05,  1.5378e-03,  ..., -7.4272e-04,
          6.8405e-04,  2.9327e-04],
        [ 0.0000e+00, -7.9829e-05,  1.5378e-03,  ..., -7.4272e-04,
          6.8405e-04,  2.9327e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(757.1024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0078, device='cuda:0')



h[100].sum tensor(52.0841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8303, device='cuda:0')



h[200].sum tensor(-5.5211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2077, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0301, 0.0373,  ..., 0.0001, 0.0174, 0.0084],
        [0.0000, 0.0172, 0.0240,  ..., 0.0000, 0.0111, 0.0053],
        [0.0000, 0.0991, 0.1087,  ..., 0.0012, 0.0509, 0.0249],
        ...,
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0027, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36996.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0313, 0.0000, 0.0000,  ..., 0.0292, 0.0000, 0.1256],
        [0.0326, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.1278],
        [0.0525, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.2236],
        ...,
        [0.0092, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0072],
        [0.0092, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0072],
        [0.0092, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(294282.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2729.3904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(25.9194, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5318.1753, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-301.2954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(466.8939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.5331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1170],
        [ 0.1011],
        [ 0.0808],
        ...,
        [-0.2542],
        [-0.2532],
        [-0.2529]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-77284.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0018],
        [1.0021],
        [1.0004],
        ...,
        [1.0003],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366491.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(505.5403, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0021],
        [1.0005],
        ...,
        [1.0004],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366505.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -8.2798e-05,  1.5573e-03,  ..., -7.4339e-04,
          6.8270e-04,  2.8773e-04],
        [ 0.0000e+00, -8.2798e-05,  1.5573e-03,  ..., -7.4339e-04,
          6.8270e-04,  2.8773e-04],
        [-6.5682e-03,  1.2926e-02,  1.4979e-02,  ..., -2.0801e-04,
          6.9870e-03,  3.3895e-03],
        ...,
        [ 0.0000e+00, -8.2798e-05,  1.5573e-03,  ..., -7.4339e-04,
          6.8270e-04,  2.8773e-04],
        [ 0.0000e+00, -8.2798e-05,  1.5573e-03,  ..., -7.4339e-04,
          6.8270e-04,  2.8773e-04],
        [ 0.0000e+00, -8.2798e-05,  1.5573e-03,  ..., -7.4339e-04,
          6.8270e-04,  2.8773e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1140.3584, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-38.0985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-38.1119, device='cuda:0')



h[100].sum tensor(93.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-25.7853, device='cuda:0')



h[200].sum tensor(-13.3678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.1604, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0200, 0.0270,  ..., 0.0000, 0.0125, 0.0060],
        [0.0000, 0.0466, 0.0546,  ..., 0.0003, 0.0254, 0.0123],
        ...,
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0027, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70037.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0169, 0.0000, 0.0000,  ..., 0.0192, 0.0000, 0.0491],
        [0.0297, 0.0000, 0.0000,  ..., 0.0281, 0.0000, 0.1168],
        [0.0480, 0.0000, 0.0000,  ..., 0.0415, 0.0000, 0.2083],
        ...,
        [0.0096, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0070],
        [0.0096, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0070],
        [0.0096, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(464777.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(4077.4927, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(179.3127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8026.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-711.5229, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(440.3133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-540.5294, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0944],
        [ 0.0977],
        [ 0.0960],
        ...,
        [-0.2665],
        [-0.2654],
        [-0.2650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-69635.6172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0021],
        [1.0005],
        ...,
        [1.0004],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366505.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0019],
        [1.0022],
        [1.0005],
        ...,
        [1.0004],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366519.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -7.5974e-05,  1.5649e-03,  ..., -7.4364e-04,
          6.8458e-04,  2.9363e-04],
        [ 0.0000e+00, -7.5974e-05,  1.5649e-03,  ..., -7.4364e-04,
          6.8458e-04,  2.9363e-04],
        [ 0.0000e+00, -7.5974e-05,  1.5649e-03,  ..., -7.4364e-04,
          6.8458e-04,  2.9363e-04],
        ...,
        [ 0.0000e+00, -7.5974e-05,  1.5649e-03,  ..., -7.4364e-04,
          6.8458e-04,  2.9363e-04],
        [ 0.0000e+00, -7.5974e-05,  1.5649e-03,  ..., -7.4364e-04,
          6.8458e-04,  2.9363e-04],
        [ 0.0000e+00, -7.5974e-05,  1.5649e-03,  ..., -7.4364e-04,
          6.8458e-04,  2.9363e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(854.3090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.9574, device='cuda:0')



h[100].sum tensor(60.1667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.5026, device='cuda:0')



h[200].sum tensor(-6.9336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9861, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0027, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0027, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0028, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0028, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0028, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45059.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0126, 0.0000, 0.0000,  ..., 0.0165, 0.0000, 0.0256],
        [0.0118, 0.0000, 0.0000,  ..., 0.0161, 0.0000, 0.0206],
        [0.0103, 0.0000, 0.0000,  ..., 0.0154, 0.0000, 0.0112],
        ...,
        [0.0096, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0069],
        [0.0096, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0069],
        [0.0096, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(348773.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3280.8247, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.7929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6042.7007, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-406.1521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(636.5095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.3582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0140],
        [-0.0157],
        [-0.0545],
        ...,
        [-0.2743],
        [-0.2732],
        [-0.2728]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-100712.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0019],
        [1.0022],
        [1.0005],
        ...,
        [1.0004],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366519.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0022],
        [1.0006],
        ...,
        [1.0004],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366532.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3198e-02,  2.6147e-02,  2.8610e-02,  ...,  3.3368e-04,
          1.3392e-02,  6.5512e-03],
        [-6.6214e-03,  1.3085e-02,  1.5136e-02,  ..., -2.0389e-04,
          7.0621e-03,  3.4356e-03],
        [-6.6125e-03,  1.3067e-02,  1.5118e-02,  ..., -2.0462e-04,
          7.0536e-03,  3.4314e-03],
        ...,
        [ 0.0000e+00, -6.6115e-05,  1.5716e-03,  ..., -7.4509e-04,
          6.8912e-04,  2.9910e-04],
        [ 0.0000e+00, -6.6115e-05,  1.5716e-03,  ..., -7.4509e-04,
          6.8912e-04,  2.9910e-04],
        [ 0.0000e+00, -6.6115e-05,  1.5716e-03,  ..., -7.4509e-04,
          6.8912e-04,  2.9910e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(843.5200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6026, device='cuda:0')



h[100].sum tensor(57.5590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.5859, device='cuda:0')



h[200].sum tensor(-6.3764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3761, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0690, 0.0777,  ..., 0.0003, 0.0363, 0.0177],
        [0.0000, 0.0714, 0.0802,  ..., 0.0005, 0.0375, 0.0183],
        [0.0000, 0.0238, 0.0310,  ..., 0.0000, 0.0144, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0028, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0028, 0.0012],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0028, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41279.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0486, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.2055],
        [0.0445, 0.0000, 0.0000,  ..., 0.0397, 0.0000, 0.1852],
        [0.0302, 0.0000, 0.0000,  ..., 0.0290, 0.0000, 0.1157],
        ...,
        [0.0096, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0068],
        [0.0096, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0068],
        [0.0096, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(320662.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3017.0422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(39.4113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5693.8857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-358.8425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(650.1207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.1718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1068],
        [ 0.1093],
        [ 0.1071],
        ...,
        [-0.2807],
        [-0.2797],
        [-0.2794]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-89807.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0022],
        [1.0006],
        ...,
        [1.0004],
        [1.0000],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366532.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0006],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366546.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3533e-02,  2.6863e-02,  2.9328e-02,  ...,  3.6085e-04,
          1.3754e-02,  6.7351e-03],
        [-9.3788e-03,  1.8600e-02,  2.0806e-02,  ...,  2.0893e-05,
          9.7496e-03,  4.7637e-03],
        [-1.3256e-02,  2.6313e-02,  2.8759e-02,  ...,  3.3819e-04,
          1.3487e-02,  6.6037e-03],
        ...,
        [ 0.0000e+00, -5.3010e-05,  1.5695e-03,  ..., -7.4654e-04,
          7.0908e-04,  3.1347e-04],
        [ 0.0000e+00, -5.3010e-05,  1.5695e-03,  ..., -7.4654e-04,
          7.0908e-04,  3.1347e-04],
        [ 0.0000e+00, -5.3010e-05,  1.5695e-03,  ..., -7.4654e-04,
          7.0908e-04,  3.1347e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(849.1559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1313, device='cuda:0')



h[100].sum tensor(56.8633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.2671, device='cuda:0')



h[200].sum tensor(-6.1474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1639, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0666, 0.0752,  ..., 0.0003, 0.0352, 0.0172],
        [0.0000, 0.0748, 0.0836,  ..., 0.0007, 0.0392, 0.0192],
        [0.0000, 0.0466, 0.0546,  ..., 0.0002, 0.0255, 0.0124],
        ...,
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0029, 0.0013],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0029, 0.0013],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0029, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39787.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0622, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.2777],
        [0.0611, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.2709],
        [0.0519, 0.0000, 0.0000,  ..., 0.0443, 0.0000, 0.2258],
        ...,
        [0.0095, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0071],
        [0.0095, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0071],
        [0.0095, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(310909.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2908.9478, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(35.0035, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5553.6748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-344.3524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(708.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.4211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0921],
        [ 0.0973],
        [ 0.1006],
        ...,
        [-0.2844],
        [-0.2833],
        [-0.2829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91467.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0006],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366546.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0007],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366559.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.2744e-05,  1.5564e-03,  ..., -7.4768e-04,
          7.4048e-04,  3.3877e-04],
        [ 0.0000e+00, -3.2744e-05,  1.5564e-03,  ..., -7.4768e-04,
          7.4048e-04,  3.3877e-04],
        [ 0.0000e+00, -3.2744e-05,  1.5564e-03,  ..., -7.4768e-04,
          7.4048e-04,  3.3877e-04],
        ...,
        [ 0.0000e+00, -3.2744e-05,  1.5564e-03,  ..., -7.4768e-04,
          7.4048e-04,  3.3877e-04],
        [ 0.0000e+00, -3.2744e-05,  1.5564e-03,  ..., -7.4768e-04,
          7.4048e-04,  3.3877e-04],
        [ 0.0000e+00, -3.2744e-05,  1.5564e-03,  ..., -7.4768e-04,
          7.4048e-04,  3.3877e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(831.9030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5618, device='cuda:0')



h[100].sum tensor(53.0743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5286, device='cuda:0')



h[200].sum tensor(-5.3453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0030, 0.0014],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0030, 0.0014],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0030, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0030, 0.0014],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0030, 0.0014],
        [0.0000, 0.0000, 0.0063,  ..., 0.0000, 0.0030, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36594.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0094, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0076],
        [0.0094, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0076],
        [0.0094, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0076],
        ...,
        [0.0094, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0076],
        [0.0094, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0076],
        [0.0094, 0.0000, 0.0000,  ..., 0.0145, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297246.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2770.8157, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.6701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5355.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-312.6026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(804.2315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.1825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3091],
        [-0.2873],
        [-0.2417],
        ...,
        [-0.2826],
        [-0.2815],
        [-0.2811]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95518.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0020],
        [1.0023],
        [1.0007],
        ...,
        [1.0004],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366559.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0024],
        [1.0007],
        ...,
        [1.0005],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366573.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -9.0220e-06,  1.5402e-03,  ..., -7.4815e-04,
          7.7709e-04,  3.6778e-04],
        [ 0.0000e+00, -9.0220e-06,  1.5402e-03,  ..., -7.4815e-04,
          7.7709e-04,  3.6778e-04],
        [ 0.0000e+00, -9.0220e-06,  1.5402e-03,  ..., -7.4815e-04,
          7.7709e-04,  3.6778e-04],
        ...,
        [ 0.0000e+00, -9.0220e-06,  1.5402e-03,  ..., -7.4815e-04,
          7.7709e-04,  3.6778e-04],
        [ 0.0000e+00, -9.0220e-06,  1.5402e-03,  ..., -7.4815e-04,
          7.7709e-04,  3.6778e-04],
        [ 0.0000e+00, -9.0220e-06,  1.5402e-03,  ..., -7.4815e-04,
          7.7709e-04,  3.6778e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(912.1331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.1263, device='cuda:0')



h[100].sum tensor(59.4012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.9403, device='cuda:0')



h[200].sum tensor(-6.4591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6119, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0031, 0.0015],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0031, 0.0015],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0031, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0031, 0.0015],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0031, 0.0015],
        [0.0000, 0.0000, 0.0062,  ..., 0.0000, 0.0031, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39689.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0082],
        [0.0090, 0.0000, 0.0000,  ..., 0.0135, 0.0000, 0.0082],
        [0.0121, 0.0000, 0.0000,  ..., 0.0158, 0.0000, 0.0239],
        ...,
        [0.0090, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0082],
        [0.0090, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0082],
        [0.0090, 0.0000, 0.0000,  ..., 0.0136, 0.0000, 0.0082]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(304262.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2717.4358, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(46.2310, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5561.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-358.2444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(853.6901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.5233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0305],
        [-0.0550],
        [-0.0417],
        ...,
        [-0.2746],
        [-0.2735],
        [-0.2731]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-90517.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0024],
        [1.0007],
        ...,
        [1.0005],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366573.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0024],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366587.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.3303e-05,  1.5313e-03,  ..., -7.5020e-04,
          8.0862e-04,  3.9089e-04],
        [ 0.0000e+00,  1.3303e-05,  1.5313e-03,  ..., -7.5020e-04,
          8.0862e-04,  3.9089e-04],
        [ 0.0000e+00,  1.3303e-05,  1.5313e-03,  ..., -7.5020e-04,
          8.0862e-04,  3.9089e-04],
        ...,
        [ 0.0000e+00,  1.3303e-05,  1.5313e-03,  ..., -7.5020e-04,
          8.0862e-04,  3.9089e-04],
        [ 0.0000e+00,  1.3303e-05,  1.5313e-03,  ..., -7.5020e-04,
          8.0862e-04,  3.9089e-04],
        [ 0.0000e+00,  1.3303e-05,  1.5313e-03,  ..., -7.5020e-04,
          8.0862e-04,  3.9089e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1049.1296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.8709, device='cuda:0')



h[100].sum tensor(72.1057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.5034, device='cuda:0')



h[200].sum tensor(-8.7826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.3279e-05, 6.1327e-03,  ..., 0.0000e+00, 3.2385e-03,
         1.5655e-03],
        [0.0000e+00, 5.3327e-05, 6.1382e-03,  ..., 0.0000e+00, 3.2414e-03,
         1.5669e-03],
        [0.0000e+00, 5.3334e-05, 6.1389e-03,  ..., 0.0000e+00, 3.2418e-03,
         1.5671e-03],
        ...,
        [0.0000e+00, 5.3496e-05, 6.1576e-03,  ..., 0.0000e+00, 3.2517e-03,
         1.5718e-03],
        [0.0000e+00, 5.3495e-05, 6.1575e-03,  ..., 0.0000e+00, 3.2517e-03,
         1.5718e-03],
        [0.0000e+00, 5.3496e-05, 6.1576e-03,  ..., 0.0000e+00, 3.2517e-03,
         1.5719e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51723.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0144, 0.0000, 0.0000,  ..., 0.0159, 0.0000, 0.0449],
        [0.0124, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0325],
        [0.0112, 0.0000, 0.0000,  ..., 0.0143, 0.0000, 0.0242],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0085],
        [0.0087, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0085],
        [0.0087, 0.0000, 0.0000,  ..., 0.0131, 0.0000, 0.0085]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(371069.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3069.4163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(111.0613, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6803.2690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-509.2761, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(782.5001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-466.7401, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1341],
        [ 0.1030],
        [ 0.0616],
        ...,
        [-0.2711],
        [-0.2700],
        [-0.2696]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-67458.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0024],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366587.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0021],
        [1.0025],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366600.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.8173e-02,  3.6388e-02,  3.8984e-02,  ...,  7.4346e-04,
          1.8440e-02,  9.0946e-03],
        [-5.0754e-03,  1.0189e-02,  1.1990e-02,  ..., -3.3334e-04,
          5.7377e-03,  2.8348e-03],
        [-1.5906e-02,  3.1854e-02,  3.4312e-02,  ...,  5.5709e-04,
          1.6241e-02,  8.0112e-03],
        ...,
        [ 0.0000e+00,  3.6936e-05,  1.5290e-03,  ..., -7.5060e-04,
          8.1572e-04,  4.0906e-04],
        [ 0.0000e+00,  3.6936e-05,  1.5290e-03,  ..., -7.5060e-04,
          8.1572e-04,  4.0906e-04],
        [ 0.0000e+00,  3.6936e-05,  1.5290e-03,  ..., -7.5060e-04,
          8.1572e-04,  4.0906e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1182.0449, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-32.3385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-32.8298, device='cuda:0')



h[100].sum tensor(84.5623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-22.2116, device='cuda:0')



h[200].sum tensor(-11.1605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.7821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0544, 0.0620,  ..., 0.0006, 0.0295, 0.0146],
        [0.0000, 0.1336, 0.1437,  ..., 0.0025, 0.0680, 0.0335],
        [0.0000, 0.0851, 0.0936,  ..., 0.0010, 0.0445, 0.0219],
        ...,
        [0.0000, 0.0001, 0.0061,  ..., 0.0000, 0.0033, 0.0016],
        [0.0000, 0.0001, 0.0061,  ..., 0.0000, 0.0033, 0.0016],
        [0.0000, 0.0001, 0.0061,  ..., 0.0000, 0.0033, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65040.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0640, 0.0000, 0.0000,  ..., 0.0523, 0.0000, 0.2923],
        [0.0948, 0.0000, 0.0000,  ..., 0.0760, 0.0000, 0.4400],
        [0.0974, 0.0000, 0.0000,  ..., 0.0778, 0.0000, 0.4532],
        ...,
        [0.0086, 0.0000, 0.0000,  ..., 0.0133, 0.0000, 0.0083],
        [0.0107, 0.0000, 0.0000,  ..., 0.0148, 0.0000, 0.0192],
        [0.0172, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0520]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(460191.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3730.6335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(169.3083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8212.5684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-674.7941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(893.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-512.8191, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0972],
        [ 0.0921],
        [ 0.0908],
        ...,
        [-0.1645],
        [-0.0788],
        [-0.0048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-71512.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0021],
        [1.0025],
        [1.0008],
        ...,
        [1.0005],
        [1.0001],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366600.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0025],
        [1.0009],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366613.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  5.3048e-05,  1.5325e-03,  ..., -7.4729e-04,
          8.1400e-04,  4.2317e-04],
        [ 0.0000e+00,  5.3048e-05,  1.5325e-03,  ..., -7.4729e-04,
          8.1400e-04,  4.2317e-04],
        [ 0.0000e+00,  5.3048e-05,  1.5325e-03,  ..., -7.4729e-04,
          8.1400e-04,  4.2317e-04],
        ...,
        [ 0.0000e+00,  5.3048e-05,  1.5325e-03,  ..., -7.4729e-04,
          8.1400e-04,  4.2317e-04],
        [ 0.0000e+00,  5.3048e-05,  1.5325e-03,  ..., -7.4729e-04,
          8.1400e-04,  4.2317e-04],
        [ 0.0000e+00,  5.3048e-05,  1.5325e-03,  ..., -7.4729e-04,
          8.1400e-04,  4.2317e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1032.8738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.9466, device='cuda:0')



h[100].sum tensor(67.6378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.2015, device='cuda:0')



h[200].sum tensor(-8.0928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0061,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0002, 0.0061,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0002, 0.0061,  ..., 0.0000, 0.0033, 0.0017],
        ...,
        [0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48659.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0121],
        [0.0086, 0.0000, 0.0000,  ..., 0.0139, 0.0000, 0.0088],
        [0.0086, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0082],
        ...,
        [0.0086, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0082],
        [0.0086, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0082],
        [0.0086, 0.0000, 0.0000,  ..., 0.0140, 0.0000, 0.0082]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(364183.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2989.9785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(91.9955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6656.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-471.0202, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1038.8059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-456.2548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1431],
        [-0.2400],
        [-0.3182],
        ...,
        [-0.2933],
        [-0.2921],
        [-0.2918]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-80008.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0025],
        [1.0009],
        ...,
        [1.0005],
        [1.0001],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366613.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0026],
        [1.0009],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366626.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  6.4481e-05,  1.5370e-03,  ..., -7.4371e-04,
          8.1729e-04,  4.3349e-04],
        [ 0.0000e+00,  6.4481e-05,  1.5370e-03,  ..., -7.4371e-04,
          8.1729e-04,  4.3349e-04],
        [ 0.0000e+00,  6.4481e-05,  1.5370e-03,  ..., -7.4371e-04,
          8.1729e-04,  4.3349e-04],
        ...,
        [ 0.0000e+00,  6.4481e-05,  1.5370e-03,  ..., -7.4371e-04,
          8.1729e-04,  4.3349e-04],
        [ 0.0000e+00,  6.4481e-05,  1.5370e-03,  ..., -7.4371e-04,
          8.1729e-04,  4.3349e-04],
        [ 0.0000e+00,  6.4481e-05,  1.5370e-03,  ..., -7.4371e-04,
          8.1729e-04,  4.3349e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(910.5887, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.1038, device='cuda:0')



h[100].sum tensor(54.3494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5719, device='cuda:0')



h[200].sum tensor(-5.7040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0003, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0003, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        ...,
        [0.0000, 0.0003, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0003, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0003, 0.0062,  ..., 0.0000, 0.0033, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39844.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0085, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0078],
        [0.0085, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0078],
        [0.0085, 0.0000, 0.0000,  ..., 0.0146, 0.0000, 0.0079],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0078],
        [0.0085, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0078],
        [0.0085, 0.0000, 0.0000,  ..., 0.0147, 0.0000, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322189.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2619.8152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(44.4502, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5871.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-360.4696, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1233.4368, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-421.1682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4188],
        [-0.4019],
        [-0.3687],
        ...,
        [-0.3018],
        [-0.3006],
        [-0.3003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-94162.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0026],
        [1.0009],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366626.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(566.6909, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0022],
        [1.0026],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366640.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  5.6721e-05,  1.5471e-03,  ..., -7.4105e-04,
          8.2594e-04,  4.3350e-04],
        [ 0.0000e+00,  5.6721e-05,  1.5471e-03,  ..., -7.4105e-04,
          8.2594e-04,  4.3350e-04],
        [ 0.0000e+00,  5.6721e-05,  1.5471e-03,  ..., -7.4105e-04,
          8.2594e-04,  4.3350e-04],
        ...,
        [ 0.0000e+00,  5.6721e-05,  1.5471e-03,  ..., -7.4105e-04,
          8.2594e-04,  4.3350e-04],
        [ 0.0000e+00,  5.6721e-05,  1.5471e-03,  ..., -7.4105e-04,
          8.2594e-04,  4.3350e-04],
        [ 0.0000e+00,  5.6721e-05,  1.5471e-03,  ..., -7.4105e-04,
          8.2594e-04,  4.3350e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(973.7914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.0079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.5625, device='cuda:0')



h[100].sum tensor(60.6489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.9119, device='cuda:0')



h[200].sum tensor(-6.8556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2586, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0105, 0.0167,  ..., 0.0000, 0.0083, 0.0042],
        ...,
        [0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017],
        [0.0000, 0.0002, 0.0062,  ..., 0.0000, 0.0033, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43077.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0000, 0.0000,  ..., 0.0152, 0.0000, 0.0104],
        [0.0119, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0279],
        [0.0184, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0649],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0079],
        [0.0085, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0079],
        [0.0085, 0.0000, 0.0000,  ..., 0.0151, 0.0000, 0.0079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(333450.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2713.1479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.2039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5972.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-401.7742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1272.8969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.0515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1052],
        [-0.0190],
        [ 0.0532],
        ...,
        [-0.3082],
        [-0.3071],
        [-0.3067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-101586.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0022],
        [1.0026],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366640.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366653.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.2090e-02,  2.4351e-02,  2.6607e-02,  ...,  2.6227e-04,
          1.2629e-02,  6.2387e-03],
        [-1.5977e-02,  3.2169e-02,  3.4659e-02,  ...,  5.8474e-04,
          1.6419e-02,  8.1074e-03],
        [-7.8954e-03,  1.5915e-02,  1.7918e-02,  ..., -8.5689e-05,
          8.5390e-03,  4.2222e-03],
        ...,
        [ 0.0000e+00,  3.4288e-05,  1.5610e-03,  ..., -7.4071e-04,
          8.4009e-04,  4.2640e-04],
        [ 0.0000e+00,  3.4288e-05,  1.5610e-03,  ..., -7.4071e-04,
          8.4009e-04,  4.2640e-04],
        [ 0.0000e+00,  3.4288e-05,  1.5610e-03,  ..., -7.4071e-04,
          8.4009e-04,  4.2640e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(841.0608, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1039, device='cuda:0')



h[100].sum tensor(47.6866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.8657, device='cuda:0')



h[200].sum tensor(-4.3631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.9002, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4566e-01, 1.5614e-01,  ..., 3.0354e-03, 7.3914e-02,
         3.6491e-02],
        [0.0000e+00, 1.2174e-01, 1.3150e-01,  ..., 2.1317e-03, 6.2319e-02,
         3.0775e-02],
        [0.0000e+00, 1.0158e-01, 1.1074e-01,  ..., 1.9559e-03, 5.2548e-02,
         2.5957e-02],
        ...,
        [0.0000e+00, 1.3797e-04, 6.2812e-03,  ..., 0.0000e+00, 3.3804e-03,
         1.7157e-03],
        [0.0000e+00, 1.3797e-04, 6.2813e-03,  ..., 0.0000e+00, 3.3804e-03,
         1.7158e-03],
        [0.0000e+00, 1.3797e-04, 6.2813e-03,  ..., 0.0000e+00, 3.3804e-03,
         1.7158e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34506.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1415, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.6473],
        [0.1285, 0.0000, 0.0000,  ..., 0.1050, 0.0000, 0.5867],
        [0.1027, 0.0000, 0.0000,  ..., 0.0855, 0.0000, 0.4633],
        ...,
        [0.0085, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0080],
        [0.0085, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0080],
        [0.0085, 0.0000, 0.0000,  ..., 0.0153, 0.0000, 0.0080]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(300751.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2421.2361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(17.7625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5426.9346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-297.3654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1218.2395, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-396.8536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0270],
        [-0.0272],
        [-0.0270],
        ...,
        [-0.3124],
        [-0.3113],
        [-0.3110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92786.8203, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366653.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366667.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8603e-03,  7.7778e-03,  9.5852e-03,  ..., -4.2096e-04,
          4.6234e-03,  2.2731e-03],
        [-9.4708e-03,  1.9078e-02,  2.1224e-02,  ...,  4.5141e-05,
          1.0102e-02,  4.9742e-03],
        [ 0.0000e+00,  2.9207e-06,  1.5773e-03,  ..., -7.4166e-04,
          8.5372e-04,  4.1459e-04],
        ...,
        [ 0.0000e+00,  2.9207e-06,  1.5773e-03,  ..., -7.4166e-04,
          8.5372e-04,  4.1459e-04],
        [ 0.0000e+00,  2.9207e-06,  1.5773e-03,  ..., -7.4166e-04,
          8.5372e-04,  4.1459e-04],
        [ 0.0000e+00,  2.9207e-06,  1.5773e-03,  ..., -7.4166e-04,
          8.5372e-04,  4.1459e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1107.1089, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.1078, device='cuda:0')



h[100].sum tensor(74.7552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-19.0168, device='cuda:0')



h[200].sum tensor(-9.1858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.6559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.8383e-02, 7.6737e-02,  ..., 2.1641e-04, 3.6570e-02,
         1.8004e-02],
        [0.0000e+00, 2.9983e-02, 3.7193e-02,  ..., 0.0000e+00, 1.7954e-02,
         8.8264e-03],
        [0.0000e+00, 2.5505e-02, 3.2582e-02,  ..., 4.5264e-05, 1.5784e-02,
         7.7564e-03],
        ...,
        [0.0000e+00, 1.1754e-05, 6.3476e-03,  ..., 0.0000e+00, 3.4356e-03,
         1.6684e-03],
        [0.0000e+00, 1.1754e-05, 6.3477e-03,  ..., 0.0000e+00, 3.4357e-03,
         1.6685e-03],
        [0.0000e+00, 1.1754e-05, 6.3477e-03,  ..., 0.0000e+00, 3.4357e-03,
         1.6685e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50894.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0412, 0.0000, 0.0000,  ..., 0.0369, 0.0000, 0.1874],
        [0.0326, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.1444],
        [0.0265, 0.0000, 0.0000,  ..., 0.0267, 0.0000, 0.1096],
        ...,
        [0.0086, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0082],
        [0.0086, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0082],
        [0.0086, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0082]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(368323.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(3005.5171, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(91.6013, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6376.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-505.2639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1228.1621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.4108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1369],
        [ 0.1371],
        [ 0.1190],
        ...,
        [-0.3115],
        [-0.3017],
        [-0.2819]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110262.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0023],
        [1.0027],
        [1.0010],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366667.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0028],
        [1.0011],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366681.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.1888e-05,  1.5880e-03,  ..., -7.4612e-04,
          8.7168e-04,  4.0284e-04],
        [ 0.0000e+00, -2.1888e-05,  1.5880e-03,  ..., -7.4612e-04,
          8.7168e-04,  4.0284e-04],
        [ 0.0000e+00, -2.1888e-05,  1.5880e-03,  ..., -7.4612e-04,
          8.7168e-04,  4.0284e-04],
        ...,
        [ 0.0000e+00, -2.1888e-05,  1.5880e-03,  ..., -7.4612e-04,
          8.7168e-04,  4.0284e-04],
        [ 0.0000e+00, -2.1888e-05,  1.5880e-03,  ..., -7.4612e-04,
          8.7168e-04,  4.0284e-04],
        [ 0.0000e+00, -2.1888e-05,  1.5880e-03,  ..., -7.4612e-04,
          8.7168e-04,  4.0284e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(877.6265, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1533, device='cuda:0')



h[100].sum tensor(52.7643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.2522, device='cuda:0')



h[200].sum tensor(-5.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0064,  ..., 0.0000, 0.0035, 0.0016],
        [0.0000, 0.0000, 0.0064,  ..., 0.0000, 0.0035, 0.0016],
        [0.0000, 0.0000, 0.0064,  ..., 0.0000, 0.0035, 0.0016],
        ...,
        [0.0000, 0.0000, 0.0064,  ..., 0.0000, 0.0035, 0.0016],
        [0.0000, 0.0000, 0.0064,  ..., 0.0000, 0.0035, 0.0016],
        [0.0000, 0.0000, 0.0064,  ..., 0.0000, 0.0035, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36160.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0000, 0.0000,  ..., 0.0158, 0.0000, 0.0138],
        [0.0087, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0089],
        [0.0087, 0.0000, 0.0000,  ..., 0.0155, 0.0000, 0.0083],
        ...,
        [0.0086, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0083],
        [0.0086, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0083],
        [0.0086, 0.0000, 0.0000,  ..., 0.0156, 0.0000, 0.0083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(306508.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2470.9902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.7578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5396.5645, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-321.9669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1114.7637, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.4902, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0938],
        [-0.1395],
        [-0.1706],
        ...,
        [-0.3161],
        [-0.3149],
        [-0.3145]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-95155.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0028],
        [1.0011],
        ...,
        [1.0006],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366681.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0024],
        [1.0028],
        [1.0011],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366695.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.0401e-05,  1.6001e-03,  ..., -7.4940e-04,
          8.8273e-04,  3.9107e-04],
        [-2.3235e-02,  4.6900e-02,  4.9904e-02,  ...,  1.1819e-03,
          2.3653e-02,  1.1611e-02],
        [-1.0947e-02,  2.2074e-02,  2.4357e-02,  ...,  1.6049e-04,
          1.1610e-02,  5.6771e-03],
        ...,
        [-8.3625e-03,  1.6854e-02,  1.8985e-02,  ..., -5.4302e-05,
          9.0777e-03,  4.4293e-03],
        [-5.8699e-03,  1.1818e-02,  1.3803e-02,  ..., -2.6149e-04,
          6.6350e-03,  3.2256e-03],
        [-8.3625e-03,  1.6854e-02,  1.8985e-02,  ..., -5.4302e-05,
          9.0777e-03,  4.4293e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(939.7906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.0228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7988, device='cuda:0')



h[100].sum tensor(59.2593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.7187, device='cuda:0')



h[200].sum tensor(-6.1160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1574, 0.1686,  ..., 0.0035, 0.0800, 0.0392],
        [0.0000, 0.0605, 0.0688,  ..., 0.0002, 0.0329, 0.0161],
        [0.0000, 0.1589, 0.1701,  ..., 0.0035, 0.0807, 0.0396],
        ...,
        [0.0000, 0.0257, 0.0330,  ..., 0.0000, 0.0161, 0.0077],
        [0.0000, 0.0713, 0.0800,  ..., 0.0002, 0.0382, 0.0187],
        [0.0000, 0.0571, 0.0653,  ..., 0.0002, 0.0313, 0.0153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39228.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1094, 0.0000, 0.0000,  ..., 0.0900, 0.0000, 0.5099],
        [0.0841, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.3902],
        [0.1123, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.5246],
        ...,
        [0.0269, 0.0000, 0.0000,  ..., 0.0278, 0.0000, 0.1093],
        [0.0390, 0.0000, 0.0000,  ..., 0.0363, 0.0000, 0.1734],
        [0.0419, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.1897]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(316544., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2587.4016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(37.1797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5481.9678, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-359.7450, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1178.4799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.2433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1558],
        [0.1563],
        [0.1584],
        ...,
        [0.0215],
        [0.0785],
        [0.0970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-105581.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0024],
        [1.0028],
        [1.0011],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366695.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0029],
        [1.0012],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366709.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.3291e-05,  1.6121e-03,  ..., -7.5275e-04,
          8.8492e-04,  3.7882e-04],
        [ 0.0000e+00, -5.3291e-05,  1.6121e-03,  ..., -7.5275e-04,
          8.8492e-04,  3.7882e-04],
        [ 0.0000e+00, -5.3291e-05,  1.6121e-03,  ..., -7.5275e-04,
          8.8492e-04,  3.7882e-04],
        ...,
        [ 0.0000e+00, -5.3291e-05,  1.6121e-03,  ..., -7.5275e-04,
          8.8492e-04,  3.7882e-04],
        [ 0.0000e+00, -5.3291e-05,  1.6121e-03,  ..., -7.5275e-04,
          8.8492e-04,  3.7882e-04],
        [ 0.0000e+00, -5.3291e-05,  1.6121e-03,  ..., -7.5275e-04,
          8.8492e-04,  3.7882e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(876.5635, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3002, device='cuda:0')



h[100].sum tensor(53.2247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3516, device='cuda:0')



h[200].sum tensor(-5.0112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8891, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0036, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0036, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0036, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36784.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0188, 0.0000, 0.0000,  ..., 0.0229, 0.0000, 0.0623],
        [0.0119, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0269],
        [0.0096, 0.0000, 0.0000,  ..., 0.0164, 0.0000, 0.0157],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.0080],
        [0.0088, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.0080],
        [0.0088, 0.0000, 0.0000,  ..., 0.0163, 0.0000, 0.0080]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313475.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2565.3970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(22.2290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5406.1377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-326.6723, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1229.9606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.2364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0917],
        [ 0.0743],
        [ 0.0721],
        ...,
        [-0.3297],
        [-0.3284],
        [-0.3280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-111001.0703, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0029],
        [1.0012],
        ...,
        [1.0007],
        [1.0002],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366709.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0025],
        [1.0030],
        [1.0012],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366722.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.2168e-05,  1.6217e-03,  ..., -7.4940e-04,
          8.7942e-04,  3.7101e-04],
        [ 0.0000e+00, -6.2168e-05,  1.6217e-03,  ..., -7.4940e-04,
          8.7942e-04,  3.7101e-04],
        [ 0.0000e+00, -6.2168e-05,  1.6217e-03,  ..., -7.4940e-04,
          8.7942e-04,  3.7101e-04],
        ...,
        [ 0.0000e+00, -6.2168e-05,  1.6217e-03,  ..., -7.4940e-04,
          8.7942e-04,  3.7101e-04],
        [ 0.0000e+00, -6.2168e-05,  1.6217e-03,  ..., -7.4940e-04,
          8.7942e-04,  3.7101e-04],
        [ 0.0000e+00, -6.2168e-05,  1.6217e-03,  ..., -7.4940e-04,
          8.7942e-04,  3.7101e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(924.5465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.7005, device='cuda:0')



h[100].sum tensor(57.1405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.9756, device='cuda:0')



h[200].sum tensor(-5.7727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9699, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40634.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0148, 0.0000, 0.0000,  ..., 0.0212, 0.0000, 0.0380],
        [0.0104, 0.0000, 0.0000,  ..., 0.0180, 0.0000, 0.0160],
        [0.0107, 0.0000, 0.0000,  ..., 0.0181, 0.0000, 0.0181],
        ...,
        [0.0088, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0078],
        [0.0088, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0078],
        [0.0088, 0.0000, 0.0000,  ..., 0.0169, 0.0000, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(332081.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2655.6338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(38.5146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5842.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-370.6479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1193.2830, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.6982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0696],
        [-0.0755],
        [-0.0336],
        ...,
        [-0.3373],
        [-0.3362],
        [-0.3360]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97955.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0025],
        [1.0030],
        [1.0012],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366722.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0026],
        [1.0030],
        [1.0013],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366735.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.2093e-05,  1.6261e-03,  ..., -7.4652e-04,
          8.7119e-04,  3.6887e-04],
        [ 0.0000e+00, -6.2093e-05,  1.6261e-03,  ..., -7.4652e-04,
          8.7119e-04,  3.6887e-04],
        [ 0.0000e+00, -6.2093e-05,  1.6261e-03,  ..., -7.4652e-04,
          8.7119e-04,  3.6887e-04],
        ...,
        [ 0.0000e+00, -6.2093e-05,  1.6261e-03,  ..., -7.4652e-04,
          8.7119e-04,  3.6887e-04],
        [ 0.0000e+00, -6.2093e-05,  1.6261e-03,  ..., -7.4652e-04,
          8.7119e-04,  3.6887e-04],
        [ 0.0000e+00, -6.2093e-05,  1.6261e-03,  ..., -7.4652e-04,
          8.7119e-04,  3.6887e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(827.3327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2794, device='cuda:0')



h[100].sum tensor(46.6049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.3079, device='cuda:0')



h[200].sum tensor(-3.9786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.5290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0067, 0.0134,  ..., 0.0000, 0.0068, 0.0031],
        [0.0000, 0.0133, 0.0204,  ..., 0.0000, 0.0100, 0.0047],
        ...,
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0035, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31561.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0138, 0.0000, 0.0000,  ..., 0.0198, 0.0000, 0.0402],
        [0.0176, 0.0000, 0.0000,  ..., 0.0217, 0.0000, 0.0631],
        [0.0211, 0.0000, 0.0000,  ..., 0.0237, 0.0000, 0.0835],
        ...,
        [0.0087, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0076],
        [0.0087, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0076],
        [0.0087, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(286774.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2371.6938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-13.1432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5016.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-260.5576, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1495.4546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-364.0413, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0671],
        [ 0.1018],
        [ 0.1203],
        ...,
        [-0.3453],
        [-0.3440],
        [-0.3437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-130798.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0026],
        [1.0030],
        [1.0013],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366735.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0027],
        [1.0031],
        [1.0013],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366748.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.0742e-03,  1.8380e-02,  2.0580e-02,  ...,  1.4522e-05,
          9.8202e-03,  4.7719e-03],
        [-3.7476e-03,  7.5539e-03,  9.4559e-03,  ..., -4.3096e-04,
          4.5691e-03,  2.1859e-03],
        [-5.3266e-03,  1.0763e-02,  1.2754e-02,  ..., -2.9890e-04,
          6.1258e-03,  2.9525e-03],
        ...,
        [ 0.0000e+00, -6.2612e-05,  1.6292e-03,  ..., -7.4438e-04,
          8.7467e-04,  3.6649e-04],
        [ 0.0000e+00, -6.2612e-05,  1.6292e-03,  ..., -7.4438e-04,
          8.7467e-04,  3.6649e-04],
        [ 0.0000e+00, -6.2612e-05,  1.6292e-03,  ..., -7.4438e-04,
          8.7467e-04,  3.6649e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(942.6300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.8902, device='cuda:0')



h[100].sum tensor(56.4186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1039, device='cuda:0')



h[200].sum tensor(-5.7859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0304, 0.0379,  ..., 0.0000, 0.0183, 0.0088],
        [0.0000, 0.0688, 0.0775,  ..., 0.0002, 0.0370, 0.0180],
        [0.0000, 0.0305, 0.0380,  ..., 0.0002, 0.0184, 0.0088],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0035, 0.0015],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0035, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39193.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0333, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.1447],
        [0.0422, 0.0000, 0.0000,  ..., 0.0383, 0.0000, 0.1880],
        [0.0358, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.1536],
        ...,
        [0.0086, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0076],
        [0.0086, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0076],
        [0.0086, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322714.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2621.3970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.4657, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5705.7646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-354.5347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1528.3285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.0401, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1219],
        [ 0.1244],
        [ 0.1202],
        ...,
        [-0.3486],
        [-0.3474],
        [-0.3471]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132931.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0027],
        [1.0031],
        [1.0013],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366748.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0028],
        [1.0032],
        [1.0014],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366762., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -6.1406e-05,  1.6267e-03,  ..., -7.4328e-04,
          8.8615e-04,  3.6499e-04],
        [ 0.0000e+00, -6.1406e-05,  1.6267e-03,  ..., -7.4328e-04,
          8.8615e-04,  3.6499e-04],
        [ 0.0000e+00, -6.1406e-05,  1.6267e-03,  ..., -7.4328e-04,
          8.8615e-04,  3.6499e-04],
        ...,
        [ 0.0000e+00, -6.1406e-05,  1.6267e-03,  ..., -7.4328e-04,
          8.8615e-04,  3.6499e-04],
        [ 0.0000e+00, -6.1406e-05,  1.6267e-03,  ..., -7.4328e-04,
          8.8615e-04,  3.6499e-04],
        [ 0.0000e+00, -6.1406e-05,  1.6267e-03,  ..., -7.4328e-04,
          8.8615e-04,  3.6499e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(936.4675, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.2765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9855, device='cuda:0')



h[100].sum tensor(54.3236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.4918, device='cuda:0')



h[200].sum tensor(-5.4564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0036, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0036, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0036, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0036, 0.0015],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0036, 0.0015],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0036, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40215.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0124, 0.0000, 0.0000,  ..., 0.0194, 0.0000, 0.0320],
        [0.0094, 0.0000, 0.0000,  ..., 0.0180, 0.0000, 0.0141],
        [0.0084, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0076],
        ...,
        [0.0084, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0076],
        [0.0084, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0076],
        [0.0084, 0.0000, 0.0000,  ..., 0.0176, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(333711.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2581.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.1424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6196.9585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-362.6292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1341.9581, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.2223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0092],
        [-0.0830],
        [-0.1679],
        ...,
        [-0.3513],
        [-0.3500],
        [-0.3496]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103098.4141, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0028],
        [1.0032],
        [1.0014],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366762., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 350 loss: tensor(548.8920, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0029],
        [1.0033],
        [1.0015],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366775.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3382e-02,  2.7218e-02,  2.9635e-02,  ...,  3.8030e-04,
          1.4144e-02,  6.8765e-03],
        [-1.7856e-02,  3.6338e-02,  3.9002e-02,  ...,  7.5561e-04,
          1.8568e-02,  9.0549e-03],
        [-2.3012e-02,  4.6848e-02,  4.9796e-02,  ...,  1.1881e-03,
          2.3666e-02,  1.1565e-02],
        ...,
        [ 0.0000e+00, -5.9332e-05,  1.6213e-03,  ..., -7.4224e-04,
          9.1255e-04,  3.6115e-04],
        [ 0.0000e+00, -5.9332e-05,  1.6213e-03,  ..., -7.4224e-04,
          9.1255e-04,  3.6115e-04],
        [ 0.0000e+00, -5.9332e-05,  1.6213e-03,  ..., -7.4224e-04,
          9.1255e-04,  3.6115e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1083.0985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.7204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.5788, device='cuda:0')



h[100].sum tensor(66.1053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.9526, device='cuda:0')



h[200].sum tensor(-7.5978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1224, 0.1324,  ..., 0.0021, 0.0631, 0.0307],
        [0.0000, 0.1341, 0.1445,  ..., 0.0026, 0.0688, 0.0335],
        [0.0000, 0.1595, 0.1705,  ..., 0.0036, 0.0811, 0.0396],
        ...,
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0037, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0037, 0.0015],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0037, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47955.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0883, 0.0000, 0.0000,  ..., 0.0723, 0.0000, 0.3999],
        [0.1049, 0.0000, 0.0000,  ..., 0.0843, 0.0000, 0.4775],
        [0.1192, 0.0000, 0.0000,  ..., 0.0943, 0.0000, 0.5452],
        ...,
        [0.0082, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0076],
        [0.0082, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0076],
        [0.0082, 0.0000, 0.0000,  ..., 0.0174, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363852.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2743.1685, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(75.0169, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6869.6909, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-460.6004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1290.5706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.8145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0575],
        [ 0.0626],
        [ 0.0735],
        ...,
        [-0.3514],
        [-0.3502],
        [-0.3498]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-97975.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0029],
        [1.0033],
        [1.0015],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366775.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0034],
        [1.0015],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366788.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.8740e-05,  1.6206e-03,  ..., -7.3993e-04,
          9.4198e-04,  3.5216e-04],
        [ 0.0000e+00, -5.8740e-05,  1.6206e-03,  ..., -7.3993e-04,
          9.4198e-04,  3.5216e-04],
        [ 0.0000e+00, -5.8740e-05,  1.6206e-03,  ..., -7.3993e-04,
          9.4198e-04,  3.5216e-04],
        ...,
        [ 0.0000e+00, -5.8740e-05,  1.6206e-03,  ..., -7.3993e-04,
          9.4198e-04,  3.5216e-04],
        [ 0.0000e+00, -5.8740e-05,  1.6206e-03,  ..., -7.3993e-04,
          9.4198e-04,  3.5216e-04],
        [ 0.0000e+00, -5.8740e-05,  1.6206e-03,  ..., -7.3993e-04,
          9.4198e-04,  3.5216e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(995.8621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.4754, device='cuda:0')



h[100].sum tensor(56.2219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.4999, device='cuda:0')



h[200].sum tensor(-5.8918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3188, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0235, 0.0308,  ..., 0.0000, 0.0152, 0.0071],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0038, 0.0014],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0038, 0.0014],
        ...,
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0038, 0.0014],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0038, 0.0014],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0038, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42054.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0246, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.1037],
        [0.0151, 0.0000, 0.0000,  ..., 0.0211, 0.0000, 0.0507],
        [0.0108, 0.0000, 0.0000,  ..., 0.0188, 0.0000, 0.0253],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0075],
        [0.0080, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0075],
        [0.0080, 0.0000, 0.0000,  ..., 0.0175, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341549.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2511.8799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(53.2060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6667.3218, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-391.7119, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1228.8728, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-406.0753, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1334],
        [ 0.0753],
        [-0.0064],
        ...,
        [-0.3514],
        [-0.3501],
        [-0.3492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91492.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0034],
        [1.0015],
        ...,
        [1.0007],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366788.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0030],
        [1.0034],
        [1.0016],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366802.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.7879e-05,  1.6251e-03,  ..., -7.3771e-04,
          9.5840e-04,  3.4298e-04],
        [-7.4270e-03,  1.5125e-02,  1.7212e-02,  ..., -1.1228e-04,
          8.3238e-03,  3.9691e-03],
        [-5.3708e-03,  1.0921e-02,  1.2897e-02,  ..., -2.8544e-04,
          6.2847e-03,  2.9652e-03],
        ...,
        [ 0.0000e+00, -5.7879e-05,  1.6251e-03,  ..., -7.3771e-04,
          9.5840e-04,  3.4298e-04],
        [ 0.0000e+00, -5.7879e-05,  1.6251e-03,  ..., -7.3771e-04,
          9.5840e-04,  3.4298e-04],
        [ 0.0000e+00, -5.7879e-05,  1.6251e-03,  ..., -7.3771e-04,
          9.5840e-04,  3.4298e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1004.8719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.6735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.5251, device='cuda:0')



h[100].sum tensor(55.8162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.5335, device='cuda:0')



h[200].sum tensor(-5.8810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3412, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0359, 0.0435,  ..., 0.0001, 0.0213, 0.0100],
        [0.0000, 0.0323, 0.0398,  ..., 0.0001, 0.0196, 0.0091],
        [0.0000, 0.0751, 0.0839,  ..., 0.0005, 0.0404, 0.0194],
        ...,
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0039, 0.0014],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0039, 0.0014],
        [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0039, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43387.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0335, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.1403],
        [0.0349, 0.0000, 0.0000,  ..., 0.0349, 0.0000, 0.1503],
        [0.0472, 0.0000, 0.0000,  ..., 0.0429, 0.0000, 0.2132],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0178, 0.0000, 0.0075],
        [0.0079, 0.0000, 0.0000,  ..., 0.0178, 0.0000, 0.0075],
        [0.0079, 0.0000, 0.0000,  ..., 0.0178, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(352143.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2558.0630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(62.2133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6892.0962, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-410.7349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1237.4021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.8591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1224],
        [ 0.1245],
        [ 0.1279],
        ...,
        [-0.3553],
        [-0.3540],
        [-0.3536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-92184.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0030],
        [1.0034],
        [1.0016],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366802.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0035],
        [1.0017],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366815.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.7796e-05,  1.6324e-03,  ..., -7.3502e-04,
          9.6395e-04,  3.3414e-04],
        [-3.5077e-03,  7.1228e-03,  9.0035e-03,  ..., -4.3904e-04,
          4.4474e-03,  2.0490e-03],
        [-3.5077e-03,  7.1228e-03,  9.0035e-03,  ..., -4.3904e-04,
          4.4474e-03,  2.0490e-03],
        ...,
        [ 0.0000e+00, -5.7796e-05,  1.6324e-03,  ..., -7.3502e-04,
          9.6395e-04,  3.3414e-04],
        [ 0.0000e+00, -5.7796e-05,  1.6324e-03,  ..., -7.3502e-04,
          9.6395e-04,  3.3414e-04],
        [ 0.0000e+00, -5.7796e-05,  1.6324e-03,  ..., -7.3502e-04,
          9.6395e-04,  3.3414e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1028.6818, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.5724, device='cuda:0')



h[100].sum tensor(56.7539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.2420, device='cuda:0')



h[200].sum tensor(-6.1403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8127, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0200,  ..., 0.0000, 0.0102, 0.0045],
        [0.0000, 0.0130, 0.0200,  ..., 0.0000, 0.0102, 0.0045],
        [0.0000, 0.0130, 0.0200,  ..., 0.0000, 0.0102, 0.0045],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42565.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0449],
        [0.0169, 0.0000, 0.0000,  ..., 0.0223, 0.0000, 0.0671],
        [0.0272, 0.0000, 0.0000,  ..., 0.0293, 0.0000, 0.1173],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0184, 0.0000, 0.0075],
        [0.0079, 0.0000, 0.0000,  ..., 0.0184, 0.0000, 0.0075],
        [0.0079, 0.0000, 0.0000,  ..., 0.0184, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344510.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2498.1880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(58.3730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6798.1270, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-401.9678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1260.6154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.4456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0479],
        [ 0.0405],
        [ 0.0857],
        ...,
        [-0.3631],
        [-0.3618],
        [-0.3614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-91974.6953, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0035],
        [1.0017],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366815.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0031],
        [1.0036],
        [1.0017],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366829.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.4122e-05,  1.6401e-03,  ..., -7.3270e-04,
          9.6511e-04,  3.2684e-04],
        [ 0.0000e+00, -5.4122e-05,  1.6401e-03,  ..., -7.3270e-04,
          9.6511e-04,  3.2684e-04],
        [ 0.0000e+00, -5.4122e-05,  1.6401e-03,  ..., -7.3270e-04,
          9.6511e-04,  3.2684e-04],
        ...,
        [ 0.0000e+00, -5.4122e-05,  1.6401e-03,  ..., -7.3270e-04,
          9.6511e-04,  3.2684e-04],
        [ 0.0000e+00, -5.4122e-05,  1.6401e-03,  ..., -7.3270e-04,
          9.6511e-04,  3.2684e-04],
        [ 0.0000e+00, -5.4122e-05,  1.6401e-03,  ..., -7.3270e-04,
          9.6511e-04,  3.2684e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(946.8566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0043, device='cuda:0')



h[100].sum tensor(48.2866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.1514, device='cuda:0')



h[200].sum tensor(-4.7500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.7559, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35890.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0189, 0.0000, 0.0075],
        [0.0079, 0.0000, 0.0000,  ..., 0.0190, 0.0000, 0.0075],
        [0.0080, 0.0000, 0.0000,  ..., 0.0190, 0.0000, 0.0075],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0190, 0.0000, 0.0075],
        [0.0080, 0.0000, 0.0000,  ..., 0.0190, 0.0000, 0.0075],
        [0.0080, 0.0000, 0.0000,  ..., 0.0190, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(311732.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2334.3093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(21.8722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6025.1152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-323.7304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1572.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.2260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5134],
        [-0.5027],
        [-0.4802],
        ...,
        [-0.3730],
        [-0.3717],
        [-0.3713]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131044.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0031],
        [1.0036],
        [1.0017],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366829.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0036],
        [1.0018],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366843., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.2003e-05,  1.6494e-03,  ..., -7.3040e-04,
          9.6986e-04,  3.1748e-04],
        [ 0.0000e+00, -5.2003e-05,  1.6494e-03,  ..., -7.3040e-04,
          9.6986e-04,  3.1748e-04],
        [ 0.0000e+00, -5.2003e-05,  1.6494e-03,  ..., -7.3040e-04,
          9.6986e-04,  3.1748e-04],
        ...,
        [ 0.0000e+00, -5.2003e-05,  1.6494e-03,  ..., -7.3040e-04,
          9.6986e-04,  3.1748e-04],
        [ 0.0000e+00, -5.2003e-05,  1.6494e-03,  ..., -7.3040e-04,
          9.6986e-04,  3.1748e-04],
        [ 0.0000e+00, -5.2003e-05,  1.6494e-03,  ..., -7.3040e-04,
          9.6986e-04,  3.1748e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1005.2493, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9047, device='cuda:0')



h[100].sum tensor(52.8538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1137, device='cuda:0')



h[200].sum tensor(-5.6092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0293, 0.0368,  ..., 0.0002, 0.0182, 0.0083],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0039, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43625.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0285, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.1201],
        [0.0151, 0.0000, 0.0000,  ..., 0.0232, 0.0000, 0.0504],
        [0.0101, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0223],
        ...,
        [0.0081, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0076],
        [0.0081, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0076],
        [0.0081, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(369993.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2776.4233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.1576, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7056.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-418.5563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1499.1031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.7389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0722],
        [-0.0253],
        [-0.1466],
        ...,
        [-0.3820],
        [-0.3807],
        [-0.3802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116650.7109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0036],
        [1.0018],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366843., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0032],
        [1.0037],
        [1.0018],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366856.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.1588e-05,  1.6555e-03,  ..., -7.3001e-04,
          9.8626e-04,  3.0916e-04],
        [ 0.0000e+00, -5.1588e-05,  1.6555e-03,  ..., -7.3001e-04,
          9.8626e-04,  3.0916e-04],
        [ 0.0000e+00, -5.1588e-05,  1.6555e-03,  ..., -7.3001e-04,
          9.8626e-04,  3.0916e-04],
        ...,
        [ 0.0000e+00, -5.1588e-05,  1.6555e-03,  ..., -7.3001e-04,
          9.8626e-04,  3.0916e-04],
        [ 0.0000e+00, -5.1588e-05,  1.6555e-03,  ..., -7.3001e-04,
          9.8626e-04,  3.0916e-04],
        [ 0.0000e+00, -5.1588e-05,  1.6555e-03,  ..., -7.3001e-04,
          9.8626e-04,  3.0916e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(969.9995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9564, device='cuda:0')



h[100].sum tensor(49.0313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7956, device='cuda:0')



h[200].sum tensor(-4.9826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0141, 0.0212,  ..., 0.0000, 0.0108, 0.0046],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0040, 0.0012],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0040, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0040, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0040, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0040, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39316.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0201, 0.0000, 0.0000,  ..., 0.0278, 0.0000, 0.0702],
        [0.0110, 0.0000, 0.0000,  ..., 0.0218, 0.0000, 0.0238],
        [0.0086, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0108],
        ...,
        [0.0080, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0078],
        [0.0080, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0078],
        [0.0080, 0.0000, 0.0000,  ..., 0.0201, 0.0000, 0.0078]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341287.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2503.2646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(38.5810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6657.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-365.5074, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1488.4641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.6518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0539],
        [-0.1828],
        [-0.3060],
        ...,
        [-0.3894],
        [-0.3881],
        [-0.3877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113783.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0032],
        [1.0037],
        [1.0018],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366856.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0033],
        [1.0038],
        [1.0019],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366870., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -5.0519e-05,  1.6585e-03,  ..., -7.3002e-04,
          9.9550e-04,  3.0327e-04],
        [ 0.0000e+00, -5.0519e-05,  1.6585e-03,  ..., -7.3002e-04,
          9.9550e-04,  3.0327e-04],
        [ 0.0000e+00, -5.0519e-05,  1.6585e-03,  ..., -7.3002e-04,
          9.9550e-04,  3.0327e-04],
        ...,
        [ 0.0000e+00, -5.0519e-05,  1.6585e-03,  ..., -7.3002e-04,
          9.9550e-04,  3.0327e-04],
        [ 0.0000e+00, -5.0519e-05,  1.6585e-03,  ..., -7.3002e-04,
          9.9550e-04,  3.0327e-04],
        [ 0.0000e+00, -5.0519e-05,  1.6585e-03,  ..., -7.3002e-04,
          9.9550e-04,  3.0327e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(935.0396, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1401, device='cuda:0')



h[100].sum tensor(45.3319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.5667, device='cuda:0')



h[200].sum tensor(-4.3907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.3668, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0040, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0040, 0.0012],
        [0.0000, 0.0320, 0.0397,  ..., 0.0002, 0.0196, 0.0089],
        ...,
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0040, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0040, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0040, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36110.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0099, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0180],
        [0.0167, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0531],
        [0.0338, 0.0000, 0.0000,  ..., 0.0371, 0.0000, 0.1405],
        ...,
        [0.0080, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0080],
        [0.0080, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0080],
        [0.0080, 0.0000, 0.0000,  ..., 0.0204, 0.0000, 0.0080]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324070.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2379.0500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(23.0268, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6349.1592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-328.3133, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1533.2950, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.8928, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1993],
        [-0.0782],
        [ 0.0168],
        ...,
        [-0.3971],
        [-0.3957],
        [-0.3953]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-120927.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0033],
        [1.0038],
        [1.0019],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366870., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0038],
        [1.0020],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366883.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5104e-03,  9.2481e-03,  1.1200e-02,  ..., -3.4685e-04,
          5.5246e-03,  2.5191e-03],
        [ 0.0000e+00, -4.7170e-05,  1.6567e-03,  ..., -7.3020e-04,
          1.0149e-03,  2.9897e-04],
        [ 0.0000e+00, -4.7170e-05,  1.6567e-03,  ..., -7.3020e-04,
          1.0149e-03,  2.9897e-04],
        ...,
        [ 0.0000e+00, -4.7170e-05,  1.6567e-03,  ..., -7.3020e-04,
          1.0149e-03,  2.9897e-04],
        [ 0.0000e+00, -4.7170e-05,  1.6567e-03,  ..., -7.3020e-04,
          1.0149e-03,  2.9897e-04],
        [ 0.0000e+00, -4.7170e-05,  1.6567e-03,  ..., -7.3020e-04,
          1.0149e-03,  2.9897e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(939.9483, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.0535, device='cuda:0')



h[100].sum tensor(45.0462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.5082, device='cuda:0')



h[200].sum tensor(-4.3767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.3278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0306, 0.0381,  ..., 0.0002, 0.0190, 0.0085],
        [0.0000, 0.0093, 0.0162,  ..., 0.0000, 0.0086, 0.0034],
        [0.0000, 0.0507, 0.0588,  ..., 0.0006, 0.0287, 0.0133],
        ...,
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0041, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0041, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0041, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36133.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0419, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.1837],
        [0.0364, 0.0000, 0.0000,  ..., 0.0395, 0.0000, 0.1532],
        [0.0518, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.2235],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0083],
        [0.0079, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0083],
        [0.0079, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324046.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2321.1536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(24.0140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6422.4917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-329.9484, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1502.8315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.9331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1102],
        [ 0.0996],
        [ 0.0859],
        ...,
        [-0.4011],
        [-0.3998],
        [-0.3996]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-119217.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0038],
        [1.0020],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366883.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0034],
        [1.0039],
        [1.0021],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366897.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -4.3656e-05,  1.6571e-03,  ..., -7.3078e-04,
          1.0322e-03,  2.9482e-04],
        [ 0.0000e+00, -4.3656e-05,  1.6571e-03,  ..., -7.3078e-04,
          1.0322e-03,  2.9482e-04],
        [ 0.0000e+00, -4.3656e-05,  1.6571e-03,  ..., -7.3078e-04,
          1.0322e-03,  2.9482e-04],
        ...,
        [ 0.0000e+00, -4.3656e-05,  1.6571e-03,  ..., -7.3078e-04,
          1.0322e-03,  2.9482e-04],
        [ 0.0000e+00, -4.3656e-05,  1.6571e-03,  ..., -7.3078e-04,
          1.0322e-03,  2.9482e-04],
        [ 0.0000e+00, -4.3656e-05,  1.6571e-03,  ..., -7.3078e-04,
          1.0322e-03,  2.9482e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(960.0524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8231, device='cuda:0')



h[100].sum tensor(46.2355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.0288, device='cuda:0')



h[200].sum tensor(-4.5759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6743, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0041, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0041, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0041, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0042, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0042, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0042, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37632.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0086],
        [0.0083, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0108],
        [0.0118, 0.0000, 0.0000,  ..., 0.0230, 0.0000, 0.0298],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0086],
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0086],
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334910.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2444.1021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(28.6376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6533.8174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-352.1921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1599.8718, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.7554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3856],
        [-0.2582],
        [-0.0971],
        ...,
        [-0.4102],
        [-0.4088],
        [-0.4084]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134874.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0034],
        [1.0039],
        [1.0021],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366897.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(543.9714, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0035],
        [1.0040],
        [1.0022],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366910.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -3.4724e-05,  1.6505e-03,  ..., -7.3114e-04,
          1.0616e-03,  2.9652e-04],
        [-9.4414e-03,  1.9474e-02,  2.1682e-02,  ...,  7.3045e-05,
          1.0528e-02,  4.9570e-03],
        [-4.7727e-03,  9.8271e-03,  1.1777e-02,  ..., -3.2462e-04,
          5.8470e-03,  2.6524e-03],
        ...,
        [ 0.0000e+00, -3.4724e-05,  1.6505e-03,  ..., -7.3114e-04,
          1.0616e-03,  2.9652e-04],
        [ 0.0000e+00, -3.4724e-05,  1.6505e-03,  ..., -7.3114e-04,
          1.0616e-03,  2.9652e-04],
        [ 0.0000e+00, -3.4724e-05,  1.6505e-03,  ..., -7.3114e-04,
          1.0616e-03,  2.9652e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1039.8228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.0466, device='cuda:0')



h[100].sum tensor(52.4573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.2097, device='cuda:0')



h[200].sum tensor(-5.6024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.7546e-02, 3.4971e-02,  ..., 7.3194e-05, 1.7655e-02,
         7.7855e-03],
        [0.0000e+00, 3.1413e-02, 3.8988e-02,  ..., 0.0000e+00, 1.9555e-02,
         8.7197e-03],
        [0.0000e+00, 1.1346e-01, 1.2327e-01,  ..., 1.9231e-03, 5.9385e-02,
         2.8328e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 6.6579e-03,  ..., 0.0000e+00, 4.2824e-03,
         1.1961e-03],
        [0.0000e+00, 0.0000e+00, 6.6583e-03,  ..., 0.0000e+00, 4.2827e-03,
         1.1962e-03],
        [0.0000e+00, 0.0000e+00, 6.6585e-03,  ..., 0.0000e+00, 4.2828e-03,
         1.1962e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39846.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0295, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.1266],
        [0.0467, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.2130],
        [0.0833, 0.0000, 0.0000,  ..., 0.0731, 0.0000, 0.3855],
        ...,
        [0.0078, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0090],
        [0.0078, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0090],
        [0.0078, 0.0000, 0.0000,  ..., 0.0206, 0.0000, 0.0090]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(338149.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2363.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(41.0785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6830.3901, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-378.9080, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1443.1943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.8730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1151],
        [ 0.1419],
        [ 0.1414],
        ...,
        [-0.4138],
        [-0.4124],
        [-0.4120]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-113810.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0035],
        [1.0040],
        [1.0022],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366910.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0036],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366923.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.8085e-05,  1.6493e-03,  ..., -7.3129e-04,
          1.0663e-03,  2.9766e-04],
        [ 0.0000e+00, -2.8085e-05,  1.6493e-03,  ..., -7.3129e-04,
          1.0663e-03,  2.9766e-04],
        [ 0.0000e+00, -2.8085e-05,  1.6493e-03,  ..., -7.3129e-04,
          1.0663e-03,  2.9766e-04],
        ...,
        [ 0.0000e+00, -2.8085e-05,  1.6493e-03,  ..., -7.3129e-04,
          1.0663e-03,  2.9766e-04],
        [ 0.0000e+00, -2.8085e-05,  1.6493e-03,  ..., -7.3129e-04,
          1.0663e-03,  2.9766e-04],
        [ 0.0000e+00, -2.8085e-05,  1.6493e-03,  ..., -7.3129e-04,
          1.0663e-03,  2.9766e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1111.4174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.1258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3270, device='cuda:0')



h[100].sum tensor(58.2734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.4292, device='cuda:0')



h[200].sum tensor(-6.5479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6028, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0043, 0.0012],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0043, 0.0012],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0043, 0.0012],
        ...,
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0043, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0043, 0.0012],
        [0.0000, 0.0000, 0.0067,  ..., 0.0000, 0.0043, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48173.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0092],
        [0.0078, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0093],
        [0.0079, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0093],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0093],
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0093],
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389992.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2759.1597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(79.9266, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7743.8774, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-482.1388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1414.2456, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.5078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3256],
        [-0.3809],
        [-0.4260],
        ...,
        [-0.4231],
        [-0.4216],
        [-0.4212]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110642.3516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0036],
        [1.0041],
        [1.0022],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366923.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0042],
        [1.0023],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366935.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -1.3736e-05,  1.6442e-03,  ..., -7.3047e-04,
          1.0593e-03,  3.0694e-04],
        [ 0.0000e+00, -1.3736e-05,  1.6442e-03,  ..., -7.3047e-04,
          1.0593e-03,  3.0694e-04],
        [ 0.0000e+00, -1.3736e-05,  1.6442e-03,  ..., -7.3047e-04,
          1.0593e-03,  3.0694e-04],
        ...,
        [ 0.0000e+00, -1.3736e-05,  1.6442e-03,  ..., -7.3047e-04,
          1.0593e-03,  3.0694e-04],
        [ 0.0000e+00, -1.3736e-05,  1.6442e-03,  ..., -7.3047e-04,
          1.0593e-03,  3.0694e-04],
        [ 0.0000e+00, -1.3736e-05,  1.6442e-03,  ..., -7.3047e-04,
          1.0593e-03,  3.0694e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1200.1770, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.6119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.9253, device='cuda:0')



h[100].sum tensor(65.1294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.8636, device='cuda:0')



h[200].sum tensor(-7.6627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.2229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0138,  ..., 0.0000, 0.0077, 0.0029],
        [0.0000, 0.0141, 0.0211,  ..., 0.0000, 0.0111, 0.0046],
        [0.0000, 0.0070, 0.0139,  ..., 0.0000, 0.0077, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0043, 0.0012],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0043, 0.0012],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0043, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50706.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0154, 0.0000, 0.0000,  ..., 0.0242, 0.0000, 0.0606],
        [0.0165, 0.0000, 0.0000,  ..., 0.0247, 0.0000, 0.0683],
        [0.0164, 0.0000, 0.0000,  ..., 0.0249, 0.0000, 0.0661],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0095],
        [0.0079, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0095],
        [0.0079, 0.0000, 0.0000,  ..., 0.0209, 0.0000, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(397970.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2859.3354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(90.1542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7787.2134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-517.7457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1594.0986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-439.9521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1790],
        [ 0.1825],
        [ 0.1812],
        ...,
        [-0.4311],
        [-0.4296],
        [-0.4292]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-129385.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0042],
        [1.0023],
        ...,
        [1.0009],
        [1.0004],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366935.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0037],
        [1.0043],
        [1.0024],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366948.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00, -2.5591e-06,  1.6433e-03,  ..., -7.2975e-04,
          1.0487e-03,  3.1534e-04],
        [ 0.0000e+00, -2.5591e-06,  1.6433e-03,  ..., -7.2975e-04,
          1.0487e-03,  3.1534e-04],
        [ 0.0000e+00, -2.5591e-06,  1.6433e-03,  ..., -7.2975e-04,
          1.0487e-03,  3.1534e-04],
        ...,
        [ 0.0000e+00, -2.5591e-06,  1.6433e-03,  ..., -7.2975e-04,
          1.0487e-03,  3.1534e-04],
        [ 0.0000e+00, -2.5591e-06,  1.6433e-03,  ..., -7.2975e-04,
          1.0487e-03,  3.1534e-04],
        [ 0.0000e+00, -2.5591e-06,  1.6433e-03,  ..., -7.2975e-04,
          1.0487e-03,  3.1534e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1032.0021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7732, device='cuda:0')



h[100].sum tensor(50.3323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3482, device='cuda:0')



h[200].sum tensor(-5.0851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5524, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0042, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0042, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0042, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0042, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0042, 0.0013],
        [0.0000, 0.0000, 0.0066,  ..., 0.0000, 0.0042, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40226.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0248, 0.0000, 0.0000,  ..., 0.0317, 0.0000, 0.0969],
        [0.0179, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0622],
        [0.0153, 0.0000, 0.0000,  ..., 0.0256, 0.0000, 0.0474],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0097],
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0097],
        [0.0079, 0.0000, 0.0000,  ..., 0.0208, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(349328.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2488.6311, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(36.1155, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6986.7319, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-385.7401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1751.0598, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.9121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0208],
        [ 0.0107],
        [ 0.0049],
        ...,
        [-0.4379],
        [-0.4363],
        [-0.4351]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-133973.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0037],
        [1.0043],
        [1.0024],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366948.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0038],
        [1.0043],
        [1.0025],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366960.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6527e-02,  5.5126e-02,  5.8231e-02,  ...,  1.5445e-03,
          2.7804e-02,  1.3503e-02],
        [-1.9047e-02,  3.9586e-02,  4.2274e-02,  ...,  9.0342e-04,
          2.0263e-02,  9.7877e-03],
        [-1.0049e-02,  2.0893e-02,  2.3078e-02,  ...,  1.3228e-04,
          1.1191e-02,  5.3189e-03],
        ...,
        [ 0.0000e+00,  1.4373e-05,  1.6387e-03,  ..., -7.2898e-04,
          1.0585e-03,  3.2787e-04],
        [ 0.0000e+00,  1.4373e-05,  1.6387e-03,  ..., -7.2898e-04,
          1.0585e-03,  3.2787e-04],
        [ 0.0000e+00,  1.4373e-05,  1.6387e-03,  ..., -7.2898e-04,
          1.0585e-03,  3.2787e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1034.1182, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.4199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3030, device='cuda:0')



h[100].sum tensor(50.1514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.0301, device='cuda:0')



h[200].sum tensor(-4.9788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3406, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.6923e-01, 1.8028e-01,  ..., 4.0563e-03, 8.6341e-02,
         4.1755e-02],
        [0.0000e+00, 1.5571e-01, 1.6642e-01,  ..., 3.4941e-03, 7.9790e-02,
         3.8527e-02],
        [0.0000e+00, 1.2885e-01, 1.3884e-01,  ..., 2.3856e-03, 6.6756e-02,
         3.2106e-02],
        ...,
        [0.0000e+00, 5.8013e-05, 6.6141e-03,  ..., 0.0000e+00, 4.2722e-03,
         1.3233e-03],
        [0.0000e+00, 5.8019e-05, 6.6147e-03,  ..., 0.0000e+00, 4.2726e-03,
         1.3234e-03],
        [0.0000e+00, 5.8020e-05, 6.6148e-03,  ..., 0.0000e+00, 4.2727e-03,
         1.3235e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38370.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1237, 0.0000, 0.0000,  ..., 0.1020, 0.0000, 0.5607],
        [0.1086, 0.0000, 0.0000,  ..., 0.0911, 0.0000, 0.4918],
        [0.0882, 0.0000, 0.0000,  ..., 0.0760, 0.0000, 0.4005],
        ...,
        [0.0079, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0100],
        [0.0079, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0100],
        [0.0079, 0.0000, 0.0000,  ..., 0.0207, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(336158.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2401.5723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(27.2887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6746.7163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-369.4520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1842.0564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-394.4065, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0190],
        [ 0.0401],
        [ 0.0646],
        ...,
        [-0.4446],
        [-0.4431],
        [-0.4427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142453.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0038],
        [1.0043],
        [1.0025],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366960.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0039],
        [1.0044],
        [1.0025],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366973.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.9757e-02,  6.1940e-02,  6.5208e-02,  ...,  1.8257e-03,
          3.1133e-02,  1.5140e-02],
        [-9.0652e-03,  1.8892e-02,  2.1002e-02,  ...,  4.9513e-05,
          1.0238e-02,  4.8465e-03],
        [ 0.0000e+00,  3.2261e-05,  1.6354e-03,  ..., -7.2862e-04,
          1.0842e-03,  3.3692e-04],
        ...,
        [ 0.0000e+00,  3.2261e-05,  1.6354e-03,  ..., -7.2862e-04,
          1.0842e-03,  3.3692e-04],
        [ 0.0000e+00,  3.2261e-05,  1.6354e-03,  ..., -7.2862e-04,
          1.0842e-03,  3.3692e-04],
        [ 0.0000e+00,  3.2261e-05,  1.6354e-03,  ..., -7.2862e-04,
          1.0842e-03,  3.3692e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1037.8087, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3333, device='cuda:0')



h[100].sum tensor(50.3781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.0505, device='cuda:0')



h[200].sum tensor(-4.9157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3543, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.0428e-01, 1.1350e-01,  ..., 1.3762e-03, 5.4898e-02,
         2.6254e-02],
        [0.0000e+00, 1.2387e-01, 1.3364e-01,  ..., 2.9113e-03, 6.4416e-02,
         3.0942e-02],
        [0.0000e+00, 4.9627e-02, 5.7396e-02,  ..., 5.7860e-04, 2.8379e-02,
         1.3189e-02],
        ...,
        [0.0000e+00, 1.3023e-04, 6.6018e-03,  ..., 0.0000e+00, 4.3764e-03,
         1.3600e-03],
        [0.0000e+00, 1.3024e-04, 6.6025e-03,  ..., 0.0000e+00, 4.3769e-03,
         1.3602e-03],
        [0.0000e+00, 1.3025e-04, 6.6026e-03,  ..., 0.0000e+00, 4.3770e-03,
         1.3602e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39414.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0897, 0.0000, 0.0000,  ..., 0.0766, 0.0000, 0.4125],
        [0.0900, 0.0000, 0.0000,  ..., 0.0770, 0.0000, 0.4129],
        [0.0672, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.3055],
        ...,
        [0.0077, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0103],
        [0.0077, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0103],
        [0.0077, 0.0000, 0.0000,  ..., 0.0203, 0.0000, 0.0103]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(344099.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2386.5757, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(34.8442, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7037.8862, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-384.7544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1784.7871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.3590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0479],
        [ 0.0618],
        [ 0.0778],
        ...,
        [-0.4484],
        [-0.4469],
        [-0.4465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132730.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0039],
        [1.0044],
        [1.0025],
        ...,
        [1.0009],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366973.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0026],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366985.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04],
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04],
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04],
        ...,
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04],
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04],
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1239.9647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.3872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.3150, device='cuda:0')



h[100].sum tensor(66.2319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.1273, device='cuda:0')



h[200].sum tensor(-7.5128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0065,  ..., 0.0000, 0.0045, 0.0014],
        [0.0000, 0.0003, 0.0065,  ..., 0.0000, 0.0045, 0.0014],
        [0.0000, 0.0003, 0.0065,  ..., 0.0000, 0.0045, 0.0014],
        ...,
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51343.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0074, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0103],
        [0.0074, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0104],
        [0.0074, 0.0000, 0.0000,  ..., 0.0196, 0.0000, 0.0104],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0104],
        [0.0074, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0104],
        [0.0074, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405392.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2721.3843, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(96.7612, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8380.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-532.9026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1603.1267, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.9000, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5352],
        [-0.5620],
        [-0.5730],
        ...,
        [-0.4438],
        [-0.4422],
        [-0.4417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-109660.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0026],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366985.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0026],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366985.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0754e-02,  2.2472e-02,  2.4633e-02,  ...,  1.9662e-04,
          1.2008e-02,  5.7009e-03],
        [-2.2408e-02,  4.6751e-02,  4.9565e-02,  ...,  1.1987e-03,
          2.3795e-02,  1.1508e-02],
        [-1.7904e-02,  3.7368e-02,  3.9930e-02,  ...,  8.1142e-04,
          1.9240e-02,  9.2637e-03],
        ...,
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04],
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04],
        [ 0.0000e+00,  6.8710e-05,  1.6278e-03,  ..., -7.2796e-04,
          1.1322e-03,  3.4290e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1187.1716, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.1139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.3467, device='cuda:0')



h[100].sum tensor(61.8616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.1191, device='cuda:0')



h[200].sum tensor(-6.7826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1152, 0.1246,  ..., 0.0019, 0.0603, 0.0289],
        [0.0000, 0.1226, 0.1321,  ..., 0.0021, 0.0639, 0.0306],
        [0.0000, 0.1515, 0.1618,  ..., 0.0033, 0.0780, 0.0375],
        ...,
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48749.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0731, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.3410],
        [0.0882, 0.0000, 0.0000,  ..., 0.0753, 0.0000, 0.4150],
        [0.0992, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.4680],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0104],
        [0.0074, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0104],
        [0.0074, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(388093.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2582.0142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(85.3316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8123.8018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-500.0055, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1556.3127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-437.4347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0987],
        [ 0.0820],
        [ 0.0647],
        ...,
        [-0.4458],
        [-0.4443],
        [-0.4439]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-103896.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0026],
        ...,
        [1.0008],
        [1.0003],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366985.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0027],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(366998.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.1926e-03,  8.8298e-03,  1.0611e-02,  ..., -3.7016e-04,
          5.4299e-03,  2.4376e-03],
        [-1.3892e-02,  2.9064e-02,  3.1391e-02,  ...,  4.6455e-04,
          1.5257e-02,  7.2786e-03],
        [ 0.0000e+00,  8.3770e-05,  1.6295e-03,  ..., -7.3096e-04,
          1.1823e-03,  3.4510e-04],
        ...,
        [ 0.0000e+00,  8.3770e-05,  1.6295e-03,  ..., -7.3096e-04,
          1.1823e-03,  3.4510e-04],
        [ 0.0000e+00,  8.3770e-05,  1.6295e-03,  ..., -7.3096e-04,
          1.1823e-03,  3.4510e-04],
        [ 0.0000e+00,  8.3770e-05,  1.6295e-03,  ..., -7.3096e-04,
          1.1823e-03,  3.4510e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1332.2216, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.9864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.7280, device='cuda:0')



h[100].sum tensor(73.9540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-19.4364, device='cuda:0')



h[200].sum tensor(-8.6468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0935, 0.1022,  ..., 0.0014, 0.0500, 0.0237],
        [0.0000, 0.0386, 0.0459,  ..., 0.0000, 0.0233, 0.0105],
        [0.0000, 0.0631, 0.0710,  ..., 0.0005, 0.0352, 0.0164],
        ...,
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0048, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0048, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0048, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56069.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0674, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.3319],
        [0.0525, 0.0000, 0.0000,  ..., 0.0491, 0.0000, 0.2586],
        [0.0466, 0.0000, 0.0000,  ..., 0.0450, 0.0000, 0.2278],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0191, 0.0000, 0.0105],
        [0.0070, 0.0000, 0.0000,  ..., 0.0191, 0.0000, 0.0105],
        [0.0070, 0.0000, 0.0000,  ..., 0.0191, 0.0000, 0.0105]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(420476.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2672.3188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(124.1724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8835.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-592.9982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1477.1536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.4557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1765],
        [ 0.1751],
        [ 0.1687],
        ...,
        [-0.4426],
        [-0.4411],
        [-0.4407]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-93923.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0040],
        [1.0045],
        [1.0027],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(366998.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0041],
        [1.0046],
        [1.0027],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367010., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.9420e-05,  1.6408e-03,  ..., -7.3462e-04,
          1.1872e-03,  3.4177e-04],
        [ 0.0000e+00,  7.9420e-05,  1.6408e-03,  ..., -7.3462e-04,
          1.1872e-03,  3.4177e-04],
        [-4.4573e-03,  9.3898e-03,  1.1203e-02,  ..., -3.5070e-04,
          5.7093e-03,  2.5696e-03],
        ...,
        [ 0.0000e+00,  7.9420e-05,  1.6408e-03,  ..., -7.3462e-04,
          1.1872e-03,  3.4177e-04],
        [ 0.0000e+00,  7.9420e-05,  1.6408e-03,  ..., -7.3462e-04,
          1.1872e-03,  3.4177e-04],
        [ 0.0000e+00,  7.9420e-05,  1.6408e-03,  ..., -7.3462e-04,
          1.1872e-03,  3.4177e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1294.8647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.3967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.8988, device='cuda:0')



h[100].sum tensor(71.0600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.1988, device='cuda:0')



h[200].sum tensor(-8.1166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.1115, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0048, 0.0014],
        [0.0000, 0.0097, 0.0162,  ..., 0.0000, 0.0093, 0.0036],
        [0.0000, 0.0319, 0.0390,  ..., 0.0002, 0.0201, 0.0089],
        ...,
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0048, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0048, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0048, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55452.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0111, 0.0000, 0.0000,  ..., 0.0216, 0.0000, 0.0354],
        [0.0183, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0801],
        [0.0291, 0.0000, 0.0000,  ..., 0.0327, 0.0000, 0.1409],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0106],
        [0.0071, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0106],
        [0.0071, 0.0000, 0.0000,  ..., 0.0195, 0.0000, 0.0106]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425009.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2749.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(114.7842, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8849.6143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-585.5843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1561.9429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-459.8711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0370],
        [ 0.0924],
        [ 0.1751],
        ...,
        [-0.4534],
        [-0.4519],
        [-0.4515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-104781.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0041],
        [1.0046],
        [1.0027],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367010., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 450 loss: tensor(502.9626, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0042],
        [1.0047],
        [1.0028],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367020.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.3267e-05,  1.6471e-03,  ..., -7.3748e-04,
          1.1622e-03,  3.4617e-04],
        [ 0.0000e+00,  7.3267e-05,  1.6471e-03,  ..., -7.3748e-04,
          1.1622e-03,  3.4617e-04],
        [ 0.0000e+00,  7.3267e-05,  1.6471e-03,  ..., -7.3748e-04,
          1.1622e-03,  3.4617e-04],
        ...,
        [ 0.0000e+00,  7.3267e-05,  1.6471e-03,  ..., -7.3748e-04,
          1.1622e-03,  3.4617e-04],
        [ 0.0000e+00,  7.3267e-05,  1.6471e-03,  ..., -7.3748e-04,
          1.1622e-03,  3.4617e-04],
        [ 0.0000e+00,  7.3267e-05,  1.6471e-03,  ..., -7.3748e-04,
          1.1622e-03,  3.4617e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1086.9131, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9942, device='cuda:0')



h[100].sum tensor(54.7322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1743, device='cuda:0')



h[200].sum tensor(-5.4192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1021, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0047, 0.0014],
        ...,
        [0.0000, 0.0003, 0.0067,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0003, 0.0067,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0003, 0.0067,  ..., 0.0000, 0.0047, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45119.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0202, 0.0000, 0.0209],
        [0.0078, 0.0000, 0.0000,  ..., 0.0197, 0.0000, 0.0166],
        [0.0106, 0.0000, 0.0000,  ..., 0.0214, 0.0000, 0.0331],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0108],
        [0.0074, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0108],
        [0.0074, 0.0000, 0.0000,  ..., 0.0199, 0.0000, 0.0108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387550.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2539.1597, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(56.6172, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8074.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-456.7965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1738.3755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.6094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0227],
        [ 0.0051],
        [ 0.0359],
        ...,
        [-0.4686],
        [-0.4670],
        [-0.4666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-116375.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0042],
        [1.0047],
        [1.0028],
        ...,
        [1.0008],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367020.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0048],
        [1.0029],
        ...,
        [1.0007],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367032.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.5112e-05,  1.6547e-03,  ..., -7.3697e-04,
          1.1537e-03,  3.4283e-04],
        [ 0.0000e+00,  7.5112e-05,  1.6547e-03,  ..., -7.3697e-04,
          1.1537e-03,  3.4283e-04],
        [ 0.0000e+00,  7.5112e-05,  1.6547e-03,  ..., -7.3697e-04,
          1.1537e-03,  3.4283e-04],
        ...,
        [ 0.0000e+00,  7.5112e-05,  1.6547e-03,  ..., -7.3697e-04,
          1.1537e-03,  3.4283e-04],
        [ 0.0000e+00,  7.5112e-05,  1.6547e-03,  ..., -7.3697e-04,
          1.1537e-03,  3.4283e-04],
        [ 0.0000e+00,  7.5112e-05,  1.6547e-03,  ..., -7.3697e-04,
          1.1537e-03,  3.4283e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1028.7668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9940, device='cuda:0')



h[100].sum tensor(50.0399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8210, device='cuda:0')



h[200].sum tensor(-4.7025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014],
        [0.0000, 0.0003, 0.0066,  ..., 0.0000, 0.0046, 0.0014],
        ...,
        [0.0000, 0.0003, 0.0067,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0003, 0.0067,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0003, 0.0067,  ..., 0.0000, 0.0047, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38478.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0134, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0406],
        [0.0106, 0.0000, 0.0000,  ..., 0.0221, 0.0000, 0.0284],
        [0.0145, 0.0000, 0.0000,  ..., 0.0243, 0.0000, 0.0525],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0107],
        [0.0076, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0108],
        [0.0076, 0.0000, 0.0000,  ..., 0.0205, 0.0000, 0.0108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(339629.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2288.7163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(18.4606, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7078.7002, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-377.5237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1986.5341, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.5729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0016],
        [ 0.0075],
        [ 0.0352],
        ...,
        [-0.4788],
        [-0.4766],
        [-0.4721]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-145620.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0048],
        [1.0029],
        ...,
        [1.0007],
        [1.0002],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367032.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0043],
        [1.0049],
        [1.0029],
        ...,
        [1.0007],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367043.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.8462e-03,  1.0260e-02,  1.2099e-02,  ..., -3.1714e-04,
          6.1041e-03,  2.7680e-03],
        [-3.8540e-03,  8.1788e-03,  9.9620e-03,  ..., -4.0300e-04,
          5.0934e-03,  2.2700e-03],
        [-8.7002e-03,  1.8343e-02,  2.0402e-02,  ...,  1.6419e-05,
          1.0030e-02,  4.7025e-03],
        ...,
        [ 0.0000e+00,  9.5609e-05,  1.6591e-03,  ..., -7.3656e-04,
          1.1671e-03,  3.3560e-04],
        [ 0.0000e+00,  9.5609e-05,  1.6591e-03,  ..., -7.3656e-04,
          1.1671e-03,  3.3560e-04],
        [ 0.0000e+00,  9.5609e-05,  1.6591e-03,  ..., -7.3656e-04,
          1.1671e-03,  3.3560e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(971.1504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.4218, device='cuda:0')



h[100].sum tensor(44.9522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.0808, device='cuda:0')



h[200].sum tensor(-3.9527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.0434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0307, 0.0378,  ..., 0.0002, 0.0194, 0.0086],
        [0.0000, 0.0669, 0.0750,  ..., 0.0002, 0.0370, 0.0173],
        [0.0000, 0.0308, 0.0379,  ..., 0.0000, 0.0195, 0.0086],
        ...,
        [0.0000, 0.0004, 0.0067,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0004, 0.0067,  ..., 0.0000, 0.0047, 0.0014],
        [0.0000, 0.0004, 0.0067,  ..., 0.0000, 0.0047, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35591.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0293, 0.0000, 0.0000,  ..., 0.0335, 0.0000, 0.1358],
        [0.0391, 0.0000, 0.0000,  ..., 0.0398, 0.0000, 0.1883],
        [0.0309, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.1470],
        ...,
        [0.0076, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0107],
        [0.0076, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0107],
        [0.0076, 0.0000, 0.0000,  ..., 0.0210, 0.0000, 0.0107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(330172., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2191.3965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(3.8431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6933.6831, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-340.7151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1985.7251, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.1399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1312],
        [ 0.1443],
        [ 0.1246],
        ...,
        [-0.4819],
        [-0.4745],
        [-0.4461]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-141620.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0043],
        [1.0049],
        [1.0029],
        ...,
        [1.0007],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367043.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0044],
        [1.0050],
        [1.0030],
        ...,
        [1.0007],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367055.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [-0.0053,  0.0113,  0.0131,  ..., -0.0003,  0.0066,  0.0030],
        ...,
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1147.9529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.4723, device='cuda:0')



h[100].sum tensor(58.5606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.5275, device='cuda:0')



h[200].sum tensor(-6.2342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0333, 0.0404,  ..., 0.0006, 0.0208, 0.0091],
        [0.0000, 0.0117, 0.0182,  ..., 0.0000, 0.0103, 0.0040],
        [0.0000, 0.0199, 0.0266,  ..., 0.0000, 0.0142, 0.0059],
        ...,
        [0.0000, 0.0005, 0.0067,  ..., 0.0000, 0.0048, 0.0013],
        [0.0000, 0.0005, 0.0067,  ..., 0.0000, 0.0048, 0.0013],
        [0.0000, 0.0005, 0.0067,  ..., 0.0000, 0.0048, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46019.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0351, 0.0000, 0.0000,  ..., 0.0404, 0.0000, 0.1507],
        [0.0237, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0990],
        [0.0263, 0.0000, 0.0000,  ..., 0.0329, 0.0000, 0.1180],
        ...,
        [0.0074, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0107],
        [0.0074, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0107],
        [0.0074, 0.0000, 0.0000,  ..., 0.0215, 0.0000, 0.0107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381097.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2538.2405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.9697, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7796.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-469.2551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1909.2755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.3461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1052],
        [ 0.1232],
        [ 0.1419],
        ...,
        [-0.4886],
        [-0.4870],
        [-0.4866]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142692.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0044],
        [1.0050],
        [1.0030],
        ...,
        [1.0007],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367055.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0050],
        [1.0031],
        ...,
        [1.0007],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367066.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [-0.0055,  0.0118,  0.0136,  ..., -0.0003,  0.0069,  0.0031],
        ...,
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003],
        [ 0.0000,  0.0001,  0.0017,  ..., -0.0007,  0.0012,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1187.6530, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.2272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8518, device='cuda:0')



h[100].sum tensor(61.0745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.4608, device='cuda:0')



h[200].sum tensor(-6.6968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2893, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0067,  ..., 0.0000, 0.0050, 0.0012],
        [0.0000, 0.0122, 0.0187,  ..., 0.0000, 0.0107, 0.0040],
        [0.0000, 0.0101, 0.0165,  ..., 0.0000, 0.0096, 0.0035],
        ...,
        [0.0000, 0.0005, 0.0068,  ..., 0.0000, 0.0050, 0.0012],
        [0.0000, 0.0005, 0.0068,  ..., 0.0000, 0.0050, 0.0012],
        [0.0000, 0.0005, 0.0068,  ..., 0.0000, 0.0050, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50224.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0225, 0.0000, 0.0241],
        [0.0127, 0.0000, 0.0000,  ..., 0.0247, 0.0000, 0.0488],
        [0.0158, 0.0000, 0.0000,  ..., 0.0263, 0.0000, 0.0685],
        ...,
        [0.0072, 0.0000, 0.0000,  ..., 0.0220, 0.0000, 0.0108],
        [0.0072, 0.0000, 0.0000,  ..., 0.0220, 0.0000, 0.0108],
        [0.0072, 0.0000, 0.0000,  ..., 0.0220, 0.0000, 0.0108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(408648.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2645.4148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(78.4249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8329.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-518.8654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1773.4998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.3198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1662],
        [-0.0034],
        [ 0.1196],
        ...,
        [-0.4890],
        [-0.4874],
        [-0.4870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128080.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0050],
        [1.0031],
        ...,
        [1.0007],
        [1.0001],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367066.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0045],
        [1.0051],
        [1.0032],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367077.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1300.8479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.5249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.1663, device='cuda:0')



h[100].sum tensor(68.7680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.3798, device='cuda:0')



h[200].sum tensor(-8.0318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.2320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0067,  ..., 0.0000, 0.0051, 0.0012],
        [0.0000, 0.0006, 0.0067,  ..., 0.0000, 0.0051, 0.0012],
        [0.0000, 0.0006, 0.0067,  ..., 0.0000, 0.0051, 0.0012],
        ...,
        [0.0000, 0.0006, 0.0068,  ..., 0.0000, 0.0051, 0.0012],
        [0.0000, 0.0006, 0.0068,  ..., 0.0000, 0.0051, 0.0012],
        [0.0000, 0.0006, 0.0068,  ..., 0.0000, 0.0051, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54855.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0070, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0108],
        [0.0070, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0108],
        [0.0070, 0.0000, 0.0000,  ..., 0.0224, 0.0000, 0.0109],
        ...,
        [0.0070, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0109],
        [0.0070, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0109],
        [0.0070, 0.0000, 0.0000,  ..., 0.0226, 0.0000, 0.0109]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422641.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2671.2163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.2536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8616.5410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-573.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1679.4882, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-450.2537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6345],
        [-0.6218],
        [-0.5902],
        ...,
        [-0.4851],
        [-0.4849],
        [-0.4849]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112766.3828, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0045],
        [1.0051],
        [1.0032],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367077.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0046],
        [1.0052],
        [1.0033],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367088.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045,  0.0097,  0.0115,  ..., -0.0003,  0.0059,  0.0026],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1078.1853, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.7623, device='cuda:0')



h[100].sum tensor(50.5925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.0174, device='cuda:0')



h[200].sum tensor(-5.1461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0327, 0.0397,  ..., 0.0002, 0.0207, 0.0088],
        [0.0000, 0.0103, 0.0167,  ..., 0.0000, 0.0098, 0.0034],
        [0.0000, 0.0007, 0.0068,  ..., 0.0000, 0.0051, 0.0011],
        ...,
        [0.0000, 0.0007, 0.0068,  ..., 0.0000, 0.0051, 0.0011],
        [0.0000, 0.0007, 0.0068,  ..., 0.0000, 0.0051, 0.0011],
        [0.0000, 0.0007, 0.0068,  ..., 0.0000, 0.0051, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41672.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0297, 0.0000, 0.0000,  ..., 0.0373, 0.0000, 0.1441],
        [0.0185, 0.0000, 0.0000,  ..., 0.0301, 0.0000, 0.0805],
        [0.0116, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0381],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0109],
        [0.0069, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0109],
        [0.0069, 0.0000, 0.0000,  ..., 0.0233, 0.0000, 0.0109]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(359434.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2194.3813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(40.4002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7455.2646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-412.7692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1823.1118, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.0612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1633],
        [ 0.0746],
        [-0.0452],
        ...,
        [-0.4959],
        [-0.4944],
        [-0.4940]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128601.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0046],
        [1.0052],
        [1.0033],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367088.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0047],
        [1.0053],
        [1.0033],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367099.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0264,  0.0559,  0.0589,  ...,  0.0016,  0.0284,  0.0136],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [-0.0156,  0.0332,  0.0356,  ...,  0.0006,  0.0173,  0.0082],
        ...,
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1138.2478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.9788, device='cuda:0')



h[100].sum tensor(54.7316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.5170, device='cuda:0')



h[200].sum tensor(-5.8439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9957, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0989, 0.1077,  ..., 0.0018, 0.0529, 0.0246],
        [0.0000, 0.1439, 0.1540,  ..., 0.0029, 0.0748, 0.0353],
        [0.0000, 0.0278, 0.0347,  ..., 0.0004, 0.0183, 0.0075],
        ...,
        [0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0052, 0.0011],
        [0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0052, 0.0011],
        [0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0052, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45509.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0940, 0.0000, 0.0000,  ..., 0.0861, 0.0000, 0.4466],
        [0.0772, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.3632],
        [0.0393, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.1746],
        ...,
        [0.0069, 0.0000, 0.0000,  ..., 0.0239, 0.0000, 0.0108],
        [0.0069, 0.0000, 0.0000,  ..., 0.0239, 0.0000, 0.0108],
        [0.0069, 0.0000, 0.0000,  ..., 0.0239, 0.0000, 0.0108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(384813.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2355.8596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.9235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7849.6997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.1514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1878.5112, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.0260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1142],
        [ 0.1073],
        [ 0.0740],
        ...,
        [-0.5033],
        [-0.5017],
        [-0.5013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-140597.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0047],
        [1.0053],
        [1.0033],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367099.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0053],
        [1.0034],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367110.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.6598e-04,  1.7127e-03,  ..., -7.4086e-04,
          1.2983e-03,  2.5281e-04],
        [ 0.0000e+00,  1.6598e-04,  1.7127e-03,  ..., -7.4086e-04,
          1.2983e-03,  2.5281e-04],
        [-7.3840e-03,  1.5780e-02,  1.7755e-02,  ..., -9.6512e-05,
          8.8871e-03,  3.9888e-03],
        ...,
        [ 0.0000e+00,  1.6598e-04,  1.7127e-03,  ..., -7.4086e-04,
          1.2983e-03,  2.5281e-04],
        [ 0.0000e+00,  1.6598e-04,  1.7127e-03,  ..., -7.4086e-04,
          1.2983e-03,  2.5281e-04],
        [ 0.0000e+00,  1.6598e-04,  1.7127e-03,  ..., -7.4086e-04,
          1.2983e-03,  2.5281e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1223.7875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.2187, device='cuda:0')



h[100].sum tensor(60.6346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.7090, device='cuda:0')



h[200].sum tensor(-6.8251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4545, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0052, 0.0010],
        [0.0000, 0.0164, 0.0230,  ..., 0.0000, 0.0128, 0.0048],
        [0.0000, 0.0552, 0.0629,  ..., 0.0008, 0.0317, 0.0141],
        ...,
        [0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0052, 0.0010],
        [0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0053, 0.0010],
        [0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0053, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52264.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0159, 0.0000, 0.0000,  ..., 0.0303, 0.0000, 0.0612],
        [0.0356, 0.0000, 0.0000,  ..., 0.0443, 0.0000, 0.1611],
        [0.0703, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.3317],
        ...,
        [0.0068, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0108],
        [0.0068, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0108],
        [0.0068, 0.0000, 0.0000,  ..., 0.0245, 0.0000, 0.0108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(419287.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2592.8132, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(87.9150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8484.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-540.9720, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1815.9768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.3287, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0374],
        [ 0.0586],
        [ 0.0739],
        ...,
        [-0.5115],
        [-0.5099],
        [-0.5095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132048.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0053],
        [1.0034],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367110.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0048],
        [1.0054],
        [1.0034],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367121.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        ...,
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(978.3889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0844, device='cuda:0')



h[100].sum tensor(41.1153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.8525, device='cuda:0')



h[200].sum tensor(-3.7389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0052, 0.0010],
        [0.0000, 0.0007, 0.0069,  ..., 0.0000, 0.0053, 0.0010],
        [0.0000, 0.0346, 0.0418,  ..., 0.0007, 0.0218, 0.0091],
        ...,
        [0.0000, 0.0007, 0.0070,  ..., 0.0000, 0.0053, 0.0010],
        [0.0000, 0.0007, 0.0070,  ..., 0.0000, 0.0053, 0.0010],
        [0.0000, 0.0007, 0.0070,  ..., 0.0000, 0.0053, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35537.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0180],
        [0.0130, 0.0000, 0.0000,  ..., 0.0293, 0.0000, 0.0423],
        [0.0284, 0.0000, 0.0000,  ..., 0.0404, 0.0000, 0.1195],
        ...,
        [0.0067, 0.0000, 0.0000,  ..., 0.0250, 0.0000, 0.0107],
        [0.0067, 0.0000, 0.0000,  ..., 0.0250, 0.0000, 0.0108],
        [0.0067, 0.0000, 0.0000,  ..., 0.0250, 0.0000, 0.0108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(335425.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1984.0942, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(9.5961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7009.0034, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-336.6868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2012.3606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.1316, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3830],
        [-0.1845],
        [ 0.0013],
        ...,
        [-0.4510],
        [-0.5017],
        [-0.5132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151736.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0048],
        [1.0054],
        [1.0034],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367121.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(522.2551, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0049],
        [1.0054],
        [1.0035],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367132.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [-0.0045,  0.0096,  0.0115,  ..., -0.0004,  0.0059,  0.0025],
        ...,
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0017,  ..., -0.0007,  0.0013,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1004.7662, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9165, device='cuda:0')



h[100].sum tensor(42.6648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.4155, device='cuda:0')



h[200].sum tensor(-3.9941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2661, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0070,  ..., 0.0000, 0.0053, 0.0010],
        [0.0000, 0.0102, 0.0168,  ..., 0.0000, 0.0099, 0.0033],
        [0.0000, 0.0084, 0.0150,  ..., 0.0000, 0.0091, 0.0028],
        ...,
        [0.0000, 0.0007, 0.0070,  ..., 0.0000, 0.0053, 0.0010],
        [0.0000, 0.0007, 0.0070,  ..., 0.0000, 0.0053, 0.0010],
        [0.0000, 0.0007, 0.0070,  ..., 0.0000, 0.0053, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36766.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0079, 0.0000, 0.0000,  ..., 0.0258, 0.0000, 0.0195],
        [0.0107, 0.0000, 0.0000,  ..., 0.0274, 0.0000, 0.0389],
        [0.0120, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0488],
        ...,
        [0.0067, 0.0000, 0.0000,  ..., 0.0254, 0.0000, 0.0108],
        [0.0067, 0.0000, 0.0000,  ..., 0.0254, 0.0000, 0.0108],
        [0.0068, 0.0000, 0.0000,  ..., 0.0254, 0.0000, 0.0117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(341924.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1989.0161, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(15.5598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7183.6572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-350.2253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1979.3987, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-371.2109, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3235],
        [-0.1846],
        [-0.0327],
        ...,
        [-0.5162],
        [-0.4958],
        [-0.4677]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148458.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0049],
        [1.0054],
        [1.0035],
        ...,
        [1.0006],
        [1.0000],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367132.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0050],
        [1.0055],
        [1.0035],
        ...,
        [1.0006],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367144.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0104,  0.0123,  ..., -0.0003,  0.0063,  0.0027],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [-0.0048,  0.0104,  0.0123,  ..., -0.0003,  0.0063,  0.0027],
        ...,
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0007,  0.0013,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1295.7866, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.2451, device='cuda:0')



h[100].sum tensor(64.3895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.7566, device='cuda:0')



h[200].sum tensor(-7.4194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.8172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0323, 0.0396,  ..., 0.0000, 0.0208, 0.0085],
        [0.0000, 0.0532, 0.0611,  ..., 0.0000, 0.0309, 0.0135],
        [0.0000, 0.0166, 0.0235,  ..., 0.0000, 0.0131, 0.0048],
        ...,
        [0.0000, 0.0006, 0.0071,  ..., 0.0000, 0.0054, 0.0010],
        [0.0000, 0.0006, 0.0071,  ..., 0.0000, 0.0054, 0.0010],
        [0.0000, 0.0006, 0.0071,  ..., 0.0000, 0.0054, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55530.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0309, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.1559],
        [0.0302, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.1555],
        [0.0204, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0980],
        ...,
        [0.0066, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0109],
        [0.0066, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0109],
        [0.0066, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0109]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(435710.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2582.6765, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.7767, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8981.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-575.5703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1659.7729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-448.8851, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2071],
        [ 0.1948],
        [ 0.1442],
        ...,
        [-0.5336],
        [-0.5320],
        [-0.5315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-110505.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0050],
        [1.0055],
        [1.0035],
        ...,
        [1.0006],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367144.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0051],
        [1.0055],
        [1.0036],
        ...,
        [1.0006],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367155.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        ...,
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1187.4697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2597, device='cuda:0')



h[100].sum tensor(55.8988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.3836, device='cuda:0')



h[200].sum tensor(-6.0441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5725, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0071,  ..., 0.0000, 0.0054, 0.0009],
        [0.0000, 0.0006, 0.0071,  ..., 0.0000, 0.0054, 0.0009],
        [0.0000, 0.0093, 0.0161,  ..., 0.0000, 0.0096, 0.0030],
        ...,
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0009],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0009],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46977.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0078, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0262],
        [0.0093, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0315],
        [0.0139, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.0564],
        ...,
        [0.0071, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0135],
        [0.0067, 0.0000, 0.0000,  ..., 0.0263, 0.0000, 0.0109],
        [0.0067, 0.0000, 0.0000,  ..., 0.0263, 0.0000, 0.0109]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391445.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2309.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(59.3146, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8085.0986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-475.4284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1984.3506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.5864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0496],
        [-0.0270],
        [ 0.0283],
        ...,
        [-0.4952],
        [-0.5273],
        [-0.5386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-148036.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0051],
        [1.0055],
        [1.0036],
        ...,
        [1.0006],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367155.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0052],
        [1.0056],
        [1.0037],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367166.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [-0.0068,  0.0145,  0.0166,  ..., -0.0002,  0.0083,  0.0037],
        [-0.0058,  0.0125,  0.0145,  ..., -0.0002,  0.0073,  0.0032],
        ...,
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(978.9114, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.2586, device='cuda:0')



h[100].sum tensor(39.7583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.2938, device='cuda:0')



h[200].sum tensor(-3.4771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.5196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0150, 0.0220,  ..., 0.0000, 0.0124, 0.0044],
        [0.0000, 0.0350, 0.0426,  ..., 0.0002, 0.0221, 0.0092],
        [0.0000, 0.0857, 0.0947,  ..., 0.0009, 0.0467, 0.0214],
        ...,
        [0.0000, 0.0005, 0.0072,  ..., 0.0000, 0.0054, 0.0010],
        [0.0000, 0.0005, 0.0072,  ..., 0.0000, 0.0054, 0.0010],
        [0.0000, 0.0005, 0.0072,  ..., 0.0000, 0.0054, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34862.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0273, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.1225],
        [0.0371, 0.0000, 0.0000,  ..., 0.0466, 0.0000, 0.1749],
        [0.0548, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.2672],
        ...,
        [0.0067, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0110],
        [0.0067, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0110],
        [0.0067, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0110]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334892.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1933.7740, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(0.8680, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7067.6016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-330.3483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2196.9653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-358.1684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1015],
        [ 0.0999],
        [ 0.0937],
        ...,
        [-0.5505],
        [-0.5484],
        [-0.5457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165555.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0052],
        [1.0056],
        [1.0037],
        ...,
        [1.0006],
        [0.9999],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367166.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0057],
        [1.0037],
        ...,
        [1.0005],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367177.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.6649e-03,  1.8611e-02,  2.0766e-02,  ...,  1.2759e-05,
          1.0314e-02,  4.6814e-03],
        [-1.4709e-02,  3.1496e-02,  3.4008e-02,  ...,  5.4352e-04,
          1.6578e-02,  7.7674e-03],
        [-1.5312e-02,  3.2781e-02,  3.5329e-02,  ...,  5.9648e-04,
          1.7203e-02,  8.0753e-03],
        ...,
        [ 0.0000e+00,  1.3761e-04,  1.7808e-03,  ..., -7.4822e-04,
          1.3318e-03,  2.5696e-04],
        [ 0.0000e+00,  1.3761e-04,  1.7808e-03,  ..., -7.4822e-04,
          1.3318e-03,  2.5696e-04],
        [ 0.0000e+00,  1.3761e-04,  1.7808e-03,  ..., -7.4822e-04,
          1.3318e-03,  2.5696e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1077.2400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0322, device='cuda:0')



h[100].sum tensor(46.3771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8468, device='cuda:0')



h[200].sum tensor(-4.4837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0983, 0.1076,  ..., 0.0015, 0.0529, 0.0245],
        [0.0000, 0.1311, 0.1413,  ..., 0.0024, 0.0688, 0.0323],
        [0.0000, 0.1159, 0.1257,  ..., 0.0018, 0.0615, 0.0287],
        ...,
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0010],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0010],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40869.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0826, 0.0000, 0.0000,  ..., 0.0777, 0.0000, 0.4085],
        [0.0890, 0.0000, 0.0000,  ..., 0.0828, 0.0000, 0.4344],
        [0.0759, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.3678],
        ...,
        [0.0065, 0.0000, 0.0000,  ..., 0.0261, 0.0000, 0.0113],
        [0.0065, 0.0000, 0.0000,  ..., 0.0261, 0.0000, 0.0113],
        [0.0065, 0.0000, 0.0000,  ..., 0.0261, 0.0000, 0.0113]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(369931.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2157.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(30.6241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7733.2583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-404.1347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2138.5471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.6501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0917],
        [ 0.0849],
        [ 0.0788],
        ...,
        [-0.5537],
        [-0.5520],
        [-0.5515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-159260.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0057],
        [1.0037],
        ...,
        [1.0005],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367177.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0053],
        [1.0058],
        [1.0038],
        ...,
        [1.0005],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367188.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0112,  0.0240,  0.0263,  ...,  0.0002,  0.0129,  0.0060],
        [-0.0066,  0.0142,  0.0162,  ..., -0.0002,  0.0082,  0.0036],
        [-0.0232,  0.0497,  0.0527,  ...,  0.0013,  0.0255,  0.0121],
        ...,
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0007,  0.0013,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1110.1964, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.8434, device='cuda:0')



h[100].sum tensor(48.0930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3957, device='cuda:0')



h[200].sum tensor(-4.7163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5839, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0712, 0.0797,  ..., 0.0005, 0.0397, 0.0180],
        [0.0000, 0.1251, 0.1351,  ..., 0.0021, 0.0660, 0.0309],
        [0.0000, 0.0554, 0.0635,  ..., 0.0002, 0.0321, 0.0142],
        ...,
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0011],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0011],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0054, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41275.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0451, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.2186],
        [0.0620, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.2995],
        [0.0505, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.2432],
        ...,
        [0.0064, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0116],
        [0.0064, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0116],
        [0.0064, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0116]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(365565.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2039.7562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(36.6446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7773.6025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-407.7474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2039.8787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.9634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0329],
        [ 0.0739],
        [ 0.0943],
        ...,
        [-0.5557],
        [-0.5540],
        [-0.5536]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142584.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0053],
        [1.0058],
        [1.0038],
        ...,
        [1.0005],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367188.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0054],
        [1.0058],
        [1.0039],
        ...,
        [1.0005],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367200.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1086.9946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3670, device='cuda:0')



h[100].sum tensor(45.6907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3968, device='cuda:0')



h[200].sum tensor(-4.3015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9192, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0186,  ..., 0.0000, 0.0109, 0.0038],
        [0.0000, 0.0006, 0.0071,  ..., 0.0000, 0.0055, 0.0011],
        [0.0000, 0.0006, 0.0071,  ..., 0.0000, 0.0055, 0.0011],
        ...,
        [0.0000, 0.0007, 0.0071,  ..., 0.0000, 0.0055, 0.0011],
        [0.0000, 0.0007, 0.0072,  ..., 0.0000, 0.0055, 0.0011],
        [0.0000, 0.0007, 0.0072,  ..., 0.0000, 0.0055, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39233.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0139, 0.0000, 0.0000,  ..., 0.0292, 0.0000, 0.0647],
        [0.0088, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0314],
        [0.0065, 0.0000, 0.0000,  ..., 0.0252, 0.0000, 0.0161],
        ...,
        [0.0062, 0.0000, 0.0000,  ..., 0.0252, 0.0000, 0.0120],
        [0.0062, 0.0000, 0.0000,  ..., 0.0252, 0.0000, 0.0120],
        [0.0062, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0120]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(354754.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1896.4117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(32.1463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7672.2944, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-382.6292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1976.7627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.2712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0772],
        [-0.0316],
        [-0.1293],
        ...,
        [-0.5561],
        [-0.5545],
        [-0.5540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134021.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0054],
        [1.0058],
        [1.0039],
        ...,
        [1.0005],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367200.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0055],
        [1.0059],
        [1.0040],
        ...,
        [1.0005],
        [0.9998],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367211.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036,  0.0079,  0.0098,  ..., -0.0004,  0.0052,  0.0021],
        [-0.0036,  0.0079,  0.0098,  ..., -0.0004,  0.0052,  0.0021],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0014,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1459.2925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.6444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.3040, device='cuda:0')



h[100].sum tensor(72.9002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-20.5027, device='cuda:0')



h[200].sum tensor(-8.4268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.0279e-02, 5.8099e-02,  ..., 2.3011e-05, 2.9761e-02,
         1.2987e-02],
        [0.0000e+00, 3.0051e-02, 3.7328e-02,  ..., 0.0000e+00, 1.9933e-02,
         8.1421e-03],
        [0.0000e+00, 5.9399e-02, 6.7485e-02,  ..., 8.0411e-05, 3.4211e-02,
         1.5175e-02],
        ...,
        [0.0000e+00, 6.3510e-04, 7.1423e-03,  ..., 0.0000e+00, 5.6561e-03,
         1.0987e-03],
        [0.0000e+00, 6.3524e-04, 7.1439e-03,  ..., 0.0000e+00, 5.6574e-03,
         1.0990e-03],
        [0.0000e+00, 6.3530e-04, 7.1445e-03,  ..., 0.0000e+00, 5.6579e-03,
         1.0991e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62346.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0363, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.2038],
        [0.0331, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.1850],
        [0.0394, 0.0000, 0.0000,  ..., 0.0453, 0.0000, 0.2149],
        ...,
        [0.0060, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0121],
        [0.0060, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0121],
        [0.0060, 0.0000, 0.0000,  ..., 0.0251, 0.0000, 0.0121]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478504.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2798.0225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(140.4283, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9787.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-668.8587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1854.8807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-451.7758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2497],
        [ 0.2487],
        [ 0.2401],
        ...,
        [-0.5577],
        [-0.5561],
        [-0.5556]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135283.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0055],
        [1.0059],
        [1.0040],
        ...,
        [1.0005],
        [0.9998],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367211.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0056],
        [1.0060],
        [1.0041],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367223.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        ...,
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1214.9651, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.7338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.5429, device='cuda:0')



h[100].sum tensor(55.5053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.8986, device='cuda:0')



h[200].sum tensor(-5.6953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2497, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0071,  ..., 0.0000, 0.0057, 0.0010],
        [0.0000, 0.0005, 0.0071,  ..., 0.0000, 0.0057, 0.0010],
        [0.0000, 0.0352, 0.0427,  ..., 0.0007, 0.0225, 0.0093],
        ...,
        [0.0000, 0.0005, 0.0072,  ..., 0.0000, 0.0057, 0.0011],
        [0.0000, 0.0005, 0.0072,  ..., 0.0000, 0.0057, 0.0011],
        [0.0000, 0.0005, 0.0072,  ..., 0.0000, 0.0057, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46792.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0102, 0.0000, 0.0000,  ..., 0.0277, 0.0000, 0.0378],
        [0.0159, 0.0000, 0.0000,  ..., 0.0318, 0.0000, 0.0669],
        [0.0317, 0.0000, 0.0000,  ..., 0.0429, 0.0000, 0.1492],
        ...,
        [0.0060, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0122],
        [0.0060, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0122],
        [0.0060, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393234.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2106.8054, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(68.6348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8466.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-477.1087, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1810.7549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.0322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1588],
        [ 0.1654],
        [ 0.1822],
        ...,
        [-0.5205],
        [-0.5349],
        [-0.5484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-127596.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0056],
        [1.0060],
        [1.0041],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367223.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0061],
        [1.0042],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367233.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04],
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04],
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04],
        ...,
        [-9.7881e-03,  2.1076e-02,  2.3411e-02,  ...,  9.8975e-05,
          1.1623e-02,  5.2648e-03],
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04],
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1305.6206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.6683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.5994, device='cuda:0')



h[100].sum tensor(63.5006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.6432, device='cuda:0')



h[200].sum tensor(-6.8729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0762, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0072,  ..., 0.0000, 0.0056, 0.0009],
        [0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0057, 0.0009],
        [0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0057, 0.0009],
        ...,
        [0.0000, 0.0346, 0.0426,  ..., 0.0000, 0.0224, 0.0092],
        [0.0000, 0.0302, 0.0381,  ..., 0.0001, 0.0202, 0.0081],
        [0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0057, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55022.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0075, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0209],
        [0.0061, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0121],
        [0.0062, 0.0000, 0.0000,  ..., 0.0261, 0.0000, 0.0121],
        ...,
        [0.0335, 0.0000, 0.0000,  ..., 0.0439, 0.0000, 0.1754],
        [0.0265, 0.0000, 0.0000,  ..., 0.0396, 0.0000, 0.1313],
        [0.0129, 0.0000, 0.0000,  ..., 0.0306, 0.0000, 0.0520]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(444798.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2533.6040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.4019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9225.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-581.7646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1843.7437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.9213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1738],
        [-0.3159],
        [-0.4067],
        ...,
        [ 0.1469],
        [ 0.0382],
        [-0.1490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136001.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0061],
        [1.0042],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367233.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0057],
        [1.0061],
        [1.0042],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367233.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.6233e-03,  7.8471e-03,  9.8037e-03,  ..., -4.4274e-04,
          5.1871e-03,  2.0941e-03],
        [-4.6468e-03,  1.0043e-02,  1.2063e-02,  ..., -3.5280e-04,
          6.2557e-03,  2.6205e-03],
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04],
        ...,
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04],
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04],
        [ 0.0000e+00,  7.1931e-05,  1.8058e-03,  ..., -7.6113e-04,
          1.4044e-03,  2.3051e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1195.6853, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.6579, device='cuda:0')



h[100].sum tensor(55.5549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.9765, device='cuda:0')



h[200].sum tensor(-5.6572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0777, 0.0868,  ..., 0.0005, 0.0433, 0.0195],
        [0.0000, 0.0321, 0.0400,  ..., 0.0002, 0.0211, 0.0086],
        [0.0000, 0.0103, 0.0176,  ..., 0.0000, 0.0105, 0.0033],
        ...,
        [0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0057, 0.0009],
        [0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0057, 0.0009],
        [0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0057, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46287.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0454, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.2371],
        [0.0320, 0.0000, 0.0000,  ..., 0.0431, 0.0000, 0.1622],
        [0.0190, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0884],
        ...,
        [0.0062, 0.0000, 0.0000,  ..., 0.0262, 0.0000, 0.0122],
        [0.0062, 0.0000, 0.0000,  ..., 0.0262, 0.0000, 0.0122],
        [0.0062, 0.0000, 0.0000,  ..., 0.0262, 0.0000, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(393875., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2147.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(59.4446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8369.7744, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-473.6857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1890.4929, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.4854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1938],
        [ 0.1594],
        [ 0.0657],
        ...,
        [-0.5829],
        [-0.5811],
        [-0.5806]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139836.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0057],
        [1.0061],
        [1.0042],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367233.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0058],
        [1.0062],
        [1.0042],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367243.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.8456e-05,  1.8308e-03,  ..., -7.6443e-04,
          1.3845e-03,  2.1657e-04],
        [ 0.0000e+00,  2.8456e-05,  1.8308e-03,  ..., -7.6443e-04,
          1.3845e-03,  2.1657e-04],
        [-3.6250e-03,  7.8162e-03,  9.8454e-03,  ..., -4.4615e-04,
          5.1729e-03,  2.0833e-03],
        ...,
        [ 0.0000e+00,  2.8456e-05,  1.8308e-03,  ..., -7.6443e-04,
          1.3845e-03,  2.1657e-04],
        [ 0.0000e+00,  2.8456e-05,  1.8308e-03,  ..., -7.6443e-04,
          1.3845e-03,  2.1657e-04],
        [ 0.0000e+00,  2.8456e-05,  1.8308e-03,  ..., -7.6443e-04,
          1.3845e-03,  2.1657e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1326.3557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.7115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9477, device='cuda:0')



h[100].sum tensor(65.5161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.5554, device='cuda:0')



h[200].sum tensor(-7.1698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.4526e-03, 1.3872e-02,  ..., 0.0000e+00, 8.6412e-03,
         2.3887e-03],
        [0.0000e+00, 3.3010e-02, 4.1221e-02,  ..., 0.0000e+00, 2.1574e-02,
         8.7568e-03],
        [0.0000e+00, 5.6977e-02, 6.5886e-02,  ..., 9.4048e-05, 3.3233e-02,
         1.4502e-02],
        ...,
        [0.0000e+00, 1.1524e-04, 7.4142e-03,  ..., 0.0000e+00, 5.6070e-03,
         8.7706e-04],
        [0.0000e+00, 1.1526e-04, 7.4159e-03,  ..., 0.0000e+00, 5.6083e-03,
         8.7725e-04],
        [0.0000e+00, 1.1527e-04, 7.4165e-03,  ..., 0.0000e+00, 5.6087e-03,
         8.7733e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56304.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0000, 0.0000,  ..., 0.0329, 0.0000, 0.0816],
        [0.0294, 0.0000, 0.0000,  ..., 0.0411, 0.0000, 0.1547],
        [0.0406, 0.0000, 0.0000,  ..., 0.0490, 0.0000, 0.2132],
        ...,
        [0.0064, 0.0000, 0.0000,  ..., 0.0269, 0.0000, 0.0121],
        [0.0064, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0121],
        [0.0064, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0121]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449184.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2583.2073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(98.8040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9256.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-597.4535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1861.3630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.4237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0122],
        [ 0.1197],
        [ 0.1646],
        ...,
        [-0.5953],
        [-0.5935],
        [-0.5930]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-139289.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0058],
        [1.0062],
        [1.0042],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367243.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0059],
        [1.0063],
        [1.0043],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367254.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.7638e-05,  1.8428e-03,  ..., -7.6910e-04,
          1.3757e-03,  2.2225e-04],
        [ 0.0000e+00,  1.7638e-05,  1.8428e-03,  ..., -7.6910e-04,
          1.3757e-03,  2.2225e-04],
        [ 0.0000e+00,  1.7638e-05,  1.8428e-03,  ..., -7.6910e-04,
          1.3757e-03,  2.2225e-04],
        ...,
        [ 0.0000e+00,  1.7638e-05,  1.8428e-03,  ..., -7.6910e-04,
          1.3757e-03,  2.2225e-04],
        [ 0.0000e+00,  1.7638e-05,  1.8428e-03,  ..., -7.6910e-04,
          1.3757e-03,  2.2225e-04],
        [ 0.0000e+00,  1.7638e-05,  1.8428e-03,  ..., -7.6910e-04,
          1.3757e-03,  2.2225e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1085.5923, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3566, device='cuda:0')



h[100].sum tensor(47.5586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.0663, device='cuda:0')



h[200].sum tensor(-4.4768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 7.0810e-05, 7.3981e-03,  ..., 0.0000e+00, 5.5227e-03,
         8.9225e-04],
        [0.0000e+00, 7.0983e-05, 7.4163e-03,  ..., 0.0000e+00, 5.5362e-03,
         8.9444e-04],
        [0.0000e+00, 7.0980e-05, 7.4159e-03,  ..., 0.0000e+00, 5.5360e-03,
         8.9440e-04],
        ...,
        [0.0000e+00, 7.1440e-05, 7.4640e-03,  ..., 0.0000e+00, 5.5718e-03,
         9.0019e-04],
        [0.0000e+00, 7.1456e-05, 7.4656e-03,  ..., 0.0000e+00, 5.5731e-03,
         9.0039e-04],
        [0.0000e+00, 7.1462e-05, 7.4663e-03,  ..., 0.0000e+00, 5.5735e-03,
         9.0047e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41427.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0000, 0.0000,  ..., 0.0269, 0.0000, 0.0119],
        [0.0064, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0120],
        [0.0064, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0120],
        ...,
        [0.0064, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0120],
        [0.0064, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0121],
        [0.0064, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0121]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(374814.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2119.7700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.2100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7803.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-421.0539, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2181.1401, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-379.0172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7351],
        [-0.6599],
        [-0.5491],
        ...,
        [-0.6025],
        [-0.6006],
        [-0.6001]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172900.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0059],
        [1.0063],
        [1.0043],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367254.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0060],
        [1.0065],
        [1.0044],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367264.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.2426e-05,  1.8412e-03,  ..., -7.7440e-04,
          1.3776e-03,  2.3854e-04],
        [ 0.0000e+00,  3.2426e-05,  1.8412e-03,  ..., -7.7440e-04,
          1.3776e-03,  2.3854e-04],
        [-4.3905e-03,  9.4883e-03,  1.1579e-02,  ..., -3.8968e-04,
          5.9770e-03,  2.5069e-03],
        ...,
        [ 0.0000e+00,  3.2426e-05,  1.8412e-03,  ..., -7.7440e-04,
          1.3776e-03,  2.3854e-04],
        [ 0.0000e+00,  3.2426e-05,  1.8412e-03,  ..., -7.7440e-04,
          1.3776e-03,  2.3854e-04],
        [ 0.0000e+00,  3.2426e-05,  1.8412e-03,  ..., -7.7440e-04,
          1.3776e-03,  2.3854e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1188.8524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.1567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1642, device='cuda:0')



h[100].sum tensor(53.5487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.6424, device='cuda:0')



h[200].sum tensor(-5.4605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0792, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0001, 0.0074,  ..., 0.0000, 0.0055, 0.0010],
        [0.0000, 0.0096, 0.0172,  ..., 0.0000, 0.0102, 0.0032],
        [0.0000, 0.0143, 0.0220,  ..., 0.0000, 0.0124, 0.0044],
        ...,
        [0.0000, 0.0001, 0.0075,  ..., 0.0000, 0.0056, 0.0010],
        [0.0000, 0.0001, 0.0075,  ..., 0.0000, 0.0056, 0.0010],
        [0.0000, 0.0001, 0.0075,  ..., 0.0000, 0.0056, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47191.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0085, 0.0000, 0.0000,  ..., 0.0282, 0.0000, 0.0269],
        [0.0142, 0.0000, 0.0000,  ..., 0.0317, 0.0000, 0.0626],
        [0.0194, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0957],
        ...,
        [0.0063, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0121],
        [0.0063, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122],
        [0.0063, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(404091.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2238.8389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.7949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8428.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-487.8063, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1965.0435, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-406.6837, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3969],
        [-0.1786],
        [ 0.0079],
        ...,
        [-0.6056],
        [-0.6038],
        [-0.6033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-142005.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0060],
        [1.0065],
        [1.0044],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367264.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0061],
        [1.0066],
        [1.0045],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367274.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.4822e-05,  1.8242e-03,  ..., -7.8275e-04,
          1.4041e-03,  2.6281e-04],
        [ 0.0000e+00,  7.4822e-05,  1.8242e-03,  ..., -7.8275e-04,
          1.4041e-03,  2.6281e-04],
        [ 0.0000e+00,  7.4822e-05,  1.8242e-03,  ..., -7.8275e-04,
          1.4041e-03,  2.6281e-04],
        ...,
        [ 0.0000e+00,  7.4822e-05,  1.8242e-03,  ..., -7.8275e-04,
          1.4041e-03,  2.6281e-04],
        [ 0.0000e+00,  7.4822e-05,  1.8242e-03,  ..., -7.8275e-04,
          1.4041e-03,  2.6281e-04],
        [ 0.0000e+00,  7.4822e-05,  1.8242e-03,  ..., -7.8275e-04,
          1.4041e-03,  2.6281e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1166.7795, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.6131, device='cuda:0')



h[100].sum tensor(50.2510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.5930, device='cuda:0')



h[200].sum tensor(-5.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3808, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0056, 0.0011],
        [0.0000, 0.0003, 0.0073,  ..., 0.0000, 0.0057, 0.0011],
        [0.0000, 0.0181, 0.0257,  ..., 0.0000, 0.0143, 0.0053],
        ...,
        [0.0000, 0.0003, 0.0074,  ..., 0.0000, 0.0057, 0.0011],
        [0.0000, 0.0003, 0.0074,  ..., 0.0000, 0.0057, 0.0011],
        [0.0000, 0.0003, 0.0074,  ..., 0.0000, 0.0057, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43496.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0067, 0.0000, 0.0000,  ..., 0.0267, 0.0000, 0.0185],
        [0.0097, 0.0000, 0.0000,  ..., 0.0286, 0.0000, 0.0381],
        [0.0173, 0.0000, 0.0000,  ..., 0.0332, 0.0000, 0.0870],
        ...,
        [0.0059, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0123],
        [0.0059, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0123],
        [0.0059, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(382891.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2002.2571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(45.4821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8083.9209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-442.9167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1975.7749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.9897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4246],
        [-0.2166],
        [-0.0133],
        ...,
        [-0.5976],
        [-0.5941],
        [-0.5927]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-144958.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0061],
        [1.0066],
        [1.0045],
        ...,
        [1.0005],
        [0.9998],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367274.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0062],
        [1.0067],
        [1.0046],
        ...,
        [1.0005],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367285.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0234,  0.0507,  0.0539,  ...,  0.0013,  0.0260,  0.0124],
        [-0.0059,  0.0128,  0.0148,  ..., -0.0003,  0.0076,  0.0033],
        [-0.0190,  0.0412,  0.0441,  ...,  0.0009,  0.0214,  0.0102],
        ...,
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0014,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1471.9014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.8854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-29.7337, device='cuda:0')



h[100].sum tensor(69.9367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-20.1168, device='cuda:0')



h[200].sum tensor(-8.0417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3880, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0915, 0.1010,  ..., 0.0014, 0.0500, 0.0230],
        [0.0000, 0.2055, 0.2184,  ..., 0.0051, 0.1055, 0.0504],
        [0.0000, 0.1305, 0.1411,  ..., 0.0024, 0.0690, 0.0324],
        ...,
        [0.0000, 0.0005, 0.0073,  ..., 0.0000, 0.0058, 0.0012],
        [0.0000, 0.0005, 0.0073,  ..., 0.0000, 0.0058, 0.0012],
        [0.0000, 0.0005, 0.0073,  ..., 0.0000, 0.0058, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61667.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0875, 0.0000, 0.0000,  ..., 0.0835, 0.0000, 0.4426],
        [0.1202, 0.0000, 0.0000,  ..., 0.1079, 0.0000, 0.5954],
        [0.1097, 0.0000, 0.0000,  ..., 0.1003, 0.0000, 0.5455],
        ...,
        [0.0054, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0126],
        [0.0054, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0126],
        [0.0054, 0.0000, 0.0000,  ..., 0.0260, 0.0000, 0.0126]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(475788.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2496.7368, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(138.5670, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9843.4365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-662.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1657.1997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-458.8014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0752],
        [ 0.0469],
        [ 0.0238],
        ...,
        [-0.5954],
        [-0.5889],
        [-0.5830]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-115297.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0062],
        [1.0067],
        [1.0046],
        ...,
        [1.0005],
        [0.9999],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367285.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0068],
        [1.0047],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367295.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [-0.0135,  0.0294,  0.0319,  ...,  0.0004,  0.0157,  0.0073],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1154.2407, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0064, device='cuda:0')



h[100].sum tensor(46.2614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5060, device='cuda:0')



h[200].sum tensor(-4.5320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0299, 0.0374,  ..., 0.0004, 0.0202, 0.0083],
        [0.0000, 0.0246, 0.0320,  ..., 0.0002, 0.0176, 0.0070],
        [0.0000, 0.1074, 0.1173,  ..., 0.0011, 0.0579, 0.0269],
        ...,
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0059, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42515.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0215, 0.0000, 0.0000,  ..., 0.0371, 0.0000, 0.1002],
        [0.0261, 0.0000, 0.0000,  ..., 0.0403, 0.0000, 0.1250],
        [0.0412, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.2050],
        ...,
        [0.0051, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0127],
        [0.0051, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0127],
        [0.0051, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381401.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1753.2837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(54.9128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8183.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-429.9787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1837.3627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-381.7188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1435],
        [-0.0160],
        [ 0.0751],
        ...,
        [-0.6000],
        [-0.5983],
        [-0.5978]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-132400.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0068],
        [1.0047],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367295.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0063],
        [1.0068],
        [1.0047],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367305.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0002,  0.0018,  ..., -0.0008,  0.0015,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1177.3652, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.2993, device='cuda:0')



h[100].sum tensor(46.9329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.7041, device='cuda:0')



h[200].sum tensor(-4.6622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0060, 0.0012],
        [0.0000, 0.0006, 0.0072,  ..., 0.0000, 0.0060, 0.0012],
        ...,
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0060, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42588.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0000, 0.0000,  ..., 0.0255, 0.0000, 0.0125],
        [0.0049, 0.0000, 0.0000,  ..., 0.0254, 0.0000, 0.0133],
        [0.0048, 0.0000, 0.0000,  ..., 0.0252, 0.0000, 0.0152],
        ...,
        [0.0049, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0126],
        [0.0049, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0126],
        [0.0049, 0.0000, 0.0000,  ..., 0.0257, 0.0000, 0.0126]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381029.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1660.6849, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.3935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8184.2705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-428.4609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1820.2322, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.7001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4453],
        [-0.4849],
        [-0.4819],
        ...,
        [-0.5850],
        [-0.5999],
        [-0.6023]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-125099.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0063],
        [1.0068],
        [1.0047],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367305.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0064],
        [1.0069],
        [1.0048],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367315.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [-0.0036,  0.0079,  0.0098,  ..., -0.0005,  0.0053,  0.0022],
        ...,
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1142.3174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9748, device='cuda:0')



h[100].sum tensor(43.6816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8080, device='cuda:0')



h[200].sum tensor(-4.2085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1929, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0148, 0.0220,  ..., 0.0000, 0.0129, 0.0047],
        [0.0000, 0.0208, 0.0281,  ..., 0.0000, 0.0158, 0.0061],
        ...,
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0006, 0.0073,  ..., 0.0000, 0.0060, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39738.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.0342],
        [0.0135, 0.0000, 0.0000,  ..., 0.0304, 0.0000, 0.0763],
        [0.0169, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.1040],
        ...,
        [0.0049, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0124],
        [0.0049, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0124],
        [0.0049, 0.0000, 0.0000,  ..., 0.0259, 0.0000, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(366760.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1642.8142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(38.3061, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7752.3652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-397.4656, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2042.4641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.9255, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1215],
        [ 0.0539],
        [ 0.1702],
        ...,
        [-0.6134],
        [-0.6116],
        [-0.6111]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155701.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0064],
        [1.0069],
        [1.0048],
        ...,
        [1.0006],
        [0.9999],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367315.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0065],
        [1.0070],
        [1.0049],
        ...,
        [1.0006],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367325.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [-0.0043,  0.0095,  0.0115,  ..., -0.0005,  0.0061,  0.0026],
        ...,
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003],
        [-0.0116,  0.0253,  0.0277,  ...,  0.0002,  0.0137,  0.0064],
        [ 0.0000,  0.0001,  0.0018,  ..., -0.0008,  0.0015,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1301.3650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.7459, device='cuda:0')



h[100].sum tensor(54.1454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.7125, device='cuda:0')



h[200].sum tensor(-5.7472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.5098e-04, 7.3232e-03,  ..., 0.0000e+00, 5.9824e-03,
         1.2623e-03],
        [0.0000e+00, 9.9991e-03, 1.7077e-02,  ..., 0.0000e+00, 1.0597e-02,
         3.5376e-03],
        [0.0000e+00, 3.2001e-02, 3.9750e-02,  ..., 4.7784e-05, 2.1311e-02,
         8.8297e-03],
        ...,
        [0.0000e+00, 2.6029e-02, 3.3642e-02,  ..., 1.7989e-04, 1.8442e-02,
         7.4010e-03],
        [0.0000e+00, 2.1359e-02, 2.8831e-02,  ..., 0.0000e+00, 1.6169e-02,
         6.2780e-03],
        [0.0000e+00, 9.3121e-02, 1.0278e-01,  ..., 3.5985e-04, 5.1113e-02,
         2.3538e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50155.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0122, 0.0000, 0.0000,  ..., 0.0306, 0.0000, 0.0572],
        [0.0166, 0.0000, 0.0000,  ..., 0.0332, 0.0000, 0.0887],
        [0.0255, 0.0000, 0.0000,  ..., 0.0388, 0.0000, 0.1428],
        ...,
        [0.0189, 0.0000, 0.0000,  ..., 0.0359, 0.0000, 0.0891],
        [0.0228, 0.0000, 0.0000,  ..., 0.0385, 0.0000, 0.1110],
        [0.0357, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.1817]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(424205.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1884.2896, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(85.3063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8881.6289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-518.5997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1776.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.0892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1494],
        [ 0.1737],
        [ 0.1960],
        ...,
        [-0.1690],
        [-0.0133],
        [ 0.0627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-112064.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0065],
        [1.0070],
        [1.0049],
        ...,
        [1.0006],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367325.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(1025.7559, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0066],
        [1.0071],
        [1.0050],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367335.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0019,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0008,  0.0015,  0.0003],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0008,  0.0015,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0008,  0.0015,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1206.3293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9046, device='cuda:0')



h[100].sum tensor(47.0518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1137, device='cuda:0')



h[200].sum tensor(-4.7229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0074,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0004, 0.0075,  ..., 0.0000, 0.0059, 0.0013],
        [0.0000, 0.0004, 0.0075,  ..., 0.0000, 0.0059, 0.0013],
        ...,
        [0.0000, 0.0004, 0.0075,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0004, 0.0075,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0004, 0.0075,  ..., 0.0000, 0.0060, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45662.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0059, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0177],
        [0.0051, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.0122],
        [0.0051, 0.0000, 0.0000,  ..., 0.0266, 0.0000, 0.0119],
        ...,
        [0.0051, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0119],
        [0.0051, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0120],
        [0.0051, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0120]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(406927.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1922.1155, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.9161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8290.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-468.5434, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2126.5508, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.3640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4638],
        [-0.6367],
        [-0.7459],
        ...,
        [-0.6357],
        [-0.6339],
        [-0.6334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-156573.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0066],
        [1.0071],
        [1.0050],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367335.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0072],
        [1.0051],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367344.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.7732e-05,  1.8738e-03,  ..., -8.3899e-04,
          1.4592e-03,  3.0691e-04],
        [ 0.0000e+00,  7.7732e-05,  1.8738e-03,  ..., -8.3899e-04,
          1.4592e-03,  3.0691e-04],
        [ 0.0000e+00,  7.7732e-05,  1.8738e-03,  ..., -8.3899e-04,
          1.4592e-03,  3.0691e-04],
        ...,
        [ 0.0000e+00,  7.7732e-05,  1.8738e-03,  ..., -8.3899e-04,
          1.4592e-03,  3.0691e-04],
        [ 0.0000e+00,  7.7732e-05,  1.8738e-03,  ..., -8.3899e-04,
          1.4592e-03,  3.0691e-04],
        [ 0.0000e+00,  7.7732e-05,  1.8738e-03,  ..., -8.3899e-04,
          1.4592e-03,  3.0691e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1144.7972, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.6915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2242, device='cuda:0')



h[100].sum tensor(42.1469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3002, device='cuda:0')



h[200].sum tensor(-4.0283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8549, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0173,  ..., 0.0000, 0.0105, 0.0035],
        [0.0000, 0.0003, 0.0075,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0193, 0.0271,  ..., 0.0000, 0.0151, 0.0058],
        ...,
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41513.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0099, 0.0000, 0.0000,  ..., 0.0295, 0.0000, 0.0438],
        [0.0096, 0.0000, 0.0000,  ..., 0.0298, 0.0000, 0.0381],
        [0.0157, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0708],
        ...,
        [0.0053, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0117],
        [0.0053, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0117],
        [0.0053, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386747.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1802.1167, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.9778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7844.4829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-417.1510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2255.0098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.7639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5076],
        [-0.5114],
        [-0.4480],
        ...,
        [-0.6469],
        [-0.6449],
        [-0.6444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160886.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0072],
        [1.0051],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367344.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0067],
        [1.0073],
        [1.0053],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367354.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.0670e-05,  1.8829e-03,  ..., -8.4625e-04,
          1.4655e-03,  3.0575e-04],
        [ 0.0000e+00,  7.0670e-05,  1.8829e-03,  ..., -8.4625e-04,
          1.4655e-03,  3.0575e-04],
        [ 0.0000e+00,  7.0670e-05,  1.8829e-03,  ..., -8.4625e-04,
          1.4655e-03,  3.0575e-04],
        ...,
        [ 0.0000e+00,  7.0670e-05,  1.8829e-03,  ..., -8.4625e-04,
          1.4655e-03,  3.0575e-04],
        [ 0.0000e+00,  7.0670e-05,  1.8829e-03,  ..., -8.4625e-04,
          1.4655e-03,  3.0575e-04],
        [ 0.0000e+00,  7.0670e-05,  1.8829e-03,  ..., -8.4625e-04,
          1.4655e-03,  3.0575e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1302.2379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.6665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.7698, device='cuda:0')



h[100].sum tensor(52.3174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.0522, device='cuda:0')



h[200].sum tensor(-5.4769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.3519, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012],
        ...,
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47683.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0055, 0.0000, 0.0000,  ..., 0.0274, 0.0000, 0.0139],
        [0.0055, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0153],
        [0.0066, 0.0000, 0.0000,  ..., 0.0276, 0.0000, 0.0256],
        ...,
        [0.0053, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0117],
        [0.0053, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0117],
        [0.0053, 0.0000, 0.0000,  ..., 0.0276, 0.0000, 0.0117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410244.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1870.2690, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(60.8374, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8374.3857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-487.0166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2025.2583, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.5161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4989],
        [-0.3771],
        [-0.1966],
        ...,
        [-0.6536],
        [-0.6518],
        [-0.6513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-131298.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0067],
        [1.0073],
        [1.0053],
        ...,
        [1.0007],
        [1.0000],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367354.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0068],
        [1.0074],
        [1.0054],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367364.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  6.5183e-05,  1.8926e-03,  ..., -8.5333e-04,
          1.4719e-03,  3.1202e-04],
        [ 0.0000e+00,  6.5183e-05,  1.8926e-03,  ..., -8.5333e-04,
          1.4719e-03,  3.1202e-04],
        [ 0.0000e+00,  6.5183e-05,  1.8926e-03,  ..., -8.5333e-04,
          1.4719e-03,  3.1202e-04],
        ...,
        [ 0.0000e+00,  6.5183e-05,  1.8926e-03,  ..., -8.5333e-04,
          1.4719e-03,  3.1202e-04],
        [ 0.0000e+00,  6.5183e-05,  1.8926e-03,  ..., -8.5333e-04,
          1.4719e-03,  3.1202e-04],
        [ 0.0000e+00,  6.5183e-05,  1.8926e-03,  ..., -8.5333e-04,
          1.4719e-03,  3.1202e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1205.9415, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.8435, device='cuda:0')



h[100].sum tensor(45.3861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3957, device='cuda:0')



h[200].sum tensor(-4.3948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0824, 0.0923,  ..., 0.0010, 0.0459, 0.0210],
        [0.0000, 0.0136, 0.0214,  ..., 0.0000, 0.0124, 0.0045],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0059, 0.0013],
        ...,
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0060, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42398.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0571, 0.0000, 0.0000,  ..., 0.0618, 0.0000, 0.2947],
        [0.0364, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.1852],
        [0.0277, 0.0000, 0.0000,  ..., 0.0421, 0.0000, 0.1363],
        ...,
        [0.0053, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0117],
        [0.0053, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0117],
        [0.0053, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386627.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1735.0817, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(36.3625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7872.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-424.8731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2264.6050, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-381.3312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1548],
        [ 0.1499],
        [ 0.1409],
        ...,
        [-0.6601],
        [-0.6582],
        [-0.6577]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155121.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0068],
        [1.0074],
        [1.0054],
        ...,
        [1.0007],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367364.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0069],
        [1.0075],
        [1.0055],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367374.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  6.8530e-05,  1.8945e-03,  ..., -8.5928e-04,
          1.4914e-03,  3.1534e-04],
        [ 0.0000e+00,  6.8530e-05,  1.8945e-03,  ..., -8.5928e-04,
          1.4914e-03,  3.1534e-04],
        [ 0.0000e+00,  6.8530e-05,  1.8945e-03,  ..., -8.5928e-04,
          1.4914e-03,  3.1534e-04],
        ...,
        [ 0.0000e+00,  6.8530e-05,  1.8945e-03,  ..., -8.5928e-04,
          1.4914e-03,  3.1534e-04],
        [ 0.0000e+00,  6.8530e-05,  1.8945e-03,  ..., -8.5928e-04,
          1.4914e-03,  3.1534e-04],
        [ 0.0000e+00,  6.8530e-05,  1.8945e-03,  ..., -8.5928e-04,
          1.4914e-03,  3.1534e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1224.5376, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0517, device='cuda:0')



h[100].sum tensor(46.2660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5367, device='cuda:0')



h[200].sum tensor(-4.4230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0060, 0.0013],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0060, 0.0013],
        ...,
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0061, 0.0013],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0061, 0.0013],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0061, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43992.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0068, 0.0000, 0.0000,  ..., 0.0279, 0.0000, 0.0249],
        [0.0054, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0149],
        [0.0050, 0.0000, 0.0000,  ..., 0.0271, 0.0000, 0.0118],
        ...,
        [0.0050, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0119],
        [0.0051, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0119],
        [0.0051, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0119]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398430.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1749.0402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(46.3184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8151.7100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-443.5218, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2181.4951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.6038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4170],
        [-0.6146],
        [-0.7484],
        ...,
        [-0.6632],
        [-0.6614],
        [-0.6609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-143660.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0069],
        [1.0075],
        [1.0055],
        ...,
        [1.0008],
        [1.0001],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367374.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0070],
        [1.0076],
        [1.0056],
        ...,
        [1.0008],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367385., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  7.0071e-05,  1.8953e-03,  ..., -8.6400e-04,
          1.5173e-03,  3.1048e-04],
        [ 0.0000e+00,  7.0071e-05,  1.8953e-03,  ..., -8.6400e-04,
          1.5173e-03,  3.1048e-04],
        [ 0.0000e+00,  7.0071e-05,  1.8953e-03,  ..., -8.6400e-04,
          1.5173e-03,  3.1048e-04],
        ...,
        [ 0.0000e+00,  7.0071e-05,  1.8953e-03,  ..., -8.6400e-04,
          1.5173e-03,  3.1048e-04],
        [ 0.0000e+00,  7.0071e-05,  1.8953e-03,  ..., -8.6400e-04,
          1.5173e-03,  3.1048e-04],
        [ 0.0000e+00,  7.0071e-05,  1.8953e-03,  ..., -8.6400e-04,
          1.5173e-03,  3.1048e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1252.5964, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6059, device='cuda:0')



h[100].sum tensor(47.9899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.9116, device='cuda:0')



h[200].sum tensor(-4.5692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0061, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0061, 0.0013],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0061, 0.0013],
        ...,
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0062, 0.0013],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0062, 0.0013],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0062, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43658.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0106, 0.0000, 0.0000,  ..., 0.0302, 0.0000, 0.0528],
        [0.0089, 0.0000, 0.0000,  ..., 0.0293, 0.0000, 0.0414],
        [0.0070, 0.0000, 0.0000,  ..., 0.0282, 0.0000, 0.0297],
        ...,
        [0.0047, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.0120],
        [0.0047, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.0120],
        [0.0047, 0.0000, 0.0000,  ..., 0.0272, 0.0000, 0.0120]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(391008.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1554.3591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8134.2300, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-438.7654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2142.4492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.9507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1200],
        [ 0.0953],
        [ 0.0615],
        ...,
        [-0.6661],
        [-0.6645],
        [-0.6644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135052.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0070],
        [1.0076],
        [1.0056],
        ...,
        [1.0008],
        [1.0001],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367385., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0071],
        [1.0077],
        [1.0057],
        ...,
        [1.0008],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367395.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.3356e-05,  1.8881e-03,  ..., -8.6816e-04,
          1.5440e-03,  3.0530e-04],
        [ 0.0000e+00,  8.3356e-05,  1.8881e-03,  ..., -8.6816e-04,
          1.5440e-03,  3.0530e-04],
        [ 0.0000e+00,  8.3356e-05,  1.8881e-03,  ..., -8.6816e-04,
          1.5440e-03,  3.0530e-04],
        ...,
        [ 0.0000e+00,  8.3356e-05,  1.8881e-03,  ..., -8.6816e-04,
          1.5440e-03,  3.0530e-04],
        [ 0.0000e+00,  8.3356e-05,  1.8881e-03,  ..., -8.6816e-04,
          1.5440e-03,  3.0530e-04],
        [ 0.0000e+00,  8.3356e-05,  1.8881e-03,  ..., -8.6816e-04,
          1.5440e-03,  3.0530e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1256.5303, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.3100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.1587, device='cuda:0')



h[100].sum tensor(47.6922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.6090, device='cuda:0')



h[200].sum tensor(-4.4422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0062, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0062, 0.0012],
        [0.0000, 0.0003, 0.0076,  ..., 0.0000, 0.0062, 0.0012],
        ...,
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0063, 0.0012],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0063, 0.0012],
        [0.0000, 0.0003, 0.0077,  ..., 0.0000, 0.0063, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42671.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0120],
        [0.0042, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0121],
        [0.0043, 0.0000, 0.0000,  ..., 0.0268, 0.0000, 0.0121],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122],
        [0.0043, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122],
        [0.0043, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387421.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1508.9700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(44.9922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8057.7729, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-431.6826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2263.8813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.0701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6090],
        [-0.6366],
        [-0.6480],
        ...,
        [-0.6697],
        [-0.6679],
        [-0.6674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161121.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0071],
        [1.0077],
        [1.0057],
        ...,
        [1.0008],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367395.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0073],
        [1.0078],
        [1.0058],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367406.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  9.2643e-05,  1.8839e-03,  ..., -8.6937e-04,
          1.5666e-03,  2.9527e-04],
        [ 0.0000e+00,  9.2643e-05,  1.8839e-03,  ..., -8.6937e-04,
          1.5666e-03,  2.9527e-04],
        [ 0.0000e+00,  9.2643e-05,  1.8839e-03,  ..., -8.6937e-04,
          1.5666e-03,  2.9527e-04],
        ...,
        [ 0.0000e+00,  9.2643e-05,  1.8839e-03,  ..., -8.6937e-04,
          1.5666e-03,  2.9527e-04],
        [ 0.0000e+00,  9.2643e-05,  1.8839e-03,  ..., -8.6937e-04,
          1.5666e-03,  2.9527e-04],
        [ 0.0000e+00,  9.2643e-05,  1.8839e-03,  ..., -8.6937e-04,
          1.5666e-03,  2.9527e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1269.2615, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.1260, device='cuda:0')



h[100].sum tensor(47.9050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5869, device='cuda:0')



h[200].sum tensor(-4.4206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0063, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0063, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0063, 0.0012],
        ...,
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0064, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0064, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0064, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43769.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0050, 0.0000, 0.0000,  ..., 0.0273, 0.0000, 0.0185],
        [0.0085, 0.0000, 0.0000,  ..., 0.0296, 0.0000, 0.0398],
        [0.0126, 0.0000, 0.0000,  ..., 0.0321, 0.0000, 0.0650],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122],
        [0.0040, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122],
        [0.0040, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(395782.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1428.4722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(51.3922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8419.9434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-443.1637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2040.9968, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.3257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0222],
        [ 0.1019],
        [ 0.1715],
        ...,
        [-0.6738],
        [-0.6719],
        [-0.6715]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-134776.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0073],
        [1.0078],
        [1.0058],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367406.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0074],
        [1.0080],
        [1.0059],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367417.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1281.1371, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.3008, device='cuda:0')



h[100].sum tensor(47.6939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.7052, device='cuda:0')



h[200].sum tensor(-4.3892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7899, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0063, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0063, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0063, 0.0012],
        ...,
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0064, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0064, 0.0012],
        [0.0000, 0.0004, 0.0076,  ..., 0.0000, 0.0064, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43073.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0000, 0.0000,  ..., 0.0264, 0.0000, 0.0162],
        [0.0039, 0.0000, 0.0000,  ..., 0.0265, 0.0000, 0.0159],
        [0.0047, 0.0000, 0.0000,  ..., 0.0270, 0.0000, 0.0191],
        ...,
        [0.0056, 0.0000, 0.0000,  ..., 0.0280, 0.0000, 0.0217],
        [0.0086, 0.0000, 0.0000,  ..., 0.0298, 0.0000, 0.0404],
        [0.0101, 0.0000, 0.0000,  ..., 0.0307, 0.0000, 0.0498]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(389276.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1445.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(46.4159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8294.2559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-437.0446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2102.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.1071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4627],
        [-0.3943],
        [-0.2771],
        ...,
        [-0.4406],
        [-0.2819],
        [-0.1829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152388.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0074],
        [1.0080],
        [1.0059],
        ...,
        [1.0009],
        [1.0002],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367417.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0075],
        [1.0081],
        [1.0060],
        ...,
        [1.0009],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367427.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.0228e-04,  1.8843e-03,  ..., -8.7008e-04,
          1.5742e-03,  2.8467e-04],
        [ 0.0000e+00,  1.0228e-04,  1.8843e-03,  ..., -8.7008e-04,
          1.5742e-03,  2.8467e-04],
        [-1.0720e-02,  2.3701e-02,  2.6217e-02,  ...,  5.3796e-05,
          1.3080e-02,  5.9738e-03],
        ...,
        [ 0.0000e+00,  1.0228e-04,  1.8843e-03,  ..., -8.7008e-04,
          1.5742e-03,  2.8467e-04],
        [ 0.0000e+00,  1.0228e-04,  1.8843e-03,  ..., -8.7008e-04,
          1.5742e-03,  2.8467e-04],
        [ 0.0000e+00,  1.0228e-04,  1.8843e-03,  ..., -8.7008e-04,
          1.5742e-03,  2.8467e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1647.5100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-26.6160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.1094, device='cuda:0')



h[100].sum tensor(70.8101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-20.3710, device='cuda:0')



h[200].sum tensor(-7.6579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.5571, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.1116e-04, 7.5748e-03,  ..., 0.0000e+00, 6.3281e-03,
         1.1443e-03],
        [0.0000e+00, 4.3616e-02, 5.2142e-02,  ..., 5.4221e-05, 2.7409e-02,
         1.1563e-02],
        [0.0000e+00, 4.3596e-02, 5.2123e-02,  ..., 5.4154e-05, 2.7401e-02,
         1.1558e-02],
        ...,
        [0.0000e+00, 4.1522e-04, 7.6496e-03,  ..., 0.0000e+00, 6.3906e-03,
         1.1556e-03],
        [0.0000e+00, 4.1532e-04, 7.6515e-03,  ..., 0.0000e+00, 6.3922e-03,
         1.1559e-03],
        [0.0000e+00, 4.1536e-04, 7.6522e-03,  ..., 0.0000e+00, 6.3927e-03,
         1.1560e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60153.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0903],
        [0.0308, 0.0000, 0.0000,  ..., 0.0442, 0.0000, 0.1640],
        [0.0384, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.2040],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0123],
        [0.0041, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0123],
        [0.0041, 0.0000, 0.0000,  ..., 0.0275, 0.0000, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(474550.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2028.4329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(122.9303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9807.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-645.2200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1922.6990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.6130, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2178],
        [ 0.2161],
        [ 0.2159],
        ...,
        [-0.6868],
        [-0.6849],
        [-0.6844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-135713.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0075],
        [1.0081],
        [1.0060],
        ...,
        [1.0009],
        [1.0002],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367427.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(563.8568, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0076],
        [1.0082],
        [1.0061],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367437.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036,  0.0080,  0.0100,  ..., -0.0006,  0.0054,  0.0022],
        [-0.0157,  0.0347,  0.0375,  ...,  0.0005,  0.0184,  0.0086],
        [-0.0243,  0.0537,  0.0572,  ...,  0.0012,  0.0277,  0.0132],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1465.7621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.3779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.1328, device='cuda:0')



h[100].sum tensor(57.9507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.6509, device='cuda:0')



h[200].sum tensor(-5.8465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4158, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1141, 0.1248,  ..., 0.0010, 0.0618, 0.0285],
        [0.0000, 0.1281, 0.1393,  ..., 0.0021, 0.0686, 0.0319],
        [0.0000, 0.1698, 0.1823,  ..., 0.0031, 0.0889, 0.0419],
        ...,
        [0.0000, 0.0004, 0.0077,  ..., 0.0000, 0.0064, 0.0011],
        [0.0000, 0.0004, 0.0077,  ..., 0.0000, 0.0064, 0.0011],
        [0.0000, 0.0004, 0.0077,  ..., 0.0000, 0.0064, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53150.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0951, 0.0000, 0.0000,  ..., 0.0872, 0.0000, 0.4982],
        [0.1033, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.5398],
        [0.1167, 0.0000, 0.0000,  ..., 0.1021, 0.0000, 0.6054],
        ...,
        [0.0042, 0.0000, 0.0000,  ..., 0.0279, 0.0000, 0.0124],
        [0.0042, 0.0000, 0.0000,  ..., 0.0279, 0.0000, 0.0124],
        [0.0042, 0.0000, 0.0000,  ..., 0.0279, 0.0000, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(447054.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1853.5187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(89.0486, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9344.1436, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-559.9092, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1997.4280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.7720, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1654],
        [ 0.1630],
        [ 0.1578],
        ...,
        [-0.6951],
        [-0.6932],
        [-0.6928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146218.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0076],
        [1.0082],
        [1.0061],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367437.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0077],
        [1.0083],
        [1.0062],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367448.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9749e-03,  1.9922e-02,  2.2319e-02,  ..., -8.7630e-05,
          1.1250e-02,  5.0305e-03],
        [ 0.0000e+00,  1.0724e-04,  1.8899e-03,  ..., -8.6442e-04,
          1.5916e-03,  2.5516e-04],
        [-2.4586e-02,  5.4386e-02,  5.7853e-02,  ...,  1.2635e-03,
          2.8051e-02,  1.3336e-02],
        ...,
        [ 0.0000e+00,  1.0724e-04,  1.8899e-03,  ..., -8.6442e-04,
          1.5916e-03,  2.5516e-04],
        [ 0.0000e+00,  1.0724e-04,  1.8899e-03,  ..., -8.6442e-04,
          1.5916e-03,  2.5516e-04],
        [ 0.0000e+00,  1.0724e-04,  1.8899e-03,  ..., -8.6442e-04,
          1.5916e-03,  2.5516e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1642.6760, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-25.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.8602, device='cuda:0')



h[100].sum tensor(68.5414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-19.5258, device='cuda:0')



h[200].sum tensor(-7.3273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.9947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0167, 0.0244,  ..., 0.0000, 0.0143, 0.0049],
        [0.0000, 0.1205, 0.1314,  ..., 0.0015, 0.0650, 0.0300],
        [0.0000, 0.1123, 0.1230,  ..., 0.0018, 0.0609, 0.0280],
        ...,
        [0.0000, 0.0004, 0.0077,  ..., 0.0000, 0.0065, 0.0010],
        [0.0000, 0.0004, 0.0077,  ..., 0.0000, 0.0065, 0.0010],
        [0.0000, 0.0004, 0.0077,  ..., 0.0000, 0.0065, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57108.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0295, 0.0000, 0.0000,  ..., 0.0442, 0.0000, 0.1539],
        [0.0645, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.3371],
        [0.0813, 0.0000, 0.0000,  ..., 0.0790, 0.0000, 0.4225],
        ...,
        [0.0042, 0.0000, 0.0000,  ..., 0.0285, 0.0000, 0.0124],
        [0.0056, 0.0000, 0.0000,  ..., 0.0292, 0.0000, 0.0223],
        [0.0077, 0.0000, 0.0000,  ..., 0.0303, 0.0000, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449159.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1887.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(105.4666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9365.2207, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-610.2390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2006.3335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.2415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1089],
        [ 0.1621],
        [ 0.1728],
        ...,
        [-0.6349],
        [-0.5211],
        [-0.3565]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-155486.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0077],
        [1.0083],
        [1.0062],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367448.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0078],
        [1.0084],
        [1.0063],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367459.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [-0.0087,  0.0194,  0.0217,  ..., -0.0001,  0.0110,  0.0049],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1220.8650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.5196, device='cuda:0')



h[100].sum tensor(40.5225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.1469, device='cuda:0')



h[200].sum tensor(-3.4052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.0874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0076,  ..., 0.0000, 0.0065, 0.0010],
        [0.0000, 0.0270, 0.0350,  ..., 0.0000, 0.0194, 0.0073],
        [0.0000, 0.0250, 0.0329,  ..., 0.0000, 0.0185, 0.0069],
        ...,
        [0.0000, 0.0005, 0.0077,  ..., 0.0000, 0.0065, 0.0010],
        [0.0000, 0.0005, 0.0077,  ..., 0.0000, 0.0065, 0.0010],
        [0.0000, 0.0005, 0.0077,  ..., 0.0000, 0.0065, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38618.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0081, 0.0000, 0.0000,  ..., 0.0308, 0.0000, 0.0397],
        [0.0171, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0978],
        [0.0197, 0.0000, 0.0000,  ..., 0.0368, 0.0000, 0.1207],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0290, 0.0000, 0.0123],
        [0.0041, 0.0000, 0.0000,  ..., 0.0290, 0.0000, 0.0123],
        [0.0041, 0.0000, 0.0000,  ..., 0.0290, 0.0000, 0.0123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(374260.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1363.8303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.5095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8067.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-384.4448, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2111.7754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-343.5567, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1943],
        [ 0.0055],
        [ 0.1383],
        ...,
        [-0.7075],
        [-0.7056],
        [-0.7051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168917.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0078],
        [1.0084],
        [1.0063],
        ...,
        [1.0010],
        [1.0003],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367459.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0085],
        [1.0064],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367470.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0157,  0.0349,  0.0377,  ...,  0.0005,  0.0186,  0.0086],
        [-0.0158,  0.0351,  0.0379,  ...,  0.0005,  0.0187,  0.0086],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1473.5220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.1950, device='cuda:0')



h[100].sum tensor(55.7595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.0164, device='cuda:0')



h[200].sum tensor(-5.5431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1437, 0.1552,  ..., 0.0021, 0.0764, 0.0354],
        [0.0000, 0.1039, 0.1141,  ..., 0.0014, 0.0569, 0.0258],
        [0.0000, 0.0956, 0.1056,  ..., 0.0007, 0.0529, 0.0238],
        ...,
        [0.0000, 0.0005, 0.0077,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0005, 0.0077,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0005, 0.0077,  ..., 0.0000, 0.0066, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51056.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0821, 0.0000, 0.0000,  ..., 0.0808, 0.0000, 0.4308],
        [0.0820, 0.0000, 0.0000,  ..., 0.0806, 0.0000, 0.4334],
        [0.0768, 0.0000, 0.0000,  ..., 0.0766, 0.0000, 0.4121],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0295, 0.0000, 0.0124],
        [0.0040, 0.0000, 0.0000,  ..., 0.0295, 0.0000, 0.0124],
        [0.0040, 0.0000, 0.0000,  ..., 0.0295, 0.0000, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(437030.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1709.6388, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(76.7314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9200.8418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-535.4044, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1911.2209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.7863, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1819],
        [ 0.1850],
        [ 0.1884],
        ...,
        [-0.7106],
        [-0.7087],
        [-0.7083]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-146565.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0085],
        [1.0064],
        ...,
        [1.0010],
        [1.0003],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367470.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0079],
        [1.0086],
        [1.0065],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367481.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0034,  0.0076,  0.0096,  ..., -0.0006,  0.0053,  0.0020],
        [-0.0046,  0.0104,  0.0124,  ..., -0.0005,  0.0066,  0.0027],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1295.9686, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2217, device='cuda:0')



h[100].sum tensor(43.4847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.2985, device='cuda:0')



h[200].sum tensor(-3.8362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0681, 0.0771,  ..., 0.0002, 0.0395, 0.0171],
        [0.0000, 0.0227, 0.0304,  ..., 0.0000, 0.0174, 0.0062],
        [0.0000, 0.0109, 0.0183,  ..., 0.0000, 0.0117, 0.0033],
        ...,
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41447.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0429, 0.0000, 0.0000,  ..., 0.0538, 0.0000, 0.2418],
        [0.0241, 0.0000, 0.0000,  ..., 0.0417, 0.0000, 0.1377],
        [0.0127, 0.0000, 0.0000,  ..., 0.0347, 0.0000, 0.0699],
        ...,
        [0.0039, 0.0000, 0.0000,  ..., 0.0299, 0.0000, 0.0124],
        [0.0039, 0.0000, 0.0000,  ..., 0.0299, 0.0000, 0.0124],
        [0.0039, 0.0000, 0.0000,  ..., 0.0299, 0.0000, 0.0124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390421., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1338.6746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(32.6654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8390.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-418.3638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1972.0813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-356.1041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1950],
        [ 0.1345],
        [ 0.0164],
        ...,
        [-0.7153],
        [-0.7134],
        [-0.7129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-151149.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0079],
        [1.0086],
        [1.0065],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367481.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0080],
        [1.0086],
        [1.0065],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367492., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1351.5181, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7783, device='cuda:0')



h[100].sum tensor(45.5997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3517, device='cuda:0')



h[200].sum tensor(-4.1255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0076,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0006, 0.0076,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0135, 0.0209,  ..., 0.0000, 0.0129, 0.0039],
        ...,
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43795.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0089, 0.0000, 0.0000,  ..., 0.0326, 0.0000, 0.0433],
        [0.0124, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0641],
        [0.0189, 0.0000, 0.0000,  ..., 0.0387, 0.0000, 0.1038],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0301, 0.0000, 0.0126],
        [0.0040, 0.0000, 0.0000,  ..., 0.0301, 0.0000, 0.0126],
        [0.0040, 0.0000, 0.0000,  ..., 0.0301, 0.0000, 0.0126]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407720.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1519.3621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(40.5583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8707.2754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-448.0674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2046.8237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-365.1930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0975],
        [ 0.1302],
        [ 0.1683],
        ...,
        [-0.7227],
        [-0.7207],
        [-0.7202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152913.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0080],
        [1.0086],
        [1.0065],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367492., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0087],
        [1.0066],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367502.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1364.1442, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.2948, device='cuda:0')



h[100].sum tensor(45.2364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.0245, device='cuda:0')



h[200].sum tensor(-4.0672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0076,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0066, 0.0008],
        ...,
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008],
        [0.0000, 0.0006, 0.0077,  ..., 0.0000, 0.0067, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42943.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0049, 0.0000, 0.0000,  ..., 0.0302, 0.0000, 0.0203],
        [0.0043, 0.0000, 0.0000,  ..., 0.0301, 0.0000, 0.0152],
        [0.0042, 0.0000, 0.0000,  ..., 0.0302, 0.0000, 0.0130],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.0127],
        [0.0043, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.0127],
        [0.0043, 0.0000, 0.0000,  ..., 0.0305, 0.0000, 0.0127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(398372.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1526.2859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(31.3816, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8581.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-437.7206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2204.2964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.8165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1480],
        [-0.3060],
        [-0.4700],
        ...,
        [-0.7292],
        [-0.7293],
        [-0.7298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162735., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0087],
        [1.0066],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367502.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0081],
        [1.0087],
        [1.0067],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367513.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.7009e-03,  1.7272e-02,  1.9590e-02,  ..., -1.9666e-04,
          9.9956e-03,  4.3198e-03],
        [-8.7035e-03,  1.9505e-02,  2.1892e-02,  ..., -1.0938e-04,
          1.1084e-02,  4.8576e-03],
        [-8.9015e-03,  1.9946e-02,  2.2346e-02,  ..., -9.2144e-05,
          1.1299e-02,  4.9639e-03],
        ...,
        [ 0.0000e+00,  1.2308e-04,  1.9126e-03,  ..., -8.6710e-04,
          1.6353e-03,  1.8809e-04],
        [ 0.0000e+00,  1.2308e-04,  1.9126e-03,  ..., -8.6710e-04,
          1.6353e-03,  1.8809e-04],
        [ 0.0000e+00,  1.2308e-04,  1.9126e-03,  ..., -8.6710e-04,
          1.6353e-03,  1.8809e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1395.2092, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0130, device='cuda:0')



h[100].sum tensor(46.2628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5104, device='cuda:0')



h[200].sum tensor(-4.1898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0636, 0.0727,  ..., 0.0000, 0.0373, 0.0159],
        [0.0000, 0.0741, 0.0836,  ..., 0.0003, 0.0425, 0.0185],
        [0.0000, 0.0935, 0.1036,  ..., 0.0004, 0.0519, 0.0232],
        ...,
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44141.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0711, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.3806],
        [0.0643, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.3482],
        [0.0570, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.3099],
        ...,
        [0.0045, 0.0000, 0.0000,  ..., 0.0309, 0.0000, 0.0128],
        [0.0045, 0.0000, 0.0000,  ..., 0.0309, 0.0000, 0.0128],
        [0.0045, 0.0000, 0.0000,  ..., 0.0309, 0.0000, 0.0128]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407738.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1638.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.2137, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8767.4434, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-452.2029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2299.0630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.0532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1639],
        [ 0.1752],
        [ 0.1740],
        ...,
        [-0.7449],
        [-0.7428],
        [-0.7422]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-161585.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0081],
        [1.0087],
        [1.0067],
        ...,
        [1.0011],
        [1.0004],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367513.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0082],
        [1.0088],
        [1.0068],
        ...,
        [1.0012],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367524.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0082,  0.0183,  0.0207,  ..., -0.0002,  0.0105,  0.0046],
        [-0.0034,  0.0078,  0.0098,  ..., -0.0006,  0.0054,  0.0020],
        [-0.0047,  0.0106,  0.0128,  ..., -0.0005,  0.0068,  0.0027],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1363.5079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3150, device='cuda:0')



h[100].sum tensor(43.3603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3616, device='cuda:0')



h[200].sum tensor(-3.7757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8958, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.0702e-02, 3.8885e-02,  ..., 0.0000e+00, 2.1292e-02,
         8.0426e-03],
        [0.0000e+00, 6.8278e-02, 7.7638e-02,  ..., 5.8032e-06, 3.9627e-02,
         1.7098e-02],
        [0.0000e+00, 3.0814e-02, 3.9022e-02,  ..., 5.8068e-06, 2.1365e-02,
         8.0714e-03],
        ...,
        [0.0000e+00, 4.7578e-04, 7.8030e-03,  ..., 0.0000e+00, 6.6228e-03,
         7.6584e-04],
        [0.0000e+00, 4.7588e-04, 7.8046e-03,  ..., 0.0000e+00, 6.6243e-03,
         7.6600e-04],
        [0.0000e+00, 4.7593e-04, 7.8054e-03,  ..., 0.0000e+00, 6.6249e-03,
         7.6608e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40887.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0263, 0.0000, 0.0000,  ..., 0.0429, 0.0000, 0.1505],
        [0.0345, 0.0000, 0.0000,  ..., 0.0484, 0.0000, 0.1937],
        [0.0251, 0.0000, 0.0000,  ..., 0.0427, 0.0000, 0.1390],
        ...,
        [0.0045, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0130],
        [0.0045, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0130],
        [0.0045, 0.0000, 0.0000,  ..., 0.0313, 0.0000, 0.0130]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390953.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1577.8135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(19.9334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8277.7773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-414.9041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2563.1062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-356.6165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1797],
        [ 0.1890],
        [ 0.1066],
        ...,
        [-0.7525],
        [-0.7504],
        [-0.7499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194309.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0082],
        [1.0088],
        [1.0068],
        ...,
        [1.0012],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367524.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0083],
        [1.0088],
        [1.0068],
        ...,
        [1.0012],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367535., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0038,  0.0085,  0.0106,  ..., -0.0005,  0.0057,  0.0022],
        [-0.0085,  0.0191,  0.0214,  ..., -0.0001,  0.0109,  0.0048],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1671.8582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-22.2696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.5855, device='cuda:0')



h[100].sum tensor(61.5564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.3103, device='cuda:0')



h[200].sum tensor(-6.2272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0666, 0.0758,  ..., 0.0000, 0.0388, 0.0167],
        [0.0000, 0.0307, 0.0388,  ..., 0.0000, 0.0213, 0.0081],
        [0.0000, 0.0265, 0.0346,  ..., 0.0000, 0.0192, 0.0071],
        ...,
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60174.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0342, 0.0000, 0.0000,  ..., 0.0483, 0.0000, 0.1939],
        [0.0259, 0.0000, 0.0000,  ..., 0.0430, 0.0000, 0.1493],
        [0.0198, 0.0000, 0.0000,  ..., 0.0399, 0.0000, 0.1095],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.0132],
        [0.0043, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.0132],
        [0.0043, 0.0000, 0.0000,  ..., 0.0315, 0.0000, 0.0132]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(501859.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2259.7612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(114.6097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10198.5352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-645.9233, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2154.5017, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-430.2476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1937],
        [ 0.1544],
        [ 0.0388],
        ...,
        [-0.7549],
        [-0.7528],
        [-0.7521]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-149205.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0083],
        [1.0088],
        [1.0068],
        ...,
        [1.0012],
        [1.0005],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367535., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(510.6434, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0084],
        [1.0089],
        [1.0069],
        ...,
        [1.0012],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367546., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0173,  0.0388,  0.0418,  ...,  0.0006,  0.0205,  0.0095],
        [-0.0345,  0.0774,  0.0815,  ...,  0.0021,  0.0393,  0.0188],
        [-0.0174,  0.0390,  0.0420,  ...,  0.0006,  0.0206,  0.0096],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1379.5356, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.9308, device='cuda:0')



h[100].sum tensor(42.5740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.1017, device='cuda:0')



h[200].sum tensor(-3.6303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.7228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1902, 0.2032,  ..., 0.0039, 0.0990, 0.0466],
        [0.0000, 0.1775, 0.1901,  ..., 0.0034, 0.0928, 0.0435],
        [0.0000, 0.1931, 0.2062,  ..., 0.0040, 0.1004, 0.0473],
        ...,
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40783.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1065, 0.0000, 0.0000,  ..., 0.0983, 0.0000, 0.5507],
        [0.1133, 0.0000, 0.0000,  ..., 0.1029, 0.0000, 0.5856],
        [0.1129, 0.0000, 0.0000,  ..., 0.1026, 0.0000, 0.5843],
        ...,
        [0.0039, 0.0000, 0.0000,  ..., 0.0317, 0.0000, 0.0136],
        [0.0039, 0.0000, 0.0000,  ..., 0.0317, 0.0000, 0.0136],
        [0.0039, 0.0000, 0.0000,  ..., 0.0317, 0.0000, 0.0136]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(392975.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1391.0281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(33.3544, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8193.7988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-407.4710, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2352.9561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.9778, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1925],
        [ 0.1886],
        [ 0.1918],
        ...,
        [-0.7567],
        [-0.7547],
        [-0.7542]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172157.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0084],
        [1.0089],
        [1.0069],
        ...,
        [1.0012],
        [1.0005],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367546., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0085],
        [1.0090],
        [1.0070],
        ...,
        [1.0013],
        [1.0006],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367556.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [-0.0042,  0.0096,  0.0117,  ..., -0.0005,  0.0063,  0.0025],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1429.8743, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.1810, device='cuda:0')



h[100].sum tensor(44.9319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.9475, device='cuda:0')



h[200].sum tensor(-3.9216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0503, 0.0590,  ..., 0.0000, 0.0308, 0.0129],
        [0.0000, 0.0084, 0.0158,  ..., 0.0000, 0.0104, 0.0027],
        [0.0000, 0.0101, 0.0176,  ..., 0.0000, 0.0113, 0.0032],
        ...,
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42469.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0225, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.1364],
        [0.0125, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0740],
        [0.0119, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0669],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0140],
        [0.0037, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0140],
        [0.0037, 0.0000, 0.0000,  ..., 0.0319, 0.0000, 0.0140]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(400103.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1397.1573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(46.7670, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8243.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-428.7334, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2331.4604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-352.6053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1361],
        [-0.2662],
        [-0.3251],
        ...,
        [-0.7605],
        [-0.7585],
        [-0.7580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170663.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0085],
        [1.0090],
        [1.0070],
        ...,
        [1.0013],
        [1.0006],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367556.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0086],
        [1.0090],
        [1.0070],
        ...,
        [1.0013],
        [1.0006],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367567.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  1.2645e-04,  1.9239e-03,  ..., -8.7977e-04,
          1.6346e-03,  2.2087e-04],
        [-1.0903e-02,  2.4580e-02,  2.7130e-02,  ...,  7.1225e-05,
          1.3559e-02,  6.1198e-03],
        [-1.6308e-02,  3.6703e-02,  3.9626e-02,  ...,  5.4270e-04,
          1.9471e-02,  9.0443e-03],
        ...,
        [ 0.0000e+00,  1.2645e-04,  1.9239e-03,  ..., -8.7977e-04,
          1.6346e-03,  2.2087e-04],
        [ 0.0000e+00,  1.2645e-04,  1.9239e-03,  ..., -8.7977e-04,
          1.6346e-03,  2.2087e-04],
        [ 0.0000e+00,  1.2645e-04,  1.9239e-03,  ..., -8.7977e-04,
          1.6346e-03,  2.2087e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1530.3152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.7288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.2179, device='cuda:0')



h[100].sum tensor(50.5470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.0022, device='cuda:0')



h[200].sum tensor(-4.6374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.7052e-02, 5.5716e-02,  ..., 7.1633e-05, 2.9273e-02,
         1.2116e-02],
        [0.0000e+00, 6.7215e-02, 7.6519e-02,  ..., 5.4752e-04, 3.9122e-02,
         1.6982e-02],
        [0.0000e+00, 1.0244e-01, 1.1283e-01,  ..., 4.9977e-04, 5.6299e-02,
         2.5479e-02],
        ...,
        [0.0000e+00, 5.1423e-04, 7.8242e-03,  ..., 0.0000e+00, 6.6474e-03,
         8.9822e-04],
        [0.0000e+00, 5.1433e-04, 7.8257e-03,  ..., 0.0000e+00, 6.6487e-03,
         8.9840e-04],
        [0.0000e+00, 5.1438e-04, 7.8265e-03,  ..., 0.0000e+00, 6.6494e-03,
         8.9849e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47114.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0600, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.3292],
        [0.0716, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.3884],
        [0.0854, 0.0000, 0.0000,  ..., 0.0842, 0.0000, 0.4599],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0143],
        [0.0034, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0143],
        [0.0034, 0.0000, 0.0000,  ..., 0.0320, 0.0000, 0.0143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423314.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1432.0608, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(73.9705, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8693.7852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-482.5944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2189.6611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.9155, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1525],
        [ 0.1590],
        [ 0.1647],
        ...,
        [-0.7646],
        [-0.7624],
        [-0.7617]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-147577.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0086],
        [1.0090],
        [1.0070],
        ...,
        [1.0013],
        [1.0006],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367567.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0087],
        [1.0091],
        [1.0071],
        ...,
        [1.0013],
        [1.0006],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367578.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [-0.0045,  0.0103,  0.0124,  ..., -0.0005,  0.0066,  0.0027],
        ...,
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002],
        [ 0.0000,  0.0001,  0.0019,  ..., -0.0009,  0.0016,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1456.0452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4257, device='cuda:0')



h[100].sum tensor(45.5533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1131, device='cuda:0')



h[200].sum tensor(-3.9308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0078,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0107, 0.0184,  ..., 0.0000, 0.0116, 0.0034],
        [0.0000, 0.0565, 0.0656,  ..., 0.0007, 0.0339, 0.0144],
        ...,
        [0.0000, 0.0005, 0.0079,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0005, 0.0079,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0005, 0.0079,  ..., 0.0000, 0.0066, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43559.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0112, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0613],
        [0.0220, 0.0000, 0.0000,  ..., 0.0430, 0.0000, 0.1242],
        [0.0438, 0.0000, 0.0000,  ..., 0.0568, 0.0000, 0.2421],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0323, 0.0000, 0.0146],
        [0.0034, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.0146],
        [0.0034, 0.0000, 0.0000,  ..., 0.0324, 0.0000, 0.0146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(410768.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1347.5540, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(59.3267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8415.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-441.2148, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2359.7466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-358.8425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0313],
        [ 0.1407],
        [ 0.2054],
        ...,
        [-0.7750],
        [-0.7730],
        [-0.7725]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-160099.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0087],
        [1.0091],
        [1.0071],
        ...,
        [1.0013],
        [1.0006],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367578.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0092],
        [1.0072],
        ...,
        [1.0013],
        [1.0006],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367589.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  8.6913e-05,  1.9587e-03,  ..., -8.8520e-04,
          1.6170e-03,  2.2316e-04],
        [ 0.0000e+00,  8.6913e-05,  1.9587e-03,  ..., -8.8520e-04,
          1.6170e-03,  2.2316e-04],
        [ 0.0000e+00,  8.6913e-05,  1.9587e-03,  ..., -8.8520e-04,
          1.6170e-03,  2.2316e-04],
        ...,
        [ 0.0000e+00,  8.6913e-05,  1.9587e-03,  ..., -8.8520e-04,
          1.6170e-03,  2.2316e-04],
        [ 0.0000e+00,  8.6913e-05,  1.9587e-03,  ..., -8.8520e-04,
          1.6170e-03,  2.2316e-04],
        [ 0.0000e+00,  8.6913e-05,  1.9587e-03,  ..., -8.8520e-04,
          1.6170e-03,  2.2316e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1318.8636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.5949, device='cuda:0')



h[100].sum tensor(37.2105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.8447, device='cuda:0')



h[200].sum tensor(-2.7895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.2208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0469, 0.0559,  ..., 0.0001, 0.0292, 0.0121],
        [0.0000, 0.0004, 0.0079,  ..., 0.0000, 0.0065, 0.0009],
        [0.0000, 0.0004, 0.0079,  ..., 0.0000, 0.0065, 0.0009],
        ...,
        [0.0000, 0.0004, 0.0080,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0004, 0.0080,  ..., 0.0000, 0.0066, 0.0009],
        [0.0000, 0.0004, 0.0080,  ..., 0.0000, 0.0066, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37812.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0287, 0.0000, 0.0000,  ..., 0.0482, 0.0000, 0.1525],
        [0.0120, 0.0000, 0.0000,  ..., 0.0379, 0.0000, 0.0605],
        [0.0068, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0315],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0329, 0.0000, 0.0146],
        [0.0037, 0.0000, 0.0000,  ..., 0.0330, 0.0000, 0.0146],
        [0.0037, 0.0000, 0.0000,  ..., 0.0330, 0.0000, 0.0147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(387415.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1248.7327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(30.1171, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7965.1377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-371.3023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2591.6726, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-341.1465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0190],
        [-0.2227],
        [-0.5059],
        ...,
        [-0.7904],
        [-0.7883],
        [-0.7877]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173565.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0092],
        [1.0072],
        ...,
        [1.0013],
        [1.0006],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367589.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0088],
        [1.0092],
        [1.0073],
        ...,
        [1.0014],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367599.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.8658e-03,  2.0028e-02,  2.2568e-02,  ..., -1.1402e-04,
          1.1344e-02,  5.0417e-03],
        [-1.7885e-02,  4.0343e-02,  4.3511e-02,  ...,  6.7340e-04,
          2.1252e-02,  9.9452e-03],
        [-2.5928e-02,  5.8456e-02,  6.2186e-02,  ...,  1.3755e-03,
          3.0086e-02,  1.4318e-02],
        ...,
        [ 0.0000e+00,  6.0573e-05,  1.9811e-03,  ..., -8.8801e-04,
          1.6056e-03,  2.2181e-04],
        [ 0.0000e+00,  6.0573e-05,  1.9811e-03,  ..., -8.8801e-04,
          1.6056e-03,  2.2181e-04],
        [ 0.0000e+00,  6.0573e-05,  1.9811e-03,  ..., -8.8801e-04,
          1.6056e-03,  2.2181e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1359.5552, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9789, device='cuda:0')



h[100].sum tensor(39.9483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7811, device='cuda:0')



h[200].sum tensor(-3.0791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8439, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.3162e-01, 1.4342e-01,  ..., 1.8405e-03, 7.0534e-02,
         3.2604e-02],
        [0.0000e+00, 1.8732e-01, 2.0086e-01,  ..., 3.7829e-03, 9.7718e-02,
         4.6051e-02],
        [0.0000e+00, 2.7650e-01, 2.9281e-01,  ..., 7.1241e-03, 1.4122e-01,
         6.7579e-02],
        ...,
        [0.0000e+00, 2.4644e-04, 8.0599e-03,  ..., 0.0000e+00, 6.5322e-03,
         9.0243e-04],
        [0.0000e+00, 2.4648e-04, 8.0615e-03,  ..., 0.0000e+00, 6.5335e-03,
         9.0260e-04],
        [0.0000e+00, 2.4651e-04, 8.0622e-03,  ..., 0.0000e+00, 6.5341e-03,
         9.0269e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38233.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1147, 0.0000, 0.0000,  ..., 0.1040, 0.0000, 0.5975],
        [0.1652, 0.0000, 0.0000,  ..., 0.1387, 0.0000, 0.8421],
        [0.2176, 0.0000, 0.0000,  ..., 0.1745, 0.0000, 1.0953],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0333, 0.0000, 0.0147],
        [0.0040, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0147],
        [0.0040, 0.0000, 0.0000,  ..., 0.0334, 0.0000, 0.0147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(386053.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1366.0491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(29.3376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7846.1431, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-379.5025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2872.1738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-339.2862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1470],
        [ 0.1106],
        [ 0.0764],
        ...,
        [-0.8034],
        [-0.8011],
        [-0.8006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201785.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0088],
        [1.0092],
        [1.0073],
        ...,
        [1.0014],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367599.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0089],
        [1.0093],
        [1.0074],
        ...,
        [1.0014],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367610.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  4.6195e-05,  1.9949e-03,  ..., -8.9037e-04,
          1.6000e-03,  2.2444e-04],
        [ 0.0000e+00,  4.6195e-05,  1.9949e-03,  ..., -8.9037e-04,
          1.6000e-03,  2.2444e-04],
        [ 0.0000e+00,  4.6195e-05,  1.9949e-03,  ..., -8.9037e-04,
          1.6000e-03,  2.2444e-04],
        ...,
        [ 0.0000e+00,  4.6195e-05,  1.9949e-03,  ..., -8.9037e-04,
          1.6000e-03,  2.2444e-04],
        [ 0.0000e+00,  4.6195e-05,  1.9949e-03,  ..., -8.9037e-04,
          1.6000e-03,  2.2444e-04],
        [ 0.0000e+00,  4.6195e-05,  1.9949e-03,  ..., -8.9037e-04,
          1.6000e-03,  2.2444e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1474.2476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.8292, device='cuda:0')



h[100].sum tensor(47.0663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3861, device='cuda:0')



h[200].sum tensor(-3.9442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0116, 0.0198,  ..., 0.0000, 0.0120, 0.0037],
        [0.0000, 0.0002, 0.0081,  ..., 0.0000, 0.0065, 0.0009],
        [0.0000, 0.0143, 0.0226,  ..., 0.0000, 0.0133, 0.0043],
        ...,
        [0.0000, 0.0002, 0.0081,  ..., 0.0000, 0.0065, 0.0009],
        [0.0000, 0.0002, 0.0081,  ..., 0.0000, 0.0065, 0.0009],
        [0.0000, 0.0002, 0.0081,  ..., 0.0000, 0.0065, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44696.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0098, 0.0000, 0.0000,  ..., 0.0362, 0.0000, 0.0523],
        [0.0083, 0.0000, 0.0000,  ..., 0.0355, 0.0000, 0.0413],
        [0.0139, 0.0000, 0.0000,  ..., 0.0390, 0.0000, 0.0731],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0147],
        [0.0041, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0147],
        [0.0041, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(422638.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1616.5852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(57.8897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8484.3271, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-456.1584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2827.4624, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-367.8038, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4571],
        [-0.3617],
        [-0.1785],
        ...,
        [-0.8135],
        [-0.8113],
        [-0.8107]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192838.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0089],
        [1.0093],
        [1.0074],
        ...,
        [1.0014],
        [1.0007],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367610.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0090],
        [1.0094],
        [1.0075],
        ...,
        [1.0014],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367621.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.3080e-03,  7.5044e-03,  9.7078e-03,  ..., -6.0364e-04,
          5.2478e-03,  2.0266e-03],
        [ 0.0000e+00,  3.2877e-05,  2.0044e-03,  ..., -8.9269e-04,
          1.6033e-03,  2.2231e-04],
        [ 0.0000e+00,  3.2877e-05,  2.0044e-03,  ..., -8.9269e-04,
          1.6033e-03,  2.2231e-04],
        ...,
        [ 0.0000e+00,  3.2877e-05,  2.0044e-03,  ..., -8.9269e-04,
          1.6033e-03,  2.2231e-04],
        [ 0.0000e+00,  3.2877e-05,  2.0044e-03,  ..., -8.9269e-04,
          1.6033e-03,  2.2231e-04],
        [ 0.0000e+00,  3.2877e-05,  2.0044e-03,  ..., -8.9269e-04,
          1.6033e-03,  2.2231e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1652.7019, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.2910, device='cuda:0')



h[100].sum tensor(58.3134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.0814, device='cuda:0')



h[200].sum tensor(-5.3268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0263, 0.0351,  ..., 0.0000, 0.0192, 0.0072],
        [0.0000, 0.0480, 0.0575,  ..., 0.0000, 0.0298, 0.0125],
        [0.0000, 0.0799, 0.0904,  ..., 0.0004, 0.0454, 0.0202],
        ...,
        [0.0000, 0.0001, 0.0082,  ..., 0.0000, 0.0065, 0.0009],
        [0.0000, 0.0001, 0.0082,  ..., 0.0000, 0.0065, 0.0009],
        [0.0000, 0.0001, 0.0082,  ..., 0.0000, 0.0065, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52715.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0442, 0.0000, 0.0000,  ..., 0.0577, 0.0000, 0.2443],
        [0.0592, 0.0000, 0.0000,  ..., 0.0677, 0.0000, 0.3212],
        [0.0765, 0.0000, 0.0000,  ..., 0.0795, 0.0000, 0.4060],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0148],
        [0.0041, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0148],
        [0.0041, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(459301.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1839.8364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(94.3497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9096.4111, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.4011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2753.1846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.3454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1899],
        [ 0.1869],
        [ 0.1835],
        ...,
        [-0.8218],
        [-0.8195],
        [-0.8189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185666.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0090],
        [1.0094],
        [1.0075],
        ...,
        [1.0014],
        [1.0007],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367621.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0094],
        [1.0076],
        ...,
        [1.0015],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367632.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  5.8708e-05,  1.9908e-03,  ..., -8.9154e-04,
          1.6149e-03,  2.4014e-04],
        [ 0.0000e+00,  5.8708e-05,  1.9908e-03,  ..., -8.9154e-04,
          1.6149e-03,  2.4014e-04],
        [ 0.0000e+00,  5.8708e-05,  1.9908e-03,  ..., -8.9154e-04,
          1.6149e-03,  2.4014e-04],
        ...,
        [ 0.0000e+00,  5.8708e-05,  1.9908e-03,  ..., -8.9154e-04,
          1.6149e-03,  2.4014e-04],
        [ 0.0000e+00,  5.8708e-05,  1.9908e-03,  ..., -8.9154e-04,
          1.6149e-03,  2.4014e-04],
        [ 0.0000e+00,  5.8708e-05,  1.9908e-03,  ..., -8.9154e-04,
          1.6149e-03,  2.4014e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1392.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6357, device='cuda:0')



h[100].sum tensor(42.3859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.2255, device='cuda:0')



h[200].sum tensor(-3.2093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.1397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0122, 0.0203,  ..., 0.0000, 0.0123, 0.0039],
        [0.0000, 0.0062, 0.0142,  ..., 0.0000, 0.0094, 0.0024],
        [0.0000, 0.0002, 0.0080,  ..., 0.0000, 0.0065, 0.0010],
        ...,
        [0.0000, 0.0002, 0.0081,  ..., 0.0000, 0.0066, 0.0010],
        [0.0000, 0.0002, 0.0081,  ..., 0.0000, 0.0066, 0.0010],
        [0.0000, 0.0002, 0.0081,  ..., 0.0000, 0.0066, 0.0010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41137.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0077, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0551],
        [0.0066, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0434],
        [0.0053, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0296],
        ...,
        [0.0039, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0151],
        [0.0039, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0151],
        [0.0039, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0151]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(407435.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1393.0200, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(48.1678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8250.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-410.3137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2640.7617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-358.4581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6198],
        [-0.6702],
        [-0.7424],
        ...,
        [-0.8249],
        [-0.8226],
        [-0.8221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174497.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0094],
        [1.0076],
        ...,
        [1.0015],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367632.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0091],
        [1.0095],
        [1.0076],
        ...,
        [1.0015],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367643.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.1485e-03,  9.4966e-03,  1.1656e-02,  ..., -5.2685e-04,
          6.2146e-03,  2.5402e-03],
        [-4.0400e-03,  9.2508e-03,  1.1403e-02,  ..., -5.3636e-04,
          6.0946e-03,  2.4807e-03],
        [-8.1885e-03,  1.8649e-02,  2.1090e-02,  ..., -1.7270e-04,
          1.0679e-02,  4.7521e-03],
        ...,
        [ 0.0000e+00,  9.8538e-05,  1.9685e-03,  ..., -8.9051e-04,
          1.6302e-03,  2.6880e-04],
        [ 0.0000e+00,  9.8538e-05,  1.9685e-03,  ..., -8.9051e-04,
          1.6302e-03,  2.6880e-04],
        [ 0.0000e+00,  9.8538e-05,  1.9685e-03,  ..., -8.9051e-04,
          1.6302e-03,  2.6880e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1537.7941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.0274, device='cuda:0')



h[100].sum tensor(50.4597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1967, device='cuda:0')



h[200].sum tensor(-4.2243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0312, 0.0397,  ..., 0.0000, 0.0216, 0.0085],
        [0.0000, 0.0644, 0.0739,  ..., 0.0000, 0.0378, 0.0165],
        [0.0000, 0.0313, 0.0398,  ..., 0.0000, 0.0217, 0.0086],
        ...,
        [0.0000, 0.0004, 0.0080,  ..., 0.0000, 0.0066, 0.0011],
        [0.0000, 0.0004, 0.0080,  ..., 0.0000, 0.0066, 0.0011],
        [0.0000, 0.0004, 0.0080,  ..., 0.0000, 0.0066, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46129.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0269, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.1597],
        [0.0376, 0.0000, 0.0000,  ..., 0.0528, 0.0000, 0.2236],
        [0.0334, 0.0000, 0.0000,  ..., 0.0500, 0.0000, 0.2015],
        ...,
        [0.0036, 0.0000, 0.0000,  ..., 0.0336, 0.0000, 0.0155],
        [0.0036, 0.0000, 0.0000,  ..., 0.0336, 0.0000, 0.0155],
        [0.0036, 0.0000, 0.0000,  ..., 0.0336, 0.0000, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425331.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1420.9893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(80.1956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8592.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-469.6510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2402.4016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-375.1044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2458],
        [ 0.2683],
        [ 0.2777],
        ...,
        [-0.8225],
        [-0.8204],
        [-0.8199]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-158858.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0091],
        [1.0095],
        [1.0076],
        ...,
        [1.0015],
        [1.0008],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367643.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 150.0 event: 750 loss: tensor(498.0736, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0092],
        [1.0095],
        [1.0077],
        ...,
        [1.0016],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367654.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0016,  0.0003],
        ...,
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0016,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0016,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1508.7607, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0754, device='cuda:0')



h[100].sum tensor(48.4597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5526, device='cuda:0')



h[200].sum tensor(-3.9215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0079,  ..., 0.0000, 0.0066, 0.0011],
        [0.0000, 0.0005, 0.0079,  ..., 0.0000, 0.0066, 0.0011],
        [0.0000, 0.0005, 0.0079,  ..., 0.0000, 0.0066, 0.0011],
        ...,
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0067, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0067, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0067, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44906.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0000, 0.0000,  ..., 0.0333, 0.0000, 0.0154],
        [0.0034, 0.0000, 0.0000,  ..., 0.0333, 0.0000, 0.0163],
        [0.0034, 0.0000, 0.0000,  ..., 0.0331, 0.0000, 0.0184],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0336, 0.0000, 0.0156],
        [0.0035, 0.0000, 0.0000,  ..., 0.0336, 0.0000, 0.0156],
        [0.0035, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425161.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1442.4666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(77.9653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8473.4785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-458.2652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2549.2495, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.1599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6580],
        [-0.6728],
        [-0.6695],
        ...,
        [-0.8262],
        [-0.8244],
        [-0.8242]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185431.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0092],
        [1.0095],
        [1.0077],
        ...,
        [1.0016],
        [1.0008],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367654.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0093],
        [1.0096],
        [1.0078],
        ...,
        [1.0016],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367665.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        ...,
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1408.6871, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.4758, device='cuda:0')



h[100].sum tensor(42.6982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.1173, device='cuda:0')



h[200].sum tensor(-3.1185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.0677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0140,  ..., 0.0000, 0.0096, 0.0026],
        [0.0000, 0.0125, 0.0202,  ..., 0.0000, 0.0125, 0.0041],
        [0.0000, 0.0065, 0.0141,  ..., 0.0000, 0.0096, 0.0026],
        ...,
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0067, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0067, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0067, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41607.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0556],
        [0.0088, 0.0000, 0.0000,  ..., 0.0350, 0.0000, 0.0668],
        [0.0072, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0531],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0157],
        [0.0033, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0157],
        [0.0033, 0.0000, 0.0000,  ..., 0.0339, 0.0000, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415528.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1268.9263, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(64.9608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8433.4795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-415.0245, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2383.9431, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-358.1146, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1606],
        [-0.0603],
        [-0.0245],
        ...,
        [-0.8356],
        [-0.8334],
        [-0.8329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-165108.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0093],
        [1.0096],
        [1.0078],
        ...,
        [1.0016],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367665.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0097],
        [1.0079],
        ...,
        [1.0016],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367676.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [-0.0086,  0.0198,  0.0222,  ..., -0.0001,  0.0112,  0.0050],
        ...,
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1657.7524, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.4309, device='cuda:0')



h[100].sum tensor(57.7007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.4994, device='cuda:0')



h[200].sum tensor(-4.9670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0189,  ..., 0.0000, 0.0119, 0.0037],
        [0.0000, 0.0590, 0.0682,  ..., 0.0002, 0.0352, 0.0153],
        [0.0000, 0.0887, 0.0988,  ..., 0.0008, 0.0497, 0.0225],
        ...,
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0068, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0068, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0068, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50937.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0251, 0.0000, 0.0000,  ..., 0.0469, 0.0000, 0.1454],
        [0.0546, 0.0000, 0.0000,  ..., 0.0662, 0.0000, 0.3033],
        [0.0808, 0.0000, 0.0000,  ..., 0.0838, 0.0000, 0.4381],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0156],
        [0.0033, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0156],
        [0.0033, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(450858.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1497.1686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.5432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9067.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-530.5981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2326.7451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-394.9136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1840],
        [ 0.2515],
        [ 0.2795],
        ...,
        [-0.8469],
        [-0.8447],
        [-0.8442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-167463.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0097],
        [1.0079],
        ...,
        [1.0016],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367676.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0094],
        [1.0097],
        [1.0080],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367687.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0057,  0.0132,  0.0154,  ..., -0.0004,  0.0080,  0.0035],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        ...,
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1424.4213, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.6955, device='cuda:0')



h[100].sum tensor(43.9264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.2659, device='cuda:0')



h[200].sum tensor(-3.1314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.1666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0424, 0.0511,  ..., 0.0000, 0.0271, 0.0113],
        [0.0000, 0.0137, 0.0215,  ..., 0.0000, 0.0131, 0.0044],
        [0.0000, 0.0005, 0.0079,  ..., 0.0000, 0.0067, 0.0012],
        ...,
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0068, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0068, 0.0012],
        [0.0000, 0.0005, 0.0080,  ..., 0.0000, 0.0068, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40537.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0403, 0.0000, 0.0000,  ..., 0.0569, 0.0000, 0.2230],
        [0.0200, 0.0000, 0.0000,  ..., 0.0441, 0.0000, 0.1133],
        [0.0082, 0.0000, 0.0000,  ..., 0.0368, 0.0000, 0.0473],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0155],
        [0.0033, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0155],
        [0.0033, 0.0000, 0.0000,  ..., 0.0345, 0.0000, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(405150.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1196.4451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(55.7059, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8327.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-405.4973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2520.7454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.0556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2003],
        [ 0.1113],
        [-0.0258],
        ...,
        [-0.8579],
        [-0.8556],
        [-0.8551]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186623.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0094],
        [1.0097],
        [1.0080],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367687.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0095],
        [1.0098],
        [1.0081],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367698.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [-0.0153,  0.0351,  0.0380,  ...,  0.0005,  0.0187,  0.0088],
        [-0.0179,  0.0411,  0.0442,  ...,  0.0007,  0.0216,  0.0102],
        ...,
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.6421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.1810, device='cuda:0')



h[100].sum tensor(45.3050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.5944, device='cuda:0')



h[200].sum tensor(-3.2540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.3852, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0357, 0.0442,  ..., 0.0005, 0.0238, 0.0098],
        [0.0000, 0.0783, 0.0880,  ..., 0.0012, 0.0446, 0.0201],
        [0.0000, 0.1847, 0.1977,  ..., 0.0036, 0.0965, 0.0459],
        ...,
        [0.0000, 0.0006, 0.0080,  ..., 0.0000, 0.0067, 0.0013],
        [0.0000, 0.0006, 0.0080,  ..., 0.0000, 0.0067, 0.0013],
        [0.0000, 0.0006, 0.0080,  ..., 0.0000, 0.0067, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42020.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0346, 0.0000, 0.0000,  ..., 0.0536, 0.0000, 0.1829],
        [0.0618, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.3254],
        [0.0949, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.4969],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0155],
        [0.0032, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0155],
        [0.0032, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(415415.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1291.6028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(61.5334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8514.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-426.8158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2697.3105, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-353.7031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1482],
        [ 0.2024],
        [ 0.2268],
        ...,
        [-0.8653],
        [-0.8630],
        [-0.8624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209083.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0095],
        [1.0098],
        [1.0081],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367698.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0096],
        [1.0099],
        [1.0083],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367710.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1700.2925, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.5609, device='cuda:0')



h[100].sum tensor(58.5116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.5874, device='cuda:0')



h[200].sum tensor(-4.8908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7081, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0079,  ..., 0.0000, 0.0067, 0.0014],
        [0.0000, 0.0006, 0.0079,  ..., 0.0000, 0.0067, 0.0014],
        [0.0000, 0.0006, 0.0079,  ..., 0.0000, 0.0067, 0.0014],
        ...,
        [0.0000, 0.0006, 0.0080,  ..., 0.0000, 0.0067, 0.0014],
        [0.0000, 0.0006, 0.0080,  ..., 0.0000, 0.0067, 0.0014],
        [0.0000, 0.0006, 0.0080,  ..., 0.0000, 0.0067, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49718.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0088, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0540],
        [0.0045, 0.0000, 0.0000,  ..., 0.0344, 0.0000, 0.0276],
        [0.0031, 0.0000, 0.0000,  ..., 0.0338, 0.0000, 0.0173],
        ...,
        [0.0031, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0157],
        [0.0031, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0157],
        [0.0031, 0.0000, 0.0000,  ..., 0.0343, 0.0000, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441177.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1380.9851, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(99.6898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9155.5039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-518.5262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2452.7725, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.0496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0476],
        [-0.2511],
        [-0.4582],
        ...,
        [-0.8669],
        [-0.8648],
        [-0.8644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175229.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0096],
        [1.0099],
        [1.0083],
        ...,
        [1.0017],
        [1.0009],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367710.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0100],
        [1.0084],
        ...,
        [1.0017],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367721.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1526.1538, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4692, device='cuda:0')



h[100].sum tensor(47.8392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.4659, device='cuda:0')



h[200].sum tensor(-3.4962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0079,  ..., 0.0000, 0.0067, 0.0014],
        [0.0000, 0.0007, 0.0079,  ..., 0.0000, 0.0067, 0.0014],
        [0.0000, 0.0007, 0.0079,  ..., 0.0000, 0.0067, 0.0014],
        ...,
        [0.0000, 0.0007, 0.0080,  ..., 0.0000, 0.0068, 0.0015],
        [0.0000, 0.0007, 0.0080,  ..., 0.0000, 0.0068, 0.0015],
        [0.0000, 0.0007, 0.0080,  ..., 0.0000, 0.0068, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43316.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0029, 0.0000, 0.0000,  ..., 0.0337, 0.0000, 0.0159],
        [0.0041, 0.0000, 0.0000,  ..., 0.0346, 0.0000, 0.0221],
        [0.0056, 0.0000, 0.0000,  ..., 0.0355, 0.0000, 0.0301],
        ...,
        [0.0030, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0162],
        [0.0030, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0162],
        [0.0030, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0162]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(417200.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1165.4005, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(74.7494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8814.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-440.2240, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2441.9001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.3116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8725],
        [-0.7884],
        [-0.6072],
        ...,
        [-0.8722],
        [-0.8699],
        [-0.8694]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174413.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0100],
        [1.0084],
        ...,
        [1.0017],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367721.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0097],
        [1.0101],
        [1.0085],
        ...,
        [1.0017],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367732.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0047,  0.0110,  0.0131,  ..., -0.0005,  0.0070,  0.0030],
        [-0.0111,  0.0257,  0.0282,  ...,  0.0001,  0.0141,  0.0065],
        [-0.0141,  0.0324,  0.0352,  ...,  0.0004,  0.0174,  0.0082],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2072.6060, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.5984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.5517, device='cuda:0')



h[100].sum tensor(78.7796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-21.3469, device='cuda:0')



h[200].sum tensor(-7.3186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.2066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.9384e-02, 5.7961e-02,  ..., 1.1101e-04, 3.0504e-02,
         1.3234e-02],
        [0.0000e+00, 1.0970e-01, 1.2012e-01,  ..., 1.1294e-03, 5.9932e-02,
         2.7846e-02],
        [0.0000e+00, 1.3231e-01, 1.4342e-01,  ..., 1.5381e-03, 7.0957e-02,
         3.3322e-02],
        ...,
        [0.0000e+00, 7.8323e-04, 7.9774e-03,  ..., 0.0000e+00, 6.8863e-03,
         1.4790e-03],
        [0.0000e+00, 7.8337e-04, 7.9788e-03,  ..., 0.0000e+00, 6.8875e-03,
         1.4792e-03],
        [0.0000e+00, 7.8343e-04, 7.9795e-03,  ..., 0.0000e+00, 6.8881e-03,
         1.4794e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67616.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0468, 0.0000, 0.0000,  ..., 0.0602, 0.0000, 0.2639],
        [0.0769, 0.0000, 0.0000,  ..., 0.0794, 0.0000, 0.4209],
        [0.0988, 0.0000, 0.0000,  ..., 0.0937, 0.0000, 0.5313],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0167],
        [0.0027, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0167],
        [0.0027, 0.0000, 0.0000,  ..., 0.0341, 0.0000, 0.0167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537648.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1992.4215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(191.5109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10816.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-742.7794, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2368.6970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.7349, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2101],
        [ 0.1906],
        [ 0.1679],
        ...,
        [-0.8685],
        [-0.8693],
        [-0.8697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-179184.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0097],
        [1.0101],
        [1.0085],
        ...,
        [1.0017],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367732.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0098],
        [1.0102],
        [1.0086],
        ...,
        [1.0017],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367743.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0133,  0.0307,  0.0333,  ...,  0.0003,  0.0166,  0.0077],
        [-0.0233,  0.0536,  0.0570,  ...,  0.0012,  0.0278,  0.0133],
        [-0.0126,  0.0291,  0.0317,  ...,  0.0002,  0.0158,  0.0074],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1533.1594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0107, device='cuda:0')



h[100].sum tensor(47.5052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.1558, device='cuda:0')



h[200].sum tensor(-3.3557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.7588, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1511, 0.1627,  ..., 0.0023, 0.0802, 0.0379],
        [0.0000, 0.1565, 0.1683,  ..., 0.0025, 0.0828, 0.0392],
        [0.0000, 0.1655, 0.1775,  ..., 0.0032, 0.0872, 0.0414],
        ...,
        [0.0000, 0.0009, 0.0080,  ..., 0.0000, 0.0070, 0.0015],
        [0.0000, 0.0009, 0.0080,  ..., 0.0000, 0.0070, 0.0015],
        [0.0000, 0.0009, 0.0080,  ..., 0.0000, 0.0070, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42558.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0803, 0.0000, 0.0000,  ..., 0.0822, 0.0000, 0.4378],
        [0.0931, 0.0000, 0.0000,  ..., 0.0901, 0.0000, 0.5092],
        [0.0947, 0.0000, 0.0000,  ..., 0.0909, 0.0000, 0.5206],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0173],
        [0.0024, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0173],
        [0.0024, 0.0000, 0.0000,  ..., 0.0340, 0.0000, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(412829.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(994.0009, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(83.7357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8845.1611, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-432.8735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2339.6099, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-356.7915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1993],
        [ 0.2406],
        [ 0.2600],
        ...,
        [-0.8752],
        [-0.8730],
        [-0.8724]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169806.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0098],
        [1.0102],
        [1.0086],
        ...,
        [1.0017],
        [1.0009],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367743.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0099],
        [1.0103],
        [1.0087],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367753.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0168,  0.0389,  0.0419,  ...,  0.0006,  0.0206,  0.0098],
        [-0.0328,  0.0755,  0.0796,  ...,  0.0020,  0.0385,  0.0186],
        [-0.0321,  0.0739,  0.0779,  ...,  0.0020,  0.0377,  0.0182],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1554.3572, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4139, device='cuda:0')



h[100].sum tensor(48.8226, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.4285, device='cuda:0')



h[200].sum tensor(-3.4510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9403, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1927, 0.2055,  ..., 0.0039, 0.1005, 0.0480],
        [0.0000, 0.2553, 0.2701,  ..., 0.0063, 0.1311, 0.0632],
        [0.0000, 0.2672, 0.2823,  ..., 0.0067, 0.1369, 0.0660],
        ...,
        [0.0000, 0.0009, 0.0080,  ..., 0.0000, 0.0071, 0.0015],
        [0.0000, 0.0009, 0.0080,  ..., 0.0000, 0.0071, 0.0015],
        [0.0000, 0.0009, 0.0080,  ..., 0.0000, 0.0071, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44349.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1188, 0.0000, 0.0000,  ..., 0.1082, 0.0000, 0.6298],
        [0.1544, 0.0000, 0.0000,  ..., 0.1312, 0.0000, 0.8129],
        [0.1592, 0.0000, 0.0000,  ..., 0.1341, 0.0000, 0.8407],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0175],
        [0.0023, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0175],
        [0.0023, 0.0000, 0.0000,  ..., 0.0342, 0.0000, 0.0175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(425932.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1004.3674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(93.3309, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9156.2881, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-454.1955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2296.0908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-367.5964, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1996],
        [ 0.2317],
        [ 0.2501],
        ...,
        [-0.8822],
        [-0.8798],
        [-0.8788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-162732.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0099],
        [1.0103],
        [1.0087],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367753.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(523.8232, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0100],
        [1.0104],
        [1.0088],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367764.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0162,  0.0184,  ..., -0.0003,  0.0095,  0.0042],
        [-0.0167,  0.0386,  0.0416,  ...,  0.0006,  0.0205,  0.0097],
        [-0.0143,  0.0331,  0.0359,  ...,  0.0004,  0.0178,  0.0083],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1767.2362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.8217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.2429, device='cuda:0')



h[100].sum tensor(61.7177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.0488, device='cuda:0')



h[200].sum tensor(-4.9461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0152, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1087, 0.1193,  ..., 0.0009, 0.0597, 0.0276],
        [0.0000, 0.1082, 0.1188,  ..., 0.0008, 0.0594, 0.0275],
        [0.0000, 0.1127, 0.1234,  ..., 0.0010, 0.0616, 0.0286],
        ...,
        [0.0000, 0.0008, 0.0081,  ..., 0.0000, 0.0071, 0.0015],
        [0.0000, 0.0008, 0.0081,  ..., 0.0000, 0.0071, 0.0015],
        [0.0000, 0.0008, 0.0081,  ..., 0.0000, 0.0071, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52480.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0555, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.3147],
        [0.0620, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.3509],
        [0.0614, 0.0000, 0.0000,  ..., 0.0709, 0.0000, 0.3481],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0174],
        [0.0024, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0174],
        [0.0024, 0.0000, 0.0000,  ..., 0.0348, 0.0000, 0.0174]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465006.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1301.1674, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(124.1569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9875.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.2745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2332.3345, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.6537, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2665],
        [ 0.2797],
        [ 0.2813],
        ...,
        [-0.8983],
        [-0.8967],
        [-0.8965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168154.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0100],
        [1.0104],
        [1.0088],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367764.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0102],
        [1.0105],
        [1.0089],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367774.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.3218e-03,  2.1639e-02,  2.4167e-02,  ..., -5.8219e-05,
          1.2223e-02,  5.5512e-03],
        [-8.3988e-03,  1.9510e-02,  2.1974e-02,  ..., -1.4051e-04,
          1.1185e-02,  5.0353e-03],
        [-9.1501e-03,  2.1243e-02,  2.3759e-02,  ..., -7.3521e-05,
          1.2030e-02,  5.4552e-03],
        ...,
        [ 0.0000e+00,  1.4407e-04,  2.0184e-03,  ..., -8.8932e-04,
          1.7385e-03,  3.4095e-04],
        [-1.0474e-02,  2.4295e-02,  2.6904e-02,  ...,  4.4479e-05,
          1.3519e-02,  6.1950e-03],
        [ 0.0000e+00,  1.4407e-04,  2.0184e-03,  ..., -8.8932e-04,
          1.7385e-03,  3.4095e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1693.0825, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.9253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1865, device='cuda:0')



h[100].sum tensor(58.4562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.6575, device='cuda:0')



h[200].sum tensor(-4.4343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 7.5272e-02, 8.5095e-02,  ..., 0.0000e+00, 4.3436e-02,
         1.9478e-02],
        [0.0000e+00, 7.7425e-02, 8.7339e-02,  ..., 0.0000e+00, 4.4509e-02,
         2.0004e-02],
        [0.0000e+00, 7.4987e-02, 8.4829e-02,  ..., 0.0000e+00, 4.3321e-02,
         1.9414e-02],
        ...,
        [0.0000e+00, 2.5208e-02, 3.3601e-02,  ..., 4.5345e-05, 1.9099e-02,
         7.3583e-03],
        [0.0000e+00, 2.0693e-02, 2.8949e-02,  ..., 0.0000e+00, 1.6897e-02,
         6.2641e-03],
        [0.0000e+00, 9.0054e-02, 1.0042e-01,  ..., 9.0708e-05, 5.0731e-02,
         2.3077e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49261.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0449, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.2684],
        [0.0452, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.2680],
        [0.0444, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.2639],
        ...,
        [0.0160, 0.0000, 0.0000,  ..., 0.0439, 0.0000, 0.0927],
        [0.0197, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.1143],
        [0.0320, 0.0000, 0.0000,  ..., 0.0537, 0.0000, 0.1838]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446177.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1179.8748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(100.4533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9694.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-513.4178, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2359.6978, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.7265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3029],
        [ 0.3041],
        [ 0.3038],
        ...,
        [-0.2971],
        [-0.1970],
        [-0.1491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164524.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0102],
        [1.0105],
        [1.0089],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367774.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0103],
        [1.0106],
        [1.0090],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367785.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        ...,
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1800.3250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.9877, device='cuda:0')



h[100].sum tensor(64.0900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.5527, device='cuda:0')



h[200].sum tensor(-5.0993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3505, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0006, 0.0082,  ..., 0.0000, 0.0070, 0.0013],
        [0.0000, 0.0006, 0.0082,  ..., 0.0000, 0.0071, 0.0014],
        [0.0000, 0.0006, 0.0082,  ..., 0.0000, 0.0071, 0.0014],
        ...,
        [0.0000, 0.0006, 0.0083,  ..., 0.0000, 0.0071, 0.0014],
        [0.0000, 0.0006, 0.0083,  ..., 0.0000, 0.0071, 0.0014],
        [0.0000, 0.0006, 0.0083,  ..., 0.0000, 0.0071, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53508.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0061, 0.0000, 0.0000,  ..., 0.0374, 0.0000, 0.0420],
        [0.0039, 0.0000, 0.0000,  ..., 0.0362, 0.0000, 0.0276],
        [0.0032, 0.0000, 0.0000,  ..., 0.0359, 0.0000, 0.0224],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0170],
        [0.0027, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0170],
        [0.0027, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465372.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1332.9915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(117.4835, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10033.1533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.7446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2377.3579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.4362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0854],
        [-0.2804],
        [-0.4708],
        ...,
        [-0.9304],
        [-0.9279],
        [-0.9272]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-170130.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0103],
        [1.0106],
        [1.0090],
        ...,
        [1.0018],
        [1.0010],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367785.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0104],
        [1.0108],
        [1.0091],
        ...,
        [1.0019],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367796.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [-0.0083,  0.0192,  0.0217,  ..., -0.0001,  0.0111,  0.0050],
        ...,
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003],
        [ 0.0000,  0.0001,  0.0020,  ..., -0.0009,  0.0017,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1771.9391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.3350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.7180, device='cuda:0')



h[100].sum tensor(61.4616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.6937, device='cuda:0')



h[200].sum tensor(-4.7745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0005, 0.0082,  ..., 0.0000, 0.0070, 0.0014],
        [0.0000, 0.0356, 0.0444,  ..., 0.0000, 0.0242, 0.0099],
        [0.0000, 0.0490, 0.0582,  ..., 0.0000, 0.0307, 0.0131],
        ...,
        [0.0000, 0.0005, 0.0083,  ..., 0.0000, 0.0071, 0.0014],
        [0.0000, 0.0005, 0.0083,  ..., 0.0000, 0.0071, 0.0014],
        [0.0000, 0.0261, 0.0347,  ..., 0.0000, 0.0196, 0.0076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50408.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0113, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0670],
        [0.0274, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.1589],
        [0.0404, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.2355],
        ...,
        [0.0049, 0.0000, 0.0000,  ..., 0.0376, 0.0000, 0.0312],
        [0.0093, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0604],
        [0.0204, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.1290]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449676.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1272.7196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(100.9065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9730.1973, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-528.9805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2500.3796, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.2709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0577],
        [ 0.0749],
        [ 0.1828],
        ...,
        [-0.5961],
        [-0.2916],
        [-0.0094]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182338.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0104],
        [1.0108],
        [1.0091],
        ...,
        [1.0019],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367796.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0105],
        [1.0109],
        [1.0092],
        ...,
        [1.0019],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367807.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0075,  0.0176,  0.0200,  ..., -0.0002,  0.0102,  0.0046],
        [-0.0200,  0.0465,  0.0498,  ...,  0.0009,  0.0244,  0.0116],
        [-0.0034,  0.0080,  0.0101,  ..., -0.0006,  0.0056,  0.0023],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0017,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1581.3831, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1985, device='cuda:0')



h[100].sum tensor(48.8619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.2828, device='cuda:0')



h[200].sum tensor(-3.3124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1663, 0.1789,  ..., 0.0029, 0.0878, 0.0416],
        [0.0000, 0.0844, 0.0946,  ..., 0.0006, 0.0479, 0.0218],
        [0.0000, 0.0940, 0.1044,  ..., 0.0009, 0.0526, 0.0241],
        ...,
        [0.0000, 0.0007, 0.0083,  ..., 0.0000, 0.0071, 0.0014],
        [0.0000, 0.0007, 0.0083,  ..., 0.0000, 0.0071, 0.0014],
        [0.0000, 0.0007, 0.0083,  ..., 0.0000, 0.0071, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41872.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0877, 0.0000, 0.0000,  ..., 0.0896, 0.0000, 0.4784],
        [0.0669, 0.0000, 0.0000,  ..., 0.0758, 0.0000, 0.3770],
        [0.0556, 0.0000, 0.0000,  ..., 0.0686, 0.0000, 0.3170],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0170],
        [0.0026, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0170],
        [0.0026, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(411579.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1026.2859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(65.3809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8897.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-425.1780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2717.4758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-364.4292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2362],
        [ 0.2340],
        [ 0.2159],
        ...,
        [-0.9429],
        [-0.9404],
        [-0.9398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213779.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0105],
        [1.0109],
        [1.0092],
        ...,
        [1.0019],
        [1.0011],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367807.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0110],
        [1.0094],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367818.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0066,  0.0155,  0.0178,  ..., -0.0003,  0.0092,  0.0041],
        [-0.0132,  0.0309,  0.0337,  ...,  0.0003,  0.0167,  0.0078],
        [-0.0066,  0.0156,  0.0179,  ..., -0.0003,  0.0093,  0.0041],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1873.7103, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.7006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.2724, device='cuda:0')



h[100].sum tensor(62.8847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.7454, device='cuda:0')



h[200].sum tensor(-5.0989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1027, 0.1131,  ..., 0.0013, 0.0568, 0.0262],
        [0.0000, 0.1029, 0.1134,  ..., 0.0010, 0.0569, 0.0263],
        [0.0000, 0.1019, 0.1124,  ..., 0.0012, 0.0564, 0.0260],
        ...,
        [0.0000, 0.0008, 0.0083,  ..., 0.0000, 0.0072, 0.0015],
        [0.0000, 0.0008, 0.0083,  ..., 0.0000, 0.0072, 0.0015],
        [0.0000, 0.0008, 0.0083,  ..., 0.0000, 0.0072, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55981.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1007, 0.0000, 0.0000,  ..., 0.0988, 0.0000, 0.5400],
        [0.1101, 0.0000, 0.0000,  ..., 0.1048, 0.0000, 0.5909],
        [0.1058, 0.0000, 0.0000,  ..., 0.1022, 0.0000, 0.5666],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0364, 0.0000, 0.0174],
        [0.0024, 0.0000, 0.0000,  ..., 0.0364, 0.0000, 0.0174],
        [0.0024, 0.0000, 0.0000,  ..., 0.0364, 0.0000, 0.0174]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(484856.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1417.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.4119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10135.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-593.9908, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2405.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.3660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1822],
        [ 0.1803],
        [ 0.1811],
        ...,
        [-0.9397],
        [-0.9373],
        [-0.9367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174792.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0110],
        [1.0094],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367818.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0106],
        [1.0110],
        [1.0094],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367818.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1635.2933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0982, device='cuda:0')



h[100].sum tensor(49.6525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8915, device='cuda:0')



h[200].sum tensor(-3.5008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2484, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0082,  ..., 0.0000, 0.0071, 0.0015],
        [0.0000, 0.0008, 0.0082,  ..., 0.0000, 0.0071, 0.0015],
        [0.0000, 0.0008, 0.0082,  ..., 0.0000, 0.0071, 0.0015],
        ...,
        [0.0000, 0.0008, 0.0083,  ..., 0.0000, 0.0072, 0.0015],
        [0.0000, 0.0008, 0.0083,  ..., 0.0000, 0.0072, 0.0015],
        [0.0000, 0.0008, 0.0083,  ..., 0.0000, 0.0072, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45313.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0358, 0.0000, 0.0214],
        [0.0023, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0179],
        [0.0024, 0.0000, 0.0000,  ..., 0.0361, 0.0000, 0.0172],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0364, 0.0000, 0.0174],
        [0.0024, 0.0000, 0.0000,  ..., 0.0364, 0.0000, 0.0174],
        [0.0024, 0.0000, 0.0000,  ..., 0.0364, 0.0000, 0.0174]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(435718.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1057.6024, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(88.6591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9334.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-461.7075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2425.8350, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-381.4902, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5173],
        [-0.7345],
        [-0.8995],
        ...,
        [-0.9397],
        [-0.9373],
        [-0.9367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172871.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0106],
        [1.0110],
        [1.0094],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367818.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0107],
        [1.0111],
        [1.0095],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367829.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0020,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1613.4425, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.5460, device='cuda:0')



h[100].sum tensor(46.3272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8413, device='cuda:0')



h[200].sum tensor(-3.1810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0081,  ..., 0.0000, 0.0071, 0.0016],
        [0.0000, 0.0010, 0.0081,  ..., 0.0000, 0.0071, 0.0016],
        [0.0000, 0.0010, 0.0081,  ..., 0.0000, 0.0071, 0.0016],
        ...,
        [0.0000, 0.0010, 0.0082,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0010, 0.0082,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0010, 0.0082,  ..., 0.0000, 0.0072, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42902.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0000, 0.0000,  ..., 0.0359, 0.0000, 0.0176],
        [0.0020, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0176],
        [0.0020, 0.0000, 0.0000,  ..., 0.0360, 0.0000, 0.0177],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0363, 0.0000, 0.0178],
        [0.0021, 0.0000, 0.0000,  ..., 0.0363, 0.0000, 0.0178],
        [0.0021, 0.0000, 0.0000,  ..., 0.0363, 0.0000, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(423555.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(888.5541, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(85.5307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9063.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-431.2653, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2378.7515, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-369.7924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0286],
        [-1.0796],
        [-1.1185],
        ...,
        [-0.9356],
        [-0.9333],
        [-0.9327]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168799.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0107],
        [1.0111],
        [1.0095],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367829.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0108],
        [1.0113],
        [1.0096],
        ...,
        [1.0021],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367840.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.3299e-03,  2.1979e-02,  2.4371e-02,  ..., -3.7323e-05,
          1.2361e-02,  5.6673e-03],
        [-1.5679e-02,  3.6753e-02,  3.9589e-02,  ...,  5.3578e-04,
          1.9565e-02,  9.2539e-03],
        [-1.0690e-02,  2.5144e-02,  2.7631e-02,  ...,  8.5458e-05,
          1.3905e-02,  6.4357e-03],
        ...,
        [ 0.0000e+00,  2.7088e-04,  2.0105e-03,  ..., -8.7942e-04,
          1.7759e-03,  3.9712e-04],
        [ 0.0000e+00,  2.7088e-04,  2.0105e-03,  ..., -8.7942e-04,
          1.7759e-03,  3.9712e-04],
        [ 0.0000e+00,  2.7088e-04,  2.0105e-03,  ..., -8.7942e-04,
          1.7759e-03,  3.9712e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2331.0637, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-30.2775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-36.0095, device='cuda:0')



h[100].sum tensor(84.2889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-24.3628, device='cuda:0')



h[200].sum tensor(-7.7884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2137, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0615, 0.0704,  ..., 0.0005, 0.0366, 0.0163],
        [0.0000, 0.0898, 0.0995,  ..., 0.0006, 0.0505, 0.0232],
        [0.0000, 0.1711, 0.1832,  ..., 0.0030, 0.0901, 0.0429],
        ...,
        [0.0000, 0.0011, 0.0082,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0011, 0.0082,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0011, 0.0082,  ..., 0.0000, 0.0072, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(79324.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0544, 0.0000, 0.0000,  ..., 0.0682, 0.0000, 0.3217],
        [0.0786, 0.0000, 0.0000,  ..., 0.0841, 0.0000, 0.4465],
        [0.1140, 0.0000, 0.0000,  ..., 0.1077, 0.0000, 0.6225],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0180],
        [0.0019, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0180],
        [0.0019, 0.0000, 0.0000,  ..., 0.0365, 0.0000, 0.0180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(641622.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(2270.5635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(258.0542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12707.1221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-876.3159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2021.5148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-508.4419, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3066],
        [ 0.2962],
        [ 0.2838],
        ...,
        [-0.9364],
        [-0.9341],
        [-0.9335]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-128635.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0108],
        [1.0113],
        [1.0096],
        ...,
        [1.0021],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367840.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0109],
        [1.0114],
        [1.0098],
        ...,
        [1.0021],
        [1.0013],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367852., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0082,  0.0193,  0.0216,  ..., -0.0001,  0.0111,  0.0050],
        [-0.0037,  0.0089,  0.0109,  ..., -0.0005,  0.0060,  0.0025],
        [-0.0142,  0.0334,  0.0361,  ...,  0.0004,  0.0179,  0.0084],
        ...,
        [ 0.0000,  0.0003,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0020,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1869.5056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.5336, device='cuda:0')



h[100].sum tensor(57.8461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.5689, device='cuda:0')



h[200].sum tensor(-4.6719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6958, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0441, 0.0524,  ..., 0.0000, 0.0281, 0.0120],
        [0.0000, 0.0782, 0.0876,  ..., 0.0004, 0.0447, 0.0203],
        [0.0000, 0.0444, 0.0527,  ..., 0.0000, 0.0282, 0.0121],
        ...,
        [0.0000, 0.0011, 0.0082,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0011, 0.0082,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0011, 0.0083,  ..., 0.0000, 0.0072, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56050.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0412, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.2591],
        [0.0414, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.2577],
        [0.0343, 0.0000, 0.0000,  ..., 0.0553, 0.0000, 0.2165],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0369, 0.0000, 0.0178],
        [0.0020, 0.0000, 0.0000,  ..., 0.0369, 0.0000, 0.0178],
        [0.0020, 0.0000, 0.0000,  ..., 0.0369, 0.0000, 0.0178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503790.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1341.5054, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(149.3655, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10329.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-584.7623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2136.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.9074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2891],
        [ 0.2922],
        [ 0.2937],
        ...,
        [-0.9459],
        [-0.9435],
        [-0.9430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-136452.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0109],
        [1.0114],
        [1.0098],
        ...,
        [1.0021],
        [1.0013],
        [1.0006]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367852., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(502.9749, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0111],
        [1.0116],
        [1.0099],
        ...,
        [1.0021],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367863.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0061,  0.0144,  0.0166,  ..., -0.0003,  0.0086,  0.0038],
        [-0.0062,  0.0147,  0.0169,  ..., -0.0003,  0.0088,  0.0039],
        [ 0.0000,  0.0003,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0020,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0020,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1855.7758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.7172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.2113, device='cuda:0')



h[100].sum tensor(56.1360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.3508, device='cuda:0')



h[200].sum tensor(-4.5294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0797, 0.0892,  ..., 0.0002, 0.0454, 0.0207],
        [0.0000, 0.0273, 0.0352,  ..., 0.0000, 0.0198, 0.0080],
        [0.0000, 0.0157, 0.0233,  ..., 0.0000, 0.0142, 0.0052],
        ...,
        [0.0000, 0.0011, 0.0083,  ..., 0.0000, 0.0072, 0.0017],
        [0.0000, 0.0011, 0.0083,  ..., 0.0000, 0.0072, 0.0017],
        [0.0000, 0.0011, 0.0083,  ..., 0.0000, 0.0072, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54292.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0388, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.2247],
        [0.0220, 0.0000, 0.0000,  ..., 0.0489, 0.0000, 0.1330],
        [0.0124, 0.0000, 0.0000,  ..., 0.0432, 0.0000, 0.0777],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.0172],
        [0.0021, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.0172],
        [0.0021, 0.0000, 0.0000,  ..., 0.0375, 0.0000, 0.0172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(495738.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1355.9626, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(135.5868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10026.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.1939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2405.6206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.0356, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0456],
        [-0.2055],
        [-0.5132],
        ...,
        [-0.9604],
        [-0.9580],
        [-0.9575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164816.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0111],
        [1.0116],
        [1.0099],
        ...,
        [1.0021],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367863.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0112],
        [1.0117],
        [1.0100],
        ...,
        [1.0020],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367874.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [-0.0185,  0.0435,  0.0466,  ...,  0.0008,  0.0228,  0.0109],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1838.1080, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.2976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.6214, device='cuda:0')



h[100].sum tensor(55.0062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.9518, device='cuda:0')



h[200].sum tensor(-4.4085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2851, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1000, 0.1102,  ..., 0.0008, 0.0553, 0.0256],
        [0.0000, 0.0368, 0.0451,  ..., 0.0000, 0.0245, 0.0103],
        [0.0000, 0.1045, 0.1148,  ..., 0.0008, 0.0575, 0.0267],
        ...,
        [0.0000, 0.0011, 0.0084,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0011, 0.0084,  ..., 0.0000, 0.0072, 0.0016],
        [0.0000, 0.0011, 0.0084,  ..., 0.0000, 0.0072, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52877.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0406, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.2278],
        [0.0327, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.1857],
        [0.0420, 0.0000, 0.0000,  ..., 0.0623, 0.0000, 0.2344],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0382, 0.0000, 0.0169],
        [0.0021, 0.0000, 0.0000,  ..., 0.0382, 0.0000, 0.0169],
        [0.0021, 0.0000, 0.0000,  ..., 0.0382, 0.0000, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480201.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1257.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(124.8901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9653.1924, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-548.4305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2592.2603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-405.3856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0452],
        [-0.0609],
        [-0.1044],
        ...,
        [-0.9707],
        [-0.9685],
        [-0.9686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188705.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0112],
        [1.0117],
        [1.0100],
        ...,
        [1.0020],
        [1.0013],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367874.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0113],
        [1.0119],
        [1.0102],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367885.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0114,  0.0135,  ..., -0.0005,  0.0072,  0.0031],
        [-0.0142,  0.0335,  0.0363,  ...,  0.0004,  0.0180,  0.0085],
        [-0.0175,  0.0413,  0.0443,  ...,  0.0007,  0.0218,  0.0103],
        ...,
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1730.4191, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.5305, device='cuda:0')



h[100].sum tensor(49.3213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.8605, device='cuda:0')



h[200].sum tensor(-3.7259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8933, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0498, 0.0585,  ..., 0.0004, 0.0309, 0.0134],
        [0.0000, 0.1161, 0.1269,  ..., 0.0016, 0.0632, 0.0295],
        [0.0000, 0.1915, 0.2046,  ..., 0.0038, 0.1000, 0.0478],
        ...,
        [0.0000, 0.0011, 0.0085,  ..., 0.0000, 0.0072, 0.0015],
        [0.0000, 0.0011, 0.0085,  ..., 0.0000, 0.0072, 0.0015],
        [0.0000, 0.0011, 0.0085,  ..., 0.0000, 0.0072, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48052.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0488, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.2735],
        [0.0834, 0.0000, 0.0000,  ..., 0.0892, 0.0000, 0.4508],
        [0.1223, 0.0000, 0.0000,  ..., 0.1142, 0.0000, 0.6438],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0389, 0.0000, 0.0169],
        [0.0020, 0.0000, 0.0000,  ..., 0.0390, 0.0000, 0.0169],
        [0.0020, 0.0000, 0.0000,  ..., 0.0390, 0.0000, 0.0169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(458008.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1010.3549, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(101.7250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9326.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-485.4373, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2515.6970, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-394.0704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1869],
        [ 0.1605],
        [ 0.1302],
        ...,
        [-0.9889],
        [-0.9865],
        [-0.9859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-172221.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0113],
        [1.0119],
        [1.0102],
        ...,
        [1.0020],
        [1.0012],
        [1.0005]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367885.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0114],
        [1.0120],
        [1.0103],
        ...,
        [1.0019],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367895.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0072,  0.0173,  0.0196,  ..., -0.0002,  0.0101,  0.0045],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1705.8055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0664, device='cuda:0')



h[100].sum tensor(48.3436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5465, device='cuda:0')



h[200].sum tensor(-3.5658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6844, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0356, 0.0440,  ..., 0.0000, 0.0241, 0.0098],
        [0.0000, 0.0182, 0.0261,  ..., 0.0000, 0.0156, 0.0056],
        [0.0000, 0.0011, 0.0084,  ..., 0.0000, 0.0073, 0.0014],
        ...,
        [0.0000, 0.0011, 0.0085,  ..., 0.0000, 0.0073, 0.0015],
        [0.0000, 0.0011, 0.0085,  ..., 0.0000, 0.0073, 0.0015],
        [0.0000, 0.0011, 0.0085,  ..., 0.0000, 0.0073, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46106.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0278, 0.0000, 0.0000,  ..., 0.0550, 0.0000, 0.1657],
        [0.0155, 0.0000, 0.0000,  ..., 0.0473, 0.0000, 0.1001],
        [0.0060, 0.0000, 0.0000,  ..., 0.0413, 0.0000, 0.0490],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0396, 0.0000, 0.0172],
        [0.0018, 0.0000, 0.0000,  ..., 0.0396, 0.0000, 0.0172],
        [0.0018, 0.0000, 0.0000,  ..., 0.0396, 0.0000, 0.0172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(446900.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(928.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(94.8093, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9087.3662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-461.4212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2584.8618, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-381.1447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0683],
        [-0.0893],
        [-0.2788],
        ...,
        [-0.9982],
        [-0.9957],
        [-0.9951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194493.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0114],
        [1.0120],
        [1.0103],
        ...,
        [1.0019],
        [1.0012],
        [1.0004]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367895.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0115],
        [1.0121],
        [1.0104],
        ...,
        [1.0019],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367905.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0079,  0.0187,  0.0211,  ..., -0.0002,  0.0108,  0.0048],
        [-0.0034,  0.0083,  0.0104,  ..., -0.0006,  0.0057,  0.0023],
        [-0.0044,  0.0107,  0.0128,  ..., -0.0005,  0.0069,  0.0029],
        ...,
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0003,  0.0021,  ..., -0.0009,  0.0018,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1658.5791, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5951, device='cuda:0')



h[100].sum tensor(46.2859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5511, device='cuda:0')



h[200].sum tensor(-3.2660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0318, 0.0401,  ..., 0.0000, 0.0223, 0.0088],
        [0.0000, 0.0691, 0.0786,  ..., 0.0000, 0.0406, 0.0179],
        [0.0000, 0.0319, 0.0403,  ..., 0.0000, 0.0224, 0.0089],
        ...,
        [0.0000, 0.0010, 0.0086,  ..., 0.0000, 0.0074, 0.0014],
        [0.0000, 0.0011, 0.0086,  ..., 0.0000, 0.0074, 0.0014],
        [0.0000, 0.0011, 0.0086,  ..., 0.0000, 0.0074, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44270.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0226, 0.0000, 0.0000,  ..., 0.0515, 0.0000, 0.1572],
        [0.0305, 0.0000, 0.0000,  ..., 0.0566, 0.0000, 0.2018],
        [0.0213, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.1481],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0401, 0.0000, 0.0176],
        [0.0015, 0.0000, 0.0000,  ..., 0.0401, 0.0000, 0.0177],
        [0.0015, 0.0000, 0.0000,  ..., 0.0401, 0.0000, 0.0177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441060.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(798.7785, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(87.8921, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8992.8467, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-437.2921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2577.4507, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.3303, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2084],
        [ 0.2749],
        [ 0.2340],
        ...,
        [-1.0075],
        [-1.0050],
        [-1.0044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201834.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0115],
        [1.0121],
        [1.0104],
        ...,
        [1.0019],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367905.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0116],
        [1.0122],
        [1.0105],
        ...,
        [1.0019],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367916.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0039,  0.0095,  0.0116,  ..., -0.0005,  0.0063,  0.0026],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0018,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0018,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1698.0605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.8470, device='cuda:0')



h[100].sum tensor(49.0518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3981, device='cuda:0')



h[200].sum tensor(-3.5061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5856, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0325, 0.0409,  ..., 0.0000, 0.0228, 0.0089],
        [0.0000, 0.0103, 0.0182,  ..., 0.0000, 0.0120, 0.0036],
        [0.0000, 0.0010, 0.0086,  ..., 0.0000, 0.0075, 0.0013],
        ...,
        [0.0000, 0.0010, 0.0086,  ..., 0.0000, 0.0075, 0.0013],
        [0.0000, 0.0010, 0.0086,  ..., 0.0000, 0.0075, 0.0013],
        [0.0000, 0.0010, 0.0086,  ..., 0.0000, 0.0075, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46737.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0216, 0.0000, 0.0000,  ..., 0.0518, 0.0000, 0.1527],
        [0.0114, 0.0000, 0.0000,  ..., 0.0457, 0.0000, 0.0905],
        [0.0047, 0.0000, 0.0000,  ..., 0.0418, 0.0000, 0.0461],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0405, 0.0000, 0.0181],
        [0.0011, 0.0000, 0.0000,  ..., 0.0405, 0.0000, 0.0181],
        [0.0011, 0.0000, 0.0000,  ..., 0.0405, 0.0000, 0.0181]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(452198.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(762.5294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(100.4225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9289.0400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-465.4891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2412.6736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.5819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2372],
        [ 0.0756],
        [-0.1372],
        ...,
        [-1.0020],
        [-1.0095],
        [-1.0115]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-183573.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0116],
        [1.0122],
        [1.0105],
        ...,
        [1.0019],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367916.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0117],
        [1.0123],
        [1.0106],
        ...,
        [1.0018],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367926.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0037,  0.0090,  0.0112,  ..., -0.0006,  0.0061,  0.0024],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0019,  0.0003],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0019,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0019,  0.0003],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0019,  0.0003],
        [ 0.0000,  0.0002,  0.0021,  ..., -0.0009,  0.0019,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1709.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0219, device='cuda:0')



h[100].sum tensor(50.2593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5165, device='cuda:0')



h[200].sum tensor(-3.5807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0230, 0.0314,  ..., 0.0000, 0.0183, 0.0066],
        [0.0000, 0.0171, 0.0253,  ..., 0.0000, 0.0154, 0.0051],
        [0.0000, 0.0009, 0.0087,  ..., 0.0000, 0.0075, 0.0012],
        ...,
        [0.0000, 0.0009, 0.0087,  ..., 0.0000, 0.0076, 0.0012],
        [0.0000, 0.0009, 0.0087,  ..., 0.0000, 0.0076, 0.0012],
        [0.0000, 0.0009, 0.0087,  ..., 0.0000, 0.0076, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48875.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.1194],
        [0.0102, 0.0000, 0.0000,  ..., 0.0453, 0.0000, 0.0911],
        [0.0042, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0466],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0182],
        [0.0012, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0182],
        [0.0012, 0.0000, 0.0000,  ..., 0.0412, 0.0000, 0.0182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(470154.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(870.6454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.5326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9598.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-492.8687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2546.8638, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-398.2900, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1878],
        [ 0.0346],
        [-0.2084],
        ...,
        [-1.0337],
        [-1.0311],
        [-1.0305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190577.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0117],
        [1.0123],
        [1.0106],
        ...,
        [1.0018],
        [1.0011],
        [1.0003]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367926.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0119],
        [1.0124],
        [1.0107],
        ...,
        [1.0018],
        [1.0010],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367936., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0165,  0.0391,  0.0422,  ...,  0.0006,  0.0208,  0.0097],
        [-0.0265,  0.0627,  0.0666,  ...,  0.0015,  0.0323,  0.0155],
        [-0.0188,  0.0446,  0.0479,  ...,  0.0008,  0.0235,  0.0111],
        ...,
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1557.9185, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9270, device='cuda:0')



h[100].sum tensor(42.5489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7460, device='cuda:0')



h[200].sum tensor(-2.6894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1701, 0.1832,  ..., 0.0030, 0.0900, 0.0423],
        [0.0000, 0.2183, 0.2330,  ..., 0.0048, 0.1136, 0.0540],
        [0.0000, 0.2311, 0.2461,  ..., 0.0053, 0.1198, 0.0571],
        ...,
        [0.0000, 0.0007, 0.0089,  ..., 0.0000, 0.0076, 0.0011],
        [0.0000, 0.0007, 0.0089,  ..., 0.0000, 0.0076, 0.0011],
        [0.0000, 0.0007, 0.0089,  ..., 0.0000, 0.0076, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41014.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1046, 0.0000, 0.0000,  ..., 0.1078, 0.0000, 0.5677],
        [0.1323, 0.0000, 0.0000,  ..., 0.1259, 0.0000, 0.7111],
        [0.1358, 0.0000, 0.0000,  ..., 0.1281, 0.0000, 0.7326],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0179],
        [0.0015, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0179],
        [0.0015, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(427256.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(678.8370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(63.6199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(8830.0400, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-400.1105, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2959.1951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.0128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2626],
        [ 0.2770],
        [ 0.2863],
        ...,
        [-1.0549],
        [-1.0522],
        [-1.0515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232243.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0119],
        [1.0124],
        [1.0107],
        ...,
        [1.0018],
        [1.0010],
        [1.0002]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367936., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0120],
        [1.0125],
        [1.0108],
        ...,
        [1.0017],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367945.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0078,  0.0187,  0.0213,  ..., -0.0002,  0.0109,  0.0048],
        [-0.0038,  0.0091,  0.0114,  ..., -0.0005,  0.0062,  0.0025],
        [-0.0041,  0.0098,  0.0121,  ..., -0.0005,  0.0065,  0.0026],
        ...,
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1765.0388, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7778, device='cuda:0')



h[100].sum tensor(53.0816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.7044, device='cuda:0')



h[200].sum tensor(-3.8891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4550, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0315, 0.0406,  ..., 0.0000, 0.0224, 0.0086],
        [0.0000, 0.0655, 0.0756,  ..., 0.0000, 0.0390, 0.0169],
        [0.0000, 0.0316, 0.0408,  ..., 0.0000, 0.0225, 0.0087],
        ...,
        [0.0000, 0.0007, 0.0090,  ..., 0.0000, 0.0075, 0.0012],
        [0.0000, 0.0007, 0.0090,  ..., 0.0000, 0.0075, 0.0012],
        [0.0000, 0.0007, 0.0090,  ..., 0.0000, 0.0075, 0.0012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50298.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0221, 0.0000, 0.0000,  ..., 0.0533, 0.0000, 0.1535],
        [0.0292, 0.0000, 0.0000,  ..., 0.0579, 0.0000, 0.1931],
        [0.0208, 0.0000, 0.0000,  ..., 0.0529, 0.0000, 0.1416],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0181],
        [0.0015, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0181],
        [0.0015, 0.0000, 0.0000,  ..., 0.0420, 0.0000, 0.0181]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471787.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(921.3035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(106.2862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9717.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-511.3980, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2820.7451, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.5700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1518],
        [ 0.1472],
        [-0.0368],
        ...,
        [-1.0654],
        [-1.0627],
        [-1.0620]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199759.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0120],
        [1.0125],
        [1.0108],
        ...,
        [1.0017],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367945.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0122],
        [1.0126],
        [1.0109],
        ...,
        [1.0016],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367955.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0127,  0.0302,  0.0331,  ...,  0.0003,  0.0164,  0.0076],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        ...,
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1827.9053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.0280, device='cuda:0')



h[100].sum tensor(54.9264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.5503, device='cuda:0')



h[200].sum tensor(-4.1216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0179, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1121, 0.1235,  ..., 0.0016, 0.0616, 0.0284],
        [0.0000, 0.0765, 0.0869,  ..., 0.0005, 0.0443, 0.0198],
        [0.0000, 0.0460, 0.0555,  ..., 0.0000, 0.0294, 0.0123],
        ...,
        [0.0000, 0.0008, 0.0090,  ..., 0.0000, 0.0075, 0.0013],
        [0.0000, 0.0008, 0.0090,  ..., 0.0000, 0.0075, 0.0013],
        [0.0000, 0.0008, 0.0090,  ..., 0.0000, 0.0075, 0.0013]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51726.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0869, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.4788],
        [0.0720, 0.0000, 0.0000,  ..., 0.0851, 0.0000, 0.4057],
        [0.0569, 0.0000, 0.0000,  ..., 0.0752, 0.0000, 0.3327],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0187],
        [0.0013, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0187],
        [0.0013, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0187]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476104.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(929.8965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(120.5986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9904.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-529.4102, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2787.7056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.4927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2004],
        [ 0.2090],
        [ 0.2173],
        ...,
        [-1.0647],
        [-1.0620],
        [-1.0614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-185093.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0122],
        [1.0126],
        [1.0109],
        ...,
        [1.0016],
        [1.0009],
        [1.0001]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367955.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(550.5071, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0123],
        [1.0127],
        [1.0110],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367964.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1797.2622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7564, device='cuda:0')



h[100].sum tensor(51.9280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.6900, device='cuda:0')



h[200].sum tensor(-3.7855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4453, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0009, 0.0088,  ..., 0.0000, 0.0073, 0.0015],
        [0.0000, 0.0009, 0.0089,  ..., 0.0000, 0.0074, 0.0015],
        [0.0000, 0.0108, 0.0190,  ..., 0.0000, 0.0121, 0.0039],
        ...,
        [0.0000, 0.0010, 0.0089,  ..., 0.0000, 0.0074, 0.0015],
        [0.0000, 0.0010, 0.0089,  ..., 0.0000, 0.0074, 0.0015],
        [0.0000, 0.0010, 0.0089,  ..., 0.0000, 0.0074, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49892.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0000, 0.0000,  ..., 0.0401, 0.0000, 0.0245],
        [0.0033, 0.0000, 0.0000,  ..., 0.0413, 0.0000, 0.0381],
        [0.0093, 0.0000, 0.0000,  ..., 0.0447, 0.0000, 0.0772],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0196],
        [0.0010, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0196],
        [0.0010, 0.0000, 0.0000,  ..., 0.0406, 0.0000, 0.0196]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469932.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(874.5147, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(122.9779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9842.7666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-508.8971, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2880.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.1354, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5611],
        [-0.3120],
        [-0.0628],
        ...,
        [-1.0590],
        [-1.0564],
        [-1.0558]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198973.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0123],
        [1.0127],
        [1.0110],
        ...,
        [1.0016],
        [1.0008],
        [1.0000]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367964.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0124],
        [1.0128],
        [1.0112],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367974.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0116,  0.0139,  ..., -0.0004,  0.0074,  0.0032],
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [-0.0048,  0.0116,  0.0139,  ..., -0.0004,  0.0074,  0.0032],
        ...,
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2401.2002, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-29.2844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.9316, device='cuda:0')



h[100].sum tensor(82.8868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-23.6336, device='cuda:0')



h[200].sum tensor(-7.2331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.7284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0248, 0.0333,  ..., 0.0000, 0.0189, 0.0074],
        [0.0000, 0.0501, 0.0594,  ..., 0.0000, 0.0313, 0.0136],
        [0.0000, 0.0105, 0.0185,  ..., 0.0000, 0.0120, 0.0039],
        ...,
        [0.0000, 0.0011, 0.0089,  ..., 0.0000, 0.0074, 0.0016],
        [0.0000, 0.0011, 0.0089,  ..., 0.0000, 0.0074, 0.0016],
        [0.0000, 0.0011, 0.0089,  ..., 0.0000, 0.0074, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(78884.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0185, 0.0000, 0.0000,  ..., 0.0493, 0.0000, 0.1395],
        [0.0212, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.1569],
        [0.0127, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.1032],
        ...,
        [0.0007, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0204],
        [0.0007, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0204],
        [0.0007, 0.0000, 0.0000,  ..., 0.0400, 0.0000, 0.0204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635972.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1996.7046, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(264.7672, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12645.9551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-868.7366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2767.4995, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-493.0573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2050],
        [ 0.2015],
        [ 0.1196],
        ...,
        [-1.0548],
        [-1.0523],
        [-1.0517]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188522.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0124],
        [1.0128],
        [1.0112],
        ...,
        [1.0015],
        [1.0008],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367974.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0126],
        [1.0130],
        [1.0113],
        ...,
        [1.0015],
        [1.0007],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367984.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0037,  0.0091,  0.0113,  ..., -0.0005,  0.0061,  0.0026],
        [-0.0037,  0.0091,  0.0113,  ..., -0.0005,  0.0061,  0.0026],
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0022,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1850.3187, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.2514, device='cuda:0')



h[100].sum tensor(53.7402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.0249, device='cuda:0')



h[200].sum tensor(-3.8841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0296, 0.0381,  ..., 0.0000, 0.0213, 0.0086],
        [0.0000, 0.0236, 0.0319,  ..., 0.0000, 0.0183, 0.0072],
        [0.0000, 0.0175, 0.0256,  ..., 0.0000, 0.0153, 0.0057],
        ...,
        [0.0000, 0.0012, 0.0089,  ..., 0.0000, 0.0075, 0.0017],
        [0.0000, 0.0012, 0.0089,  ..., 0.0000, 0.0075, 0.0017],
        [0.0000, 0.0012, 0.0089,  ..., 0.0000, 0.0075, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50515.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0174, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.1494],
        [0.0141, 0.0000, 0.0000,  ..., 0.0456, 0.0000, 0.1294],
        [0.0103, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.1014],
        ...,
        [0.0003, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0213],
        [0.0003, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0213],
        [0.0003, 0.0000, 0.0000,  ..., 0.0394, 0.0000, 0.0213]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469672., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(626.6232, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(145.5278, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10172.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-512.2277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2557.5479, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.5044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2987],
        [ 0.2466],
        [ 0.1417],
        ...,
        [-1.0485],
        [-1.0460],
        [-1.0452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-152457.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0126],
        [1.0130],
        [1.0113],
        ...,
        [1.0015],
        [1.0007],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367984.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0127],
        [1.0131],
        [1.0115],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(367994.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.8803e-04,  2.1757e-03,  ..., -8.8621e-04,
          1.8398e-03,  4.2414e-04],
        [-9.6619e-03,  2.3281e-02,  2.5849e-02,  ..., -6.3485e-06,
          1.3050e-02,  6.0376e-03],
        [ 0.0000e+00,  2.8803e-04,  2.1757e-03,  ..., -8.8621e-04,
          1.8398e-03,  4.2414e-04],
        ...,
        [ 0.0000e+00,  2.8803e-04,  2.1757e-03,  ..., -8.8621e-04,
          1.8398e-03,  4.2414e-04],
        [ 0.0000e+00,  2.8803e-04,  2.1757e-03,  ..., -8.8621e-04,
          1.8398e-03,  4.2414e-04],
        [ 0.0000e+00,  2.8803e-04,  2.1757e-03,  ..., -8.8621e-04,
          1.8398e-03,  4.2414e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1739.9752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8806, device='cuda:0')



h[100].sum tensor(48.4898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7443, device='cuda:0')



h[200].sum tensor(-3.2059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1505, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0499, 0.0589,  ..., 0.0000, 0.0312, 0.0136],
        [0.0000, 0.0202, 0.0284,  ..., 0.0000, 0.0167, 0.0064],
        [0.0000, 0.0602, 0.0696,  ..., 0.0000, 0.0362, 0.0161],
        ...,
        [0.0000, 0.0012, 0.0089,  ..., 0.0000, 0.0075, 0.0017],
        [0.0000, 0.0012, 0.0089,  ..., 0.0000, 0.0075, 0.0017],
        [0.0000, 0.0012, 0.0089,  ..., 0.0000, 0.0075, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46498.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[2.2238e-02, 0.0000e+00, 0.0000e+00,  ..., 5.0427e-02, 0.0000e+00,
         1.7475e-01],
        [1.9821e-02, 0.0000e+00, 0.0000e+00,  ..., 4.9223e-02, 0.0000e+00,
         1.5936e-01],
        [2.7334e-02, 0.0000e+00, 0.0000e+00,  ..., 5.3690e-02, 0.0000e+00,
         2.0309e-01],
        ...,
        [4.9998e-05, 0.0000e+00, 0.0000e+00,  ..., 3.9132e-02, 0.0000e+00,
         2.1860e-02],
        [5.0020e-05, 0.0000e+00, 0.0000e+00,  ..., 3.9138e-02, 0.0000e+00,
         2.1865e-02],
        [5.0053e-05, 0.0000e+00, 0.0000e+00,  ..., 3.9139e-02, 0.0000e+00,
         2.1866e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(451366.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(429.8776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(130.9691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9955.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-464.2798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2586.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.7383, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3315],
        [ 0.3318],
        [ 0.3338],
        ...,
        [-1.0504],
        [-1.0473],
        [-1.0453]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-154307.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0127],
        [1.0131],
        [1.0115],
        ...,
        [1.0014],
        [1.0007],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(367994.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0129],
        [1.0133],
        [1.0116],
        ...,
        [1.0014],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368003.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.8129e-03,  2.1275e-02,  2.3810e-02,  ..., -9.1494e-05,
          1.2090e-02,  5.5499e-03],
        [-7.7787e-03,  1.8810e-02,  2.1273e-02,  ..., -1.8532e-04,
          1.0888e-02,  4.9479e-03],
        [ 0.0000e+00,  2.6940e-04,  2.1884e-03,  ..., -8.9107e-04,
          1.8471e-03,  4.1985e-04],
        ...,
        [ 0.0000e+00,  2.6940e-04,  2.1884e-03,  ..., -8.9107e-04,
          1.8471e-03,  4.1985e-04],
        [ 0.0000e+00,  2.6940e-04,  2.1884e-03,  ..., -8.9107e-04,
          1.8471e-03,  4.1985e-04],
        [ 0.0000e+00,  2.6940e-04,  2.1884e-03,  ..., -8.9107e-04,
          1.8471e-03,  4.1985e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1842.9727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.2213, device='cuda:0')



h[100].sum tensor(54.6948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.0045, device='cuda:0')



h[200].sum tensor(-3.8183, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0864, 0.0966,  ..., 0.0003, 0.0491, 0.0225],
        [0.0000, 0.0377, 0.0466,  ..., 0.0000, 0.0253, 0.0106],
        [0.0000, 0.0199, 0.0282,  ..., 0.0000, 0.0166, 0.0063],
        ...,
        [0.0000, 0.0011, 0.0090,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0011, 0.0090,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0011, 0.0090,  ..., 0.0000, 0.0076, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49077.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0354, 0.0000, 0.0000,  ..., 0.0597, 0.0000, 0.2303],
        [0.0232, 0.0000, 0.0000,  ..., 0.0526, 0.0000, 0.1612],
        [0.0130, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.1028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0393, 0.0000, 0.0221],
        [0.0000, 0.0000, 0.0000,  ..., 0.0393, 0.0000, 0.0221],
        [0.0000, 0.0000, 0.0000,  ..., 0.0393, 0.0000, 0.0221]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(456384.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(475.2957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(141.0207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10012.8193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-501.1918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2756.2202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.8214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0491],
        [-0.0319],
        [-0.1559],
        ...,
        [-1.0670],
        [-1.0646],
        [-1.0640]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-178192.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0129],
        [1.0133],
        [1.0116],
        ...,
        [1.0014],
        [1.0006],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368003.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0130],
        [1.0134],
        [1.0117],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368013.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035,  0.0086,  0.0108,  ..., -0.0006,  0.0059,  0.0025],
        [-0.0086,  0.0207,  0.0233,  ..., -0.0001,  0.0118,  0.0054],
        [-0.0046,  0.0113,  0.0136,  ..., -0.0005,  0.0072,  0.0031],
        ...,
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1688.0006, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7732, device='cuda:0')



h[100].sum tensor(47.3651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.9951, device='cuda:0')



h[200].sum tensor(-2.9591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0607, 0.0703,  ..., 0.0000, 0.0366, 0.0162],
        [0.0000, 0.0376, 0.0466,  ..., 0.0000, 0.0254, 0.0106],
        [0.0000, 0.0618, 0.0715,  ..., 0.0000, 0.0372, 0.0165],
        ...,
        [0.0000, 0.0010, 0.0090,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0010, 0.0090,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0010, 0.0090,  ..., 0.0000, 0.0076, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44875.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0227, 0.0000, 0.0000,  ..., 0.0515, 0.0000, 0.1777],
        [0.0225, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.1836],
        [0.0296, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.2224],
        ...,
        [0.0004, 0.0000, 0.0000,  ..., 0.0399, 0.0000, 0.0250],
        [0.0000, 0.0000, 0.0000,  ..., 0.0397, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0397, 0.0000, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(444397.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(323.0658, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(118.7235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9905.2568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-450.5866, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2775.5305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-383.8603, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1396],
        [ 0.0348],
        [ 0.1372],
        ...,
        [-0.8112],
        [-0.9451],
        [-1.0333]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-169691.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0130],
        [1.0134],
        [1.0117],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368013.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0131],
        [1.0135],
        [1.0118],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368022.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1611.2893, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8975, device='cuda:0')



h[100].sum tensor(44.1956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7260, device='cuda:0')



h[200].sum tensor(-2.5641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0009, 0.0090,  ..., 0.0000, 0.0075, 0.0016],
        [0.0000, 0.0009, 0.0090,  ..., 0.0000, 0.0075, 0.0016],
        [0.0000, 0.0009, 0.0090,  ..., 0.0000, 0.0075, 0.0016],
        ...,
        [0.0000, 0.0009, 0.0091,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0009, 0.0091,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0009, 0.0091,  ..., 0.0000, 0.0076, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42527.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0396, 0.0000, 0.0219],
        [0.0002, 0.0000, 0.0000,  ..., 0.0399, 0.0000, 0.0241],
        [0.0005, 0.0000, 0.0000,  ..., 0.0401, 0.0000, 0.0274],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.0000, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(433905.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(247.1609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.0588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9707.8613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-426.9902, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2968.5359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.7689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0521],
        [-0.8873],
        [-0.6587],
        ...,
        [-1.0978],
        [-1.0916],
        [-1.0868]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191303.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0131],
        [1.0135],
        [1.0118],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368022.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0132],
        [1.0136],
        [1.0118],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368031.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004],
        [ 0.0000,  0.0002,  0.0022,  ..., -0.0009,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1657.8608, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4394, device='cuda:0')



h[100].sum tensor(47.3796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.7692, device='cuda:0')



h[200].sum tensor(-2.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0091,  ..., 0.0000, 0.0075, 0.0015],
        [0.0000, 0.0007, 0.0091,  ..., 0.0000, 0.0075, 0.0016],
        [0.0000, 0.0007, 0.0091,  ..., 0.0000, 0.0075, 0.0016],
        ...,
        [0.0000, 0.0007, 0.0092,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0007, 0.0092,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0007, 0.0092,  ..., 0.0000, 0.0076, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44049.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0403, 0.0000, 0.0218],
        [0.0000, 0.0000, 0.0000,  ..., 0.0404, 0.0000, 0.0219],
        [0.0000, 0.0000, 0.0000,  ..., 0.0404, 0.0000, 0.0220],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.0000, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(440052.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(315.7710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.2878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9777.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-451.1627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3173.5452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-383.9055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0665],
        [-1.1785],
        [-1.2635],
        ...,
        [-1.1204],
        [-1.1178],
        [-1.1173]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209280.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0132],
        [1.0136],
        [1.0118],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368031.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0133],
        [1.0137],
        [1.0119],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368041.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [-0.0081,  0.0197,  0.0223,  ..., -0.0002,  0.0114,  0.0051],
        [-0.0121,  0.0293,  0.0322,  ...,  0.0002,  0.0161,  0.0075],
        ...,
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1595.4443, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8546, device='cuda:0')



h[100].sum tensor(44.3669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.6970, device='cuda:0')



h[200].sum tensor(-2.5348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.7880, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0560, 0.0660,  ..., 0.0000, 0.0345, 0.0150],
        [0.0000, 0.0463, 0.0561,  ..., 0.0002, 0.0298, 0.0127],
        [0.0000, 0.0446, 0.0543,  ..., 0.0000, 0.0289, 0.0122],
        ...,
        [0.0000, 0.0007, 0.0093,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0007, 0.0093,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0007, 0.0093,  ..., 0.0000, 0.0076, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41811.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0232, 0.0000, 0.0000,  ..., 0.0537, 0.0000, 0.1750],
        [0.0288, 0.0000, 0.0000,  ..., 0.0574, 0.0000, 0.2021],
        [0.0326, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.2246],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0413, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0413, 0.0000, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(431358.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(282.9874, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(87.8235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9640.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-425.8762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3278.4639, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.3650, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2163],
        [ 0.3048],
        [ 0.3349],
        ...,
        [-1.1359],
        [-1.1331],
        [-1.1325]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226300.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0133],
        [1.0137],
        [1.0119],
        ...,
        [1.0013],
        [1.0005],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368041.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0134],
        [1.0138],
        [1.0121],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368050.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1698.7128, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5547, device='cuda:0')



h[100].sum tensor(49.1548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5238, device='cuda:0')



h[200].sum tensor(-3.0747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0091,  ..., 0.0000, 0.0075, 0.0015],
        [0.0000, 0.0007, 0.0092,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0007, 0.0092,  ..., 0.0000, 0.0076, 0.0015],
        ...,
        [0.0000, 0.0007, 0.0093,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0007, 0.0093,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0007, 0.0093,  ..., 0.0000, 0.0076, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47786.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0006, 0.0000, 0.0000,  ..., 0.0414, 0.0000, 0.0275],
        [0.0020, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0394],
        [0.0027, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0455],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0000, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0000, 0.0223]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(467681.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(529.5551, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(115.1403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10291.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-498.3997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3153.8008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.6312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3734],
        [-0.3577],
        [-0.2842],
        ...,
        [-1.1432],
        [-1.1404],
        [-1.1397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215660.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0134],
        [1.0138],
        [1.0121],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368050.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(510.0027, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0139],
        [1.0122],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368060.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0113,  0.0274,  0.0302,  ...,  0.0001,  0.0151,  0.0070],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1721.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8231, device='cuda:0')



h[100].sum tensor(48.6315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7054, device='cuda:0')



h[200].sum tensor(-3.1200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0862, 0.0969,  ..., 0.0015, 0.0492, 0.0224],
        [0.0000, 0.0508, 0.0606,  ..., 0.0001, 0.0320, 0.0137],
        [0.0000, 0.0008, 0.0092,  ..., 0.0000, 0.0076, 0.0015],
        ...,
        [0.0000, 0.0008, 0.0093,  ..., 0.0000, 0.0077, 0.0015],
        [0.0000, 0.0008, 0.0093,  ..., 0.0000, 0.0077, 0.0015],
        [0.0000, 0.0008, 0.0093,  ..., 0.0000, 0.0077, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47486.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[5.2731e-02, 0.0000e+00, 0.0000e+00,  ..., 7.2690e-02, 0.0000e+00,
         3.1157e-01],
        [3.6037e-02, 0.0000e+00, 0.0000e+00,  ..., 6.2793e-02, 0.0000e+00,
         2.2439e-01],
        [1.7871e-02, 0.0000e+00, 0.0000e+00,  ..., 5.1953e-02, 0.0000e+00,
         1.2439e-01],
        ...,
        [4.2439e-05, 0.0000e+00, 0.0000e+00,  ..., 4.1780e-02, 0.0000e+00,
         2.2434e-02],
        [4.2463e-05, 0.0000e+00, 0.0000e+00,  ..., 4.1786e-02, 0.0000e+00,
         2.2439e-02],
        [4.2499e-05, 0.0000e+00, 0.0000e+00,  ..., 4.1787e-02, 0.0000e+00,
         2.2440e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(465365.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(522.3351, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(117.1911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10344.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-491.1128, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2967.6143, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.2913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2349],
        [ 0.2429],
        [ 0.2541],
        ...,
        [-1.1471],
        [-1.1443],
        [-1.1437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199689.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0139],
        [1.0122],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368060.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0136],
        [1.0140],
        [1.0123],
        ...,
        [1.0012],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368070.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0128,  0.0311,  0.0340,  ...,  0.0003,  0.0169,  0.0079],
        [-0.0058,  0.0141,  0.0165,  ..., -0.0004,  0.0086,  0.0038],
        [-0.0074,  0.0180,  0.0206,  ..., -0.0002,  0.0106,  0.0047],
        ...,
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0023,  ..., -0.0009,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1897.7883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.6516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4896, device='cuda:0')



h[100].sum tensor(55.3523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.8626, device='cuda:0')



h[200].sum tensor(-3.9830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0907, 0.1014,  ..., 0.0002, 0.0514, 0.0235],
        [0.0000, 0.0913, 0.1021,  ..., 0.0004, 0.0517, 0.0236],
        [0.0000, 0.0298, 0.0388,  ..., 0.0000, 0.0217, 0.0086],
        ...,
        [0.0000, 0.0010, 0.0092,  ..., 0.0000, 0.0077, 0.0016],
        [0.0000, 0.0010, 0.0092,  ..., 0.0000, 0.0077, 0.0016],
        [0.0000, 0.0010, 0.0092,  ..., 0.0000, 0.0077, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53686.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0780, 0.0000, 0.0000,  ..., 0.0880, 0.0000, 0.4625],
        [0.0617, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.3778],
        [0.0386, 0.0000, 0.0000,  ..., 0.0642, 0.0000, 0.2519],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0422, 0.0000, 0.0267],
        [0.0014, 0.0000, 0.0000,  ..., 0.0425, 0.0000, 0.0349],
        [0.0018, 0.0000, 0.0000,  ..., 0.0426, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492551.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(724.0404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(149.6541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10886.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-564.9697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2759.4622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.1366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3731],
        [ 0.3692],
        [ 0.3633],
        ...,
        [-1.0383],
        [-0.9455],
        [-0.8834]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-182546.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0136],
        [1.0140],
        [1.0123],
        ...,
        [1.0012],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368070.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0137],
        [1.0142],
        [1.0124],
        ...,
        [1.0012],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368080.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0009,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0009,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1769.4281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4590, device='cuda:0')



h[100].sum tensor(46.0381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1356, device='cuda:0')



h[200].sum tensor(-3.1766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0091,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0011, 0.0092,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0011, 0.0092,  ..., 0.0000, 0.0076, 0.0016],
        ...,
        [0.0000, 0.0011, 0.0092,  ..., 0.0000, 0.0077, 0.0016],
        [0.0000, 0.0011, 0.0092,  ..., 0.0000, 0.0077, 0.0016],
        [0.0000, 0.0011, 0.0092,  ..., 0.0000, 0.0077, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46732.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0106, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.0843],
        [0.0031, 0.0000, 0.0000,  ..., 0.0433, 0.0000, 0.0408],
        [0.0011, 0.0000, 0.0000,  ..., 0.0423, 0.0000, 0.0266],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0424, 0.0000, 0.0227],
        [0.0006, 0.0000, 0.0000,  ..., 0.0424, 0.0000, 0.0227],
        [0.0006, 0.0000, 0.0000,  ..., 0.0424, 0.0000, 0.0227]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(457880.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(654.6061, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(120.8057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10217.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-479.9840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2922.2117, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-396.8161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0111],
        [-0.2475],
        [-0.4603],
        ...,
        [-1.1536],
        [-1.1509],
        [-1.1503]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207621.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0137],
        [1.0142],
        [1.0124],
        ...,
        [1.0012],
        [1.0004],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368080.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0138],
        [1.0143],
        [1.0125],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368091., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0067,  0.0166,  0.0190,  ..., -0.0002,  0.0098,  0.0044],
        [-0.0049,  0.0122,  0.0145,  ..., -0.0004,  0.0077,  0.0033],
        [-0.0034,  0.0086,  0.0108,  ..., -0.0005,  0.0059,  0.0024],
        ...,
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1711.3698, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4172, device='cuda:0')



h[100].sum tensor(40.2945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.7542, device='cuda:0')



h[200].sum tensor(-2.7572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.4915, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.8313e-02, 6.7792e-02,  ..., 3.1241e-05, 3.5321e-02,
         1.5631e-02],
        [0.0000e+00, 4.2874e-02, 5.1955e-02,  ..., 0.0000e+00, 2.7828e-02,
         1.1862e-02],
        [0.0000e+00, 3.4606e-02, 4.3458e-02,  ..., 0.0000e+00, 2.3802e-02,
         9.8414e-03],
        ...,
        [0.0000e+00, 1.2473e-03, 9.2454e-03,  ..., 0.0000e+00, 7.6165e-03,
         1.6989e-03],
        [0.0000e+00, 1.2475e-03, 9.2469e-03,  ..., 0.0000e+00, 7.6178e-03,
         1.6992e-03],
        [0.0000e+00, 1.2475e-03, 9.2469e-03,  ..., 0.0000e+00, 7.6178e-03,
         1.6992e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44008.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0278, 0.0000, 0.0000,  ..., 0.0578, 0.0000, 0.1922],
        [0.0264, 0.0000, 0.0000,  ..., 0.0567, 0.0000, 0.1923],
        [0.0231, 0.0000, 0.0000,  ..., 0.0546, 0.0000, 0.1776],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.0228],
        [0.0010, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.0228],
        [0.0010, 0.0000, 0.0000,  ..., 0.0428, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(449591.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(632.9454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(112.4837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10124.5527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-443.4836, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2837.7798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.2982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2814],
        [ 0.3141],
        [ 0.3062],
        ...,
        [-1.1566],
        [-1.1539],
        [-1.1533]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194983.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0138],
        [1.0143],
        [1.0125],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368091., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0139],
        [1.0144],
        [1.0126],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368101.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.2410e-04,  2.2627e-03,  ..., -8.2291e-04,
          1.8527e-03,  4.2473e-04],
        [ 0.0000e+00,  3.2410e-04,  2.2627e-03,  ..., -8.2291e-04,
          1.8527e-03,  4.2473e-04],
        [-9.6810e-03,  2.3768e-02,  2.6357e-02,  ...,  7.2133e-05,
          1.3268e-02,  6.1565e-03],
        ...,
        [ 0.0000e+00,  3.2410e-04,  2.2627e-03,  ..., -8.2291e-04,
          1.8527e-03,  4.2473e-04],
        [ 0.0000e+00,  3.2410e-04,  2.2627e-03,  ..., -8.2291e-04,
          1.8527e-03,  4.2473e-04],
        [ 0.0000e+00,  3.2410e-04,  2.2627e-03,  ..., -8.2291e-04,
          1.8527e-03,  4.2473e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2021.0745, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.3286, device='cuda:0')



h[100].sum tensor(53.5031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.1068, device='cuda:0')



h[200].sum tensor(-4.3190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.3095e-03, 9.1425e-03,  ..., 0.0000e+00, 7.4860e-03,
         1.7162e-03],
        [0.0000e+00, 2.5105e-02, 3.3630e-02,  ..., 7.3197e-05, 1.9100e-02,
         7.5394e-03],
        [0.0000e+00, 7.9950e-02, 8.9998e-02,  ..., 1.3332e-03, 4.5806e-02,
         2.0948e-02],
        ...,
        [0.0000e+00, 1.3273e-03, 9.2667e-03,  ..., 0.0000e+00, 7.5878e-03,
         1.7395e-03],
        [0.0000e+00, 1.3276e-03, 9.2683e-03,  ..., 0.0000e+00, 7.5891e-03,
         1.7398e-03],
        [0.0000e+00, 1.3276e-03, 9.2684e-03,  ..., 0.0000e+00, 7.5891e-03,
         1.7398e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57177.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0000, 0.0000,  ..., 0.0477, 0.0000, 0.0673],
        [0.0332, 0.0000, 0.0000,  ..., 0.0628, 0.0000, 0.1878],
        [0.0731, 0.0000, 0.0000,  ..., 0.0877, 0.0000, 0.3898],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0435, 0.0000, 0.0228],
        [0.0013, 0.0000, 0.0000,  ..., 0.0435, 0.0000, 0.0228],
        [0.0013, 0.0000, 0.0000,  ..., 0.0435, 0.0000, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(520559.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1223.9521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(174.7587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11297.1826, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-605.2330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2735.0120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-438.3864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0271],
        [ 0.0845],
        [ 0.1686],
        ...,
        [-1.1636],
        [-1.1608],
        [-1.1602]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190219.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0139],
        [1.0144],
        [1.0126],
        ...,
        [1.0012],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368101.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0144],
        [1.0127],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368111.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0008,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1810.3109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.6433, device='cuda:0')



h[100].sum tensor(40.8362, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.2603, device='cuda:0')



h[200].sum tensor(-3.1194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0092,  ..., 0.0000, 0.0075, 0.0017],
        [0.0000, 0.0014, 0.0092,  ..., 0.0000, 0.0075, 0.0018],
        [0.0000, 0.0014, 0.0092,  ..., 0.0000, 0.0075, 0.0018],
        ...,
        [0.0000, 0.0014, 0.0093,  ..., 0.0000, 0.0076, 0.0018],
        [0.0000, 0.0014, 0.0093,  ..., 0.0000, 0.0076, 0.0018],
        [0.0000, 0.0014, 0.0093,  ..., 0.0000, 0.0076, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47651.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0000, 0.0000,  ..., 0.0434, 0.0000, 0.0225],
        [0.0017, 0.0000, 0.0000,  ..., 0.0436, 0.0000, 0.0233],
        [0.0016, 0.0000, 0.0000,  ..., 0.0435, 0.0000, 0.0252],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0440, 0.0000, 0.0229],
        [0.0017, 0.0000, 0.0000,  ..., 0.0441, 0.0000, 0.0229],
        [0.0017, 0.0000, 0.0000,  ..., 0.0441, 0.0000, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473628.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(944.9083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(133.0379, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10608.3545, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-486.0961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2641.8154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.7301, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2093],
        [-1.0780],
        [-0.8812],
        ...,
        [-1.1695],
        [-1.1662],
        [-1.1651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-173758.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0144],
        [1.0127],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368111.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0140],
        [1.0145],
        [1.0128],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368121.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0114,  0.0281,  0.0308,  ...,  0.0003,  0.0154,  0.0072],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1647.0734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.6527, device='cuda:0')



h[100].sum tensor(31.4960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.8839, device='cuda:0')



h[200].sum tensor(-2.2137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.2468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0804, 0.0904,  ..., 0.0012, 0.0459, 0.0210],
        [0.0000, 0.0526, 0.0619,  ..., 0.0004, 0.0324, 0.0142],
        [0.0000, 0.0014, 0.0092,  ..., 0.0000, 0.0075, 0.0017],
        ...,
        [0.0000, 0.0014, 0.0093,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0014, 0.0093,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0014, 0.0093,  ..., 0.0000, 0.0076, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41075.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0560, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.3080],
        [0.0373, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.2085],
        [0.0130, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.0817],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0447, 0.0000, 0.0230],
        [0.0020, 0.0000, 0.0000,  ..., 0.0447, 0.0000, 0.0230],
        [0.0020, 0.0000, 0.0000,  ..., 0.0447, 0.0000, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(441695.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(857.8055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(102.3100, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10001.9951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-407.5303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2823.2866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.8563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1825],
        [ 0.0568],
        [-0.2107],
        ...,
        [-1.1808],
        [-1.1781],
        [-1.1775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208115.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0140],
        [1.0145],
        [1.0128],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368121.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0146],
        [1.0129],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368130.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [-0.0039,  0.0098,  0.0119,  ..., -0.0004,  0.0064,  0.0027],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2485.2642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-27.7548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-34.7028, device='cuda:0')



h[100].sum tensor(72.2943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-23.4788, device='cuda:0')



h[200].sum tensor(-6.5101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.6254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0633, 0.0729,  ..., 0.0007, 0.0377, 0.0168],
        [0.0000, 0.0092, 0.0173,  ..., 0.0000, 0.0114, 0.0035],
        [0.0000, 0.0613, 0.0708,  ..., 0.0006, 0.0367, 0.0163],
        ...,
        [0.0000, 0.0015, 0.0094,  ..., 0.0000, 0.0077, 0.0017],
        [0.0000, 0.0015, 0.0094,  ..., 0.0000, 0.0077, 0.0017],
        [0.0000, 0.0015, 0.0094,  ..., 0.0000, 0.0077, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72933.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0512, 0.0000, 0.0000,  ..., 0.0767, 0.0000, 0.2929],
        [0.0462, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.2633],
        [0.0667, 0.0000, 0.0000,  ..., 0.0874, 0.0000, 0.3645],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0231],
        [0.0023, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0231],
        [0.0023, 0.0000, 0.0000,  ..., 0.0452, 0.0000, 0.0231]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594329.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1995.1523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(247.8773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12796.9336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-801.3843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2367.9116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.3488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0936],
        [ 0.0829],
        [ 0.0717],
        ...,
        [-1.1894],
        [-1.1869],
        [-1.1866]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-176858.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0146],
        [1.0129],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368130.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0147],
        [1.0131],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368140.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0147,  0.0362,  0.0392,  ...,  0.0006,  0.0193,  0.0091],
        [-0.0159,  0.0392,  0.0422,  ...,  0.0008,  0.0208,  0.0099],
        [-0.0149,  0.0366,  0.0395,  ...,  0.0007,  0.0195,  0.0092],
        ...,
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0023,  ..., -0.0008,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1960.9875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1381, device='cuda:0')



h[100].sum tensor(46.1049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.6248, device='cuda:0')



h[200].sum tensor(-3.7679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0674, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1387, 0.1503,  ..., 0.0023, 0.0744, 0.0351],
        [0.0000, 0.1601, 0.1724,  ..., 0.0031, 0.0849, 0.0403],
        [0.0000, 0.1665, 0.1789,  ..., 0.0034, 0.0880, 0.0419],
        ...,
        [0.0000, 0.0015, 0.0094,  ..., 0.0000, 0.0078, 0.0016],
        [0.0000, 0.0015, 0.0094,  ..., 0.0000, 0.0078, 0.0016],
        [0.0000, 0.0015, 0.0094,  ..., 0.0000, 0.0078, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54075.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1218, 0.0000, 0.0000,  ..., 0.1267, 0.0000, 0.6292],
        [0.1250, 0.0000, 0.0000,  ..., 0.1291, 0.0000, 0.6446],
        [0.1183, 0.0000, 0.0000,  ..., 0.1245, 0.0000, 0.6131],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0458, 0.0000, 0.0233],
        [0.0025, 0.0000, 0.0000,  ..., 0.0458, 0.0000, 0.0233],
        [0.0025, 0.0000, 0.0000,  ..., 0.0458, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508335.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1394.0398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(160.8712, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11499.9473, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-568.7559, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2333.0171, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-441.0856, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1298],
        [ 0.1354],
        [ 0.1483],
        ...,
        [-1.2022],
        [-1.1994],
        [-1.1988]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168013.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0147],
        [1.0131],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368140.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0148],
        [1.0132],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368149.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0168,  0.0414,  0.0445,  ...,  0.0009,  0.0219,  0.0104],
        [-0.0117,  0.0290,  0.0318,  ...,  0.0004,  0.0158,  0.0074],
        [-0.0039,  0.0099,  0.0122,  ..., -0.0004,  0.0066,  0.0027],
        ...,
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1878.8274, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1212, device='cuda:0')



h[100].sum tensor(41.8609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.2602, device='cuda:0')



h[200].sum tensor(-3.3669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1593, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1516, 0.1638,  ..., 0.0029, 0.0808, 0.0382],
        [0.0000, 0.0856, 0.0960,  ..., 0.0010, 0.0487, 0.0220],
        [0.0000, 0.0736, 0.0836,  ..., 0.0005, 0.0429, 0.0191],
        ...,
        [0.0000, 0.0014, 0.0095,  ..., 0.0000, 0.0078, 0.0015],
        [0.0000, 0.0014, 0.0095,  ..., 0.0000, 0.0078, 0.0015],
        [0.0000, 0.0014, 0.0095,  ..., 0.0000, 0.0078, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49925.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0977, 0.0000, 0.0000,  ..., 0.1123, 0.0000, 0.5154],
        [0.0720, 0.0000, 0.0000,  ..., 0.0945, 0.0000, 0.3973],
        [0.0560, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.3230],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.0233],
        [0.0029, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.0233],
        [0.0029, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487493.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1322.0425, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.5560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11153.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-518.8113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2420.0684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.0067, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2927],
        [ 0.3156],
        [ 0.3325],
        ...,
        [-1.2209],
        [-1.2180],
        [-1.2174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-184411.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0148],
        [1.0132],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368149.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0141],
        [1.0148],
        [1.0132],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368149.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004],
        [-0.0045,  0.0114,  0.0137,  ..., -0.0003,  0.0073,  0.0031],
        ...,
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1806.3789, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.1174, device='cuda:0')



h[100].sum tensor(38.3140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.9045, device='cuda:0')



h[200].sum tensor(-2.9986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2571, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0094,  ..., 0.0000, 0.0077, 0.0015],
        [0.0000, 0.0126, 0.0210,  ..., 0.0000, 0.0132, 0.0042],
        [0.0000, 0.0241, 0.0328,  ..., 0.0000, 0.0188, 0.0070],
        ...,
        [0.0000, 0.0014, 0.0095,  ..., 0.0000, 0.0078, 0.0015],
        [0.0000, 0.0014, 0.0095,  ..., 0.0000, 0.0078, 0.0015],
        [0.0000, 0.0014, 0.0095,  ..., 0.0000, 0.0078, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48827.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0067, 0.0000, 0.0000,  ..., 0.0488, 0.0000, 0.0520],
        [0.0112, 0.0000, 0.0000,  ..., 0.0520, 0.0000, 0.0797],
        [0.0175, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.1206],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.0233],
        [0.0029, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.0233],
        [0.0029, 0.0000, 0.0000,  ..., 0.0468, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(487315.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1337.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(132.3154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11108.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-505.6259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2480.4971, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.2213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2311],
        [-0.0144],
        [ 0.1542],
        ...,
        [-1.2209],
        [-1.2180],
        [-1.2174]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-194983.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0141],
        [1.0148],
        [1.0132],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368149.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0149],
        [1.0133],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368158.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0127,  0.0312,  0.0341,  ...,  0.0005,  0.0170,  0.0079],
        [-0.0117,  0.0290,  0.0318,  ...,  0.0004,  0.0159,  0.0073],
        [-0.0030,  0.0076,  0.0098,  ..., -0.0004,  0.0054,  0.0021],
        ...,
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0023,  ..., -0.0007,  0.0019,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1932.3236, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.6587, device='cuda:0')



h[100].sum tensor(44.3347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.3004, device='cuda:0')



h[200].sum tensor(-3.6607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8516, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1424, 0.1545,  ..., 0.0026, 0.0764, 0.0358],
        [0.0000, 0.1053, 0.1164,  ..., 0.0016, 0.0584, 0.0268],
        [0.0000, 0.1331, 0.1450,  ..., 0.0022, 0.0719, 0.0336],
        ...,
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0078, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51344.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1056, 0.0000, 0.0000,  ..., 0.1201, 0.0000, 0.5474],
        [0.0979, 0.0000, 0.0000,  ..., 0.1148, 0.0000, 0.5122],
        [0.0982, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.5136],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0233],
        [0.0032, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0233],
        [0.0032, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492961.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1480.1965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(139.7280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11165.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-538.0914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2564.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.2809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1949],
        [ 0.1899],
        [ 0.1888],
        ...,
        [-1.2390],
        [-1.2359],
        [-1.2352]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211289.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0149],
        [1.0133],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368158.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0149],
        [1.0134],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368167.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [-0.0048,  0.0120,  0.0144,  ..., -0.0003,  0.0076,  0.0032],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1861.6580, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9533, device='cuda:0')



h[100].sum tensor(40.7308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1466, device='cuda:0')



h[200].sum tensor(-3.3009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0837, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0217,  ..., 0.0000, 0.0135, 0.0042],
        [0.0000, 0.0109, 0.0195,  ..., 0.0000, 0.0125, 0.0037],
        [0.0000, 0.0507, 0.0605,  ..., 0.0000, 0.0319, 0.0135],
        ...,
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50834.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0106, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.0715],
        [0.0144, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0970],
        [0.0227, 0.0000, 0.0000,  ..., 0.0618, 0.0000, 0.1483],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0234],
        [0.0035, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0234],
        [0.0035, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(499829.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1621.5129, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(134.9725, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11192.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-534.2411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2708.2148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.2366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2459],
        [-0.0209],
        [ 0.1178],
        ...,
        [-1.2581],
        [-1.2550],
        [-1.2544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-228453.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0149],
        [1.0134],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368167.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0150],
        [1.0135],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368177.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1929.3506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.4759, device='cuda:0')



h[100].sum tensor(43.4768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.1767, device='cuda:0')



h[200].sum tensor(-3.5826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.7693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0096,  ..., 0.0000, 0.0077, 0.0014],
        [0.0000, 0.0012, 0.0096,  ..., 0.0000, 0.0077, 0.0014],
        [0.0000, 0.0012, 0.0096,  ..., 0.0000, 0.0077, 0.0014],
        ...,
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49583.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0038, 0.0000, 0.0000,  ..., 0.0485, 0.0000, 0.0253],
        [0.0035, 0.0000, 0.0000,  ..., 0.0484, 0.0000, 0.0242],
        [0.0035, 0.0000, 0.0000,  ..., 0.0484, 0.0000, 0.0265],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0489, 0.0000, 0.0236],
        [0.0035, 0.0000, 0.0000,  ..., 0.0490, 0.0000, 0.0236],
        [0.0035, 0.0000, 0.0000,  ..., 0.0490, 0.0000, 0.0236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482442.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1491.7070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(132.2696, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10846.0996, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-519.2271, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2751.3044, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-425.7271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0092],
        [-1.1308],
        [-1.1633],
        ...,
        [-1.2658],
        [-1.2631],
        [-1.2628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235128.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0150],
        [1.0135],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368177.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0150],
        [1.0135],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368177.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [-0.0037,  0.0095,  0.0118,  ..., -0.0004,  0.0064,  0.0026],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1789.9104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.6649, device='cuda:0')



h[100].sum tensor(36.7222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5984, device='cuda:0')



h[200].sum tensor(-2.8909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0096,  ..., 0.0000, 0.0077, 0.0014],
        [0.0000, 0.0105, 0.0192,  ..., 0.0000, 0.0123, 0.0037],
        [0.0000, 0.0323, 0.0416,  ..., 0.0001, 0.0229, 0.0090],
        ...,
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014],
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0078, 0.0014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47251.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0241, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.1370],
        [0.0271, 0.0000, 0.0000,  ..., 0.0654, 0.0000, 0.1584],
        [0.0333, 0.0000, 0.0000,  ..., 0.0698, 0.0000, 0.1954],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0489, 0.0000, 0.0236],
        [0.0057, 0.0000, 0.0000,  ..., 0.0505, 0.0000, 0.0372],
        [0.0108, 0.0000, 0.0000,  ..., 0.0542, 0.0000, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(480299.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1430.5416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(121.7139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10849.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-489.6164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2707.5957, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.8884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1586],
        [ 0.1822],
        [ 0.2020],
        ...,
        [-1.0819],
        [-0.8109],
        [-0.4396]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217233.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0150],
        [1.0135],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368177.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0142],
        [1.0151],
        [1.0137],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368187.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040,  0.0100,  0.0123,  ..., -0.0003,  0.0066,  0.0028],
        [-0.0037,  0.0094,  0.0118,  ..., -0.0004,  0.0063,  0.0026],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1851.9152, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7378, device='cuda:0')



h[100].sum tensor(39.0682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3243, device='cuda:0')



h[200].sum tensor(-3.0904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.2608e-02, 6.2393e-02,  ..., 1.5343e-05, 3.2635e-02,
         1.4100e-02],
        [0.0000e+00, 2.6675e-02, 3.5786e-02,  ..., 0.0000e+00, 2.0048e-02,
         7.7730e-03],
        [0.0000e+00, 1.0504e-02, 1.9177e-02,  ..., 0.0000e+00, 1.2185e-02,
         3.8252e-03],
        ...,
        [0.0000e+00, 1.2344e-03, 9.7336e-03,  ..., 0.0000e+00, 7.7427e-03,
         1.5738e-03],
        [0.0000e+00, 1.2346e-03, 9.7354e-03,  ..., 0.0000e+00, 7.7441e-03,
         1.5741e-03],
        [0.0000e+00, 1.2346e-03, 9.7356e-03,  ..., 0.0000e+00, 7.7442e-03,
         1.5741e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48416.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0253, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.1630],
        [0.0188, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.1242],
        [0.0108, 0.0000, 0.0000,  ..., 0.0531, 0.0000, 0.0765],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0484, 0.0000, 0.0242],
        [0.0033, 0.0000, 0.0000,  ..., 0.0485, 0.0000, 0.0242],
        [0.0033, 0.0000, 0.0000,  ..., 0.0485, 0.0000, 0.0242]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(485799.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1456.5366, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(134.7030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10844.6494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-505.4447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2747.4243, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.5086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1636],
        [ 0.0465],
        [-0.1898],
        ...,
        [-1.2653],
        [-1.2625],
        [-1.2622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231435.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0142],
        [1.0151],
        [1.0137],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368187.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0143],
        [1.0152],
        [1.0138],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368197.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1831.0560, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.7114, device='cuda:0')



h[100].sum tensor(37.3299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.6298, device='cuda:0')



h[200].sum tensor(-2.8735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0743, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0076, 0.0017],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0076, 0.0017],
        ...,
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0077, 0.0017],
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0077, 0.0017],
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0077, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47092.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0000, 0.0000,  ..., 0.0478, 0.0000, 0.0425],
        [0.0041, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0402],
        [0.0047, 0.0000, 0.0000,  ..., 0.0484, 0.0000, 0.0423],
        ...,
        [0.0030, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0248],
        [0.0030, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0248],
        [0.0030, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(478456.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1287.6340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(136.3186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10728.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-488.4854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2680.6294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.2785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2981],
        [-0.3346],
        [-0.3017],
        ...,
        [-1.2680],
        [-1.2651],
        [-1.2645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-213829.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0143],
        [1.0152],
        [1.0138],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368197.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0153],
        [1.0139],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368207.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2228.1504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.4267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.2015, device='cuda:0')



h[100].sum tensor(55.7514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.7271, device='cuda:0')



h[200].sum tensor(-4.6658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0075, 0.0019],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0075, 0.0019],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0075, 0.0019],
        ...,
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0076, 0.0019],
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0076, 0.0019],
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0076, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61928.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0465, 0.0000, 0.0251],
        [0.0025, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0252],
        [0.0025, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0253],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0256],
        [0.0026, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0256],
        [0.0026, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0256]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555037.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1752.9073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(212.7514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11961.9805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-672.0833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2551.7620, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-460.2073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2725],
        [-1.3869],
        [-1.4600],
        ...,
        [-1.2634],
        [-1.2610],
        [-1.2609]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201538.0781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0153],
        [1.0139],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368207.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0144],
        [1.0153],
        [1.0139],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368207.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0044,  0.0112,  0.0136,  ..., -0.0003,  0.0071,  0.0031],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1824.9445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.9553, device='cuda:0')



h[100].sum tensor(36.4222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.1182, device='cuda:0')



h[200].sum tensor(-2.7144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.7338, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0243, 0.0332,  ..., 0.0000, 0.0187, 0.0075],
        [0.0000, 0.0124, 0.0210,  ..., 0.0000, 0.0129, 0.0046],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0075, 0.0019],
        ...,
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0076, 0.0019],
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0076, 0.0019],
        [0.0000, 0.0013, 0.0097,  ..., 0.0000, 0.0076, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45531.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0197, 0.0000, 0.0000,  ..., 0.0581, 0.0000, 0.1298],
        [0.0116, 0.0000, 0.0000,  ..., 0.0528, 0.0000, 0.0803],
        [0.0051, 0.0000, 0.0000,  ..., 0.0484, 0.0000, 0.0410],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0256],
        [0.0026, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0256],
        [0.0026, 0.0000, 0.0000,  ..., 0.0471, 0.0000, 0.0256]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(469473.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1095.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(137.9391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10563.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-468.4973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2641.2393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.2630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0646],
        [-0.4503],
        [-0.8502],
        ...,
        [-1.2652],
        [-1.2622],
        [-1.2616]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-206531.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0144],
        [1.0153],
        [1.0139],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368207.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0154],
        [1.0140],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368217.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.3281e-04,  2.3689e-03,  ..., -7.2022e-04,
          1.8265e-03,  4.9654e-04],
        [-8.0654e-03,  2.0228e-02,  2.2802e-02,  ...,  5.4637e-05,
          1.1502e-02,  5.3601e-03],
        [-6.9929e-03,  1.7582e-02,  2.0085e-02,  ..., -4.8402e-05,
          1.0215e-02,  4.7134e-03],
        ...,
        [ 0.0000e+00,  3.3281e-04,  2.3689e-03,  ..., -7.2022e-04,
          1.8265e-03,  4.9654e-04],
        [ 0.0000e+00,  3.3281e-04,  2.3689e-03,  ..., -7.2022e-04,
          1.8265e-03,  4.9654e-04],
        [ 0.0000e+00,  3.3281e-04,  2.3689e-03,  ..., -7.2022e-04,
          1.8265e-03,  4.9654e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1918.4386, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9192, device='cuda:0')



h[100].sum tensor(40.0638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.4469, device='cuda:0')



h[200].sum tensor(-3.0536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 2.1451e-02, 3.0229e-02,  ..., 5.5216e-05, 1.7163e-02,
         6.9229e-03],
        [0.0000e+00, 3.5373e-02, 4.4561e-02,  ..., 0.0000e+00, 2.3962e-02,
         1.0333e-02],
        [0.0000e+00, 1.0801e-01, 1.1917e-01,  ..., 1.3166e-03, 5.9290e-02,
         2.8091e-02],
        ...,
        [0.0000e+00, 1.3652e-03, 9.7174e-03,  ..., 0.0000e+00, 7.4926e-03,
         2.0368e-03],
        [0.0000e+00, 1.3654e-03, 9.7191e-03,  ..., 0.0000e+00, 7.4939e-03,
         2.0372e-03],
        [0.0000e+00, 1.3655e-03, 9.7193e-03,  ..., 0.0000e+00, 7.4940e-03,
         2.0372e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47287.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0181, 0.0000, 0.0000,  ..., 0.0567, 0.0000, 0.1098],
        [0.0319, 0.0000, 0.0000,  ..., 0.0665, 0.0000, 0.1812],
        [0.0593, 0.0000, 0.0000,  ..., 0.0855, 0.0000, 0.3162],
        ...,
        [0.0096, 0.0000, 0.0000,  ..., 0.0515, 0.0000, 0.0666],
        [0.0079, 0.0000, 0.0000,  ..., 0.0504, 0.0000, 0.0566],
        [0.0042, 0.0000, 0.0000,  ..., 0.0479, 0.0000, 0.0363]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(473793.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1116.6090, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(152.6723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10591.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-490.9685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2606.4888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.2145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0315],
        [ 0.1636],
        [ 0.2563],
        ...,
        [-0.5610],
        [-0.7143],
        [-0.9391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199900.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0154],
        [1.0140],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368217.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(864.6959, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0145],
        [1.0155],
        [1.0141],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368227.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1726.3837, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.3743, device='cuda:0')



h[100].sum tensor(30.2550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.6955, device='cuda:0')



h[200].sum tensor(-2.0404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.1214, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0073, 0.0021],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0073, 0.0021],
        [0.0000, 0.0013, 0.0096,  ..., 0.0000, 0.0073, 0.0021],
        ...,
        [0.0000, 0.0014, 0.0097,  ..., 0.0000, 0.0074, 0.0022],
        [0.0000, 0.0014, 0.0097,  ..., 0.0000, 0.0074, 0.0022],
        [0.0000, 0.0014, 0.0097,  ..., 0.0000, 0.0074, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41916.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0325],
        [0.0036, 0.0000, 0.0000,  ..., 0.0467, 0.0000, 0.0336],
        [0.0054, 0.0000, 0.0000,  ..., 0.0480, 0.0000, 0.0423],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.0267],
        [0.0024, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.0267],
        [0.0024, 0.0000, 0.0000,  ..., 0.0463, 0.0000, 0.0267]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(455514.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(926.0775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(133.0588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10245.6602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-424.6725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2666.9634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.8831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4276],
        [-0.4645],
        [-0.3887],
        ...,
        [-1.2686],
        [-1.2657],
        [-1.2650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-195650.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0145],
        [1.0155],
        [1.0141],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368227.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0146],
        [1.0156],
        [1.0142],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368236.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [-0.0038,  0.0096,  0.0119,  ..., -0.0004,  0.0063,  0.0028],
        [-0.0039,  0.0099,  0.0122,  ..., -0.0003,  0.0064,  0.0029],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1977.3010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.8669, device='cuda:0')



h[100].sum tensor(42.2984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.0881, device='cuda:0')



h[200].sum tensor(-3.1853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0107, 0.0193,  ..., 0.0000, 0.0118, 0.0045],
        [0.0000, 0.0327, 0.0420,  ..., 0.0001, 0.0226, 0.0099],
        [0.0000, 0.0763, 0.0867,  ..., 0.0006, 0.0438, 0.0205],
        ...,
        [0.0000, 0.0013, 0.0098,  ..., 0.0000, 0.0074, 0.0022],
        [0.0000, 0.0013, 0.0098,  ..., 0.0000, 0.0074, 0.0022],
        [0.0000, 0.0013, 0.0098,  ..., 0.0000, 0.0074, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50749.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0130, 0.0000, 0.0000,  ..., 0.0524, 0.0000, 0.0960],
        [0.0259, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.1661],
        [0.0397, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.2409],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.0270],
        [0.0023, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.0271],
        [0.0023, 0.0000, 0.0000,  ..., 0.0464, 0.0000, 0.0271]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(493791.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1229.3990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(175.6113, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10871.1631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-536.4468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2600.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.1100, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0520],
        [ 0.2002],
        [ 0.2932],
        ...,
        [-1.2782],
        [-1.2753],
        [-1.2746]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-192315.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0146],
        [1.0156],
        [1.0142],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368236.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0147],
        [1.0157],
        [1.0143],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368246.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0024,  ..., -0.0007,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2006.7289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7321, device='cuda:0')



h[100].sum tensor(44.2438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.6735, device='cuda:0')



h[200].sum tensor(-3.2970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0073, 0.0022],
        [0.0000, 0.0012, 0.0097,  ..., 0.0000, 0.0073, 0.0022],
        [0.0000, 0.0012, 0.0098,  ..., 0.0000, 0.0073, 0.0022],
        ...,
        [0.0000, 0.0012, 0.0098,  ..., 0.0000, 0.0074, 0.0022],
        [0.0000, 0.0012, 0.0098,  ..., 0.0000, 0.0074, 0.0022],
        [0.0000, 0.0012, 0.0098,  ..., 0.0000, 0.0074, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52329.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0292],
        [0.0022, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0270],
        [0.0022, 0.0000, 0.0000,  ..., 0.0461, 0.0000, 0.0271],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0466, 0.0000, 0.0274],
        [0.0022, 0.0000, 0.0000,  ..., 0.0466, 0.0000, 0.0274],
        [0.0022, 0.0000, 0.0000,  ..., 0.0466, 0.0000, 0.0274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(510169.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1332.3713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(183.7843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11114.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-558.8436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2665.7795, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.3221, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0103],
        [-1.2375],
        [-1.3841],
        ...,
        [-1.2908],
        [-1.2879],
        [-1.2872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-203802.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0147],
        [1.0157],
        [1.0143],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368246.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0148],
        [1.0158],
        [1.0144],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368255.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.6532e-04,  2.4166e-03,  ..., -7.0360e-04,
          1.8026e-03,  5.2980e-04],
        [-7.4274e-03,  1.8695e-02,  2.1344e-02,  ...,  1.4684e-05,
          1.0768e-02,  5.0362e-03],
        [ 0.0000e+00,  2.6532e-04,  2.4166e-03,  ..., -7.0360e-04,
          1.8026e-03,  5.2980e-04],
        ...,
        [ 0.0000e+00,  2.6532e-04,  2.4166e-03,  ..., -7.0360e-04,
          1.8026e-03,  5.2980e-04],
        [ 0.0000e+00,  2.6532e-04,  2.4166e-03,  ..., -7.0360e-04,
          1.8026e-03,  5.2980e-04],
        [ 0.0000e+00,  2.6532e-04,  2.4166e-03,  ..., -7.0360e-04,
          1.8026e-03,  5.2980e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1932.5822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.1075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.6563, device='cuda:0')



h[100].sum tensor(41.4716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.2691, device='cuda:0')



h[200].sum tensor(-2.9440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.9703e-02, 2.8908e-02,  ..., 1.4843e-05, 1.6355e-02,
         6.6983e-03],
        [0.0000e+00, 3.1030e-02, 4.0577e-02,  ..., 4.5253e-04, 2.1894e-02,
         9.4758e-03],
        [0.0000e+00, 1.4356e-01, 1.5615e-01,  ..., 2.6929e-03, 7.6640e-02,
         3.6991e-02],
        ...,
        [0.0000e+00, 1.0889e-03, 9.9183e-03,  ..., 0.0000e+00, 7.3985e-03,
         2.1744e-03],
        [0.0000e+00, 1.0891e-03, 9.9200e-03,  ..., 0.0000e+00, 7.3998e-03,
         2.1748e-03],
        [0.0000e+00, 1.0891e-03, 9.9200e-03,  ..., 0.0000e+00, 7.3998e-03,
         2.1748e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48248.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0394, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.2234],
        [0.0608, 0.0000, 0.0000,  ..., 0.0872, 0.0000, 0.3265],
        [0.0982, 0.0000, 0.0000,  ..., 0.1133, 0.0000, 0.5105],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0277],
        [0.0022, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0277],
        [0.0022, 0.0000, 0.0000,  ..., 0.0470, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(482373.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1054.4572, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(165.1637, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10677.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-509.9587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2685.7410, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.7624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2760],
        [ 0.2703],
        [ 0.2671],
        ...,
        [-1.3062],
        [-1.3032],
        [-1.3025]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202272.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0148],
        [1.0158],
        [1.0144],
        ...,
        [1.0011],
        [1.0004],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368255.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0149],
        [1.0159],
        [1.0145],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368265., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.5415e-02,  8.8247e-02,  9.2815e-02,  ...,  2.7347e-03,
          4.4628e-02,  2.2038e-02],
        [-1.4049e-02,  3.5153e-02,  3.8287e-02,  ...,  6.6396e-04,
          1.8794e-02,  9.0578e-03],
        [-7.3712e-03,  1.8558e-02,  2.1245e-02,  ...,  1.6766e-05,
          1.0720e-02,  5.0008e-03],
        ...,
        [ 0.0000e+00,  2.4070e-04,  2.4333e-03,  ..., -6.9763e-04,
          1.8077e-03,  5.2257e-04],
        [ 0.0000e+00,  2.4070e-04,  2.4333e-03,  ..., -6.9763e-04,
          1.8077e-03,  5.2257e-04],
        [ 0.0000e+00,  2.4070e-04,  2.4333e-03,  ..., -6.9763e-04,
          1.8077e-03,  5.2257e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1987.6324, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9735, device='cuda:0')



h[100].sum tensor(44.6995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1603, device='cuda:0')



h[200].sum tensor(-3.1888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0928, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1509, 0.1638,  ..., 0.0030, 0.0803, 0.0388],
        [0.0000, 0.1927, 0.2068,  ..., 0.0046, 0.1006, 0.0490],
        [0.0000, 0.1124, 0.1243,  ..., 0.0016, 0.0616, 0.0294],
        ...,
        [0.0000, 0.0010, 0.0100,  ..., 0.0000, 0.0074, 0.0021],
        [0.0000, 0.0010, 0.0100,  ..., 0.0000, 0.0074, 0.0021],
        [0.0000, 0.0010, 0.0100,  ..., 0.0000, 0.0074, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51717.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1308, 0.0000, 0.0000,  ..., 0.1373, 0.0000, 0.6683],
        [0.1332, 0.0000, 0.0000,  ..., 0.1390, 0.0000, 0.6820],
        [0.0977, 0.0000, 0.0000,  ..., 0.1136, 0.0000, 0.5204],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0279],
        [0.0021, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0279],
        [0.0021, 0.0000, 0.0000,  ..., 0.0474, 0.0000, 0.0279]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(503561.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1171.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(180.9890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11034.9170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.8035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2656.0342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.0458, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2937],
        [ 0.2990],
        [ 0.3101],
        ...,
        [-1.3213],
        [-1.3183],
        [-1.3176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197743.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0149],
        [1.0159],
        [1.0145],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368265., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0150],
        [1.0160],
        [1.0146],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368274.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.9896e-02,  4.9736e-02,  5.3295e-02,  ...,  1.2461e-03,
          2.5909e-02,  1.2620e-02],
        [-1.4291e-02,  3.5789e-02,  3.8971e-02,  ...,  7.0124e-04,
          1.9122e-02,  9.2104e-03],
        [ 0.0000e+00,  2.2276e-04,  2.4442e-03,  ..., -6.8818e-04,
          1.8143e-03,  5.1616e-04],
        ...,
        [ 0.0000e+00,  2.2276e-04,  2.4442e-03,  ..., -6.8818e-04,
          1.8143e-03,  5.1616e-04],
        [ 0.0000e+00,  2.2276e-04,  2.4442e-03,  ..., -6.8818e-04,
          1.8143e-03,  5.1616e-04],
        [-7.4122e-03,  1.8669e-02,  2.1389e-02,  ...,  3.2442e-05,
          1.0791e-02,  5.0255e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2316.9678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-20.9404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.7766, device='cuda:0')



h[100].sum tensor(60.5456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.1161, device='cuda:0')



h[200].sum tensor(-4.6716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.9037e-01, 2.0448e-01,  ..., 4.6177e-03, 9.9544e-02,
         4.8406e-02],
        [0.0000e+00, 9.2924e-02, 1.0443e-01,  ..., 1.6554e-03, 5.2150e-02,
         2.4591e-02],
        [0.0000e+00, 5.0940e-02, 6.1323e-02,  ..., 7.1244e-04, 3.1725e-02,
         1.4330e-02],
        ...,
        [0.0000e+00, 9.1453e-04, 1.0034e-02,  ..., 0.0000e+00, 7.4485e-03,
         2.1190e-03],
        [0.0000e+00, 1.9847e-02, 2.9480e-02,  ..., 3.3296e-05, 1.6663e-02,
         6.7474e-03],
        [0.0000e+00, 3.5132e-02, 4.5177e-02,  ..., 2.6231e-05, 2.4101e-02,
         1.0484e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64947.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1340, 0.0000, 0.0000,  ..., 0.1410, 0.0000, 0.6793],
        [0.0817, 0.0000, 0.0000,  ..., 0.1038, 0.0000, 0.4286],
        [0.0430, 0.0000, 0.0000,  ..., 0.0762, 0.0000, 0.2387],
        ...,
        [0.0075, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0592],
        [0.0174, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.1117],
        [0.0304, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.1804]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(577584.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1670.8571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(241.7159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12240.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-720.4910, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2579.1782, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-475.7947, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2892],
        [ 0.2003],
        [-0.0587],
        ...,
        [-0.5400],
        [-0.2530],
        [ 0.0410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-190899.5781, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0150],
        [1.0160],
        [1.0146],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368274.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0151],
        [1.0161],
        [1.0147],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368283.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0007,  0.0018,  0.0005],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0007,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2175.1497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.9108, device='cuda:0')



h[100].sum tensor(54.0301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.5007, device='cuda:0')



h[200].sum tensor(-3.9862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3159, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0099,  ..., 0.0000, 0.0073, 0.0021],
        [0.0000, 0.0009, 0.0100,  ..., 0.0000, 0.0074, 0.0021],
        [0.0000, 0.0009, 0.0100,  ..., 0.0000, 0.0074, 0.0021],
        ...,
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0021],
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0021],
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55309.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0064, 0.0000, 0.0000,  ..., 0.0503, 0.0000, 0.0590],
        [0.0065, 0.0000, 0.0000,  ..., 0.0506, 0.0000, 0.0594],
        [0.0065, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0590],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0483, 0.0000, 0.0284],
        [0.0020, 0.0000, 0.0000,  ..., 0.0483, 0.0000, 0.0284],
        [0.0020, 0.0000, 0.0000,  ..., 0.0483, 0.0000, 0.0284]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511907.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1227.8455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(197.3907, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11104.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-604.1385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2816.0464, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.9915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2015],
        [-0.1316],
        [-0.0614],
        ...,
        [-1.3484],
        [-1.3453],
        [-1.3446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221113.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0151],
        [1.0161],
        [1.0147],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368283.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0152],
        [1.0162],
        [1.0148],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368293.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005],
        [-0.0098,  0.0247,  0.0276,  ...,  0.0003,  0.0137,  0.0065],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2276.5562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.8227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.6222, device='cuda:0')



h[100].sum tensor(58.5861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.6586, device='cuda:0')



h[200].sum tensor(-4.3922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.0865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0908, 0.1024,  ..., 0.0010, 0.0511, 0.0241],
        [0.0000, 0.0211, 0.0309,  ..., 0.0002, 0.0173, 0.0070],
        [0.0000, 0.0257, 0.0356,  ..., 0.0003, 0.0195, 0.0081],
        ...,
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0021],
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0021],
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62684.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0332, 0.0000, 0.0000,  ..., 0.0700, 0.0000, 0.1943],
        [0.0201, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.1256],
        [0.0163, 0.0000, 0.0000,  ..., 0.0583, 0.0000, 0.1046],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0486, 0.0000, 0.0287],
        [0.0019, 0.0000, 0.0000,  ..., 0.0486, 0.0000, 0.0287],
        [0.0019, 0.0000, 0.0000,  ..., 0.0486, 0.0000, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(563848.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1497.7163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(231.8094, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12020.6709, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-695.1008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2645.5854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-466.3126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3845],
        [-0.5271],
        [-0.7297],
        ...,
        [-1.3582],
        [-1.3549],
        [-1.3540]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-191304.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0152],
        [1.0162],
        [1.0148],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368293.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0153],
        [1.0163],
        [1.0149],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368303.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0042,  0.0107,  0.0133,  ..., -0.0002,  0.0069,  0.0031],
        [-0.0083,  0.0210,  0.0239,  ...,  0.0003,  0.0119,  0.0056],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0006,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1831.0220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.5301, device='cuda:0')



h[100].sum tensor(37.6027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.1540, device='cuda:0')



h[200].sum tensor(-2.3370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.0921, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0546, 0.0652,  ..., 0.0007, 0.0335, 0.0152],
        [0.0000, 0.0288, 0.0388,  ..., 0.0001, 0.0210, 0.0089],
        [0.0000, 0.0777, 0.0891,  ..., 0.0008, 0.0448, 0.0208],
        ...,
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0075, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0075, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0075, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44491.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0489, 0.0000, 0.0000,  ..., 0.0815, 0.0000, 0.2870],
        [0.0326, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.2017],
        [0.0317, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.1951],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0490, 0.0000, 0.0290],
        [0.0018, 0.0000, 0.0000,  ..., 0.0490, 0.0000, 0.0290],
        [0.0018, 0.0000, 0.0000,  ..., 0.0490, 0.0000, 0.0290]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(471375.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(845.2501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(149.8881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10505.7168, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-472.3734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2894.9253, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.5332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2950],
        [ 0.2719],
        [ 0.2010],
        ...,
        [-1.3689],
        [-1.3657],
        [-1.3650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237617.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0153],
        [1.0163],
        [1.0149],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368303.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0164],
        [1.0150],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368313.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2092.9043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.6653, device='cuda:0')



h[100].sum tensor(49.1858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.3049, device='cuda:0')



h[200].sum tensor(-3.4336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0009, 0.0100,  ..., 0.0000, 0.0074, 0.0020],
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0020],
        [0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0075, 0.0020],
        ...,
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55708.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0000, 0.0000,  ..., 0.0485, 0.0000, 0.0286],
        [0.0016, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0287],
        [0.0016, 0.0000, 0.0000,  ..., 0.0487, 0.0000, 0.0288],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0494, 0.0000, 0.0312],
        [0.0016, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0292],
        [0.0016, 0.0000, 0.0000,  ..., 0.0492, 0.0000, 0.0292]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(529321.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1072.8018, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(203.3549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11708.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-608.7457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2408.0793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.2954, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6126],
        [-1.6048],
        [-1.5782],
        ...,
        [-1.3291],
        [-1.3588],
        [-1.3687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-174347.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0164],
        [1.0150],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368313.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(516.9634, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0165],
        [1.0151],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368323.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0080,  0.0201,  0.0230,  ...,  0.0004,  0.0115,  0.0054],
        [-0.0081,  0.0206,  0.0234,  ...,  0.0004,  0.0117,  0.0055],
        [-0.0108,  0.0272,  0.0303,  ...,  0.0007,  0.0150,  0.0071],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2020.8824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9798, device='cuda:0')



h[100].sum tensor(45.4811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1645, device='cuda:0')



h[200].sum tensor(-3.0634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0921, 0.1040,  ..., 0.0019, 0.0518, 0.0243],
        [0.0000, 0.0878, 0.0996,  ..., 0.0017, 0.0497, 0.0233],
        [0.0000, 0.0939, 0.1059,  ..., 0.0024, 0.0527, 0.0248],
        ...,
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50162.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0534, 0.0000, 0.0000,  ..., 0.0863, 0.0000, 0.3027],
        [0.0634, 0.0000, 0.0000,  ..., 0.0937, 0.0000, 0.3533],
        [0.0692, 0.0000, 0.0000,  ..., 0.0980, 0.0000, 0.3767],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0293],
        [0.0017, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0293],
        [0.0017, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0293]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496297.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1026.8743, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(177.0650, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11026.7998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-544.0679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2754.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.5020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2892],
        [ 0.3073],
        [ 0.3056],
        ...,
        [-1.3882],
        [-1.3850],
        [-1.3842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238299.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0165],
        [1.0151],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368323.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0165],
        [1.0151],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368323.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0131,  0.0330,  0.0362,  ...,  0.0009,  0.0178,  0.0085],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        ...,
        [-0.0056,  0.0141,  0.0168,  ...,  0.0001,  0.0086,  0.0039],
        [-0.0123,  0.0310,  0.0341,  ...,  0.0008,  0.0168,  0.0080],
        [-0.0087,  0.0220,  0.0249,  ...,  0.0004,  0.0124,  0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2098.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.5236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.7847, device='cuda:0')



h[100].sum tensor(49.0627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.3857, device='cuda:0')



h[200].sum tensor(-3.4045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9083, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1235, 0.1363,  ..., 0.0037, 0.0670, 0.0320],
        [0.0000, 0.0804, 0.0920,  ..., 0.0019, 0.0461, 0.0215],
        [0.0000, 0.0368, 0.0471,  ..., 0.0005, 0.0249, 0.0108],
        ...,
        [0.0000, 0.0588, 0.0698,  ..., 0.0014, 0.0357, 0.0162],
        [0.0000, 0.1027, 0.1150,  ..., 0.0023, 0.0570, 0.0269],
        [0.0000, 0.1206, 0.1334,  ..., 0.0030, 0.0657, 0.0313]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52661.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1174, 0.0000, 0.0000,  ..., 0.1331, 0.0000, 0.6009],
        [0.0882, 0.0000, 0.0000,  ..., 0.1119, 0.0000, 0.4664],
        [0.0707, 0.0000, 0.0000,  ..., 0.0992, 0.0000, 0.3832],
        ...,
        [0.0424, 0.0000, 0.0000,  ..., 0.0789, 0.0000, 0.2473],
        [0.0592, 0.0000, 0.0000,  ..., 0.0910, 0.0000, 0.3360],
        [0.0616, 0.0000, 0.0000,  ..., 0.0929, 0.0000, 0.3441]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(507656.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1071.2767, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(188.5078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11205.7490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-574.6924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2777.3804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-420.7682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0092],
        [0.0230],
        [0.0359],
        ...,
        [0.0491],
        [0.2546],
        [0.2397]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242682., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0165],
        [1.0151],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368323.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0154],
        [1.0165],
        [1.0151],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368323.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0175,  0.0440,  0.0475,  ...,  0.0014,  0.0231,  0.0112],
        [-0.0189,  0.0475,  0.0511,  ...,  0.0015,  0.0248,  0.0121],
        [-0.0173,  0.0436,  0.0472,  ...,  0.0013,  0.0229,  0.0111],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1966.3245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.5188, device='cuda:0')



h[100].sum tensor(42.9660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1761, device='cuda:0')



h[200].sum tensor(-2.8239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4378, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1573, 0.1710,  ..., 0.0046, 0.0834, 0.0403],
        [0.0000, 0.1620, 0.1760,  ..., 0.0048, 0.0858, 0.0415],
        [0.0000, 0.1496, 0.1632,  ..., 0.0043, 0.0798, 0.0384],
        ...,
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48689.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1039, 0.0000, 0.0000,  ..., 0.1232, 0.0000, 0.5366],
        [0.1007, 0.0000, 0.0000,  ..., 0.1212, 0.0000, 0.5189],
        [0.0886, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.4608],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0293],
        [0.0017, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0293],
        [0.0017, 0.0000, 0.0000,  ..., 0.0497, 0.0000, 0.0293]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(490539.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(889.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(170.4628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10983.8643, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-524.4070, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2676.5659, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.4810, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2400],
        [ 0.2332],
        [ 0.1882],
        ...,
        [-1.3882],
        [-1.3850],
        [-1.3842]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217521.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0154],
        [1.0165],
        [1.0151],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368323.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0155],
        [1.0165],
        [1.0152],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368333.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-4.5050e-03,  1.1510e-02,  1.4137e-02,  ...,  4.6772e-05,
          7.3396e-03,  3.2379e-03],
        [-7.0384e-03,  1.7864e-02,  2.0681e-02,  ...,  3.1755e-04,
          1.0426e-02,  4.7969e-03],
        [-1.4928e-02,  3.7651e-02,  4.1059e-02,  ...,  1.1608e-03,
          2.0039e-02,  9.6519e-03],
        ...,
        [ 0.0000e+00,  2.1091e-04,  2.5003e-03,  ..., -4.3475e-04,
          1.8503e-03,  4.6559e-04],
        [ 0.0000e+00,  2.1091e-04,  2.5003e-03,  ..., -4.3475e-04,
          1.8503e-03,  4.6559e-04],
        [ 0.0000e+00,  2.1091e-04,  2.5003e-03,  ..., -4.3475e-04,
          1.8503e-03,  4.6559e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2043.9683, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.2782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.3394, device='cuda:0')



h[100].sum tensor(46.4185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.4078, device='cuda:0')



h[200].sum tensor(-3.1206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2576, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0420, 0.0525,  ..., 0.0006, 0.0275, 0.0120],
        [0.0000, 0.0954, 0.1075,  ..., 0.0023, 0.0535, 0.0251],
        [0.0000, 0.1206, 0.1335,  ..., 0.0033, 0.0657, 0.0313],
        ...,
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0076, 0.0019],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0076, 0.0019],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0076, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51661.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0364, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.2336],
        [0.0640, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.3669],
        [0.0838, 0.0000, 0.0000,  ..., 0.1095, 0.0000, 0.4593],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0502, 0.0000, 0.0294],
        [0.0017, 0.0000, 0.0000,  ..., 0.0502, 0.0000, 0.0294],
        [0.0017, 0.0000, 0.0000,  ..., 0.0502, 0.0000, 0.0294]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(505855.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(994.2548, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(182.7720, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11287.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.3118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2664.4070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-421.6083, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2369],
        [ 0.2602],
        [ 0.2516],
        ...,
        [-1.4019],
        [-1.3985],
        [-1.3975]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226036.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0155],
        [1.0165],
        [1.0152],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368333.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0166],
        [1.0153],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368342.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7759e-03,  1.7227e-02,  2.0038e-02,  ...,  3.4521e-04,
          1.0129e-02,  4.6277e-03],
        [-1.2627e-02,  3.1920e-02,  3.5177e-02,  ...,  9.7991e-04,
          1.7267e-02,  8.2346e-03],
        [-3.1461e-03,  8.1107e-03,  1.0646e-02,  ..., -4.8548e-05,
          5.7012e-03,  2.3900e-03],
        ...,
        [ 0.0000e+00,  2.0983e-04,  2.5053e-03,  ..., -3.8983e-04,
          1.8633e-03,  4.5059e-04],
        [ 0.0000e+00,  2.0983e-04,  2.5053e-03,  ..., -3.8983e-04,
          1.8633e-03,  4.5059e-04],
        [ 0.0000e+00,  2.0983e-04,  2.5053e-03,  ..., -3.8983e-04,
          1.8633e-03,  4.5059e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1985.7073, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.5070, device='cuda:0')



h[100].sum tensor(43.3121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1681, device='cuda:0')



h[200].sum tensor(-2.8124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1318, 0.1450,  ..., 0.0041, 0.0711, 0.0340],
        [0.0000, 0.0724, 0.0839,  ..., 0.0016, 0.0423, 0.0194],
        [0.0000, 0.0680, 0.0794,  ..., 0.0014, 0.0402, 0.0183],
        ...,
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0019],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0019],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49232.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0687, 0.0000, 0.0000,  ..., 0.0989, 0.0000, 0.3882],
        [0.0547, 0.0000, 0.0000,  ..., 0.0884, 0.0000, 0.3296],
        [0.0438, 0.0000, 0.0000,  ..., 0.0802, 0.0000, 0.2813],
        ...,
        [0.0016, 0.0000, 0.0000,  ..., 0.0505, 0.0000, 0.0295],
        [0.0016, 0.0000, 0.0000,  ..., 0.0505, 0.0000, 0.0296],
        [0.0016, 0.0000, 0.0000,  ..., 0.0505, 0.0000, 0.0296]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(497019.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(937.9697, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(171.5382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11171.1182, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-534.1187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2724.0205, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.6879, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2766],
        [ 0.2786],
        [ 0.2792],
        ...,
        [-1.4130],
        [-1.4096],
        [-1.4088]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237683.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0166],
        [1.0153],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368342.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0156],
        [1.0167],
        [1.0154],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368352.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0166,  0.0421,  0.0456,  ...,  0.0015,  0.0222,  0.0107],
        [-0.0173,  0.0436,  0.0472,  ...,  0.0016,  0.0229,  0.0111],
        [-0.0078,  0.0197,  0.0226,  ...,  0.0005,  0.0114,  0.0052],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2057.7256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.0338, device='cuda:0')



h[100].sum tensor(45.3804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.2011, device='cuda:0')



h[200].sum tensor(-3.0236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1308, 0.1440,  ..., 0.0043, 0.0707, 0.0337],
        [0.0000, 0.1317, 0.1449,  ..., 0.0043, 0.0711, 0.0339],
        [0.0000, 0.1258, 0.1389,  ..., 0.0041, 0.0683, 0.0325],
        ...,
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0018],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0018],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51568.8633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0741, 0.0000, 0.0000,  ..., 0.1039, 0.0000, 0.4010],
        [0.0746, 0.0000, 0.0000,  ..., 0.1044, 0.0000, 0.4048],
        [0.0644, 0.0000, 0.0000,  ..., 0.0967, 0.0000, 0.3568],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0504, 0.0000, 0.0298],
        [0.0015, 0.0000, 0.0000,  ..., 0.0504, 0.0000, 0.0298],
        [0.0015, 0.0000, 0.0000,  ..., 0.0504, 0.0000, 0.0298]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(508253.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(919.0316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(185.3021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11529.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-563.0941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2486.2983, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.7209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2867],
        [ 0.3067],
        [ 0.3066],
        ...,
        [-1.4161],
        [-1.4099],
        [-1.3967]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-198749.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0156],
        [1.0167],
        [1.0154],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368352.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0157],
        [1.0168],
        [1.0155],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368362.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0038,  0.0098,  0.0124,  ...,  0.0001,  0.0065,  0.0028],
        [-0.0036,  0.0092,  0.0117,  ...,  0.0001,  0.0062,  0.0026],
        [-0.0074,  0.0188,  0.0216,  ...,  0.0005,  0.0109,  0.0050],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1875.6201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9727, device='cuda:0')



h[100].sum tensor(35.8950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7769, device='cuda:0')



h[200].sum tensor(-2.1627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8411, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0319, 0.0421,  ..., 0.0008, 0.0227, 0.0094],
        [0.0000, 0.0662, 0.0774,  ..., 0.0017, 0.0393, 0.0178],
        [0.0000, 0.0321, 0.0423,  ..., 0.0005, 0.0228, 0.0095],
        ...,
        [0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0077, 0.0018],
        [0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0077, 0.0018],
        [0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0077, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44527.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0220, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.1581],
        [0.0312, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.2106],
        [0.0238, 0.0000, 0.0000,  ..., 0.0662, 0.0000, 0.1704],
        ...,
        [0.0016, 0.0000, 0.0000,  ..., 0.0506, 0.0000, 0.0299],
        [0.0016, 0.0000, 0.0000,  ..., 0.0506, 0.0000, 0.0299],
        [0.0016, 0.0000, 0.0000,  ..., 0.0506, 0.0000, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(476414.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(794.8363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(154.1691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(10909.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-477.8307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2783.8064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.6488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1701],
        [ 0.2401],
        [ 0.1448],
        ...,
        [-1.4262],
        [-1.4228],
        [-1.4219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248942.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0157],
        [1.0168],
        [1.0155],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368362.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0158],
        [1.0168],
        [1.0157],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368372.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2183.0593, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.3079, device='cuda:0')



h[100].sum tensor(48.8401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.7396, device='cuda:0')



h[200].sum tensor(-3.3897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1439, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0009, 0.0101,  ..., 0.0000, 0.0076, 0.0018],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0076, 0.0018],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0077, 0.0018],
        ...,
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0018],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0018],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0077, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54882.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0507, 0.0000, 0.0370],
        [0.0027, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0393],
        [0.0029, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.0413],
        ...,
        [0.0044, 0.0000, 0.0000,  ..., 0.0529, 0.0000, 0.0464],
        [0.0022, 0.0000, 0.0000,  ..., 0.0513, 0.0000, 0.0338],
        [0.0016, 0.0000, 0.0000,  ..., 0.0508, 0.0000, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(525018.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1056.3134, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(202.3371, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11884.0586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-605.5002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2560.3362, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.6616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0231],
        [-0.9096],
        [-0.8039],
        ...,
        [-1.1965],
        [-1.3393],
        [-1.4039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-212951.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0158],
        [1.0168],
        [1.0157],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368372.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0159],
        [1.0169],
        [1.0158],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368382.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0076,  0.0195,  0.0224,  ...,  0.0007,  0.0113,  0.0052],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1971.7659, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8007, device='cuda:0')



h[100].sum tensor(38.2527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.0137, device='cuda:0')



h[200].sum tensor(-2.4274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0312, 0.0414,  ..., 0.0010, 0.0224, 0.0093],
        [0.0000, 0.0205, 0.0304,  ..., 0.0007, 0.0172, 0.0066],
        [0.0000, 0.0009, 0.0102,  ..., 0.0000, 0.0077, 0.0018],
        ...,
        [0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0078, 0.0018],
        [0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0078, 0.0018],
        [0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0078, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47960.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0397, 0.0000, 0.0000,  ..., 0.0793, 0.0000, 0.2307],
        [0.0292, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.1730],
        [0.0202, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.1244],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0302],
        [0.0015, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0302],
        [0.0015, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(496413.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(920.3294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(172.3154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11334.0283, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-521.6390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2792.3418, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.8420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0826],
        [ 0.0732],
        [ 0.0695],
        ...,
        [-1.4464],
        [-1.4430],
        [-1.4422]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250956.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0159],
        [1.0169],
        [1.0158],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368382.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0160],
        [1.0170],
        [1.0159],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368392.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0044,  0.0114,  0.0140,  ...,  0.0004,  0.0073,  0.0032],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2122.0264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1194, device='cuda:0')



h[100].sum tensor(43.9846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.2590, device='cuda:0')



h[200].sum tensor(-2.9816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1585, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0190, 0.0287,  ..., 0.0006, 0.0165, 0.0063],
        [0.0000, 0.0123, 0.0218,  ..., 0.0004, 0.0132, 0.0046],
        [0.0000, 0.0010, 0.0101,  ..., 0.0000, 0.0077, 0.0018],
        ...,
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0078, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54698.1367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0116, 0.0000, 0.0000,  ..., 0.0578, 0.0000, 0.1004],
        [0.0083, 0.0000, 0.0000,  ..., 0.0556, 0.0000, 0.0761],
        [0.0045, 0.0000, 0.0000,  ..., 0.0528, 0.0000, 0.0530],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0304],
        [0.0015, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0304],
        [0.0015, 0.0000, 0.0000,  ..., 0.0511, 0.0000, 0.0304]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541185., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1129.0413, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.2135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12226.0908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-605.1901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2611.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.8780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8304],
        [-0.8650],
        [-0.8112],
        ...,
        [-1.4545],
        [-1.4511],
        [-1.4502]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-215213.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0160],
        [1.0170],
        [1.0159],
        ...,
        [1.0011],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368392.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(402.1062, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0161],
        [1.0171],
        [1.0160],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368401.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.2417e-03,  1.8580e-02,  2.1407e-02,  ...,  7.9179e-04,
          1.0812e-02,  4.9675e-03],
        [-1.2575e-02,  3.2084e-02,  3.5341e-02,  ...,  1.4170e-03,
          1.7369e-02,  8.2921e-03],
        [-4.0616e-03,  1.0527e-02,  1.3099e-02,  ...,  4.1898e-04,
          6.9021e-03,  2.9851e-03],
        ...,
        [ 0.0000e+00,  2.4271e-04,  2.4877e-03,  ..., -5.7178e-05,
          1.9085e-03,  4.5320e-04],
        [ 0.0000e+00,  2.4271e-04,  2.4877e-03,  ..., -5.7178e-05,
          1.9085e-03,  4.5320e-04],
        [ 0.0000e+00,  2.4271e-04,  2.4877e-03,  ..., -5.7178e-05,
          1.9085e-03,  4.5320e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2136.9001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9604, device='cuda:0')



h[100].sum tensor(43.5731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1514, device='cuda:0')



h[200].sum tensor(-2.9673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1476, 0.1613,  ..., 0.0066, 0.0789, 0.0379],
        [0.0000, 0.1029, 0.1152,  ..., 0.0045, 0.0572, 0.0269],
        [0.0000, 0.0562, 0.0671,  ..., 0.0024, 0.0346, 0.0154],
        ...,
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0078, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53882.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0857, 0.0000, 0.0000,  ..., 0.1169, 0.0000, 0.4698],
        [0.0704, 0.0000, 0.0000,  ..., 0.1049, 0.0000, 0.3976],
        [0.0466, 0.0000, 0.0000,  ..., 0.0861, 0.0000, 0.2747],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0307],
        [0.0015, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0307],
        [0.0015, 0.0000, 0.0000,  ..., 0.0512, 0.0000, 0.0307]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(530162.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(953.7903, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(204.0802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12158.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-595.3651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2477.6787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.4687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2629],
        [ 0.2509],
        [ 0.2049],
        ...,
        [-1.4622],
        [-1.4587],
        [-1.4579]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-188792.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0161],
        [1.0171],
        [1.0160],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368401.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0162],
        [1.0172],
        [1.0161],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368411.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005],
        [-0.0069,  0.0176,  0.0204,  ...,  0.0007,  0.0103,  0.0047],
        [-0.0134,  0.0341,  0.0374,  ...,  0.0015,  0.0183,  0.0088],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0001,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2241.0630, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.6523, device='cuda:0')



h[100].sum tensor(47.7250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.9726, device='cuda:0')



h[200].sum tensor(-3.3757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2990, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0328, 0.0431,  ..., 0.0012, 0.0232, 0.0097],
        [0.0000, 0.0634, 0.0747,  ..., 0.0025, 0.0381, 0.0173],
        [0.0000, 0.0947, 0.1070,  ..., 0.0039, 0.0533, 0.0250],
        ...,
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0009, 0.0103,  ..., 0.0000, 0.0078, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56781.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0263, 0.0000, 0.0000,  ..., 0.0708, 0.0000, 0.1686],
        [0.0489, 0.0000, 0.0000,  ..., 0.0890, 0.0000, 0.2851],
        [0.0701, 0.0000, 0.0000,  ..., 0.1059, 0.0000, 0.3927],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0536, 0.0000, 0.0440],
        [0.0017, 0.0000, 0.0000,  ..., 0.0520, 0.0000, 0.0304],
        [0.0017, 0.0000, 0.0000,  ..., 0.0520, 0.0000, 0.0304]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(543017., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1146.0922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(214.2342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12269.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-631.9568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2642.1870, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.4608, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2347],
        [ 0.2525],
        [ 0.2563],
        ...,
        [-1.1556],
        [-1.3482],
        [-1.4386]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-208899.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0162],
        [1.0172],
        [1.0161],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368411.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0163],
        [1.0173],
        [1.0162],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368420.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0039,  0.0101,  0.0127,  ...,  0.0003,  0.0067,  0.0029],
        [-0.0100,  0.0257,  0.0288,  ...,  0.0010,  0.0143,  0.0068],
        [-0.0123,  0.0313,  0.0347,  ...,  0.0013,  0.0170,  0.0081],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2300.1140, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.0967, device='cuda:0')



h[100].sum tensor(49.9438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.9499, device='cuda:0')



h[200].sum tensor(-3.6053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.9493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0532, 0.0644,  ..., 0.0019, 0.0331, 0.0148],
        [0.0000, 0.0978, 0.1105,  ..., 0.0038, 0.0548, 0.0258],
        [0.0000, 0.1426, 0.1567,  ..., 0.0059, 0.0766, 0.0369],
        ...,
        [0.0000, 0.0007, 0.0104,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0007, 0.0104,  ..., 0.0000, 0.0078, 0.0019],
        [0.0000, 0.0007, 0.0104,  ..., 0.0000, 0.0078, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58040.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0534, 0.0000, 0.0000,  ..., 0.0936, 0.0000, 0.3073],
        [0.0678, 0.0000, 0.0000,  ..., 0.1053, 0.0000, 0.3794],
        [0.0790, 0.0000, 0.0000,  ..., 0.1144, 0.0000, 0.4319],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.0300],
        [0.0019, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.0300],
        [0.0019, 0.0000, 0.0000,  ..., 0.0530, 0.0000, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546359.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1270.8552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.2559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12171.4238, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-649.0065, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2896.6187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.2578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0135],
        [ 0.0024],
        [ 0.0194],
        ...,
        [-1.5042],
        [-1.5005],
        [-1.4996]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248788.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0163],
        [1.0173],
        [1.0162],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368420.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0174],
        [1.0163],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368430.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0002,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1990.0386, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4528, device='cuda:0')



h[100].sum tensor(35.0515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.7783, device='cuda:0')



h[200].sum tensor(-2.3184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5076, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0007, 0.0102,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0007, 0.0103,  ..., 0.0000, 0.0076, 0.0020],
        [0.0000, 0.0007, 0.0103,  ..., 0.0000, 0.0077, 0.0020],
        ...,
        [0.0000, 0.0007, 0.0104,  ..., 0.0000, 0.0077, 0.0020],
        [0.0000, 0.0007, 0.0104,  ..., 0.0000, 0.0077, 0.0020],
        [0.0000, 0.0007, 0.0104,  ..., 0.0000, 0.0077, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46928.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0000, 0.0000,  ..., 0.0526, 0.0000, 0.0293],
        [0.0019, 0.0000, 0.0000,  ..., 0.0529, 0.0000, 0.0294],
        [0.0020, 0.0000, 0.0000,  ..., 0.0529, 0.0000, 0.0295],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0299],
        [0.0020, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0299],
        [0.0020, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(497346.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(876.1356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(166.5901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11226.3027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-510.8817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3084.5193, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.6434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7759],
        [-1.7846],
        [-1.7871],
        ...,
        [-1.5129],
        [-1.5106],
        [-1.5099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250668.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0174],
        [1.0163],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368430.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0164],
        [1.0174],
        [1.0164],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368440.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0003,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1992.9041, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8586, device='cuda:0')



h[100].sum tensor(33.3689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.3763, device='cuda:0')



h[200].sum tensor(-2.2422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2400, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0008, 0.0102,  ..., 0.0000, 0.0076, 0.0021],
        [0.0000, 0.0008, 0.0102,  ..., 0.0000, 0.0076, 0.0021],
        [0.0000, 0.0008, 0.0102,  ..., 0.0000, 0.0076, 0.0021],
        ...,
        [0.0000, 0.0008, 0.0103,  ..., 0.0000, 0.0077, 0.0021],
        [0.0000, 0.0008, 0.0103,  ..., 0.0000, 0.0077, 0.0021],
        [0.0000, 0.0008, 0.0103,  ..., 0.0000, 0.0077, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46248.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0294],
        [0.0017, 0.0000, 0.0000,  ..., 0.0525, 0.0000, 0.0295],
        [0.0017, 0.0000, 0.0000,  ..., 0.0525, 0.0000, 0.0296],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0531, 0.0000, 0.0300],
        [0.0018, 0.0000, 0.0000,  ..., 0.0531, 0.0000, 0.0300],
        [0.0018, 0.0000, 0.0000,  ..., 0.0531, 0.0000, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(492993.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(823.2449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(169.3864, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11138.6299, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-501.0445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3086.8936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.1794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5901],
        [-1.6754],
        [-1.7376],
        ...,
        [-1.5149],
        [-1.5113],
        [-1.5104]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253551.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0164],
        [1.0174],
        [1.0164],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368440.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0165],
        [1.0175],
        [1.0165],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368450.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [-0.0041,  0.0108,  0.0133,  ...,  0.0002,  0.0070,  0.0031],
        ...,
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0005],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2120.7104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4564, device='cuda:0')



h[100].sum tensor(37.1298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1339, device='cuda:0')



h[200].sum tensor(-2.6445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4097, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0168, 0.0264,  ..., 0.0002, 0.0153, 0.0061],
        [0.0000, 0.0117, 0.0212,  ..., 0.0002, 0.0128, 0.0049],
        [0.0000, 0.0098, 0.0192,  ..., 0.0001, 0.0119, 0.0044],
        ...,
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0077, 0.0022],
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0077, 0.0022],
        [0.0000, 0.0010, 0.0102,  ..., 0.0000, 0.0077, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50129.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0114, 0.0000, 0.0000,  ..., 0.0597, 0.0000, 0.1082],
        [0.0086, 0.0000, 0.0000,  ..., 0.0577, 0.0000, 0.0858],
        [0.0075, 0.0000, 0.0000,  ..., 0.0569, 0.0000, 0.0772],
        ...,
        [0.0014, 0.0000, 0.0000,  ..., 0.0524, 0.0000, 0.0303],
        [0.0014, 0.0000, 0.0000,  ..., 0.0524, 0.0000, 0.0303],
        [0.0014, 0.0000, 0.0000,  ..., 0.0524, 0.0000, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(511287.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(819.6962, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.2272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11575.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-546.4545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2845.8430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-414.8420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0328],
        [-0.2763],
        [-0.5471],
        ...,
        [-1.5056],
        [-1.5021],
        [-1.5012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223983.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0165],
        [1.0175],
        [1.0165],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368450.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0176],
        [1.0166],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368460.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1485e-03,  8.3453e-03,  1.0765e-02,  ...,  7.5498e-05,
          5.7913e-03,  2.5607e-03],
        [-3.1485e-03,  8.3453e-03,  1.0765e-02,  ...,  7.5498e-05,
          5.7913e-03,  2.5607e-03],
        [ 0.0000e+00,  3.0292e-04,  2.4652e-03,  ..., -3.1278e-04,
          1.8861e-03,  5.7297e-04],
        ...,
        [ 0.0000e+00,  3.0292e-04,  2.4652e-03,  ..., -3.1278e-04,
          1.8861e-03,  5.7297e-04],
        [ 0.0000e+00,  3.0292e-04,  2.4652e-03,  ..., -3.1278e-04,
          1.8861e-03,  5.7297e-04],
        [ 0.0000e+00,  3.0292e-04,  2.4652e-03,  ..., -3.1278e-04,
          1.8861e-03,  5.7297e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2352.4839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.8495, device='cuda:0')



h[100].sum tensor(45.4035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.7827, device='cuda:0')



h[200].sum tensor(-3.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.0760e-02, 4.0469e-02,  ..., 4.7551e-04, 2.1983e-02,
         9.6208e-03],
        [0.0000e+00, 2.3489e-02, 3.3002e-02,  ..., 1.2010e-04, 1.8482e-02,
         7.8324e-03],
        [0.0000e+00, 1.6112e-02, 2.5396e-02,  ..., 8.1157e-05, 1.4906e-02,
         6.0108e-03],
        ...,
        [0.0000e+00, 1.2468e-03, 1.0147e-02,  ..., 0.0000e+00, 7.7629e-03,
         2.3583e-03],
        [0.0000e+00, 1.2469e-03, 1.0147e-02,  ..., 0.0000e+00, 7.7632e-03,
         2.3584e-03],
        [0.0000e+00, 1.2469e-03, 1.0147e-02,  ..., 0.0000e+00, 7.7632e-03,
         2.3584e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59957.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0165, 0.0000, 0.0000,  ..., 0.0639, 0.0000, 0.1466],
        [0.0138, 0.0000, 0.0000,  ..., 0.0618, 0.0000, 0.1327],
        [0.0101, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.1042],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0517, 0.0000, 0.0308],
        [0.0011, 0.0000, 0.0000,  ..., 0.0517, 0.0000, 0.0308],
        [0.0011, 0.0000, 0.0000,  ..., 0.0517, 0.0000, 0.0308]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(571906.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1177.4995, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(247.9097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12705.1934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-667.5253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2608.9209, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.6362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1315],
        [ 0.0679],
        [-0.1659],
        ...,
        [-1.4948],
        [-1.4915],
        [-1.4906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-196985.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0176],
        [1.0166],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368460.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0166],
        [1.0177],
        [1.0167],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368469.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0006],
        ...,
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0003,  0.0019,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2042.5657, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.4627, device='cuda:0')



h[100].sum tensor(30.2038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.1084, device='cuda:0')



h[200].sum tensor(-2.1511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.0617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0099,  ..., 0.0000, 0.0076, 0.0024],
        [0.0000, 0.0014, 0.0100,  ..., 0.0000, 0.0077, 0.0024],
        [0.0000, 0.0014, 0.0100,  ..., 0.0000, 0.0077, 0.0024],
        ...,
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0077, 0.0025],
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0077, 0.0025],
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0077, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48075.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0000,  ..., 0.0504, 0.0000, 0.0324],
        [0.0008, 0.0000, 0.0000,  ..., 0.0508, 0.0000, 0.0311],
        [0.0009, 0.0000, 0.0000,  ..., 0.0509, 0.0000, 0.0305],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0514, 0.0000, 0.0309],
        [0.0009, 0.0000, 0.0000,  ..., 0.0514, 0.0000, 0.0309],
        [0.0009, 0.0000, 0.0000,  ..., 0.0514, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506271.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(584.0141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(198.6738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11608.1221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-516.6204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2540.6565, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.2588, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0814],
        [-1.2964],
        [-1.4536],
        ...,
        [-1.4860],
        [-1.4827],
        [-1.4818]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-177048.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0166],
        [1.0177],
        [1.0167],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368469.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0167],
        [1.0178],
        [1.0168],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368478.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0019,  0.0006],
        ...,
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0019,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0019,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2138.1812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9762, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.7304, device='cuda:0')



h[100].sum tensor(33.2467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.6427, device='cuda:0')



h[200].sum tensor(-2.4836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0099,  ..., 0.0000, 0.0075, 0.0025],
        [0.0000, 0.0014, 0.0100,  ..., 0.0000, 0.0076, 0.0025],
        [0.0000, 0.0014, 0.0100,  ..., 0.0000, 0.0076, 0.0025],
        ...,
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0076, 0.0026],
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0076, 0.0026],
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0076, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50551.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0000,  ..., 0.0506, 0.0000, 0.0316],
        [0.0009, 0.0000, 0.0000,  ..., 0.0510, 0.0000, 0.0355],
        [0.0021, 0.0000, 0.0000,  ..., 0.0520, 0.0000, 0.0464],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0307],
        [0.0009, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0307],
        [0.0009, 0.0000, 0.0000,  ..., 0.0516, 0.0000, 0.0307]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517586.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(625.1398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(210.0210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11730.1494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-545.0393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2504.4272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.4041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1384],
        [-0.8803],
        [-0.5522],
        ...,
        [-1.4961],
        [-1.4928],
        [-1.4920]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-164912.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0167],
        [1.0178],
        [1.0168],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368478.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0168],
        [1.0178],
        [1.0168],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368487.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0018,  0.0006],
        ...,
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0004,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2785.0220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-23.9904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.9796, device='cuda:0')



h[100].sum tensor(61.0292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-20.9598, device='cuda:0')



h[200].sum tensor(-4.9574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.9490, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0100,  ..., 0.0000, 0.0074, 0.0026],
        [0.0000, 0.0014, 0.0100,  ..., 0.0000, 0.0074, 0.0026],
        [0.0000, 0.0014, 0.0100,  ..., 0.0000, 0.0074, 0.0026],
        ...,
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0075, 0.0026],
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0075, 0.0026],
        [0.0000, 0.0014, 0.0101,  ..., 0.0000, 0.0075, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(76255.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0030, 0.0000, 0.0000,  ..., 0.0532, 0.0000, 0.0421],
        [0.0012, 0.0000, 0.0000,  ..., 0.0519, 0.0000, 0.0320],
        [0.0021, 0.0000, 0.0000,  ..., 0.0527, 0.0000, 0.0381],
        ...,
        [0.0009, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0302],
        [0.0009, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0302],
        [0.0009, 0.0000, 0.0000,  ..., 0.0522, 0.0000, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660549.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1625.9465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.4700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13949.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-864.1445, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2480.3623, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-505.6981, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5516],
        [-0.8743],
        [-1.0185],
        ...,
        [-1.5097],
        [-1.5064],
        [-1.5056]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-168561.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0168],
        [1.0178],
        [1.0168],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368487.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(528.0234, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0179],
        [1.0169],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368495.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0052,  0.0137,  0.0163,  ...,  0.0002,  0.0083,  0.0039],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        ...,
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2348.5928, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.1640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.0811, device='cuda:0')



h[100].sum tensor(42.2546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.2628, device='cuda:0')



h[200].sum tensor(-3.3282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4921, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0355, 0.0454,  ..., 0.0007, 0.0239, 0.0111],
        [0.0000, 0.0148, 0.0241,  ..., 0.0002, 0.0139, 0.0060],
        [0.0000, 0.0012, 0.0101,  ..., 0.0000, 0.0073, 0.0026],
        ...,
        [0.0000, 0.0012, 0.0102,  ..., 0.0000, 0.0074, 0.0026],
        [0.0000, 0.0012, 0.0102,  ..., 0.0000, 0.0074, 0.0026],
        [0.0000, 0.0012, 0.0102,  ..., 0.0000, 0.0074, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59400.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0309, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.2023],
        [0.0139, 0.0000, 0.0000,  ..., 0.0639, 0.0000, 0.1064],
        [0.0046, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0507],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0296],
        [0.0010, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0296],
        [0.0010, 0.0000, 0.0000,  ..., 0.0535, 0.0000, 0.0296]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570676.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1121.8052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.8063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12137.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-654.1863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2887.6853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.8544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0407],
        [-0.2679],
        [-0.6148],
        ...,
        [-1.5336],
        [-1.5302],
        [-1.5294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-222864.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0179],
        [1.0169],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368495.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0169],
        [1.0180],
        [1.0169],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368503.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.7300e-04,  2.5087e-03,  ..., -4.8073e-04,
          1.7880e-03,  6.3677e-04],
        [ 0.0000e+00,  2.7300e-04,  2.5087e-03,  ..., -4.8073e-04,
          1.7880e-03,  6.3677e-04],
        [-4.6298e-03,  1.2188e-02,  1.4803e-02,  ...,  9.9098e-05,
          7.5767e-03,  3.5860e-03],
        ...,
        [ 0.0000e+00,  2.7300e-04,  2.5087e-03,  ..., -4.8073e-04,
          1.7880e-03,  6.3677e-04],
        [ 0.0000e+00,  2.7300e-04,  2.5087e-03,  ..., -4.8073e-04,
          1.7880e-03,  6.3677e-04],
        [ 0.0000e+00,  2.7300e-04,  2.5087e-03,  ..., -4.8073e-04,
          1.7880e-03,  6.3677e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2517.2935, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.4941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.8123, device='cuda:0')



h[100].sum tensor(50.1619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.4638, device='cuda:0')



h[200].sum tensor(-3.9996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.6224, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0102,  ..., 0.0000, 0.0072, 0.0026],
        [0.0000, 0.0132, 0.0227,  ..., 0.0001, 0.0132, 0.0056],
        [0.0000, 0.0110, 0.0205,  ..., 0.0000, 0.0121, 0.0051],
        ...,
        [0.0000, 0.0011, 0.0103,  ..., 0.0000, 0.0074, 0.0026],
        [0.0000, 0.0011, 0.0103,  ..., 0.0000, 0.0074, 0.0026],
        [0.0000, 0.0011, 0.0103,  ..., 0.0000, 0.0074, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64274.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0552, 0.0000, 0.0400],
        [0.0061, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0663],
        [0.0079, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0854],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0547, 0.0000, 0.0291],
        [0.0010, 0.0000, 0.0000,  ..., 0.0547, 0.0000, 0.0291],
        [0.0010, 0.0000, 0.0000,  ..., 0.0547, 0.0000, 0.0291]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594099.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1180.5835, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(263.3392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12490.9668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-713.4727, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2789.1338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-463.5814, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2361],
        [-0.8694],
        [-0.4659],
        ...,
        [-1.5525],
        [-1.5490],
        [-1.5482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-202791.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0169],
        [1.0180],
        [1.0169],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368503.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0170],
        [1.0181],
        [1.0170],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368511.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0171,  0.0199,  ...,  0.0003,  0.0100,  0.0048],
        [-0.0116,  0.0300,  0.0333,  ...,  0.0010,  0.0163,  0.0080],
        [-0.0209,  0.0540,  0.0580,  ...,  0.0021,  0.0279,  0.0139],
        ...,
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0025,  ..., -0.0005,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2170.9658, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6161, device='cuda:0')



h[100].sum tensor(35.8091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.9185, device='cuda:0')



h[200].sum tensor(-2.7184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9319, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0566, 0.0675,  ..., 0.0017, 0.0342, 0.0163],
        [0.0000, 0.1320, 0.1454,  ..., 0.0044, 0.0709, 0.0350],
        [0.0000, 0.1622, 0.1765,  ..., 0.0058, 0.0856, 0.0424],
        ...,
        [0.0000, 0.0010, 0.0104,  ..., 0.0000, 0.0073, 0.0026],
        [0.0000, 0.0010, 0.0104,  ..., 0.0000, 0.0073, 0.0026],
        [0.0000, 0.0010, 0.0104,  ..., 0.0000, 0.0073, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52572.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0445, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.2814],
        [0.0791, 0.0000, 0.0000,  ..., 0.1228, 0.0000, 0.4600],
        [0.1057, 0.0000, 0.0000,  ..., 0.1457, 0.0000, 0.5919],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0507],
        [0.0008, 0.0000, 0.0000,  ..., 0.0556, 0.0000, 0.0288],
        [0.0008, 0.0000, 0.0000,  ..., 0.0556, 0.0000, 0.0288]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532981.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(703.7892, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(208.8531, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11286.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-567.0529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3091.3613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.0336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2863],
        [ 0.3250],
        [ 0.3270],
        ...,
        [-0.8520],
        [-1.2450],
        [-1.4668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236164.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0170],
        [1.0181],
        [1.0170],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368511.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0171],
        [1.0182],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368518.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058,  0.0151,  0.0179,  ...,  0.0002,  0.0090,  0.0043],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [-0.0058,  0.0151,  0.0179,  ...,  0.0002,  0.0090,  0.0043],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2271.2173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.3300, device='cuda:0')



h[100].sum tensor(41.2289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.7546, device='cuda:0')



h[200].sum tensor(-3.1027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.3264e-02, 2.2995e-02,  ..., 8.6505e-05, 1.3230e-02,
         5.5487e-03],
        [0.0000e+00, 5.6006e-02, 6.7127e-02,  ..., 6.1931e-04, 3.4040e-02,
         1.6136e-02],
        [0.0000e+00, 1.3351e-02, 2.3133e-02,  ..., 8.7089e-05, 1.3307e-02,
         5.5821e-03],
        ...,
        [0.0000e+00, 9.6396e-04, 1.0457e-02,  ..., 0.0000e+00, 7.3591e-03,
         2.5416e-03],
        [0.0000e+00, 9.6399e-04, 1.0458e-02,  ..., 0.0000e+00, 7.3593e-03,
         2.5417e-03],
        [0.0000e+00, 9.6398e-04, 1.0457e-02,  ..., 0.0000e+00, 7.3592e-03,
         2.5417e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57378.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0093, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0878],
        [0.0168, 0.0000, 0.0000,  ..., 0.0703, 0.0000, 0.1418],
        [0.0133, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.1238],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0287],
        [0.0006, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0287],
        [0.0006, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0287]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(564979.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(798.9130, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(230.3246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11845.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-623.6544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2928.3315, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-441.8712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4641],
        [-0.0847],
        [ 0.1639],
        ...,
        [-1.5895],
        [-1.5859],
        [-1.5850]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211678.9219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0171],
        [1.0182],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368518.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0182],
        [1.0170],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368526.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  2.3245e-04,  2.5407e-03,  ..., -5.1724e-04,
          1.7942e-03,  6.0577e-04],
        [-3.5440e-03,  9.3944e-03,  1.1991e-02,  ..., -6.8776e-05,
          6.2506e-03,  2.8724e-03],
        [-3.3658e-03,  8.9336e-03,  1.1515e-02,  ..., -9.1331e-05,
          6.0265e-03,  2.7584e-03],
        ...,
        [ 0.0000e+00,  2.3245e-04,  2.5407e-03,  ..., -5.1724e-04,
          1.7942e-03,  6.0577e-04],
        [ 0.0000e+00,  2.3245e-04,  2.5407e-03,  ..., -5.1724e-04,
          1.7942e-03,  6.0577e-04],
        [ 0.0000e+00,  2.3245e-04,  2.5407e-03,  ..., -5.1724e-04,
          1.7942e-03,  6.0577e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2075.5894, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3409, device='cuda:0')



h[100].sum tensor(33.8555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3791, device='cuda:0')



h[200].sum tensor(-2.3536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9074, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0199,  ..., 0.0000, 0.0118, 0.0048],
        [0.0000, 0.0314, 0.0417,  ..., 0.0005, 0.0221, 0.0100],
        [0.0000, 0.0643, 0.0757,  ..., 0.0011, 0.0382, 0.0182],
        ...,
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49821.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0086, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0950],
        [0.0178, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.1554],
        [0.0258, 0.0000, 0.0000,  ..., 0.0793, 0.0000, 0.2071],
        ...,
        [0.0004, 0.0000, 0.0000,  ..., 0.0565, 0.0000, 0.0291],
        [0.0004, 0.0000, 0.0000,  ..., 0.0565, 0.0000, 0.0291],
        [0.0004, 0.0000, 0.0000,  ..., 0.0565, 0.0000, 0.0291]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(521441.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(473.4235, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(198.7121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11156.6934, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-524.5458, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2863.2302, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.7437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1603],
        [ 0.1405],
        [ 0.2805],
        ...,
        [-1.5955],
        [-1.5919],
        [-1.5910]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-223490.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0182],
        [1.0170],
        ...,
        [1.0010],
        [1.0003],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368526.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0183],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368534.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2313.2422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.9109, device='cuda:0')



h[100].sum tensor(44.6309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.1476, device='cuda:0')



h[200].sum tensor(-3.1739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0073, 0.0024],
        [0.0000, 0.0010, 0.0104,  ..., 0.0000, 0.0073, 0.0024],
        [0.0000, 0.0010, 0.0104,  ..., 0.0000, 0.0073, 0.0024],
        ...,
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55771.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0005, 0.0000, 0.0000,  ..., 0.0555, 0.0000, 0.0289],
        [0.0005, 0.0000, 0.0000,  ..., 0.0557, 0.0000, 0.0290],
        [0.0005, 0.0000, 0.0000,  ..., 0.0557, 0.0000, 0.0291],
        ...,
        [0.0005, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0295],
        [0.0005, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0295],
        [0.0005, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0295]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(542912.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(620.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(228.3677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11618.7832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-594.0953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2622.2036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-431.5220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4621],
        [-1.6437],
        [-1.7631],
        ...,
        [-1.5973],
        [-1.5937],
        [-1.5928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-205768.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0183],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368534.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0183],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368534.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2502.9341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.1143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.2024, device='cuda:0')



h[100].sum tensor(52.7924, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.0511, device='cuda:0')



h[200].sum tensor(-3.8656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.3477, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0103,  ..., 0.0000, 0.0073, 0.0024],
        [0.0000, 0.0010, 0.0104,  ..., 0.0000, 0.0073, 0.0024],
        [0.0000, 0.0010, 0.0104,  ..., 0.0000, 0.0073, 0.0024],
        ...,
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65266.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0045, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.0586],
        [0.0039, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0531],
        [0.0146, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.1117],
        ...,
        [0.0005, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0295],
        [0.0005, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0295],
        [0.0005, 0.0000, 0.0000,  ..., 0.0564, 0.0000, 0.0295]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607246., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1013.4895, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.7723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12729.3926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-711.9423, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2485.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-470.5264, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0464],
        [ 0.1021],
        [ 0.2306],
        ...,
        [-1.5973],
        [-1.5937],
        [-1.5928]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-186830.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0183],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368534.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0172],
        [1.0183],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368534.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0085,  0.0224,  0.0253,  ...,  0.0006,  0.0125,  0.0061],
        [-0.0125,  0.0326,  0.0360,  ...,  0.0011,  0.0176,  0.0086],
        [-0.0085,  0.0224,  0.0253,  ...,  0.0006,  0.0125,  0.0061],
        ...,
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006],
        [ 0.0000,  0.0002,  0.0025,  ..., -0.0005,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2456.1218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.2702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.2585, device='cuda:0')



h[100].sum tensor(50.7783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.4125, device='cuda:0')



h[200].sum tensor(-3.6949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1211, 0.1341,  ..., 0.0037, 0.0657, 0.0321],
        [0.0000, 0.1300, 0.1433,  ..., 0.0041, 0.0700, 0.0343],
        [0.0000, 0.2217, 0.2379,  ..., 0.0086, 0.1147, 0.0570],
        ...,
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025],
        [0.0000, 0.0010, 0.0105,  ..., 0.0000, 0.0074, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62054.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[8.2508e-02, 0.0000e+00, 0.0000e+00,  ..., 1.2904e-01, 0.0000e+00,
         4.9541e-01],
        [9.8804e-02, 0.0000e+00, 0.0000e+00,  ..., 1.4368e-01, 0.0000e+00,
         5.7799e-01],
        [1.3133e-01, 0.0000e+00, 0.0000e+00,  ..., 1.7226e-01, 0.0000e+00,
         7.4202e-01],
        ...,
        [4.7838e-04, 0.0000e+00, 0.0000e+00,  ..., 5.6374e-02, 0.0000e+00,
         2.9514e-02],
        [4.7843e-04, 0.0000e+00, 0.0000e+00,  ..., 5.6375e-02, 0.0000e+00,
         2.9515e-02],
        [1.3259e-03, 0.0000e+00, 0.0000e+00,  ..., 5.7159e-02, 0.0000e+00,
         3.6261e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(576739.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(839.5292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(257.2071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12183.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-672.0790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2581.6277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-456.3787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5066],
        [ 0.5181],
        [ 0.5258],
        ...,
        [-1.5680],
        [-1.4997],
        [-1.3656]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-207181.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0172],
        [1.0183],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368534.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0173],
        [1.0184],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368542.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        ...,
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2303.6995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.0282, device='cuda:0')



h[100].sum tensor(43.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.5504, device='cuda:0')



h[200].sum tensor(-3.0649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0103,  ..., 0.0000, 0.0072, 0.0024],
        [0.0000, 0.0011, 0.0104,  ..., 0.0000, 0.0072, 0.0024],
        [0.0000, 0.0011, 0.0104,  ..., 0.0000, 0.0072, 0.0024],
        ...,
        [0.0000, 0.0011, 0.0105,  ..., 0.0000, 0.0073, 0.0025],
        [0.0000, 0.0011, 0.0105,  ..., 0.0000, 0.0073, 0.0025],
        [0.0000, 0.0011, 0.0105,  ..., 0.0000, 0.0073, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57152.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0021, 0.0000, 0.0000,  ..., 0.0563, 0.0000, 0.0405],
        [0.0008, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0313],
        [0.0007, 0.0000, 0.0000,  ..., 0.0553, 0.0000, 0.0295],
        ...,
        [0.0007, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0299],
        [0.0007, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0299],
        [0.0007, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0299]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555317.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(708.6917, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(236.2975, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11948.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-607.9509, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2435.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-435.0345, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8042],
        [-1.1650],
        [-1.4145],
        ...,
        [-1.5908],
        [-1.5872],
        [-1.5862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-193088.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0173],
        [1.0184],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368542.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0173],
        [1.0185],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368550., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0186,  0.0487,  0.0524,  ...,  0.0017,  0.0253,  0.0125],
        [-0.0205,  0.0535,  0.0574,  ...,  0.0020,  0.0276,  0.0137],
        [-0.0130,  0.0342,  0.0375,  ...,  0.0010,  0.0182,  0.0090],
        ...,
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2047.6122, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.7455, device='cuda:0')



h[100].sum tensor(32.3581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.2998, device='cuda:0')



h[200].sum tensor(-2.0634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.1891, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.2286, 0.2448,  ..., 0.0086, 0.1177, 0.0586],
        [0.0000, 0.1919, 0.2070,  ..., 0.0068, 0.0999, 0.0495],
        [0.0000, 0.1463, 0.1600,  ..., 0.0046, 0.0777, 0.0383],
        ...,
        [0.0000, 0.0013, 0.0106,  ..., 0.0000, 0.0073, 0.0025],
        [0.0000, 0.0013, 0.0106,  ..., 0.0000, 0.0073, 0.0025],
        [0.0000, 0.0013, 0.0106,  ..., 0.0000, 0.0073, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47372.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1371, 0.0000, 0.0000,  ..., 0.1664, 0.0000, 0.7465],
        [0.1239, 0.0000, 0.0000,  ..., 0.1559, 0.0000, 0.6836],
        [0.1012, 0.0000, 0.0000,  ..., 0.1375, 0.0000, 0.5731],
        ...,
        [0.0010, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0303],
        [0.0010, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0303],
        [0.0010, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(506559.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(513.3868, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.0537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11168.7676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-482.5271, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2434.7603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.9131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3645],
        [ 0.3657],
        [ 0.3727],
        ...,
        [-1.5906],
        [-1.5872],
        [-1.5863]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209517.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0173],
        [1.0185],
        [1.0171],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368550., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(404.1780, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0186],
        [1.0172],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368557.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0028,  0.0076,  0.0101,  ..., -0.0003,  0.0053,  0.0024],
        [-0.0028,  0.0076,  0.0101,  ..., -0.0003,  0.0053,  0.0024],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        ...,
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0006,  0.0018,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2157.9690, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.1839, device='cuda:0')



h[100].sum tensor(37.2436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.9495, device='cuda:0')



h[200].sum tensor(-2.4074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2870, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0147, 0.0242,  ..., 0.0000, 0.0136, 0.0056],
        [0.0000, 0.0148, 0.0243,  ..., 0.0000, 0.0137, 0.0057],
        [0.0000, 0.0148, 0.0244,  ..., 0.0000, 0.0137, 0.0057],
        ...,
        [0.0000, 0.0014, 0.0106,  ..., 0.0000, 0.0072, 0.0024],
        [0.0000, 0.0014, 0.0106,  ..., 0.0000, 0.0072, 0.0024],
        [0.0000, 0.0014, 0.0106,  ..., 0.0000, 0.0072, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50204.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0571, 0.0000, 0.0833],
        [0.0051, 0.0000, 0.0000,  ..., 0.0575, 0.0000, 0.0885],
        [0.0047, 0.0000, 0.0000,  ..., 0.0573, 0.0000, 0.0841],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0306],
        [0.0013, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0306],
        [0.0013, 0.0000, 0.0000,  ..., 0.0554, 0.0000, 0.0306]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(518880.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(599.4550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.6303, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11530.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-513.2687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2140.5552, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.1217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3786],
        [-0.2845],
        [-0.3079],
        ...,
        [-1.5926],
        [-1.5892],
        [-1.5883]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-175369.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0186],
        [1.0172],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368557.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0174],
        [1.0186],
        [1.0172],
        ...,
        [1.0009],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368565.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005],
        [-0.0066,  0.0175,  0.0203,  ...,  0.0002,  0.0101,  0.0048],
        [-0.0126,  0.0331,  0.0364,  ...,  0.0009,  0.0177,  0.0086],
        ...,
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2030.5933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1700, device='cuda:0')



h[100].sum tensor(32.6859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.9104, device='cuda:0')



h[200].sum tensor(-1.9445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.9300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0187, 0.0285,  ..., 0.0002, 0.0155, 0.0065],
        [0.0000, 0.0622, 0.0733,  ..., 0.0016, 0.0367, 0.0172],
        [0.0000, 0.1514, 0.1653,  ..., 0.0045, 0.0800, 0.0392],
        ...,
        [0.0000, 0.0013, 0.0107,  ..., 0.0000, 0.0072, 0.0023],
        [0.0000, 0.0013, 0.0107,  ..., 0.0000, 0.0072, 0.0023],
        [0.0000, 0.0013, 0.0107,  ..., 0.0000, 0.0072, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46628.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0219, 0.0000, 0.0000,  ..., 0.0708, 0.0000, 0.1414],
        [0.0509, 0.0000, 0.0000,  ..., 0.0934, 0.0000, 0.2917],
        [0.0926, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.5040],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0307],
        [0.0015, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0307],
        [0.0015, 0.0000, 0.0000,  ..., 0.0559, 0.0000, 0.0307]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(504883.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(648.3112, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(188.3999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11285.3867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-467.0955, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2194.3569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-375.7031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3143],
        [ 0.0314],
        [ 0.2071],
        ...,
        [-1.6037],
        [-1.6002],
        [-1.5993]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-209408.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0174],
        [1.0186],
        [1.0172],
        ...,
        [1.0009],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368565.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0187],
        [1.0172],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368573., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045,  0.0120,  0.0147,  ..., -0.0001,  0.0074,  0.0034],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005],
        [-0.0073,  0.0194,  0.0223,  ...,  0.0002,  0.0110,  0.0052],
        ...,
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0026,  ..., -0.0007,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2252.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.5545, device='cuda:0')



h[100].sum tensor(43.5066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.5534, device='cuda:0')



h[200].sum tensor(-2.7371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.3544, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0622, 0.0735,  ..., 0.0011, 0.0367, 0.0171],
        [0.0000, 0.0581, 0.0693,  ..., 0.0003, 0.0347, 0.0161],
        [0.0000, 0.0408, 0.0515,  ..., 0.0005, 0.0264, 0.0118],
        ...,
        [0.0000, 0.0013, 0.0109,  ..., 0.0000, 0.0073, 0.0021],
        [0.0000, 0.0013, 0.0109,  ..., 0.0000, 0.0073, 0.0021],
        [0.0000, 0.0266, 0.0369,  ..., 0.0005, 0.0195, 0.0083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53124.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0527, 0.0000, 0.0000,  ..., 0.0944, 0.0000, 0.3195],
        [0.0393, 0.0000, 0.0000,  ..., 0.0844, 0.0000, 0.2568],
        [0.0366, 0.0000, 0.0000,  ..., 0.0825, 0.0000, 0.2369],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0576, 0.0000, 0.0364],
        [0.0070, 0.0000, 0.0000,  ..., 0.0609, 0.0000, 0.0607],
        [0.0206, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.1332]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534889.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(845.3976, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.5525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11900.8379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-544.5930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2049.7285, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.3307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2706],
        [ 0.2875],
        [ 0.2918],
        ...,
        [-1.4205],
        [-1.1244],
        [-0.6849]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-211473.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0187],
        [1.0172],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368573., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0175],
        [1.0188],
        [1.0173],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368580.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0079,  0.0209,  0.0238,  ...,  0.0003,  0.0118,  0.0055],
        [-0.0068,  0.0182,  0.0211,  ...,  0.0002,  0.0105,  0.0049],
        [-0.0067,  0.0178,  0.0207,  ...,  0.0001,  0.0103,  0.0048],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2238.8193, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.5444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9782, device='cuda:0')



h[100].sum tensor(44.1374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1634, device='cuda:0')



h[200].sum tensor(-2.6801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0949, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0764, 0.0883,  ..., 0.0009, 0.0437, 0.0204],
        [0.0000, 0.1007, 0.1133,  ..., 0.0019, 0.0555, 0.0264],
        [0.0000, 0.0962, 0.1087,  ..., 0.0017, 0.0533, 0.0253],
        ...,
        [0.0000, 0.0013, 0.0110,  ..., 0.0000, 0.0073, 0.0020],
        [0.0000, 0.0013, 0.0110,  ..., 0.0000, 0.0073, 0.0020],
        [0.0000, 0.0013, 0.0110,  ..., 0.0000, 0.0073, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53833.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0554, 0.0000, 0.0000,  ..., 0.0966, 0.0000, 0.3394],
        [0.0715, 0.0000, 0.0000,  ..., 0.1090, 0.0000, 0.4194],
        [0.0737, 0.0000, 0.0000,  ..., 0.1108, 0.0000, 0.4302],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0574, 0.0000, 0.0309],
        [0.0017, 0.0000, 0.0000,  ..., 0.0574, 0.0000, 0.0309],
        [0.0017, 0.0000, 0.0000,  ..., 0.0574, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541171.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(889.8536, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.7812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12130.6689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-550.2208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1912.3057, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-407.2838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2735],
        [ 0.3021],
        [ 0.2993],
        ...,
        [-1.5683],
        [-1.5152],
        [-1.4797]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-204905.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0175],
        [1.0188],
        [1.0173],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368580.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0176],
        [1.0188],
        [1.0173],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368588.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0034,  0.0092,  0.0119,  ..., -0.0003,  0.0061,  0.0026],
        [-0.0035,  0.0094,  0.0120,  ..., -0.0003,  0.0062,  0.0027],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2109.0332, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1647, device='cuda:0')



h[100].sum tensor(39.7527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.2599, device='cuda:0')



h[200].sum tensor(-2.2211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0642, 0.0758,  ..., 0.0005, 0.0379, 0.0173],
        [0.0000, 0.0318, 0.0424,  ..., 0.0003, 0.0222, 0.0093],
        [0.0000, 0.0105, 0.0205,  ..., 0.0000, 0.0118, 0.0041],
        ...,
        [0.0000, 0.0012, 0.0110,  ..., 0.0000, 0.0074, 0.0018],
        [0.0000, 0.0012, 0.0110,  ..., 0.0000, 0.0074, 0.0018],
        [0.0000, 0.0012, 0.0110,  ..., 0.0000, 0.0074, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48683.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0285, 0.0000, 0.0000,  ..., 0.0766, 0.0000, 0.2058],
        [0.0205, 0.0000, 0.0000,  ..., 0.0708, 0.0000, 0.1579],
        [0.0115, 0.0000, 0.0000,  ..., 0.0642, 0.0000, 0.1039],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0580, 0.0000, 0.0310],
        [0.0019, 0.0000, 0.0000,  ..., 0.0580, 0.0000, 0.0310],
        [0.0019, 0.0000, 0.0000,  ..., 0.0580, 0.0000, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(512596., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(793.9136, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(193.7144, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11689.4033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-484.6827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1915.6028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-383.5901, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2602],
        [ 0.2308],
        [ 0.1707],
        ...,
        [-1.6463],
        [-1.6425],
        [-1.6410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232987.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0176],
        [1.0188],
        [1.0173],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368588.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0176],
        [1.0189],
        [1.0174],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368596.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2340.0732, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4732, device='cuda:0')



h[100].sum tensor(50.4373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.8515, device='cuda:0')



h[200].sum tensor(-3.0026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0135, 0.0237,  ..., 0.0000, 0.0134, 0.0047],
        [0.0000, 0.0074, 0.0174,  ..., 0.0000, 0.0104, 0.0032],
        [0.0000, 0.0012, 0.0110,  ..., 0.0000, 0.0074, 0.0017],
        ...,
        [0.0000, 0.0012, 0.0111,  ..., 0.0000, 0.0075, 0.0017],
        [0.0000, 0.0012, 0.0111,  ..., 0.0000, 0.0075, 0.0017],
        [0.0000, 0.0012, 0.0111,  ..., 0.0000, 0.0075, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56954.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0044, 0.0000, 0.0000,  ..., 0.0590, 0.0000, 0.0710],
        [0.0038, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0593],
        [0.0030, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0454],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0311],
        [0.0020, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0311],
        [0.0020, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0311]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(558274.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1075.0181, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(229.6288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12597.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-584.7236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1773.0847, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-419.3446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1493],
        [-1.2948],
        [-1.4577],
        ...,
        [-1.6601],
        [-1.6563],
        [-1.6553]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-217704.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0176],
        [1.0189],
        [1.0174],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368596.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0177],
        [1.0190],
        [1.0175],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368604.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2118.8003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3388, device='cuda:0')



h[100].sum tensor(41.8193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3777, device='cuda:0')



h[200].sum tensor(-2.2477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9065, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0110,  ..., 0.0000, 0.0074, 0.0016],
        [0.0000, 0.0011, 0.0111,  ..., 0.0000, 0.0075, 0.0016],
        [0.0000, 0.0011, 0.0111,  ..., 0.0000, 0.0075, 0.0016],
        ...,
        [0.0000, 0.0011, 0.0112,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0011, 0.0112,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0011, 0.0112,  ..., 0.0000, 0.0076, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51333.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0303],
        [0.0022, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0304],
        [0.0022, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0305],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0310],
        [0.0023, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0310],
        [0.0023, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(534698.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(926.1217, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(200.4626, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12188.1445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-511.5996, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1773.7195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.4454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8410],
        [-1.8868],
        [-1.9226],
        ...,
        [-1.6721],
        [-1.6694],
        [-1.6690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227544.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0177],
        [1.0190],
        [1.0175],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368604.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0178],
        [1.0191],
        [1.0175],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368612.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0018,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2075.5200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.3191, device='cuda:0')



h[100].sum tensor(39.9521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.6878, device='cuda:0')



h[200].sum tensor(-2.0684, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.4474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0105, 0.0208,  ..., 0.0000, 0.0120, 0.0038],
        [0.0000, 0.0011, 0.0112,  ..., 0.0000, 0.0075, 0.0015],
        [0.0000, 0.0011, 0.0112,  ..., 0.0000, 0.0075, 0.0015],
        ...,
        [0.0000, 0.0011, 0.0113,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0011, 0.0113,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0011, 0.0113,  ..., 0.0000, 0.0076, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48344.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0138, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.1036],
        [0.0060, 0.0000, 0.0000,  ..., 0.0617, 0.0000, 0.0573],
        [0.0047, 0.0000, 0.0000,  ..., 0.0609, 0.0000, 0.0494],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0309],
        [0.0025, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0309],
        [0.0025, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(517249.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(965.2029, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(185.2048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11799.4775, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-473.5258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1902.0135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.0509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2424],
        [-0.5634],
        [-0.7417],
        ...,
        [-1.6931],
        [-1.6891],
        [-1.6879]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264032.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0178],
        [1.0191],
        [1.0175],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368612.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0179],
        [1.0192],
        [1.0176],
        ...,
        [1.0009],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368620.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2253.0452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.3560, device='cuda:0')



h[100].sum tensor(47.0971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.4190, device='cuda:0')



h[200].sum tensor(-2.6179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2650, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0111,  ..., 0.0000, 0.0075, 0.0015],
        [0.0000, 0.0011, 0.0112,  ..., 0.0000, 0.0076, 0.0015],
        [0.0000, 0.0011, 0.0112,  ..., 0.0000, 0.0076, 0.0015],
        ...,
        [0.0000, 0.0011, 0.0113,  ..., 0.0000, 0.0077, 0.0015],
        [0.0000, 0.0011, 0.0113,  ..., 0.0000, 0.0077, 0.0015],
        [0.0000, 0.0011, 0.0113,  ..., 0.0000, 0.0076, 0.0015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52359.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0303],
        [0.0026, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0304],
        [0.0026, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0305],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0606, 0.0000, 0.0309],
        [0.0027, 0.0000, 0.0000,  ..., 0.0606, 0.0000, 0.0309],
        [0.0027, 0.0000, 0.0000,  ..., 0.0605, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(531424.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1019.4486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(201.8507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12122.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-520.6880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1791.4919, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.9007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9290],
        [-1.9529],
        [-1.9683],
        ...,
        [-1.7039],
        [-1.6995],
        [-1.6981]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235209.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0179],
        [1.0192],
        [1.0176],
        ...,
        [1.0009],
        [1.0002],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368620.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0193],
        [1.0177],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368629.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [-0.0109,  0.0291,  0.0324,  ...,  0.0006,  0.0158,  0.0075],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2207.8286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.5992, device='cuda:0')



h[100].sum tensor(43.7832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.2305, device='cuda:0')



h[200].sum tensor(-2.3780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4740, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0012, 0.0111,  ..., 0.0000, 0.0076, 0.0016],
        [0.0000, 0.0545, 0.0661,  ..., 0.0011, 0.0335, 0.0147],
        [0.0000, 0.1079, 0.1211,  ..., 0.0029, 0.0594, 0.0278],
        ...,
        [0.0000, 0.0012, 0.0113,  ..., 0.0000, 0.0077, 0.0016],
        [0.0000, 0.0012, 0.0113,  ..., 0.0000, 0.0077, 0.0016],
        [0.0000, 0.0012, 0.0113,  ..., 0.0000, 0.0077, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51490.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0192, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.1202],
        [0.0454, 0.0000, 0.0000,  ..., 0.0909, 0.0000, 0.2566],
        [0.0731, 0.0000, 0.0000,  ..., 0.1112, 0.0000, 0.3976],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0311],
        [0.0027, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0311],
        [0.0027, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(532217.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1067.2513, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(200.3747, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12120.8760, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-508.9433, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1820.2441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.4981, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0515],
        [ 0.1900],
        [ 0.2622],
        ...,
        [-1.7094],
        [-1.7054],
        [-1.7043]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244642.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0193],
        [1.0177],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368629.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(491.4849, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0180],
        [1.0194],
        [1.0177],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368639.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0031,  0.0085,  0.0111,  ..., -0.0003,  0.0058,  0.0024],
        [-0.0038,  0.0103,  0.0129,  ..., -0.0003,  0.0067,  0.0029],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2457.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.0586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.8376, device='cuda:0')



h[100].sum tensor(52.2849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.7746, device='cuda:0')



h[200].sum tensor(-3.0966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8327, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0675, 0.0792,  ..., 0.0005, 0.0398, 0.0180],
        [0.0000, 0.0320, 0.0426,  ..., 0.0003, 0.0226, 0.0092],
        [0.0000, 0.0214, 0.0317,  ..., 0.0000, 0.0174, 0.0066],
        ...,
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0078, 0.0017],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0078, 0.0017],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0078, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61580.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0317, 0.0000, 0.0000,  ..., 0.0797, 0.0000, 0.2154],
        [0.0234, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.1684],
        [0.0176, 0.0000, 0.0000,  ..., 0.0698, 0.0000, 0.1343],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0312],
        [0.0026, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0312],
        [0.0026, 0.0000, 0.0000,  ..., 0.0598, 0.0000, 0.0312]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600560.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1556.0853, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(249.3812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13326.9736, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-633.5281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1757.5494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-430.6035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2840],
        [ 0.2459],
        [ 0.1773],
        ...,
        [-1.7085],
        [-1.7046],
        [-1.7035]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-235386.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0180],
        [1.0194],
        [1.0177],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368639.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0181],
        [1.0195],
        [1.0178],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368648.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0004],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2547.1018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8895, device='cuda:0')



h[100].sum tensor(54.1336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.4863, device='cuda:0')



h[200].sum tensor(-3.2817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0078, 0.0018],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0078, 0.0018],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0078, 0.0018],
        ...,
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0079, 0.0018],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0079, 0.0018],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0079, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62419.2383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0583, 0.0000, 0.0308],
        [0.0026, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0325],
        [0.0029, 0.0000, 0.0000,  ..., 0.0589, 0.0000, 0.0368],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0593, 0.0000, 0.0315],
        [0.0034, 0.0000, 0.0000,  ..., 0.0600, 0.0000, 0.0365],
        [0.0064, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0534]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592644.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1433.2888, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(257.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13197.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-643.3259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1771.5486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.5099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5053],
        [-1.4106],
        [-1.2612],
        ...,
        [-1.6629],
        [-1.5542],
        [-1.3142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231344.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0181],
        [1.0195],
        [1.0178],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368648.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0182],
        [1.0195],
        [1.0179],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368657., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [-0.0039,  0.0108,  0.0134,  ..., -0.0002,  0.0070,  0.0030],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2252.0273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9349, device='cuda:0')



h[100].sum tensor(40.5804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7810, device='cuda:0')



h[200].sum tensor(-2.2428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0108,  ..., 0.0000, 0.0079, 0.0019],
        [0.0000, 0.0122, 0.0219,  ..., 0.0000, 0.0131, 0.0045],
        [0.0000, 0.0103, 0.0199,  ..., 0.0000, 0.0121, 0.0040],
        ...,
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0080, 0.0019],
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0080, 0.0019],
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0080, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52958.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0428],
        [0.0065, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0630],
        [0.0075, 0.0000, 0.0000,  ..., 0.0619, 0.0000, 0.0719],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0316],
        [0.0024, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0316],
        [0.0024, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0316]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(548747.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1038.0415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.5427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12537.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-524.3514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1717.5269, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.0080, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0062],
        [-1.1913],
        [-1.2863],
        ...,
        [-1.7111],
        [-1.7073],
        [-1.7061]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-201278.2344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0182],
        [1.0195],
        [1.0179],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368657., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0183],
        [1.0196],
        [1.0179],
        ...,
        [1.0012],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368665.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0019,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2606.5657, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.4880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.8131, device='cuda:0')



h[100].sum tensor(54.1756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.1111, device='cuda:0')



h[200].sum tensor(-3.3348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0108,  ..., 0.0000, 0.0079, 0.0019],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0079, 0.0019],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0080, 0.0019],
        ...,
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0081, 0.0019],
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0081, 0.0019],
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0080, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62601.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0584, 0.0000, 0.0308],
        [0.0026, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0309],
        [0.0026, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.0310],
        ...,
        [0.0101, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.0758],
        [0.0097, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0736],
        [0.0080, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0631]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(597908.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1422.2001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(257.5872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13342.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-644.2771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1755.7417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-438.8977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8407],
        [-1.8860],
        [-1.9053],
        ...,
        [-0.4954],
        [-0.5105],
        [-0.5702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199107.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0183],
        [1.0196],
        [1.0179],
        ...,
        [1.0012],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368665.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0184],
        [1.0197],
        [1.0180],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368673.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.6452e-04,  2.6841e-03,  ..., -7.1422e-04,
          1.9677e-03,  4.5766e-04],
        [ 0.0000e+00,  3.6452e-04,  2.6841e-03,  ..., -7.1422e-04,
          1.9677e-03,  4.5766e-04],
        [ 0.0000e+00,  3.6452e-04,  2.6841e-03,  ..., -7.1422e-04,
          1.9677e-03,  4.5766e-04],
        ...,
        [ 0.0000e+00,  3.6452e-04,  2.6841e-03,  ..., -7.1422e-04,
          1.9677e-03,  4.5766e-04],
        [ 0.0000e+00,  3.6452e-04,  2.6841e-03,  ..., -7.1422e-04,
          1.9677e-03,  4.5766e-04],
        [-4.9294e-03,  1.3497e-02,  1.6212e-02,  ..., -7.5197e-05,
          8.3438e-03,  3.6937e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2344.6750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0280, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.7602, device='cuda:0')



h[100].sum tensor(42.9699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.0160, device='cuda:0')



h[200].sum tensor(-2.4750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0080, 0.0019],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0080, 0.0019],
        [0.0000, 0.0015, 0.0110,  ..., 0.0000, 0.0080, 0.0019],
        ...,
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0081, 0.0019],
        [0.0000, 0.0151, 0.0250,  ..., 0.0000, 0.0147, 0.0052],
        [0.0000, 0.0126, 0.0225,  ..., 0.0000, 0.0135, 0.0046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54357.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0052, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0469],
        [0.0038, 0.0000, 0.0000,  ..., 0.0600, 0.0000, 0.0385],
        [0.0048, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.0466],
        ...,
        [0.0049, 0.0000, 0.0000,  ..., 0.0615, 0.0000, 0.0442],
        [0.0092, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0727],
        [0.0107, 0.0000, 0.0000,  ..., 0.0658, 0.0000, 0.0845]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(551789.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1175.2175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(215.7751, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12444.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-542.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1910.7798, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.3667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4548],
        [-0.6193],
        [-0.6668],
        ...,
        [-1.5568],
        [-1.3773],
        [-1.2194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231557.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0184],
        [1.0197],
        [1.0180],
        ...,
        [1.0012],
        [1.0005],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368673.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0185],
        [1.0198],
        [1.0181],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368682.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2267.6279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9190, device='cuda:0')



h[100].sum tensor(39.4877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7703, device='cuda:0')



h[200].sum tensor(-2.2150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1677, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0110,  ..., 0.0000, 0.0081, 0.0018],
        [0.0000, 0.0014, 0.0110,  ..., 0.0000, 0.0081, 0.0018],
        [0.0000, 0.0014, 0.0110,  ..., 0.0000, 0.0081, 0.0018],
        ...,
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0082, 0.0018],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0082, 0.0018],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0082, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51744.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0060, 0.0000, 0.0000,  ..., 0.0618, 0.0000, 0.0538],
        [0.0041, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.0405],
        [0.0042, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0412],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0310],
        [0.0032, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0310],
        [0.0032, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0310]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541295.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1083.1812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(198.9904, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12273.1689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-509.0702, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1938.7886, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.5225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3280],
        [-0.5346],
        [-0.6491],
        ...,
        [-1.7645],
        [-1.7603],
        [-1.7591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226278.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0185],
        [1.0198],
        [1.0181],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368682.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0199],
        [1.0181],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368690.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2148.8721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.1969, device='cuda:0')



h[100].sum tensor(34.5294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.9286, device='cuda:0')



h[200].sum tensor(-1.8295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.9421, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0110,  ..., 0.0000, 0.0082, 0.0018],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0082, 0.0018],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0082, 0.0018],
        ...,
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0083, 0.0018],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0083, 0.0018],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0083, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47966.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0034, 0.0000, 0.0000,  ..., 0.0603, 0.0000, 0.0301],
        [0.0034, 0.0000, 0.0000,  ..., 0.0605, 0.0000, 0.0303],
        [0.0034, 0.0000, 0.0000,  ..., 0.0606, 0.0000, 0.0304],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0614, 0.0000, 0.0308],
        [0.0035, 0.0000, 0.0000,  ..., 0.0614, 0.0000, 0.0308],
        [0.0035, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.0308]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(526648.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1053.4502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(178.1891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11959.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-463.4503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2079.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.6814, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8923],
        [-1.7889],
        [-1.6115],
        ...,
        [-1.7839],
        [-1.7797],
        [-1.7784]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253363.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0199],
        [1.0181],
        ...,
        [1.0013],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368690.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0186],
        [1.0199],
        [1.0181],
        ...,
        [1.0013],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368698.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0198,  0.0534,  0.0574,  ...,  0.0019,  0.0278,  0.0135],
        [-0.0156,  0.0421,  0.0457,  ...,  0.0014,  0.0223,  0.0107],
        [-0.0133,  0.0359,  0.0393,  ...,  0.0011,  0.0193,  0.0092],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2467.9331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4477, device='cuda:0')



h[100].sum tensor(47.2103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.8342, device='cuda:0')



h[200].sum tensor(-2.7853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2068, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1601, 0.1745,  ..., 0.0050, 0.0854, 0.0409],
        [0.0000, 0.2009, 0.2166,  ..., 0.0070, 0.1052, 0.0510],
        [0.0000, 0.2119, 0.2280,  ..., 0.0075, 0.1106, 0.0537],
        ...,
        [0.0000, 0.0013, 0.0112,  ..., 0.0000, 0.0084, 0.0018],
        [0.0000, 0.0013, 0.0112,  ..., 0.0000, 0.0084, 0.0018],
        [0.0000, 0.0013, 0.0112,  ..., 0.0000, 0.0084, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55205.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1289, 0.0000, 0.0000,  ..., 0.1559, 0.0000, 0.6738],
        [0.1558, 0.0000, 0.0000,  ..., 0.1765, 0.0000, 0.8001],
        [0.1669, 0.0000, 0.0000,  ..., 0.1849, 0.0000, 0.8514],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.0309],
        [0.0035, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.0309],
        [0.0035, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.0309]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(550849.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1321.4082, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(211.2799, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12354.7520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-555.9561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2091.2224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.6324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2966],
        [ 0.2717],
        [ 0.2497],
        ...,
        [-1.7947],
        [-1.7904],
        [-1.7892]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280443.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0186],
        [1.0199],
        [1.0181],
        ...,
        [1.0013],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368698.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0187],
        [1.0200],
        [1.0182],
        ...,
        [1.0013],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368707.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0160,  0.0433,  0.0469,  ...,  0.0014,  0.0229,  0.0110],
        [-0.0108,  0.0292,  0.0324,  ...,  0.0007,  0.0161,  0.0076],
        [-0.0218,  0.0588,  0.0629,  ...,  0.0022,  0.0304,  0.0149],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2620.9949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.0977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.6079, device='cuda:0')



h[100].sum tensor(52.7273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.9723, device='cuda:0')



h[200].sum tensor(-3.1996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.6298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1266, 0.1400,  ..., 0.0034, 0.0691, 0.0327],
        [0.0000, 0.1876, 0.2029,  ..., 0.0064, 0.0988, 0.0478],
        [0.0000, 0.1033, 0.1160,  ..., 0.0022, 0.0578, 0.0270],
        ...,
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0084, 0.0019],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0084, 0.0019],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0084, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61608.5742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0869, 0.0000, 0.0000,  ..., 0.1238, 0.0000, 0.4785],
        [0.1058, 0.0000, 0.0000,  ..., 0.1384, 0.0000, 0.5715],
        [0.0826, 0.0000, 0.0000,  ..., 0.1209, 0.0000, 0.4594],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0311],
        [0.0034, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0311],
        [0.0034, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0311]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(588091.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1399.8503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(240.8671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13162.3340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-635.5655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1835.4304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-447.1005, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4004],
        [ 0.4143],
        [ 0.3848],
        ...,
        [-1.7929],
        [-1.7884],
        [-1.7867]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220197.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0187],
        [1.0200],
        [1.0182],
        ...,
        [1.0013],
        [1.0006],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368707.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0188],
        [1.0201],
        [1.0183],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368716.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0109,  0.0297,  0.0329,  ...,  0.0008,  0.0163,  0.0077],
        [-0.0171,  0.0464,  0.0501,  ...,  0.0016,  0.0244,  0.0118],
        [-0.0195,  0.0527,  0.0566,  ...,  0.0019,  0.0275,  0.0134],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0020,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2350.0198, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0884, device='cuda:0')



h[100].sum tensor(41.2652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5614, device='cuda:0')



h[200].sum tensor(-2.3016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1435, 0.1573,  ..., 0.0042, 0.0774, 0.0370],
        [0.0000, 0.1490, 0.1630,  ..., 0.0045, 0.0801, 0.0383],
        [0.0000, 0.1683, 0.1829,  ..., 0.0054, 0.0894, 0.0431],
        ...,
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0084, 0.0020],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0084, 0.0020],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0084, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51670.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1126, 0.0000, 0.0000,  ..., 0.1422, 0.0000, 0.6038],
        [0.1098, 0.0000, 0.0000,  ..., 0.1403, 0.0000, 0.5911],
        [0.1048, 0.0000, 0.0000,  ..., 0.1365, 0.0000, 0.5631],
        ...,
        [0.0030, 0.0000, 0.0000,  ..., 0.0602, 0.0000, 0.0314],
        [0.0030, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0314],
        [0.0030, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0314]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537555.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1026.2412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(201.2357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12298.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-515.7991, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1873.1237, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.6400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3966],
        [ 0.3996],
        [ 0.3859],
        ...,
        [-1.7955],
        [-1.7914],
        [-1.7906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244385.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0188],
        [1.0201],
        [1.0183],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368716.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(516.7286, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0189],
        [1.0202],
        [1.0183],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368726., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2319.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8017, device='cuda:0')



h[100].sum tensor(39.3381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.6909, device='cuda:0')



h[200].sum tensor(-2.1382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0083, 0.0020],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0083, 0.0020],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0083, 0.0020],
        ...,
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0084, 0.0021],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0084, 0.0021],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0084, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51434.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0024, 0.0000, 0.0000,  ..., 0.0581, 0.0000, 0.0331],
        [0.0060, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0560],
        [0.0115, 0.0000, 0.0000,  ..., 0.0652, 0.0000, 0.0885],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.0318],
        [0.0025, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.0318],
        [0.0025, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.0318]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(537216.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(942.9291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(205.0389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12349.6816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-515.7161, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1796.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.2093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0361],
        [-0.6112],
        [-0.1961],
        ...,
        [-1.7925],
        [-1.7881],
        [-1.7846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243589.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0189],
        [1.0202],
        [1.0183],
        ...,
        [1.0013],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368726., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0190],
        [1.0203],
        [1.0184],
        ...,
        [1.0014],
        [1.0007],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368735.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0020,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2233.5063, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.5268, device='cuda:0')



h[100].sum tensor(35.2008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.1518, device='cuda:0')



h[200].sum tensor(-1.8279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.0906, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0206,  ..., 0.0000, 0.0129, 0.0044],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0083, 0.0021],
        [0.0000, 0.0116, 0.0213,  ..., 0.0000, 0.0133, 0.0046],
        ...,
        [0.0000, 0.0015, 0.0110,  ..., 0.0000, 0.0085, 0.0021],
        [0.0000, 0.0015, 0.0110,  ..., 0.0000, 0.0085, 0.0021],
        [0.0000, 0.0015, 0.0110,  ..., 0.0000, 0.0085, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48732.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0057, 0.0000, 0.0000,  ..., 0.0602, 0.0000, 0.0626],
        [0.0043, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0502],
        [0.0062, 0.0000, 0.0000,  ..., 0.0608, 0.0000, 0.0649],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0321],
        [0.0022, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0321],
        [0.0022, 0.0000, 0.0000,  ..., 0.0586, 0.0000, 0.0321]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(528633.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(796.8074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(194.9111, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12217.0059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-484.4364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1736.9980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.0323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4117],
        [-1.5452],
        [-1.5650],
        ...,
        [-1.7987],
        [-1.7946],
        [-1.7934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-241295.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0190],
        [1.0203],
        [1.0184],
        ...,
        [1.0014],
        [1.0007],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368735.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0191],
        [1.0204],
        [1.0185],
        ...,
        [1.0014],
        [1.0007],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368743.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0021,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2527.8933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.5685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.3204, device='cuda:0')



h[100].sum tensor(46.6484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.7481, device='cuda:0')



h[200].sum tensor(-2.6853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0084, 0.0021],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0084, 0.0021],
        [0.0000, 0.0015, 0.0109,  ..., 0.0000, 0.0084, 0.0021],
        ...,
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0085, 0.0021],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0085, 0.0021],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0085, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56892.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0000, 0.0000,  ..., 0.0574, 0.0000, 0.0331],
        [0.0016, 0.0000, 0.0000,  ..., 0.0575, 0.0000, 0.0348],
        [0.0022, 0.0000, 0.0000,  ..., 0.0579, 0.0000, 0.0424],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0323],
        [0.0019, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0323],
        [0.0019, 0.0000, 0.0000,  ..., 0.0585, 0.0000, 0.0323]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(561897.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(892.1077, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(229.4603, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12852.2949, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-588.0916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1576.8137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.3493, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5315],
        [-1.3527],
        [-1.1175],
        ...,
        [-1.8081],
        [-1.8039],
        [-1.8026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-214005.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0191],
        [1.0204],
        [1.0185],
        ...,
        [1.0014],
        [1.0007],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368743.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0205],
        [1.0187],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368752.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0039,  0.0108,  0.0134,  ..., -0.0001,  0.0071,  0.0031],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [-0.0039,  0.0108,  0.0134,  ..., -0.0001,  0.0071,  0.0031],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2810.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.0891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.6073, device='cuda:0')



h[100].sum tensor(57.7106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.0016, device='cuda:0')



h[200].sum tensor(-3.5051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.9803, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0198,  ..., 0.0000, 0.0126, 0.0042],
        [0.0000, 0.0401, 0.0508,  ..., 0.0000, 0.0272, 0.0116],
        [0.0000, 0.0101, 0.0200,  ..., 0.0000, 0.0127, 0.0042],
        ...,
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0086, 0.0021],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0086, 0.0021],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0086, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67024.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0095, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.0924],
        [0.0115, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.1143],
        [0.0081, 0.0000, 0.0000,  ..., 0.0622, 0.0000, 0.0934],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.0324],
        [0.0018, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.0324],
        [0.0018, 0.0000, 0.0000,  ..., 0.0587, 0.0000, 0.0324]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617123.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1231.4939, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(271.7633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13786.7461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-715.9513, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1518.0454, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-461.0760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1925],
        [-0.2303],
        [-0.1841],
        ...,
        [-1.8249],
        [-1.8207],
        [-1.8194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210683.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0205],
        [1.0187],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368752.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0188],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368760.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [-0.0041,  0.0113,  0.0140,  ..., -0.0001,  0.0074,  0.0032],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2334.8394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8420, device='cuda:0')



h[100].sum tensor(38.7199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7182, device='cuda:0')



h[200].sum tensor(-2.0927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 3.4490e-02, 4.5156e-02,  ..., 2.6182e-05, 2.4537e-02,
         1.0251e-02],
        [0.0000e+00, 3.0578e-02, 4.1168e-02,  ..., 1.0694e-04, 2.2669e-02,
         9.2918e-03],
        [0.0000e+00, 4.8088e-02, 5.9209e-02,  ..., 8.0721e-05, 3.1188e-02,
         1.3618e-02],
        ...,
        [0.0000e+00, 1.3565e-03, 1.1207e-02,  ..., 0.0000e+00, 8.5699e-03,
         2.0985e-03],
        [0.0000e+00, 1.3564e-03, 1.1206e-02,  ..., 0.0000e+00, 8.5691e-03,
         2.0983e-03],
        [0.0000e+00, 1.3562e-03, 1.1204e-02,  ..., 0.0000e+00, 8.5679e-03,
         2.0980e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51722.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0000, 0.0000,  ..., 0.0683, 0.0000, 0.1505],
        [0.0181, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.1567],
        [0.0181, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.1591],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0325],
        [0.0018, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0325],
        [0.0018, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(541363.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(723.8428, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(199.4774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12380.9326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-527.0693, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1632.1180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.0477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2889e-01],
        [-7.8827e-04],
        [-6.6346e-02],
        ...,
        [-1.8414e+00],
        [-1.8371e+00],
        [-1.8358e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240018.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0188],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368760.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0189],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368768.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0125,  0.0342,  0.0376,  ...,  0.0010,  0.0185,  0.0089],
        [-0.0082,  0.0226,  0.0257,  ...,  0.0005,  0.0129,  0.0060],
        [-0.0096,  0.0264,  0.0296,  ...,  0.0006,  0.0148,  0.0069],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0007,  0.0021,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2361.7134, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.5984, device='cuda:0')



h[100].sum tensor(39.8598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.2300, device='cuda:0')



h[200].sum tensor(-2.1711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0969, 0.1096,  ..., 0.0021, 0.0550, 0.0257],
        [0.0000, 0.1115, 0.1246,  ..., 0.0028, 0.0621, 0.0293],
        [0.0000, 0.1012, 0.1140,  ..., 0.0023, 0.0571, 0.0267],
        ...,
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0086, 0.0021],
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0086, 0.0021],
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0086, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54411.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0673, 0.0000, 0.0000,  ..., 0.1054, 0.0000, 0.4034],
        [0.0676, 0.0000, 0.0000,  ..., 0.1059, 0.0000, 0.4038],
        [0.0616, 0.0000, 0.0000,  ..., 0.1016, 0.0000, 0.3734],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0326],
        [0.0017, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0326],
        [0.0017, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0326]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(562470., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(856.2847, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(209.2529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12716.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-561.7798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1598.3513, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-409.3225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4237],
        [ 0.4192],
        [ 0.4134],
        ...,
        [-1.8586],
        [-1.8543],
        [-1.8530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-247286.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0206],
        [1.0189],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368768.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0192],
        [1.0207],
        [1.0189],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368777.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2279.8647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6247, device='cuda:0')



h[100].sum tensor(36.3522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8946, device='cuda:0')



h[200].sum tensor(-1.9169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0190,  ..., 0.0000, 0.0123, 0.0039],
        [0.0000, 0.0013, 0.0111,  ..., 0.0000, 0.0086, 0.0020],
        [0.0000, 0.0013, 0.0112,  ..., 0.0000, 0.0086, 0.0020],
        ...,
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0087, 0.0020],
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0087, 0.0020],
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0087, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50042.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0107, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.1031],
        [0.0062, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0693],
        [0.0031, 0.0000, 0.0000,  ..., 0.0600, 0.0000, 0.0454],
        ...,
        [0.0014, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0329],
        [0.0014, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0328],
        [0.0014, 0.0000, 0.0000,  ..., 0.0595, 0.0000, 0.0328]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535031.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(578.3569, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(189.7516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12221.1455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-509.0792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1610.3049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-392.4965, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0877],
        [-0.4708],
        [-0.8773],
        ...,
        [-1.8691],
        [-1.8647],
        [-1.8634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260382.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0192],
        [1.0207],
        [1.0189],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368777.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0207],
        [1.0190],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368785.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005],
        [ 0.0000,  0.0003,  0.0027,  ..., -0.0006,  0.0021,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2964.5037, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.3050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-30.2050, device='cuda:0')



h[100].sum tensor(62.9795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-20.4357, device='cuda:0')



h[200].sum tensor(-3.8525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.6002, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0496, 0.0608,  ..., 0.0012, 0.0321, 0.0139],
        [0.0000, 0.0013, 0.0111,  ..., 0.0000, 0.0087, 0.0020],
        [0.0000, 0.0013, 0.0111,  ..., 0.0000, 0.0087, 0.0020],
        ...,
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0088, 0.0020],
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0088, 0.0020],
        [0.0000, 0.0013, 0.0113,  ..., 0.0000, 0.0088, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69872.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0477, 0.0000, 0.0000,  ..., 0.0917, 0.0000, 0.2844],
        [0.0151, 0.0000, 0.0000,  ..., 0.0686, 0.0000, 0.1100],
        [0.0046, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0532],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0593, 0.0000, 0.0331],
        [0.0011, 0.0000, 0.0000,  ..., 0.0593, 0.0000, 0.0331],
        [0.0011, 0.0000, 0.0000,  ..., 0.0593, 0.0000, 0.0331]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624692.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1129.2593, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.5101, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13799.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-756.6142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1426.5710, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-465.3817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2296],
        [ 0.0162],
        [-0.2031],
        ...,
        [-1.8740],
        [-1.8697],
        [-1.8683]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232948.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0207],
        [1.0190],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368785.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0207],
        [1.0190],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368794.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.9222e-03,  1.1022e-02,  1.3719e-02,  ..., -7.6578e-05,
          7.3287e-03,  3.1340e-03],
        [-9.0496e-03,  2.4988e-02,  2.8101e-02,  ...,  6.2171e-04,
          1.4116e-02,  6.5859e-03],
        [ 0.0000e+00,  3.3817e-04,  2.7184e-03,  ..., -6.1074e-04,
          2.1366e-03,  4.9355e-04],
        ...,
        [ 0.0000e+00,  3.3817e-04,  2.7184e-03,  ..., -6.1074e-04,
          2.1366e-03,  4.9355e-04],
        [ 0.0000e+00,  3.3817e-04,  2.7184e-03,  ..., -6.1074e-04,
          2.1366e-03,  4.9355e-04],
        [ 0.0000e+00,  3.3817e-04,  2.7184e-03,  ..., -6.1074e-04,
          2.1366e-03,  4.9355e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2441.4824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1325, device='cuda:0')



h[100].sum tensor(41.4533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.2679, device='cuda:0')



h[200].sum tensor(-2.3132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1195, 0.1327,  ..., 0.0034, 0.0661, 0.0312],
        [0.0000, 0.0626, 0.0741,  ..., 0.0013, 0.0385, 0.0171],
        [0.0000, 0.0653, 0.0769,  ..., 0.0015, 0.0398, 0.0178],
        ...,
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0088, 0.0020],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0088, 0.0020],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0088, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53528.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0711, 0.0000, 0.0000,  ..., 0.1080, 0.0000, 0.4338],
        [0.0496, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.3243],
        [0.0383, 0.0000, 0.0000,  ..., 0.0848, 0.0000, 0.2615],
        ...,
        [0.0008, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0335],
        [0.0008, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0335],
        [0.0008, 0.0000, 0.0000,  ..., 0.0591, 0.0000, 0.0335]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546809., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(497.6773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(211.2939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12507.2715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-553.0859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1425.5502, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.2661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4652],
        [ 0.4151],
        [ 0.2415],
        ...,
        [-1.8755],
        [-1.8712],
        [-1.8699]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227993.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0207],
        [1.0190],
        ...,
        [1.0014],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368794.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0208],
        [1.0191],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368803.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2260.0735, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8542, device='cuda:0')



h[100].sum tensor(33.5109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.3733, device='cuda:0')



h[200].sum tensor(-1.7769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0110,  ..., 0.0000, 0.0088, 0.0020],
        [0.0000, 0.0014, 0.0111,  ..., 0.0000, 0.0088, 0.0020],
        [0.0000, 0.0299, 0.0404,  ..., 0.0004, 0.0226, 0.0090],
        ...,
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0089, 0.0020],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0089, 0.0020],
        [0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0089, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49948.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0018, 0.0000, 0.0000,  ..., 0.0590, 0.0000, 0.0419],
        [0.0107, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0928],
        [0.0285, 0.0000, 0.0000,  ..., 0.0779, 0.0000, 0.1985],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.0338],
        [0.0006, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.0338],
        [0.0006, 0.0000, 0.0000,  ..., 0.0592, 0.0000, 0.0338]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538187.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(389.7986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(197.2727, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12344.0801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-508.7949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1404.7156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.8873, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3467],
        [-0.1067],
        [ 0.1532],
        ...,
        [-1.8810],
        [-1.8768],
        [-1.8755]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229728.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0208],
        [1.0191],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368803.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(498.6049, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0208],
        [1.0192],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368811.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2374.4148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.1888, device='cuda:0')



h[100].sum tensor(37.5058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.9528, device='cuda:0')



h[200].sum tensor(-2.0771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0110,  ..., 0.0000, 0.0088, 0.0019],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0088, 0.0020],
        [0.0000, 0.0296, 0.0401,  ..., 0.0004, 0.0225, 0.0089],
        ...,
        [0.0000, 0.0015, 0.0112,  ..., 0.0000, 0.0090, 0.0020],
        [0.0000, 0.0015, 0.0112,  ..., 0.0000, 0.0090, 0.0020],
        [0.0000, 0.0015, 0.0112,  ..., 0.0000, 0.0090, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53140.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0000,  ..., 0.0607, 0.0000, 0.0554],
        [0.0135, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.1118],
        [0.0279, 0.0000, 0.0000,  ..., 0.0774, 0.0000, 0.2016],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0340],
        [0.0006, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0340],
        [0.0006, 0.0000, 0.0000,  ..., 0.0594, 0.0000, 0.0340]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(552149.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(532.3685, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(213.0181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12531.1680, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-548.0381, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1412.5916, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.6627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2750],
        [ 0.0340],
        [ 0.2568],
        ...,
        [-1.8576],
        [-1.8650],
        [-1.8711]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243736.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0208],
        [1.0192],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368811.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0209],
        [1.0193],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368820.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2433.3042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.3878, device='cuda:0')



h[100].sum tensor(39.1406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.7640, device='cuda:0')



h[200].sum tensor(-2.2193, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0110,  ..., 0.0000, 0.0088, 0.0019],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0089, 0.0019],
        [0.0000, 0.0015, 0.0111,  ..., 0.0000, 0.0089, 0.0019],
        ...,
        [0.0000, 0.0015, 0.0112,  ..., 0.0000, 0.0090, 0.0020],
        [0.0000, 0.0015, 0.0112,  ..., 0.0000, 0.0090, 0.0020],
        [0.0000, 0.0015, 0.0112,  ..., 0.0000, 0.0090, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54374.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0099, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.0908],
        [0.0111, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0995],
        [0.0111, 0.0000, 0.0000,  ..., 0.0661, 0.0000, 0.0993],
        ...,
        [0.0005, 0.0000, 0.0000,  ..., 0.0597, 0.0000, 0.0342],
        [0.0005, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.0342],
        [0.0005, 0.0000, 0.0000,  ..., 0.0596, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556190.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(520.6590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(218.9897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12573.2754, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-563.6313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1405.9829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-396.1794, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0119],
        [ 0.1328],
        [ 0.2144],
        ...,
        [-1.8976],
        [-1.8933],
        [-1.8921]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243842.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0209],
        [1.0193],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368820.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0193],
        [1.0209],
        [1.0194],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368828.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.8183e-03,  1.0834e-02,  1.3503e-02,  ..., -6.2266e-05,
          7.2632e-03,  3.0562e-03],
        [ 0.0000e+00,  3.6595e-04,  2.7228e-03,  ..., -5.8536e-04,
          2.1818e-03,  4.6511e-04],
        [-3.8183e-03,  1.0834e-02,  1.3503e-02,  ..., -6.2266e-05,
          7.2632e-03,  3.0562e-03],
        ...,
        [ 0.0000e+00,  3.6595e-04,  2.7228e-03,  ..., -5.8536e-04,
          2.1818e-03,  4.6511e-04],
        [ 0.0000e+00,  3.6595e-04,  2.7228e-03,  ..., -5.8536e-04,
          2.1818e-03,  4.6511e-04],
        [ 0.0000e+00,  3.6595e-04,  2.7228e-03,  ..., -5.8536e-04,
          2.1818e-03,  4.6511e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2410.4380, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9232, device='cuda:0')



h[100].sum tensor(37.9899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.4497, device='cuda:0')



h[200].sum tensor(-2.1426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6199, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0178, 0.0279,  ..., 0.0002, 0.0168, 0.0059],
        [0.0000, 0.0557, 0.0669,  ..., 0.0004, 0.0352, 0.0153],
        [0.0000, 0.0179, 0.0281,  ..., 0.0002, 0.0169, 0.0060],
        ...,
        [0.0000, 0.0015, 0.0113,  ..., 0.0000, 0.0090, 0.0019],
        [0.0000, 0.0015, 0.0113,  ..., 0.0000, 0.0090, 0.0019],
        [0.0000, 0.0015, 0.0113,  ..., 0.0000, 0.0090, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53894.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0124, 0.0000, 0.0000,  ..., 0.0665, 0.0000, 0.1201],
        [0.0193, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.1680],
        [0.0128, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.1245],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0602, 0.0000, 0.0344],
        [0.0006, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0344],
        [0.0006, 0.0000, 0.0000,  ..., 0.0601, 0.0000, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(556038.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(456.5108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(217.0597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12585.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-555.3471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1374.8306, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.9050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3288],
        [-0.0996],
        [-0.1434],
        ...,
        [-1.9065],
        [-1.9020],
        [-1.8985]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220415.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0193],
        [1.0209],
        [1.0194],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368828.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0210],
        [1.0196],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368837.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.4919e-03,  2.3661e-02,  2.6759e-02,  ...,  5.8503e-04,
          1.3506e-02,  6.2268e-03],
        [-3.9023e-03,  1.1059e-02,  1.3780e-02,  ..., -4.4529e-05,
          7.3906e-03,  3.1060e-03],
        [ 0.0000e+00,  3.4388e-04,  2.7436e-03,  ..., -5.7981e-04,
          2.1909e-03,  4.5251e-04],
        ...,
        [ 0.0000e+00,  3.4388e-04,  2.7436e-03,  ..., -5.7981e-04,
          2.1909e-03,  4.5251e-04],
        [ 0.0000e+00,  3.4388e-04,  2.7436e-03,  ..., -5.7981e-04,
          2.1909e-03,  4.5251e-04],
        [ 0.0000e+00,  3.4388e-04,  2.7436e-03,  ..., -5.7981e-04,
          2.1909e-03,  4.5251e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2276.0283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2031, device='cuda:0')



h[100].sum tensor(33.4102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.6093, device='cuda:0')



h[200].sum tensor(-1.7907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.3951, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0316, 0.0423,  ..., 0.0004, 0.0236, 0.0093],
        [0.0000, 0.0341, 0.0449,  ..., 0.0006, 0.0248, 0.0099],
        [0.0000, 0.0552, 0.0667,  ..., 0.0005, 0.0351, 0.0152],
        ...,
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0019],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0019],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49496.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0285, 0.0000, 0.0000,  ..., 0.0784, 0.0000, 0.2028],
        [0.0249, 0.0000, 0.0000,  ..., 0.0760, 0.0000, 0.1916],
        [0.0267, 0.0000, 0.0000,  ..., 0.0770, 0.0000, 0.2123],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0343],
        [0.0006, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0342],
        [0.0006, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0342]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535445.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(344.1219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(195.1058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12190., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-497.9739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1457.4458, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-379.0471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4175],
        [ 0.4182],
        [ 0.4150],
        ...,
        [-1.9332],
        [-1.9288],
        [-1.9275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253645.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0210],
        [1.0196],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368837.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0194],
        [1.0211],
        [1.0197],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368845.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0055,  0.0154,  0.0183,  ...,  0.0002,  0.0095,  0.0042],
        [-0.0074,  0.0207,  0.0238,  ...,  0.0005,  0.0121,  0.0055],
        [-0.0073,  0.0204,  0.0234,  ...,  0.0004,  0.0119,  0.0054],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2809.4724, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-18.4470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.9877, device='cuda:0')



h[100].sum tensor(54.4854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.5824, device='cuda:0')



h[200].sum tensor(-3.2463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.7013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0346, 0.0455,  ..., 0.0005, 0.0251, 0.0100],
        [0.0000, 0.0542, 0.0658,  ..., 0.0009, 0.0347, 0.0149],
        [0.0000, 0.1170, 0.1304,  ..., 0.0035, 0.0651, 0.0304],
        ...,
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66658.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0312, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.2248],
        [0.0471, 0.0000, 0.0000,  ..., 0.0921, 0.0000, 0.3028],
        [0.0732, 0.0000, 0.0000,  ..., 0.1104, 0.0000, 0.4238],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0617, 0.0000, 0.0343],
        [0.0006, 0.0000, 0.0000,  ..., 0.0617, 0.0000, 0.0343],
        [0.0006, 0.0000, 0.0000,  ..., 0.0617, 0.0000, 0.0343]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619888.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(992.9647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(272.6826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13575.1416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-710.6207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1435.1613, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-438.3830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3281],
        [ 0.3631],
        [ 0.3495],
        ...,
        [-1.9514],
        [-1.9469],
        [-1.9456]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-271780.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0194],
        [1.0211],
        [1.0197],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368845.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0198],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368853.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [-0.0065,  0.0182,  0.0211,  ...,  0.0003,  0.0109,  0.0049],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2288.8350, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7342, device='cuda:0')



h[100].sum tensor(33.9287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.9687, device='cuda:0')



h[200].sum tensor(-1.8181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0112,  ..., 0.0000, 0.0090, 0.0018],
        [0.0000, 0.0195, 0.0300,  ..., 0.0003, 0.0179, 0.0063],
        [0.0000, 0.0162, 0.0267,  ..., 0.0002, 0.0163, 0.0055],
        ...,
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49947.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0000, 0.0000,  ..., 0.0634, 0.0000, 0.0557],
        [0.0125, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.1019],
        [0.0226, 0.0000, 0.0000,  ..., 0.0763, 0.0000, 0.1582],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346],
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346],
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(539849.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(419.6954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(200.2872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12230.5098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-500.2077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1500.5491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.3030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0591],
        [-0.8841],
        [-0.4990],
        ...,
        [-1.9588],
        [-1.9543],
        [-1.9492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-280081.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0198],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368853.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0198],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368853.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0098,  0.0272,  0.0304,  ...,  0.0008,  0.0153,  0.0071],
        [-0.0226,  0.0625,  0.0668,  ...,  0.0026,  0.0324,  0.0158],
        [-0.0092,  0.0257,  0.0289,  ...,  0.0007,  0.0145,  0.0067],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0006,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2455.4985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1386, device='cuda:0')



h[100].sum tensor(40.3942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.2720, device='cuda:0')



h[200].sum tensor(-2.2671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1672, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1343, 0.1482,  ..., 0.0045, 0.0735, 0.0347],
        [0.0000, 0.1065, 0.1196,  ..., 0.0030, 0.0601, 0.0278],
        [0.0000, 0.1085, 0.1217,  ..., 0.0037, 0.0610, 0.0283],
        ...,
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54946.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0754, 0.0000, 0.0000,  ..., 0.1121, 0.0000, 0.4335],
        [0.0740, 0.0000, 0.0000,  ..., 0.1115, 0.0000, 0.4265],
        [0.0671, 0.0000, 0.0000,  ..., 0.1069, 0.0000, 0.3847],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346],
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346],
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560341.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(504.0665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(223.0853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12632.2246, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.3432, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1413.8159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.7515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3616],
        [ 0.3045],
        [ 0.1511],
        ...,
        [-1.9609],
        [-1.9564],
        [-1.9551]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255706.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0198],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368853.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0198],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368853.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1123e-03,  1.9919e-02,  2.2946e-02,  ...,  4.3008e-04,
          1.1719e-02,  5.2916e-03],
        [-4.1045e-03,  1.1634e-02,  1.4412e-02,  ...,  1.5125e-05,
          7.7002e-03,  3.2381e-03],
        [ 0.0000e+00,  3.2800e-04,  2.7655e-03,  ..., -5.5115e-04,
          2.2156e-03,  4.3590e-04],
        ...,
        [ 0.0000e+00,  3.2800e-04,  2.7655e-03,  ..., -5.5115e-04,
          2.2156e-03,  4.3590e-04],
        [ 0.0000e+00,  3.2800e-04,  2.7655e-03,  ..., -5.5115e-04,
          2.2156e-03,  4.3590e-04],
        [ 0.0000e+00,  3.2800e-04,  2.7655e-03,  ..., -5.5115e-04,
          2.2156e-03,  4.3590e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2337.5293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.7287, device='cuda:0')



h[100].sum tensor(35.8177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.6415, device='cuda:0')



h[200].sum tensor(-1.9493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.7804e-02, 7.9714e-02,  ..., 1.0881e-03, 4.1253e-02,
         1.8247e-02],
        [0.0000e+00, 3.7650e-02, 4.8694e-02,  ..., 6.9432e-04, 2.6660e-02,
         1.0779e-02],
        [0.0000e+00, 1.2887e-02, 2.3201e-02,  ..., 1.5446e-05, 1.4660e-02,
         4.6440e-03],
        ...,
        [0.0000e+00, 1.3579e-03, 1.1449e-02,  ..., 0.0000e+00, 9.1728e-03,
         1.8046e-03],
        [0.0000e+00, 1.3578e-03, 1.1448e-02,  ..., 0.0000e+00, 9.1719e-03,
         1.8045e-03],
        [0.0000e+00, 1.3576e-03, 1.1447e-02,  ..., 0.0000e+00, 9.1708e-03,
         1.8043e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51581.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0308, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.2279],
        [0.0221, 0.0000, 0.0000,  ..., 0.0754, 0.0000, 0.1719],
        [0.0111, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.1044],
        ...,
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346],
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346],
        [0.0006, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0346]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546556.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(449.1802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(207.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12373.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-520.5305, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1442.9968, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.4562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2668],
        [ 0.0795],
        [-0.2271],
        ...,
        [-1.9609],
        [-1.9564],
        [-1.9551]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262609.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0198],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368853.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0199],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368862.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2288.0928, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.3764, device='cuda:0')



h[100].sum tensor(33.1856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.7266, device='cuda:0')



h[200].sum tensor(-1.7879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.4732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0090, 0.0018],
        [0.0000, 0.0014, 0.0113,  ..., 0.0000, 0.0091, 0.0018],
        [0.0000, 0.0014, 0.0113,  ..., 0.0000, 0.0091, 0.0018],
        ...,
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49554.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0007, 0.0000, 0.0000,  ..., 0.0610, 0.0000, 0.0342],
        [0.0007, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0343],
        [0.0007, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.0345],
        ...,
        [0.0007, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0350],
        [0.0007, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0350],
        [0.0007, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0350]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(536615.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(370.7437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(203.4499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12227.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-493.3131, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1433.3596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.4236, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1567],
        [-1.5287],
        [-1.7876],
        ...,
        [-1.9653],
        [-1.9608],
        [-1.9596]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255889.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0195],
        [1.0212],
        [1.0199],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368862.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0196],
        [1.0213],
        [1.0200],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368871., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2433.5159, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6230, device='cuda:0')



h[100].sum tensor(38.0999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.9232, device='cuda:0')



h[200].sum tensor(-2.1434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0112,  ..., 0.0000, 0.0091, 0.0018],
        [0.0000, 0.0014, 0.0113,  ..., 0.0000, 0.0091, 0.0018],
        [0.0000, 0.0014, 0.0113,  ..., 0.0000, 0.0091, 0.0018],
        ...,
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0092, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55008.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0009, 0.0000, 0.0000,  ..., 0.0611, 0.0000, 0.0346],
        [0.0008, 0.0000, 0.0000,  ..., 0.0613, 0.0000, 0.0357],
        [0.0007, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0381],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0624, 0.0000, 0.0403],
        [0.0022, 0.0000, 0.0000,  ..., 0.0629, 0.0000, 0.0498],
        [0.0027, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0546]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(567098., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(545.7960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(233.1651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12795.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-559.3188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1372.9276, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.5911, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9816],
        [-1.8169],
        [-1.5739],
        ...,
        [-1.8131],
        [-1.6921],
        [-1.6087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-221824.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0196],
        [1.0213],
        [1.0200],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368871., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(394.8071, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0196],
        [1.0213],
        [1.0201],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368879.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [-0.0051,  0.0144,  0.0172,  ...,  0.0002,  0.0090,  0.0039],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2179.6296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.8879, device='cuda:0')



h[100].sum tensor(28.0742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.0430, device='cuda:0')



h[200].sum tensor(-1.4591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.3527, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0113,  ..., 0.0000, 0.0091, 0.0018],
        [0.0000, 0.0157, 0.0261,  ..., 0.0002, 0.0161, 0.0054],
        [0.0000, 0.0492, 0.0605,  ..., 0.0014, 0.0323, 0.0137],
        ...,
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0093, 0.0018],
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0092, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47510.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0073, 0.0000, 0.0000,  ..., 0.0654, 0.0000, 0.0791],
        [0.0170, 0.0000, 0.0000,  ..., 0.0724, 0.0000, 0.1310],
        [0.0417, 0.0000, 0.0000,  ..., 0.0896, 0.0000, 0.2573],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0626, 0.0000, 0.0357],
        [0.0013, 0.0000, 0.0000,  ..., 0.0626, 0.0000, 0.0357],
        [0.0013, 0.0000, 0.0000,  ..., 0.0626, 0.0000, 0.0357]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(535214.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(483.1660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(201.0819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12216.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-463.4828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1490.9056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.4454, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1287],
        [ 0.0212],
        [ 0.2072],
        ...,
        [-1.9829],
        [-1.9784],
        [-1.9771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264301.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0196],
        [1.0213],
        [1.0201],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368879.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0197],
        [1.0214],
        [1.0201],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368887.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0153,  0.0428,  0.0465,  ...,  0.0017,  0.0228,  0.0110],
        [-0.0140,  0.0392,  0.0428,  ...,  0.0015,  0.0211,  0.0101],
        [-0.0131,  0.0367,  0.0403,  ...,  0.0014,  0.0199,  0.0095],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0005,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2501.2295, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.3025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.8830, device='cuda:0')



h[100].sum tensor(40.7995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.7756, device='cuda:0')



h[200].sum tensor(-2.2955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0914, 0.1041,  ..., 0.0031, 0.0527, 0.0241],
        [0.0000, 0.1293, 0.1431,  ..., 0.0046, 0.0711, 0.0335],
        [0.0000, 0.1400, 0.1542,  ..., 0.0051, 0.0763, 0.0362],
        ...,
        [0.0000, 0.0014, 0.0116,  ..., 0.0000, 0.0093, 0.0018],
        [0.0000, 0.0014, 0.0116,  ..., 0.0000, 0.0093, 0.0018],
        [0.0000, 0.0014, 0.0116,  ..., 0.0000, 0.0093, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56270.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0824, 0.0000, 0.0000,  ..., 0.1182, 0.0000, 0.4687],
        [0.0926, 0.0000, 0.0000,  ..., 0.1256, 0.0000, 0.5223],
        [0.0954, 0.0000, 0.0000,  ..., 0.1277, 0.0000, 0.5389],
        ...,
        [0.0015, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0355],
        [0.0015, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0355],
        [0.0015, 0.0000, 0.0000,  ..., 0.0632, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(574441.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(784.4861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(239.7885, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12951.4258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-570.6771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1441.6677, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.0425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3318],
        [ 0.3321],
        [ 0.3350],
        ...,
        [-2.0020],
        [-1.9975],
        [-1.9962]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255131.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0197],
        [1.0214],
        [1.0201],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368887.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0214],
        [1.0202],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368895.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.6729e-03,  2.1625e-02,  2.4748e-02,  ...,  6.4444e-04,
          1.2566e-02,  5.7253e-03],
        [-3.3449e-03,  9.6069e-03,  1.2369e-02,  ...,  3.2109e-05,
          6.7390e-03,  2.7413e-03],
        [-3.2449e-03,  9.3291e-03,  1.2083e-02,  ...,  1.7952e-05,
          6.6043e-03,  2.6723e-03],
        ...,
        [ 0.0000e+00,  3.1819e-04,  2.8017e-03,  ..., -4.4114e-04,
          2.2356e-03,  4.3499e-04],
        [ 0.0000e+00,  3.1819e-04,  2.8017e-03,  ..., -4.4114e-04,
          2.2356e-03,  4.3499e-04],
        [ 0.0000e+00,  3.1819e-04,  2.8017e-03,  ..., -4.4114e-04,
          2.2356e-03,  4.3499e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2325.9863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0963, device='cuda:0')



h[100].sum tensor(34.2597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.2136, device='cuda:0')



h[200].sum tensor(-1.8267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.7973, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0438, 0.0551,  ..., 0.0004, 0.0297, 0.0123],
        [0.0000, 0.0600, 0.0719,  ..., 0.0012, 0.0376, 0.0163],
        [0.0000, 0.0496, 0.0612,  ..., 0.0007, 0.0326, 0.0138],
        ...,
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0093, 0.0018],
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0093, 0.0018],
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0093, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51289.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0322, 0.0000, 0.0000,  ..., 0.0828, 0.0000, 0.2473],
        [0.0349, 0.0000, 0.0000,  ..., 0.0850, 0.0000, 0.2600],
        [0.0306, 0.0000, 0.0000,  ..., 0.0820, 0.0000, 0.2349],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0355],
        [0.0018, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0355],
        [0.0018, 0.0000, 0.0000,  ..., 0.0635, 0.0000, 0.0354]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549937.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(683.6031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(216.2929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12546.5801, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-506.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1501.0715, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-383.6671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4523],
        [ 0.4538],
        [ 0.4556],
        ...,
        [-2.0188],
        [-2.0142],
        [-2.0129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272261.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0214],
        [1.0202],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368895.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0215],
        [1.0203],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368903.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0108,  0.0305,  0.0338,  ...,  0.0011,  0.0168,  0.0079],
        [-0.0049,  0.0139,  0.0168,  ...,  0.0003,  0.0088,  0.0038],
        [-0.0054,  0.0153,  0.0183,  ...,  0.0004,  0.0095,  0.0042],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0004],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2374.9543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9620, device='cuda:0')



h[100].sum tensor(35.7544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7993, device='cuda:0')



h[200].sum tensor(-1.9216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0765, 0.0889,  ..., 0.0022, 0.0456, 0.0205],
        [0.0000, 0.0837, 0.0963,  ..., 0.0025, 0.0491, 0.0223],
        [0.0000, 0.0500, 0.0616,  ..., 0.0012, 0.0327, 0.0139],
        ...,
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0092, 0.0018],
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0092, 0.0018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52358.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0401, 0.0000, 0.0000,  ..., 0.0889, 0.0000, 0.2605],
        [0.0410, 0.0000, 0.0000,  ..., 0.0898, 0.0000, 0.2626],
        [0.0336, 0.0000, 0.0000,  ..., 0.0848, 0.0000, 0.2230],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0355],
        [0.0020, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0355],
        [0.0020, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0355]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(554207., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(692.6497, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(222.0014, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12665.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-519.1765, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1493.5276, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.8781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4089],
        [ 0.3219],
        [ 0.1379],
        ...,
        [-2.0297],
        [-2.0251],
        [-2.0237]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258841.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0215],
        [1.0203],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368903.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0198],
        [1.0216],
        [1.0203],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368912.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.6935e-03,  1.6188e-02,  1.9141e-02,  ...,  4.2267e-04,
          9.9205e-03,  4.4046e-03],
        [-3.6890e-03,  1.0603e-02,  1.3389e-02,  ...,  1.3581e-04,
          7.2135e-03,  3.0166e-03],
        [-2.8621e-03,  8.2995e-03,  1.1017e-02,  ...,  1.7470e-05,
          6.0968e-03,  2.4440e-03],
        ...,
        [ 0.0000e+00,  3.2536e-04,  2.8049e-03,  ..., -3.9214e-04,
          2.2314e-03,  4.6206e-04],
        [ 0.0000e+00,  3.2536e-04,  2.8049e-03,  ..., -3.9214e-04,
          2.2314e-03,  4.6206e-04],
        [ 0.0000e+00,  3.2536e-04,  2.8049e-03,  ..., -3.9214e-04,
          2.2314e-03,  4.6206e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2654.8481, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.1626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.8602, device='cuda:0')



h[100].sum tensor(45.8427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.7899, device='cuda:0')



h[200].sum tensor(-2.5856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0564, 0.0682,  ..., 0.0016, 0.0358, 0.0156],
        [0.0000, 0.0484, 0.0600,  ..., 0.0012, 0.0320, 0.0136],
        [0.0000, 0.0844, 0.0971,  ..., 0.0027, 0.0494, 0.0225],
        ...,
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0092, 0.0019],
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0092, 0.0019],
        [0.0000, 0.0013, 0.0116,  ..., 0.0000, 0.0092, 0.0019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59891.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0624, 0.0000, 0.0000,  ..., 0.1043, 0.0000, 0.3750],
        [0.0484, 0.0000, 0.0000,  ..., 0.0946, 0.0000, 0.3114],
        [0.0528, 0.0000, 0.0000,  ..., 0.0978, 0.0000, 0.3374],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0359],
        [0.0020, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0359],
        [0.0020, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0358]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587041.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1010.6908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(259.6620, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13236.5303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-613.8575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1474.9988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-412.4659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2821],
        [ 0.3089],
        [ 0.3241],
        ...,
        [-2.0323],
        [-2.0278],
        [-2.0264]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273461.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0198],
        [1.0216],
        [1.0203],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368912.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0199],
        [1.0217],
        [1.0204],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368921.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0004,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2270.1079, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9595, device='cuda:0')



h[100].sum tensor(30.4443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7679, device='cuda:0')



h[200].sum tensor(-1.5473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0584, 0.0701,  ..., 0.0022, 0.0367, 0.0162],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0020],
        [0.0000, 0.0014, 0.0114,  ..., 0.0000, 0.0091, 0.0020],
        ...,
        [0.0000, 0.0014, 0.0116,  ..., 0.0000, 0.0092, 0.0020],
        [0.0000, 0.0014, 0.0116,  ..., 0.0000, 0.0092, 0.0020],
        [0.0000, 0.0014, 0.0116,  ..., 0.0000, 0.0092, 0.0020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48854.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0386, 0.0000, 0.0000,  ..., 0.0874, 0.0000, 0.2411],
        [0.0142, 0.0000, 0.0000,  ..., 0.0705, 0.0000, 0.1094],
        [0.0064, 0.0000, 0.0000,  ..., 0.0651, 0.0000, 0.0654],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0629, 0.0000, 0.0363],
        [0.0019, 0.0000, 0.0000,  ..., 0.0629, 0.0000, 0.0363],
        [0.0019, 0.0000, 0.0000,  ..., 0.0628, 0.0000, 0.0363]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(538095.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(633.8673, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(214.5884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12446.0527, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-475.3308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1474.8416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.8372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1711],
        [-0.1781],
        [-0.6377],
        ...,
        [-2.0281],
        [-2.0228],
        [-2.0201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269504., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0199],
        [1.0217],
        [1.0204],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368921.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0199],
        [1.0218],
        [1.0205],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368929.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2677.0537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.1432, device='cuda:0')



h[100].sum tensor(44.7194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.3048, device='cuda:0')



h[200].sum tensor(-2.5002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0113,  ..., 0.0000, 0.0091, 0.0021],
        [0.0000, 0.0015, 0.0114,  ..., 0.0000, 0.0091, 0.0021],
        [0.0000, 0.0303, 0.0411,  ..., 0.0008, 0.0231, 0.0093],
        ...,
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0021],
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0021],
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61198.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0053, 0.0000, 0.0000,  ..., 0.0635, 0.0000, 0.0582],
        [0.0087, 0.0000, 0.0000,  ..., 0.0661, 0.0000, 0.0797],
        [0.0216, 0.0000, 0.0000,  ..., 0.0750, 0.0000, 0.1624],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0623, 0.0000, 0.0368],
        [0.0019, 0.0000, 0.0000,  ..., 0.0623, 0.0000, 0.0368],
        [0.0019, 0.0000, 0.0000,  ..., 0.0623, 0.0000, 0.0368]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(599945.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(954.3531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(276.4718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13632.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.2695, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1308.7295, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.3666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0123],
        [-0.7796],
        [-0.3105],
        ...,
        [-2.0257],
        [-2.0214],
        [-2.0201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-197540.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0199],
        [1.0218],
        [1.0205],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368929.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0200],
        [1.0218],
        [1.0205],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368938.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032,  0.0094,  0.0121,  ...,  0.0001,  0.0066,  0.0028],
        [-0.0034,  0.0099,  0.0126,  ...,  0.0002,  0.0068,  0.0029],
        [-0.0066,  0.0189,  0.0219,  ...,  0.0006,  0.0112,  0.0052],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2799.9973, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.3083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.3312, device='cuda:0')



h[100].sum tensor(48.7098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.7852, device='cuda:0')



h[200].sum tensor(-2.7479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.5052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0328, 0.0436,  ..., 0.0010, 0.0242, 0.0100],
        [0.0000, 0.0654, 0.0771,  ..., 0.0020, 0.0400, 0.0181],
        [0.0000, 0.0531, 0.0645,  ..., 0.0014, 0.0341, 0.0150],
        ...,
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64732.3945, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0335, 0.0000, 0.0000,  ..., 0.0825, 0.0000, 0.2338],
        [0.0341, 0.0000, 0.0000,  ..., 0.0830, 0.0000, 0.2477],
        [0.0359, 0.0000, 0.0000,  ..., 0.0843, 0.0000, 0.2572],
        ...,
        [0.0022, 0.0000, 0.0000,  ..., 0.0622, 0.0000, 0.0371],
        [0.0022, 0.0000, 0.0000,  ..., 0.0622, 0.0000, 0.0371],
        [0.0022, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0371]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621931.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1247.7504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.7825, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13952.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-672.6067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1338.4070, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.9388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4454],
        [ 0.4645],
        [ 0.4703],
        ...,
        [-2.0292],
        [-2.0241],
        [-2.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-227560.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0200],
        [1.0218],
        [1.0205],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368938.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0201],
        [1.0219],
        [1.0206],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368946.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0006],
        [-0.0064,  0.0182,  0.0211,  ...,  0.0006,  0.0108,  0.0050],
        [-0.0137,  0.0389,  0.0424,  ...,  0.0017,  0.0209,  0.0102],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0003,  0.0022,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2547.8413, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6696, device='cuda:0')



h[100].sum tensor(38.2473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.9547, device='cuda:0')



h[200].sum tensor(-2.0592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0472, 0.0584,  ..., 0.0018, 0.0311, 0.0137],
        [0.0000, 0.0815, 0.0937,  ..., 0.0033, 0.0477, 0.0222],
        [0.0000, 0.0649, 0.0766,  ..., 0.0024, 0.0397, 0.0181],
        ...,
        [0.0000, 0.0016, 0.0115,  ..., 0.0000, 0.0091, 0.0023],
        [0.0000, 0.0016, 0.0115,  ..., 0.0000, 0.0091, 0.0023],
        [0.0000, 0.0016, 0.0115,  ..., 0.0000, 0.0091, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55353.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0430, 0.0000, 0.0000,  ..., 0.0891, 0.0000, 0.2704],
        [0.0556, 0.0000, 0.0000,  ..., 0.0981, 0.0000, 0.3365],
        [0.0513, 0.0000, 0.0000,  ..., 0.0951, 0.0000, 0.3169],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0374],
        [0.0024, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0374],
        [0.0024, 0.0000, 0.0000,  ..., 0.0620, 0.0000, 0.0374]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568335.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(901.6196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(254.9916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13063.5293, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.8379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1379.5396, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.4341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3846],
        [ 0.3923],
        [ 0.3827],
        ...,
        [-2.0359],
        [-2.0315],
        [-2.0302]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220006.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0201],
        [1.0219],
        [1.0206],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368946.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0201],
        [1.0220],
        [1.0207],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368954.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        [-0.0062,  0.0179,  0.0208,  ...,  0.0007,  0.0107,  0.0049],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2801.4058, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.7721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.7733, device='cuda:0')



h[100].sum tensor(47.1940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.4077, device='cuda:0')



h[200].sum tensor(-2.6364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2540, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0661, 0.0778,  ..., 0.0024, 0.0403, 0.0184],
        [0.0000, 0.0161, 0.0264,  ..., 0.0005, 0.0161, 0.0060],
        [0.0000, 0.0194, 0.0298,  ..., 0.0007, 0.0177, 0.0068],
        ...,
        [0.0000, 0.0016, 0.0115,  ..., 0.0000, 0.0091, 0.0024],
        [0.0000, 0.0016, 0.0115,  ..., 0.0000, 0.0091, 0.0024],
        [0.0000, 0.0016, 0.0115,  ..., 0.0000, 0.0091, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61739.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0264, 0.0000, 0.0000,  ..., 0.0774, 0.0000, 0.1902],
        [0.0168, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.1311],
        [0.0222, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.1595],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0376],
        [0.0026, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0376],
        [0.0026, 0.0000, 0.0000,  ..., 0.0621, 0.0000, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596908.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1155.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(285.4491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13540.4854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-634.9797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1383.0662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.3275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3166],
        [ 0.3208],
        [ 0.3336],
        ...,
        [-2.0459],
        [-2.0415],
        [-2.0402]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231745.7656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0201],
        [1.0220],
        [1.0207],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368954.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(498.4945, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0202],
        [1.0220],
        [1.0208],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368961.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0002,  0.0022,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2627.5808, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7931, device='cuda:0')



h[100].sum tensor(40.1195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.7148, device='cuda:0')



h[200].sum tensor(-2.1692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0113,  ..., 0.0000, 0.0090, 0.0023],
        [0.0000, 0.0015, 0.0114,  ..., 0.0000, 0.0090, 0.0023],
        [0.0000, 0.0015, 0.0114,  ..., 0.0000, 0.0090, 0.0023],
        ...,
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0091, 0.0024],
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0091, 0.0024],
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0091, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57578.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0612, 0.0000, 0.0368],
        [0.0025, 0.0000, 0.0000,  ..., 0.0615, 0.0000, 0.0369],
        [0.0026, 0.0000, 0.0000,  ..., 0.0615, 0.0000, 0.0371],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0623, 0.0000, 0.0377],
        [0.0028, 0.0000, 0.0000,  ..., 0.0625, 0.0000, 0.0398],
        [0.0035, 0.0000, 0.0000,  ..., 0.0629, 0.0000, 0.0468]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(580936.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1033.3171, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.6182, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13280.1162, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-583.3883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1413.4250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-402.9211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9633],
        [-2.1034],
        [-2.2088],
        ...,
        [-2.0381],
        [-1.9972],
        [-1.9150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-229467.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0202],
        [1.0220],
        [1.0208],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368961.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0220],
        [1.0208],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368969.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0112,  0.0318,  0.0352,  ...,  0.0016,  0.0175,  0.0084],
        [-0.0067,  0.0192,  0.0222,  ...,  0.0009,  0.0113,  0.0053],
        [-0.0033,  0.0096,  0.0123,  ...,  0.0004,  0.0067,  0.0029],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0001,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0001,  0.0022,  0.0006],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0001,  0.0022,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2406.7678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9394, device='cuda:0')



h[100].sum tensor(31.5967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.4309, device='cuda:0')



h[200].sum tensor(-1.6054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0938, 0.1063,  ..., 0.0044, 0.0537, 0.0253],
        [0.0000, 0.0915, 0.1040,  ..., 0.0043, 0.0526, 0.0247],
        [0.0000, 0.0809, 0.0932,  ..., 0.0037, 0.0475, 0.0221],
        ...,
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0023],
        [0.0000, 0.0015, 0.0116,  ..., 0.0000, 0.0092, 0.0023],
        [0.0000, 0.0015, 0.0115,  ..., 0.0000, 0.0092, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50276.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0782, 0.0000, 0.0000,  ..., 0.1153, 0.0000, 0.4647],
        [0.0700, 0.0000, 0.0000,  ..., 0.1097, 0.0000, 0.4274],
        [0.0632, 0.0000, 0.0000,  ..., 0.1049, 0.0000, 0.3966],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0627, 0.0000, 0.0378],
        [0.0026, 0.0000, 0.0000,  ..., 0.0628, 0.0000, 0.0378],
        [0.0026, 0.0000, 0.0000,  ..., 0.0627, 0.0000, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546535.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(781.7861, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(234.5378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12661.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-492.3821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1495.1426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.9784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3697],
        [ 0.3949],
        [ 0.4113],
        ...,
        [-2.0705],
        [-2.0661],
        [-2.0647]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245680.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0220],
        [1.0208],
        ...,
        [1.0013],
        [1.0007],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368969.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0220],
        [1.0209],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368977.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.9965e-03,  2.5751e-02,  2.8925e-02,  ...,  1.3165e-03,
          1.4529e-02,  6.8703e-03],
        [-9.4560e-03,  2.7048e-02,  3.0260e-02,  ...,  1.3863e-03,
          1.5157e-02,  7.1934e-03],
        [-1.2867e-02,  3.6677e-02,  4.0171e-02,  ...,  1.9045e-03,
          1.9823e-02,  9.5922e-03],
        ...,
        [ 0.0000e+00,  3.5650e-04,  2.7863e-03,  ..., -4.9910e-05,
          2.2227e-03,  5.4405e-04],
        [ 0.0000e+00,  3.5650e-04,  2.7863e-03,  ..., -4.9910e-05,
          2.2227e-03,  5.4405e-04],
        [ 0.0000e+00,  3.5650e-04,  2.7863e-03,  ..., -4.9910e-05,
          2.2227e-03,  5.4405e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2785.4644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.0594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.6370, device='cuda:0')



h[100].sum tensor(45.6640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.6389, device='cuda:0')



h[200].sum tensor(-2.4870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1439, 0.1580,  ..., 0.0075, 0.0781, 0.0377],
        [0.0000, 0.1080, 0.1210,  ..., 0.0056, 0.0607, 0.0288],
        [0.0000, 0.0592, 0.0708,  ..., 0.0030, 0.0371, 0.0166],
        ...,
        [0.0000, 0.0015, 0.0116,  ..., 0.0000, 0.0092, 0.0023],
        [0.0000, 0.0015, 0.0116,  ..., 0.0000, 0.0092, 0.0023],
        [0.0000, 0.0132, 0.0237,  ..., 0.0006, 0.0149, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61526.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1129, 0.0000, 0.0000,  ..., 0.1417, 0.0000, 0.6202],
        [0.0853, 0.0000, 0.0000,  ..., 0.1220, 0.0000, 0.4823],
        [0.0560, 0.0000, 0.0000,  ..., 0.1010, 0.0000, 0.3341],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0635, 0.0000, 0.0406],
        [0.0049, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.0551],
        [0.0123, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.1039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600920.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1021.1356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.9185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13680.6543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-633.1781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1368.7200, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-424.6415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3815],
        [ 0.4089],
        [ 0.4320],
        ...,
        [-1.8867],
        [-1.5278],
        [-0.9441]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-210984.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0220],
        [1.0209],
        ...,
        [1.0013],
        [1.0006],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368977.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0203],
        [1.0220],
        [1.0210],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368985.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-9.4838e-03,  2.7150e-02,  3.0394e-02,  ...,  1.4982e-03,
          1.5230e-02,  7.2025e-03],
        [-5.5067e-03,  1.5905e-02,  1.8821e-02,  ...,  8.8797e-04,
          9.7801e-03,  4.4018e-03],
        [-3.2747e-03,  9.5950e-03,  1.2326e-02,  ...,  5.4553e-04,
          6.7217e-03,  2.8300e-03],
        ...,
        [ 0.0000e+00,  3.3662e-04,  2.7963e-03,  ...,  4.3111e-05,
          2.2347e-03,  5.2402e-04],
        [ 0.0000e+00,  3.3662e-04,  2.7963e-03,  ...,  4.3111e-05,
          2.2347e-03,  5.2402e-04],
        [ 0.0000e+00,  3.3662e-04,  2.7963e-03,  ...,  4.3111e-05,
          2.2347e-03,  5.2402e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2508.2402, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8529, device='cuda:0')



h[100].sum tensor(35.0504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7256, device='cuda:0')



h[200].sum tensor(-1.8072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0812, 0.0936,  ..., 0.0045, 0.0478, 0.0220],
        [0.0000, 0.0666, 0.0786,  ..., 0.0037, 0.0408, 0.0184],
        [0.0000, 0.0392, 0.0504,  ..., 0.0022, 0.0275, 0.0116],
        ...,
        [0.0000, 0.0014, 0.0116,  ..., 0.0002, 0.0093, 0.0022],
        [0.0000, 0.0014, 0.0116,  ..., 0.0002, 0.0093, 0.0022],
        [0.0000, 0.0014, 0.0116,  ..., 0.0002, 0.0093, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53039.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0395, 0.0000, 0.0000,  ..., 0.0896, 0.0000, 0.2731],
        [0.0341, 0.0000, 0.0000,  ..., 0.0857, 0.0000, 0.2512],
        [0.0254, 0.0000, 0.0000,  ..., 0.0792, 0.0000, 0.2146],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0640, 0.0000, 0.0380],
        [0.0023, 0.0000, 0.0000,  ..., 0.0640, 0.0000, 0.0380],
        [0.0023, 0.0000, 0.0000,  ..., 0.0640, 0.0000, 0.0380]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(560905.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(795.7967, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(243.6834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12926.0791, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-529.7899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1481.1045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.0878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4729],
        [ 0.4889],
        [ 0.4436],
        ...,
        [-2.1054],
        [-2.1010],
        [-2.0997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-259192.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0203],
        [1.0220],
        [1.0210],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368985.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0220],
        [1.0210],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(368993.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0003,  0.0028,  ...,  0.0001,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ...,  0.0001,  0.0022,  0.0005],
        [-0.0090,  0.0257,  0.0289,  ...,  0.0015,  0.0145,  0.0068],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ...,  0.0001,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ...,  0.0001,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ...,  0.0001,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2583.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.4988, device='cuda:0')



h[100].sum tensor(37.6219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.8391, device='cuda:0')



h[200].sum tensor(-1.9682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8791, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0114,  ..., 0.0004, 0.0091, 0.0021],
        [0.0000, 0.0374, 0.0486,  ..., 0.0024, 0.0266, 0.0111],
        [0.0000, 0.0717, 0.0839,  ..., 0.0043, 0.0433, 0.0196],
        ...,
        [0.0000, 0.0013, 0.0116,  ..., 0.0004, 0.0093, 0.0021],
        [0.0000, 0.0013, 0.0116,  ..., 0.0004, 0.0093, 0.0021],
        [0.0000, 0.0013, 0.0116,  ..., 0.0004, 0.0093, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54586.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0096, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0844],
        [0.0288, 0.0000, 0.0000,  ..., 0.0832, 0.0000, 0.1954],
        [0.0523, 0.0000, 0.0000,  ..., 0.1005, 0.0000, 0.3232],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0381],
        [0.0021, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0382],
        [0.0021, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0381]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570232.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(721.4088, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(248.1941, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13107.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-551.6339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1461.8344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.3061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1607],
        [-0.5983],
        [-0.0892],
        ...,
        [-2.1242],
        [-2.1195],
        [-2.1179]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254940.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0220],
        [1.0210],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(368993.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0220],
        [1.0211],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369001.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.5430e-03,  2.1720e-02,  2.4814e-02,  ...,  1.1963e-03,
          1.2619e-02,  5.8298e-03],
        [-6.7417e-03,  1.9448e-02,  2.2475e-02,  ...,  1.0723e-03,
          1.1518e-02,  5.2635e-03],
        [-6.8273e-03,  1.9690e-02,  2.2724e-02,  ...,  1.0856e-03,
          1.1635e-02,  5.3240e-03],
        ...,
        [ 0.0000e+00,  3.2477e-04,  2.7943e-03,  ...,  2.8856e-05,
          2.2508e-03,  4.9922e-04],
        [ 0.0000e+00,  3.2477e-04,  2.7943e-03,  ...,  2.8856e-05,
          2.2508e-03,  4.9922e-04],
        [ 0.0000e+00,  3.2477e-04,  2.7943e-03,  ...,  2.8856e-05,
          2.2508e-03,  4.9922e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2735.4204, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4992, device='cuda:0')



h[100].sum tensor(43.1025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.8691, device='cuda:0')



h[200].sum tensor(-2.2989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0869, 0.0994,  ..., 0.0048, 0.0506, 0.0233],
        [0.0000, 0.0589, 0.0707,  ..., 0.0033, 0.0371, 0.0164],
        [0.0000, 0.0610, 0.0728,  ..., 0.0034, 0.0381, 0.0169],
        ...,
        [0.0000, 0.0013, 0.0116,  ..., 0.0001, 0.0093, 0.0021],
        [0.0000, 0.0013, 0.0116,  ..., 0.0001, 0.0093, 0.0021],
        [0.0000, 0.0013, 0.0116,  ..., 0.0001, 0.0093, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60962.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0566, 0.0000, 0.0000,  ..., 0.1033, 0.0000, 0.3510],
        [0.0392, 0.0000, 0.0000,  ..., 0.0908, 0.0000, 0.2702],
        [0.0328, 0.0000, 0.0000,  ..., 0.0862, 0.0000, 0.2369],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0381],
        [0.0019, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.0381],
        [0.0019, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0381]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(608028.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(976.2951, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(280.9624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13678.2705, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-630.5577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1438.2546, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.2422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2129],
        [ 0.1187],
        [-0.0130],
        ...,
        [-2.1317],
        [-2.1270],
        [-2.1256]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253056.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0220],
        [1.0211],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369001.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0204],
        [1.0220],
        [1.0211],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369009.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.9364e-03,  1.1521e-02,  1.4289e-02,  ...,  5.1351e-04,
          7.6682e-03,  3.2874e-03],
        [-2.6027e-03,  7.7321e-03,  1.0390e-02,  ...,  3.0710e-04,
          5.8324e-03,  2.3429e-03],
        [-6.5391e-03,  1.8916e-02,  2.1899e-02,  ...,  9.1634e-04,
          1.1251e-02,  5.1304e-03],
        ...,
        [ 0.0000e+00,  3.3752e-04,  2.7797e-03,  ..., -9.5724e-05,
          2.2497e-03,  4.9990e-04],
        [ 0.0000e+00,  3.3752e-04,  2.7797e-03,  ..., -9.5724e-05,
          2.2497e-03,  4.9990e-04],
        [ 0.0000e+00,  3.3752e-04,  2.7797e-03,  ..., -9.5724e-05,
          2.2497e-03,  4.9990e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2428.9924, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9472, device='cuda:0')



h[100].sum tensor(31.6015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.4362, device='cuda:0')



h[200].sum tensor(-1.5587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0327, 0.0435,  ..., 0.0015, 0.0243, 0.0098],
        [0.0000, 0.0811, 0.0934,  ..., 0.0040, 0.0478, 0.0219],
        [0.0000, 0.0483, 0.0597,  ..., 0.0023, 0.0320, 0.0137],
        ...,
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0093, 0.0021],
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0093, 0.0021],
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0093, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50576.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0259, 0.0000, 0.0000,  ..., 0.0806, 0.0000, 0.1947],
        [0.0395, 0.0000, 0.0000,  ..., 0.0905, 0.0000, 0.2760],
        [0.0337, 0.0000, 0.0000,  ..., 0.0865, 0.0000, 0.2452],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.0381],
        [0.0018, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.0381],
        [0.0018, 0.0000, 0.0000,  ..., 0.0649, 0.0000, 0.0381]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555603.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(645.3402, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(237.1830, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12705.9590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-499.0959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1499.0247, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.0365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0749],
        [ 0.3273],
        [ 0.3470],
        ...,
        [-2.1351],
        [-2.1305],
        [-2.1291]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285952.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0204],
        [1.0220],
        [1.0211],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369009.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369018.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0078,  0.0225,  0.0256,  ...,  0.0009,  0.0130,  0.0060],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0003,  0.0022,  0.0005],
        [ 0.0000,  0.0003,  0.0028,  ..., -0.0003,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2874.8628, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.8043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.8448, device='cuda:0')



h[100].sum tensor(48.4548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.4560, device='cuda:0')



h[200].sum tensor(-2.5576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0198, 0.0302,  ..., 0.0007, 0.0180, 0.0067],
        [0.0000, 0.0240, 0.0346,  ..., 0.0010, 0.0201, 0.0077],
        [0.0000, 0.0014, 0.0113,  ..., 0.0000, 0.0092, 0.0021],
        ...,
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0093, 0.0021],
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0093, 0.0021],
        [0.0000, 0.0014, 0.0115,  ..., 0.0000, 0.0093, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63724.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0267, 0.0000, 0.0000,  ..., 0.0808, 0.0000, 0.1835],
        [0.0179, 0.0000, 0.0000,  ..., 0.0750, 0.0000, 0.1347],
        [0.0115, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0996],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0379],
        [0.0018, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0379],
        [0.0018, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0379]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621248.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1064.0408, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.4699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13887.6309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-661.0737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1375.7366, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-422.0842, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3049],
        [ 0.2470],
        [ 0.1843],
        ...,
        [-2.1402],
        [-2.1357],
        [-2.1343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255648.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369018.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369027.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2628.8232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.5695, device='cuda:0')



h[100].sum tensor(39.5243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.8869, device='cuda:0')



h[200].sum tensor(-1.9603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4826e-03, 1.1215e-02,  ..., 0.0000e+00, 9.0464e-03,
         2.1376e-03],
        [0.0000e+00, 1.4891e-03, 1.1264e-02,  ..., 0.0000e+00, 9.0864e-03,
         2.1471e-03],
        [0.0000e+00, 1.0634e-02, 2.0691e-02,  ..., 5.6406e-05, 1.3527e-02,
         4.4326e-03],
        ...,
        [0.0000e+00, 1.5114e-03, 1.1433e-02,  ..., 0.0000e+00, 9.2223e-03,
         2.1792e-03],
        [0.0000e+00, 1.5116e-03, 1.1434e-02,  ..., 0.0000e+00, 9.2234e-03,
         2.1794e-03],
        [0.0000e+00, 1.5113e-03, 1.1432e-02,  ..., 0.0000e+00, 9.2217e-03,
         2.1790e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56639.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0020, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0389],
        [0.0037, 0.0000, 0.0000,  ..., 0.0650, 0.0000, 0.0516],
        [0.0079, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0837],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.0378],
        [0.0018, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.0378],
        [0.0018, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(582662.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(774.7605, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(270.5077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13284.6191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-569.2211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1328.8048, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.6790, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0655],
        [-1.7163],
        [-1.2041],
        ...,
        [-2.1429],
        [-2.1385],
        [-2.1371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243579.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369027.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369027.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [-0.0065,  0.0190,  0.0219,  ...,  0.0006,  0.0112,  0.0052],
        [-0.0038,  0.0113,  0.0140,  ...,  0.0002,  0.0075,  0.0032],
        ...,
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0028,  ..., -0.0004,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2760.7222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.0270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.6284, device='cuda:0')



h[100].sum tensor(44.3835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.9565, device='cuda:0')



h[200].sum tensor(-2.2608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2882, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0461, 0.0571,  ..., 0.0007, 0.0306, 0.0133],
        [0.0000, 0.0282, 0.0388,  ..., 0.0002, 0.0220, 0.0088],
        [0.0000, 0.0716, 0.0834,  ..., 0.0021, 0.0430, 0.0197],
        ...,
        [0.0000, 0.0015, 0.0114,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0015, 0.0114,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0015, 0.0114,  ..., 0.0000, 0.0092, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61164.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0194, 0.0000, 0.0000,  ..., 0.0742, 0.0000, 0.1704],
        [0.0220, 0.0000, 0.0000,  ..., 0.0762, 0.0000, 0.1852],
        [0.0414, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.2877],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.0378],
        [0.0018, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.0378],
        [0.0018, 0.0000, 0.0000,  ..., 0.0647, 0.0000, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(607484.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(938.7804, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(291.1173, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13728.1621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-625.9254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1276.2595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.9924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3635],
        [ 0.3820],
        [ 0.4122],
        ...,
        [-2.1430],
        [-2.1385],
        [-2.1371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232203.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369027.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(463.7126, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369035.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2448.4824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8057, device='cuda:0')



h[100].sum tensor(32.9276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.3405, device='cuda:0')



h[200].sum tensor(-1.5201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2162, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0090, 0.0022],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0091, 0.0022],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0091, 0.0022],
        ...,
        [0.0000, 0.0016, 0.0114,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0016, 0.0114,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0016, 0.0114,  ..., 0.0000, 0.0092, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50697.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0000, 0.0000,  ..., 0.0631, 0.0000, 0.0405],
        [0.0017, 0.0000, 0.0000,  ..., 0.0635, 0.0000, 0.0374],
        [0.0017, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0375],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0378],
        [0.0017, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0378],
        [0.0017, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(555267.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(565.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(246.9323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12796.5596, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-492.0371, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1344.5310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.7557, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4310],
        [-1.3990],
        [-1.2708],
        ...,
        [-2.1436],
        [-2.1392],
        [-2.1377]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256850., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369035.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369035.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0006,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3271.0801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.0532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-31.2479, device='cuda:0')



h[100].sum tensor(63.1649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-21.1413, device='cuda:0')



h[200].sum tensor(-3.3793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.0698, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0090, 0.0022],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0091, 0.0022],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0091, 0.0022],
        ...,
        [0.0000, 0.0016, 0.0114,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0016, 0.0114,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0016, 0.0114,  ..., 0.0000, 0.0092, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73259.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0017, 0.0000, 0.0000,  ..., 0.0633, 0.0000, 0.0368],
        [0.0017, 0.0000, 0.0000,  ..., 0.0636, 0.0000, 0.0370],
        [0.0017, 0.0000, 0.0000,  ..., 0.0637, 0.0000, 0.0371],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0378],
        [0.0017, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0378],
        [0.0017, 0.0000, 0.0000,  ..., 0.0645, 0.0000, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661894.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1398.8486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(349.1539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14596.5049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-774.7288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1289.3667, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-446.8883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1718],
        [-2.2217],
        [-2.2110],
        ...,
        [-2.1436],
        [-2.1392],
        [-2.1378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249685.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0212],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369035.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0213],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369043.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0022,  0.0005],
        [-0.0033,  0.0099,  0.0125,  ..., -0.0002,  0.0068,  0.0029],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0007,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2358.5222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.0219, device='cuda:0')



h[100].sum tensor(29.8536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.1336, device='cuda:0')



h[200].sum tensor(-1.3022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.4130, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0090, 0.0022],
        [0.0000, 0.0113, 0.0211,  ..., 0.0000, 0.0138, 0.0046],
        [0.0000, 0.0317, 0.0421,  ..., 0.0004, 0.0237, 0.0097],
        ...,
        [0.0000, 0.0016, 0.0113,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0016, 0.0113,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0016, 0.0113,  ..., 0.0000, 0.0092, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48355.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0058, 0.0000, 0.0000,  ..., 0.0659, 0.0000, 0.0636],
        [0.0140, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.1185],
        [0.0258, 0.0000, 0.0000,  ..., 0.0786, 0.0000, 0.1921],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0378],
        [0.0017, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0378],
        [0.0017, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(546020.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(530.7808, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(238.5862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12636.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-459.9442, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1338.2361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-355.1060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3910],
        [-0.0744],
        [ 0.1818],
        ...,
        [-2.1501],
        [-2.1457],
        [-2.1442]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263168.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0220],
        [1.0213],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369043.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0205],
        [1.0221],
        [1.0214],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369051.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0142,  0.0411,  0.0446,  ...,  0.0014,  0.0219,  0.0107],
        [-0.0121,  0.0350,  0.0383,  ...,  0.0011,  0.0189,  0.0092],
        [-0.0116,  0.0335,  0.0368,  ...,  0.0010,  0.0182,  0.0088],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0008,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0008,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0008,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2562.5718, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0268, device='cuda:0')



h[100].sum tensor(37.6604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8432, device='cuda:0')



h[200].sum tensor(-1.7434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1110, 0.1235,  ..., 0.0027, 0.0619, 0.0295],
        [0.0000, 0.1536, 0.1674,  ..., 0.0049, 0.0826, 0.0402],
        [0.0000, 0.1077, 0.1202,  ..., 0.0025, 0.0604, 0.0287],
        ...,
        [0.0000, 0.0017, 0.0113,  ..., 0.0000, 0.0092, 0.0023],
        [0.0000, 0.0017, 0.0113,  ..., 0.0000, 0.0092, 0.0023],
        [0.0000, 0.0017, 0.0113,  ..., 0.0000, 0.0092, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54061.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0646, 0.0000, 0.0000,  ..., 0.1036, 0.0000, 0.3920],
        [0.0777, 0.0000, 0.0000,  ..., 0.1127, 0.0000, 0.4536],
        [0.0652, 0.0000, 0.0000,  ..., 0.1046, 0.0000, 0.3915],
        ...,
        [0.0018, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0377],
        [0.0018, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0377],
        [0.0018, 0.0000, 0.0000,  ..., 0.0646, 0.0000, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(570628.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(684.0579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.5061, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13116.2334, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-527.8055, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1256.0068, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-377.4384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4396],
        [ 0.4241],
        [ 0.4066],
        ...,
        [-2.1572],
        [-2.1527],
        [-2.1513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248309.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0205],
        [1.0221],
        [1.0214],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369051.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0214],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369059.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0009,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0009,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0009,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0009,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0009,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0009,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2828.6421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3549, device='cuda:0')



h[100].sum tensor(47.8471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.4481, device='cuda:0')



h[200].sum tensor(-2.3254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6153, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0090, 0.0022],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0091, 0.0022],
        [0.0000, 0.0247, 0.0348,  ..., 0.0000, 0.0202, 0.0080],
        ...,
        [0.0000, 0.0017, 0.0112,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0017, 0.0112,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0017, 0.0112,  ..., 0.0000, 0.0092, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63095.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0584],
        [0.0096, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.0933],
        [0.0198, 0.0000, 0.0000,  ..., 0.0747, 0.0000, 0.1595],
        ...,
        [0.0019, 0.0000, 0.0000,  ..., 0.0650, 0.0000, 0.0376],
        [0.0019, 0.0000, 0.0000,  ..., 0.0650, 0.0000, 0.0376],
        [0.0019, 0.0000, 0.0000,  ..., 0.0650, 0.0000, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618425.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(944.3810, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.7672, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14034.7275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-637.9512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1159.5520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-418.5961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6917],
        [-0.2563],
        [ 0.0797],
        ...,
        [-2.1695],
        [-2.1648],
        [-2.1631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-199925.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0214],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369059.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0215],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369067.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.0424e-02,  3.0358e-02,  3.3522e-02,  ...,  6.5136e-04,
          1.6719e-02,  8.0213e-03],
        [-5.9502e-03,  1.7499e-02,  2.0297e-02,  ..., -3.2973e-05,
          1.0498e-02,  4.8073e-03],
        [-8.4919e-03,  2.4805e-02,  2.7811e-02,  ...,  3.5585e-04,
          1.4033e-02,  6.6334e-03],
        ...,
        [ 0.0000e+00,  3.9432e-04,  2.7055e-03,  ..., -9.4325e-04,
          2.2227e-03,  5.3206e-04],
        [-5.1575e-03,  1.5220e-02,  1.7954e-02,  ..., -1.5424e-04,
          9.3956e-03,  4.2377e-03],
        [-5.1575e-03,  1.5220e-02,  1.7954e-02,  ..., -1.5424e-04,
          9.3956e-03,  4.2377e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2578.1028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3877, device='cuda:0')



h[100].sum tensor(38.9646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.0874, device='cuda:0')



h[200].sum tensor(-1.7708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0439, 0.0545,  ..., 0.0000, 0.0295, 0.0127],
        [0.0000, 0.0929, 0.1050,  ..., 0.0014, 0.0533, 0.0250],
        [0.0000, 0.0539, 0.0648,  ..., 0.0004, 0.0344, 0.0152],
        ...,
        [0.0000, 0.0296, 0.0400,  ..., 0.0000, 0.0228, 0.0092],
        [0.0000, 0.0296, 0.0400,  ..., 0.0000, 0.0228, 0.0092],
        [0.0000, 0.0296, 0.0400,  ..., 0.0000, 0.0227, 0.0092]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53848.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0343, 0.0000, 0.0000,  ..., 0.0842, 0.0000, 0.2356],
        [0.0432, 0.0000, 0.0000,  ..., 0.0904, 0.0000, 0.2793],
        [0.0347, 0.0000, 0.0000,  ..., 0.0849, 0.0000, 0.2354],
        ...,
        [0.0151, 0.0000, 0.0000,  ..., 0.0732, 0.0000, 0.1244],
        [0.0179, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.1427],
        [0.0179, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.1427]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(568807.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(750.4745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(267.7180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13072.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-520.2014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1273.5763, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.9835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3262],
        [ 0.3504],
        [ 0.3394],
        ...,
        [-1.2636],
        [-1.0222],
        [-1.0213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263613.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0206],
        [1.0221],
        [1.0215],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369067.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0207],
        [1.0222],
        [1.0215],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369074.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0010,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0010,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0010,  0.0022,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0010,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0010,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0010,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2447.6143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8761, device='cuda:0')



h[100].sum tensor(34.4477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.3881, device='cuda:0')



h[200].sum tensor(-1.4866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0091, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0091, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0091, 0.0021],
        ...,
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0093, 0.0022],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0093, 0.0022],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0093, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50658.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0000,  ..., 0.0648, 0.0000, 0.0365],
        [0.0022, 0.0000, 0.0000,  ..., 0.0650, 0.0000, 0.0374],
        [0.0029, 0.0000, 0.0000,  ..., 0.0653, 0.0000, 0.0468],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0661, 0.0000, 0.0375],
        [0.0024, 0.0000, 0.0000,  ..., 0.0661, 0.0000, 0.0375],
        [0.0024, 0.0000, 0.0000,  ..., 0.0661, 0.0000, 0.0375]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(557170.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(726.0048, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(252.5397, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12835.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-478.7166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1317.7751, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.0600, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1317],
        [-1.8818],
        [-1.5201],
        ...,
        [-2.2044],
        [-2.1997],
        [-2.1982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-284010.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0207],
        [1.0222],
        [1.0215],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369074.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0208],
        [1.0222],
        [1.0215],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369082.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0022,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0022,  0.0005],
        ...,
        [-0.0026,  0.0079,  0.0104,  ..., -0.0007,  0.0059,  0.0024],
        [-0.0043,  0.0128,  0.0155,  ..., -0.0004,  0.0082,  0.0036],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0022,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2571.2954, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4604, device='cuda:0')



h[100].sum tensor(38.7873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1366, device='cuda:0')



h[200].sum tensor(-1.7475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.4115, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0091, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0092, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0092, 0.0021],
        ...,
        [0.0000, 0.0718, 0.0834,  ..., 0.0000, 0.0433, 0.0197],
        [0.0000, 0.0263, 0.0366,  ..., 0.0000, 0.0212, 0.0083],
        [0.0000, 0.0145, 0.0245,  ..., 0.0000, 0.0155, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56129.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0076, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.0715],
        [0.0032, 0.0000, 0.0000,  ..., 0.0659, 0.0000, 0.0435],
        [0.0025, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0368],
        ...,
        [0.0376, 0.0000, 0.0000,  ..., 0.0885, 0.0000, 0.2537],
        [0.0246, 0.0000, 0.0000,  ..., 0.0803, 0.0000, 0.1769],
        [0.0137, 0.0000, 0.0000,  ..., 0.0735, 0.0000, 0.1083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590768.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(953.1489, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(277.7556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13430.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-547.1949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1308.8289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.0699, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7282],
        [-1.3227],
        [-1.7936],
        ...,
        [ 0.1740],
        [-0.2656],
        [-0.9330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265623., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0208],
        [1.0222],
        [1.0215],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369082.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0222],
        [1.0215],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369090., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0023,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0011,  0.0023,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2375.3921, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4716, device='cuda:0')



h[100].sum tensor(31.3973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.4379, device='cuda:0')



h[200].sum tensor(-1.3162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.6155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0092, 0.0021],
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0092, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0092, 0.0021],
        ...,
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0093, 0.0021],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0093, 0.0021],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0093, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49224.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0366],
        [0.0025, 0.0000, 0.0000,  ..., 0.0659, 0.0000, 0.0368],
        [0.0025, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0369],
        ...,
        [0.0055, 0.0000, 0.0000,  ..., 0.0685, 0.0000, 0.0594],
        [0.0035, 0.0000, 0.0000,  ..., 0.0674, 0.0000, 0.0449],
        [0.0026, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0376]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(553400.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(681.8349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(246.5989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12745.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-460.9193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1350.2375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-357.8064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0347],
        [-1.8302],
        [-1.5200],
        ...,
        [-1.6820],
        [-1.9377],
        [-2.1058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275786.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0222],
        [1.0215],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369090., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0209],
        [1.0223],
        [1.0216],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369097.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  3.8406e-04,  2.6973e-03,  ..., -1.1779e-03,
          2.2600e-03,  5.0443e-04],
        [ 0.0000e+00,  3.8406e-04,  2.6973e-03,  ..., -1.1779e-03,
          2.2600e-03,  5.0443e-04],
        [ 0.0000e+00,  3.8406e-04,  2.6973e-03,  ..., -1.1779e-03,
          2.2600e-03,  5.0443e-04],
        ...,
        [ 0.0000e+00,  3.8406e-04,  2.6973e-03,  ..., -1.1779e-03,
          2.2600e-03,  5.0443e-04],
        [ 0.0000e+00,  3.8406e-04,  2.6973e-03,  ..., -1.1779e-03,
          2.2600e-03,  5.0443e-04],
        [-7.5950e-03,  2.2361e-02,  2.5293e-02,  ..., -3.8390e-06,
          1.2894e-02,  5.9986e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3192.4170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-19.5652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-28.9715, device='cuda:0')



h[100].sum tensor(60.4376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-19.6011, device='cuda:0')



h[200].sum tensor(-3.0385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.0448, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0092, 0.0021],
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0092, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0093, 0.0021],
        ...,
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0094, 0.0021],
        [0.0000, 0.0244, 0.0346,  ..., 0.0000, 0.0204, 0.0078],
        [0.0000, 0.0431, 0.0538,  ..., 0.0000, 0.0295, 0.0125]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(73380.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0026, 0.0000, 0.0000,  ..., 0.0659, 0.0000, 0.0366],
        [0.0046, 0.0000, 0.0000,  ..., 0.0674, 0.0000, 0.0514],
        [0.0131, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.1006],
        ...,
        [0.0075, 0.0000, 0.0000,  ..., 0.0704, 0.0000, 0.0651],
        [0.0208, 0.0000, 0.0000,  ..., 0.0792, 0.0000, 0.1367],
        [0.0369, 0.0000, 0.0000,  ..., 0.0899, 0.0000, 0.2237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(674706., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1531.4681, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(356.1223, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14863.6221, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-765.0007, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1226.6011, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-451.6976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7551],
        [-1.2683],
        [-0.5952],
        ...,
        [-1.6772],
        [-1.0981],
        [-0.4656]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242122.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0209],
        [1.0223],
        [1.0216],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369097.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(442.8877, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0210],
        [1.0223],
        [1.0217],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369105.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0012,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0012,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0012,  0.0023,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0012,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0012,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0012,  0.0023,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2356.0957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.9618, device='cuda:0')



h[100].sum tensor(29.9328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.0930, device='cuda:0')



h[200].sum tensor(-1.2514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.3860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0128, 0.0225,  ..., 0.0000, 0.0147, 0.0048],
        [0.0000, 0.0072, 0.0168,  ..., 0.0000, 0.0120, 0.0035],
        [0.0000, 0.0516, 0.0625,  ..., 0.0005, 0.0335, 0.0146],
        ...,
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0094, 0.0021],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0094, 0.0021],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0094, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48014.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0071, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0922],
        [0.0135, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.1206],
        [0.0347, 0.0000, 0.0000,  ..., 0.0871, 0.0000, 0.2285],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0377],
        [0.0026, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0377],
        [0.0026, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(549445.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(716.7175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(239.4739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(12587.7324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-447.7352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1407.7720, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-346.0813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8710],
        [-0.3175],
        [ 0.1170],
        ...,
        [-2.2518],
        [-2.2469],
        [-2.2452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-310074.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0210],
        [1.0223],
        [1.0217],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369105.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0211],
        [1.0224],
        [1.0217],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369113.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3088.9390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.9838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-26.7271, device='cuda:0')



h[100].sum tensor(56.1133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.0827, device='cuda:0')



h[200].sum tensor(-2.7697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.0343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0320, 0.0423,  ..., 0.0000, 0.0240, 0.0096],
        [0.0000, 0.0332, 0.0435,  ..., 0.0000, 0.0246, 0.0099],
        [0.0000, 0.0125, 0.0222,  ..., 0.0000, 0.0146, 0.0048],
        ...,
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0095, 0.0021],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0095, 0.0021],
        [0.0000, 0.0016, 0.0112,  ..., 0.0000, 0.0095, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68180.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0191, 0.0000, 0.0000,  ..., 0.0760, 0.0000, 0.1542],
        [0.0195, 0.0000, 0.0000,  ..., 0.0766, 0.0000, 0.1552],
        [0.0129, 0.0000, 0.0000,  ..., 0.0726, 0.0000, 0.1121],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0377],
        [0.0027, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0377],
        [0.0027, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(649463.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1272.0647, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(333.2547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14437.6201, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-701.6808, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1239.0745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-433.7873, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6343],
        [-0.7124],
        [-1.0466],
        ...,
        [-2.2632],
        [-2.2582],
        [-2.2564]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-220852.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0211],
        [1.0224],
        [1.0217],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369113.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0225],
        [1.0218],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369121.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0033,  0.0100,  0.0126,  ..., -0.0008,  0.0069,  0.0029],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3043.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.3358, device='cuda:0')



h[100].sum tensor(54.3572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.1414, device='cuda:0')



h[200].sum tensor(-2.6371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.4078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0338, 0.0440,  ..., 0.0000, 0.0248, 0.0101],
        [0.0000, 0.0114, 0.0211,  ..., 0.0000, 0.0141, 0.0046],
        [0.0000, 0.0016, 0.0110,  ..., 0.0000, 0.0093, 0.0021],
        ...,
        [0.0000, 0.0017, 0.0111,  ..., 0.0000, 0.0094, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0094, 0.0021],
        [0.0000, 0.0016, 0.0111,  ..., 0.0000, 0.0094, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70357.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0235, 0.0000, 0.0000,  ..., 0.0786, 0.0000, 0.1758],
        [0.0157, 0.0000, 0.0000,  ..., 0.0741, 0.0000, 0.1261],
        [0.0138, 0.0000, 0.0000,  ..., 0.0733, 0.0000, 0.1069],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0674, 0.0000, 0.0377],
        [0.0029, 0.0000, 0.0000,  ..., 0.0674, 0.0000, 0.0377],
        [0.0029, 0.0000, 0.0000,  ..., 0.0674, 0.0000, 0.0377]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672001.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1616.3369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(343.7408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14713.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-726.6707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1343.7369, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-429.5826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1758],
        [-0.0124],
        [-0.1719],
        ...,
        [-2.2644],
        [-2.2593],
        [-2.2575]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-268555.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0225],
        [1.0218],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369121.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0225],
        [1.0218],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369129.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0061,  0.0181,  0.0209,  ..., -0.0004,  0.0108,  0.0050],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0013,  0.0023,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2621.2732, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7256, device='cuda:0')



h[100].sum tensor(39.2649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.3160, device='cuda:0')



h[200].sum tensor(-1.7308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5309, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0845, 0.0960,  ..., 0.0015, 0.0493, 0.0229],
        [0.0000, 0.0322, 0.0423,  ..., 0.0000, 0.0241, 0.0098],
        [0.0000, 0.0017, 0.0109,  ..., 0.0000, 0.0093, 0.0022],
        ...,
        [0.0000, 0.0017, 0.0111,  ..., 0.0000, 0.0094, 0.0022],
        [0.0000, 0.0017, 0.0111,  ..., 0.0000, 0.0094, 0.0022],
        [0.0000, 0.0017, 0.0111,  ..., 0.0000, 0.0094, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56741.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0707, 0.0000, 0.0000,  ..., 0.1100, 0.0000, 0.4049],
        [0.0491, 0.0000, 0.0000,  ..., 0.0960, 0.0000, 0.2949],
        [0.0351, 0.0000, 0.0000,  ..., 0.0870, 0.0000, 0.2182],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0380],
        [0.0029, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0380],
        [0.0029, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0379]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(594661.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(913.2738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(285.8430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13511.2197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-554.6777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1311.3662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.3040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3462],
        [ 0.3441],
        [ 0.3437],
        ...,
        [-2.2728],
        [-2.2677],
        [-2.2660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-232506.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0225],
        [1.0218],
        ...,
        [1.0012],
        [1.0006],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369129.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0226],
        [1.0219],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369137.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0005],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0005],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3077.0498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-25.7600, device='cuda:0')



h[100].sum tensor(56.0854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-17.4283, device='cuda:0')



h[200].sum tensor(-2.6404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.5988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0108,  ..., 0.0000, 0.0092, 0.0022],
        [0.0000, 0.0017, 0.0109,  ..., 0.0000, 0.0093, 0.0022],
        [0.0000, 0.0017, 0.0109,  ..., 0.0000, 0.0093, 0.0022],
        ...,
        [0.0000, 0.0017, 0.0111,  ..., 0.0000, 0.0094, 0.0022],
        [0.0000, 0.0017, 0.0111,  ..., 0.0000, 0.0094, 0.0022],
        [0.0000, 0.0017, 0.0111,  ..., 0.0000, 0.0094, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71885.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0655, 0.0000, 0.0391],
        [0.0029, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0378],
        [0.0030, 0.0000, 0.0000,  ..., 0.0663, 0.0000, 0.0373],
        ...,
        [0.0031, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0380],
        [0.0031, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0380],
        [0.0031, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0380]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689094.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1791.6960, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(356.4389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15017.7666, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-739.9159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1305.8533, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-434.0347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3885],
        [-1.7154],
        [-1.9960],
        ...,
        [-2.2840],
        [-2.2789],
        [-2.2771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267633.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0226],
        [1.0219],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369137.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0212],
        [1.0226],
        [1.0219],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369145.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0082,  0.0107,  ..., -0.0010,  0.0060,  0.0025],
        [-0.0074,  0.0220,  0.0249,  ..., -0.0003,  0.0127,  0.0060],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2637.8264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0747, device='cuda:0')



h[100].sum tensor(40.8778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.5522, device='cuda:0')



h[200].sum tensor(-1.7209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1193, 0.1317,  ..., 0.0010, 0.0661, 0.0317],
        [0.0000, 0.0342, 0.0443,  ..., 0.0000, 0.0250, 0.0104],
        [0.0000, 0.0238, 0.0336,  ..., 0.0000, 0.0200, 0.0078],
        ...,
        [0.0000, 0.0017, 0.0110,  ..., 0.0000, 0.0094, 0.0023],
        [0.0000, 0.0017, 0.0110,  ..., 0.0000, 0.0094, 0.0023],
        [0.0000, 0.0017, 0.0110,  ..., 0.0000, 0.0094, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55444.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0578, 0.0000, 0.0000,  ..., 0.1000, 0.0000, 0.3424],
        [0.0335, 0.0000, 0.0000,  ..., 0.0846, 0.0000, 0.2143],
        [0.0200, 0.0000, 0.0000,  ..., 0.0763, 0.0000, 0.1380],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0382],
        [0.0032, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0382],
        [0.0032, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0382]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(585896.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(957.2762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.5546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13328.6582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-530.4009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1342.3192, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.0703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3752],
        [ 0.0868],
        [-0.3837],
        ...,
        [-2.2822],
        [-2.2840],
        [-2.2837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-252480.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0212],
        [1.0226],
        [1.0219],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369145.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0227],
        [1.0219],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369153.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006],
        [-0.0041,  0.0124,  0.0150,  ..., -0.0008,  0.0080,  0.0036],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0027,  ..., -0.0014,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2878.0601, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.4706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.9424, device='cuda:0')



h[100].sum tensor(49.7890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.8455, device='cuda:0')



h[200].sum tensor(-2.1823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.8799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 4.6024e-02, 5.6373e-02,  ..., 0.0000e+00, 3.0626e-02,
         1.3420e-02],
        [0.0000e+00, 1.1702e-02, 2.1137e-02,  ..., 0.0000e+00, 1.4066e-02,
         4.8238e-03],
        [0.0000e+00, 4.2862e-02, 5.3176e-02,  ..., 5.2228e-05, 2.9145e-02,
         1.2638e-02],
        ...,
        [0.0000e+00, 1.7215e-03, 1.1025e-02,  ..., 0.0000e+00, 9.3750e-03,
         2.3521e-03],
        [0.0000e+00, 1.7212e-03, 1.1024e-02,  ..., 0.0000e+00, 9.3736e-03,
         2.3517e-03],
        [0.0000e+00, 1.7206e-03, 1.1020e-02,  ..., 0.0000e+00, 9.3705e-03,
         2.3510e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60246.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0000, 0.0000,  ..., 0.0724, 0.0000, 0.1268],
        [0.0198, 0.0000, 0.0000,  ..., 0.0757, 0.0000, 0.1364],
        [0.0375, 0.0000, 0.0000,  ..., 0.0872, 0.0000, 0.2253],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0383],
        [0.0035, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0382],
        [0.0035, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0382]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601563.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1183.6106, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(308.8686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13589.5986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-586.6292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1287.4430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.1274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0779],
        [-0.5965],
        [-0.0918],
        ...,
        [-2.3010],
        [-2.2959],
        [-2.2941]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255746.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0227],
        [1.0219],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369153.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0213],
        [1.0227],
        [1.0220],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369161.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036,  0.0111,  0.0136,  ..., -0.0009,  0.0074,  0.0032],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2587.6899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.6450, device='cuda:0')



h[100].sum tensor(39.4849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5849, device='cuda:0')



h[200].sum tensor(-1.5678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0351, 0.0450,  ..., 0.0000, 0.0253, 0.0107],
        [0.0000, 0.0126, 0.0220,  ..., 0.0000, 0.0145, 0.0051],
        [0.0000, 0.0017, 0.0108,  ..., 0.0000, 0.0092, 0.0024],
        ...,
        [0.0000, 0.0018, 0.0110,  ..., 0.0000, 0.0094, 0.0024],
        [0.0000, 0.0018, 0.0110,  ..., 0.0000, 0.0094, 0.0024],
        [0.0000, 0.0018, 0.0110,  ..., 0.0000, 0.0094, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55055.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0433, 0.0000, 0.0000,  ..., 0.0899, 0.0000, 0.2617],
        [0.0249, 0.0000, 0.0000,  ..., 0.0787, 0.0000, 0.1626],
        [0.0161, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.1084],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0668, 0.0000, 0.0385],
        [0.0034, 0.0000, 0.0000,  ..., 0.0668, 0.0000, 0.0385],
        [0.0034, 0.0000, 0.0000,  ..., 0.0668, 0.0000, 0.0385]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(586614.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1027.5850, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(289.6198, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13337.7646, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-519.6942, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1312.9573, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.7726, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2372],
        [ 0.2014],
        [ 0.1745],
        ...,
        [-2.2962],
        [-2.2909],
        [-2.2889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-251281.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0213],
        [1.0227],
        [1.0220],
        ...,
        [1.0012],
        [1.0005],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369161.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0228],
        [1.0221],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369169.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2760.6223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.8302, device='cuda:0')



h[100].sum tensor(45.5032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.7399, device='cuda:0')



h[200].sum tensor(-1.8854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.4785, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0207, 0.0302,  ..., 0.0000, 0.0183, 0.0071],
        [0.0000, 0.0074, 0.0166,  ..., 0.0000, 0.0120, 0.0038],
        [0.0000, 0.0121, 0.0214,  ..., 0.0000, 0.0143, 0.0050],
        ...,
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0094, 0.0024],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0094, 0.0024],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0094, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60369.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0140, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.1280],
        [0.0123, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.1146],
        [0.0139, 0.0000, 0.0000,  ..., 0.0707, 0.0000, 0.1225],
        ...,
        [0.0036, 0.0000, 0.0000,  ..., 0.0668, 0.0000, 0.0387],
        [0.0036, 0.0000, 0.0000,  ..., 0.0668, 0.0000, 0.0387],
        [0.0036, 0.0000, 0.0000,  ..., 0.0667, 0.0000, 0.0386]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615790.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1255.5364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.0286, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13853.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-584.4905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1288.3093, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-395.8070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1148],
        [ 0.0357],
        [ 0.1252],
        ...,
        [-2.3118],
        [-2.3067],
        [-2.3049]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240364.2969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0228],
        [1.0221],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369169.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0214],
        [1.0228],
        [1.0221],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369177.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [-0.0028,  0.0086,  0.0110,  ..., -0.0011,  0.0062,  0.0026],
        ...,
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2850.9482, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4683, device='cuda:0')



h[100].sum tensor(48.5806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.8482, device='cuda:0')



h[200].sum tensor(-2.0438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2161, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0107,  ..., 0.0000, 0.0092, 0.0024],
        [0.0000, 0.0243, 0.0340,  ..., 0.0000, 0.0202, 0.0081],
        [0.0000, 0.0317, 0.0416,  ..., 0.0000, 0.0238, 0.0099],
        ...,
        [0.0000, 0.0018, 0.0110,  ..., 0.0000, 0.0094, 0.0025],
        [0.0000, 0.0018, 0.0110,  ..., 0.0000, 0.0094, 0.0025],
        [0.0000, 0.0018, 0.0110,  ..., 0.0000, 0.0094, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61282.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0184, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.1373],
        [0.0188, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.1534],
        [0.0229, 0.0000, 0.0000,  ..., 0.0758, 0.0000, 0.1843],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0386],
        [0.0037, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0386],
        [0.0037, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0386]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615832.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1330.6897, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.9236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13818.6230, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-594.8552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1324.5471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-396.8128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3897],
        [ 0.3853],
        [ 0.4033],
        ...,
        [-2.3280],
        [-2.3228],
        [-2.3209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261330.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0214],
        [1.0228],
        [1.0221],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369177.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(501.8194, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0215],
        [1.0228],
        [1.0221],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369184.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.4146e-02,  4.2111e-02,  4.5462e-02,  ...,  6.5173e-04,
          2.2417e-02,  1.1066e-02],
        [-9.5247e-03,  2.8493e-02,  3.1469e-02,  ..., -6.0901e-05,
          1.5834e-02,  7.6472e-03],
        [-5.0503e-03,  1.5309e-02,  1.7921e-02,  ..., -7.5085e-04,
          9.4616e-03,  4.3367e-03],
        ...,
        [ 0.0000e+00,  4.2679e-04,  2.6289e-03,  ..., -1.5296e-03,
          2.2684e-03,  6.0010e-04],
        [ 0.0000e+00,  4.2679e-04,  2.6289e-03,  ..., -1.5296e-03,
          2.2684e-03,  6.0010e-04],
        [ 0.0000e+00,  4.2679e-04,  2.6289e-03,  ..., -1.5296e-03,
          2.2684e-03,  6.0010e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2741.4053, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1861, device='cuda:0')



h[100].sum tensor(44.4453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.3041, device='cuda:0')



h[200].sum tensor(-1.8001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.1885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1305, 0.1431,  ..., 0.0006, 0.0715, 0.0348],
        [0.0000, 0.1288, 0.1413,  ..., 0.0012, 0.0707, 0.0344],
        [0.0000, 0.0644, 0.0751,  ..., 0.0002, 0.0396, 0.0182],
        ...,
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0094, 0.0025],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0094, 0.0025],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0094, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58571.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0883, 0.0000, 0.0000,  ..., 0.1189, 0.0000, 0.4811],
        [0.0826, 0.0000, 0.0000,  ..., 0.1155, 0.0000, 0.4547],
        [0.0589, 0.0000, 0.0000,  ..., 0.1004, 0.0000, 0.3360],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0671, 0.0000, 0.0387],
        [0.0037, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0387],
        [0.0037, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(604881.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1178.5483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.8598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13612.0273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-560.3306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1335.6296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.1517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3819],
        [ 0.3736],
        [ 0.3420],
        ...,
        [-2.3353],
        [-2.3301],
        [-2.3282]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-257536.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0215],
        [1.0228],
        [1.0221],
        ...,
        [1.0012],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369184.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369192.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2852.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.4207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4025, device='cuda:0')



h[100].sum tensor(48.2514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.8036, device='cuda:0')



h[200].sum tensor(-1.9816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.1865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0107,  ..., 0.0000, 0.0093, 0.0025],
        [0.0000, 0.0018, 0.0107,  ..., 0.0000, 0.0093, 0.0025],
        [0.0000, 0.0018, 0.0107,  ..., 0.0000, 0.0093, 0.0025],
        ...,
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0095, 0.0025],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0095, 0.0025],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0095, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60707.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.0000,  ..., 0.0656, 0.0000, 0.0378],
        [0.0034, 0.0000, 0.0000,  ..., 0.0659, 0.0000, 0.0380],
        [0.0034, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0381],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0389],
        [0.0034, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0389],
        [0.0034, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614414.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1232.7632, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.3102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13785.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-585.9109, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1306.4354, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.9619, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5232],
        [-2.4314],
        [-2.2565],
        ...,
        [-2.3384],
        [-2.3332],
        [-2.3313]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-250824.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369192.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369192.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006],
        [ 0.0000,  0.0004,  0.0026,  ..., -0.0015,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2609.7427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2898, device='cuda:0')



h[100].sum tensor(39.7077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3446, device='cuda:0')



h[200].sum tensor(-1.5135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8845, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0018, 0.0107,  ..., 0.0000, 0.0093, 0.0025],
        [0.0000, 0.0018, 0.0107,  ..., 0.0000, 0.0093, 0.0025],
        [0.0000, 0.0018, 0.0107,  ..., 0.0000, 0.0093, 0.0025],
        ...,
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0095, 0.0025],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0095, 0.0025],
        [0.0000, 0.0018, 0.0109,  ..., 0.0000, 0.0095, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55266.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0069, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0608],
        [0.0036, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0401],
        [0.0034, 0.0000, 0.0000,  ..., 0.0660, 0.0000, 0.0381],
        ...,
        [0.0034, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0389],
        [0.0034, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0389],
        [0.0034, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(590148.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1071.9094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.2875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13358.2783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-517.7269, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1321.2275, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.7014, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2833],
        [-1.7196],
        [-1.9553],
        ...,
        [-2.3374],
        [-2.3320],
        [-2.3299]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261865.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369192.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0216],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369200.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0049,  0.0149,  0.0174,  ..., -0.0008,  0.0093,  0.0042],
        [-0.0057,  0.0173,  0.0199,  ..., -0.0007,  0.0104,  0.0048],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2924.0786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.1986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3060, device='cuda:0')



h[100].sum tensor(50.5888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.4150, device='cuda:0')



h[200].sum tensor(-2.0875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5933, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1489, 0.1616,  ..., 0.0020, 0.0804, 0.0394],
        [0.0000, 0.0533, 0.0635,  ..., 0.0003, 0.0342, 0.0154],
        [0.0000, 0.0190, 0.0283,  ..., 0.0000, 0.0177, 0.0068],
        ...,
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0095, 0.0025],
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0095, 0.0025],
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0095, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62709.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1280, 0.0000, 0.0000,  ..., 0.1453, 0.0000, 0.6615],
        [0.0739, 0.0000, 0.0000,  ..., 0.1107, 0.0000, 0.3998],
        [0.0375, 0.0000, 0.0000,  ..., 0.0874, 0.0000, 0.2217],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0390],
        [0.0032, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0390],
        [0.0032, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623244.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1207.4303, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.7296, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13935.2227, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-609.7849, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1277.9601, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.2170, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1778],
        [ 0.1861],
        [ 0.1792],
        ...,
        [-2.3333],
        [-2.3282],
        [-2.3267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238906.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0216],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0005],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369200.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0217],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369208.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2736.4429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.4708, device='cuda:0')



h[100].sum tensor(43.7256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.8202, device='cuda:0')



h[200].sum tensor(-1.7012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8665, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0106,  ..., 0.0000, 0.0094, 0.0025],
        [0.0000, 0.0019, 0.0106,  ..., 0.0000, 0.0094, 0.0025],
        [0.0000, 0.0168, 0.0260,  ..., 0.0000, 0.0166, 0.0062],
        ...,
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0096, 0.0025],
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0096, 0.0025],
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0096, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59241.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0199, 0.0000, 0.0000,  ..., 0.0756, 0.0000, 0.1372],
        [0.0270, 0.0000, 0.0000,  ..., 0.0802, 0.0000, 0.1773],
        [0.0343, 0.0000, 0.0000,  ..., 0.0843, 0.0000, 0.2261],
        ...,
        [0.0031, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0391],
        [0.0031, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0391],
        [0.0031, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611248.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1058.3292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.4383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13742.7412, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-565.2784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1261.0990, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.1332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1772],
        [ 0.2394],
        [ 0.2699],
        ...,
        [-2.3491],
        [-2.3437],
        [-2.3416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-226057.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0217],
        [1.0229],
        [1.0222],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369208.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0218],
        [1.0230],
        [1.0223],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369215.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000e+00,  4.7067e-04,  2.5930e-03,  ..., -1.5838e-03,
          2.3088e-03,  6.0462e-04],
        [ 0.0000e+00,  4.7067e-04,  2.5930e-03,  ..., -1.5838e-03,
          2.3088e-03,  6.0462e-04],
        [-9.7017e-03,  2.9260e-02,  3.2157e-02,  ..., -8.2341e-05,
          1.6218e-02,  7.8354e-03],
        ...,
        [ 0.0000e+00,  4.7067e-04,  2.5930e-03,  ..., -1.5838e-03,
          2.3088e-03,  6.0462e-04],
        [ 0.0000e+00,  4.7067e-04,  2.5930e-03,  ..., -1.5838e-03,
          2.3088e-03,  6.0462e-04],
        [ 0.0000e+00,  4.7067e-04,  2.5930e-03,  ..., -1.5838e-03,
          2.3088e-03,  6.0462e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2627.2317, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3527, device='cuda:0')



h[100].sum tensor(39.5785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3871, device='cuda:0')



h[200].sum tensor(-1.4726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9128, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0106,  ..., 0.0000, 0.0094, 0.0025],
        [0.0000, 0.0314, 0.0409,  ..., 0.0000, 0.0237, 0.0099],
        [0.0000, 0.0261, 0.0354,  ..., 0.0000, 0.0211, 0.0085],
        ...,
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0096, 0.0025],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0096, 0.0025],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0096, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53547.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0083, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0663],
        [0.0216, 0.0000, 0.0000,  ..., 0.0777, 0.0000, 0.1350],
        [0.0291, 0.0000, 0.0000,  ..., 0.0825, 0.0000, 0.1748],
        ...,
        [0.0031, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0391],
        [0.0031, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0391],
        [0.0031, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(578838.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(821.4320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(288.9279, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13137.9707, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-493.2859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1298.3494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-366.2516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9330],
        [-1.4362],
        [-0.9200],
        ...,
        [-2.3600],
        [-2.3549],
        [-2.3531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246593.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0218],
        [1.0230],
        [1.0223],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369215.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0219],
        [1.0230],
        [1.0223],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369223.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0080,  0.0244,  0.0272,  ..., -0.0003,  0.0139,  0.0066],
        [-0.0063,  0.0193,  0.0219,  ..., -0.0006,  0.0114,  0.0053],
        [-0.0175,  0.0524,  0.0559,  ...,  0.0011,  0.0274,  0.0136],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2846.0835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.8143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.2474, device='cuda:0')



h[100].sum tensor(46.9331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.0222, device='cuda:0')



h[200].sum tensor(-1.8599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0565, 0.0667,  ..., 0.0002, 0.0358, 0.0162],
        [0.0000, 0.1500, 0.1627,  ..., 0.0015, 0.0810, 0.0396],
        [0.0000, 0.1410, 0.1534,  ..., 0.0013, 0.0767, 0.0374],
        ...,
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0096, 0.0025],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0096, 0.0025],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0096, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59741.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0547, 0.0000, 0.0000,  ..., 0.0986, 0.0000, 0.3045],
        [0.0925, 0.0000, 0.0000,  ..., 0.1227, 0.0000, 0.4958],
        [0.1061, 0.0000, 0.0000,  ..., 0.1311, 0.0000, 0.5692],
        ...,
        [0.0030, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0390],
        [0.0030, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0390],
        [0.0030, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611656.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1106.7671, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.2993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13667.1865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-570.8735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1308.8195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.5491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0814],
        [ 0.3270],
        [ 0.3927],
        ...,
        [-2.3747],
        [-2.3694],
        [-2.3674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-263969.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0219],
        [1.0230],
        [1.0223],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369223.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0219],
        [1.0231],
        [1.0224],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369231., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0146,  0.0171,  ..., -0.0009,  0.0092,  0.0042],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [-0.0025,  0.0079,  0.0102,  ..., -0.0012,  0.0059,  0.0025],
        ...,
        [-0.0083,  0.0251,  0.0279,  ..., -0.0003,  0.0142,  0.0068],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3145.6279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.5039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.7331, device='cuda:0')



h[100].sum tensor(56.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.7336, device='cuda:0')



h[200].sum tensor(-2.3852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0545, 0.0645,  ..., 0.0000, 0.0349, 0.0156],
        [0.0000, 0.0421, 0.0518,  ..., 0.0000, 0.0289, 0.0125],
        [0.0000, 0.0232, 0.0324,  ..., 0.0000, 0.0198, 0.0078],
        ...,
        [0.0000, 0.0229, 0.0322,  ..., 0.0000, 0.0198, 0.0077],
        [0.0000, 0.0276, 0.0371,  ..., 0.0000, 0.0221, 0.0089],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0097, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66588.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0484, 0.0000, 0.0000,  ..., 0.0931, 0.0000, 0.3080],
        [0.0333, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.2437],
        [0.0217, 0.0000, 0.0000,  ..., 0.0757, 0.0000, 0.1865],
        ...,
        [0.0217, 0.0000, 0.0000,  ..., 0.0792, 0.0000, 0.1426],
        [0.0177, 0.0000, 0.0000,  ..., 0.0768, 0.0000, 0.1195],
        [0.0075, 0.0000, 0.0000,  ..., 0.0705, 0.0000, 0.0640]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(643557.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1267.9524, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(345.3369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14201.2988, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-658.1057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1286.9954, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-411.7511, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4255],
        [ 0.4446],
        [ 0.4429],
        ...,
        [-1.1829],
        [-1.5289],
        [-1.9207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253971.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0219],
        [1.0231],
        [1.0224],
        ...,
        [1.0011],
        [1.0004],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369231., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0220],
        [1.0231],
        [1.0225],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369238.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0023,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2880.4653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.7982, device='cuda:0')



h[100].sum tensor(46.8822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.3948, device='cuda:0')



h[200].sum tensor(-1.8709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9144, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0105,  ..., 0.0000, 0.0095, 0.0024],
        [0.0000, 0.0020, 0.0106,  ..., 0.0000, 0.0096, 0.0024],
        [0.0000, 0.0020, 0.0106,  ..., 0.0000, 0.0096, 0.0024],
        ...,
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0097, 0.0025],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0097, 0.0025],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0097, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62272.1680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0027, 0.0000, 0.0000,  ..., 0.0666, 0.0000, 0.0377],
        [0.0027, 0.0000, 0.0000,  ..., 0.0669, 0.0000, 0.0379],
        [0.0027, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0380],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0388],
        [0.0028, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0388],
        [0.0028, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(633407., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1136.1255, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.2648, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14035.6152, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-604.8090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1327.8561, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.0223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6339],
        [-2.6391],
        [-2.6303],
        ...,
        [-2.3962],
        [-2.3909],
        [-2.3889]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-256759.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0220],
        [1.0231],
        [1.0225],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369238.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0221],
        [1.0232],
        [1.0225],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369245.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3158.9033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-16.4283, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.8263, device='cuda:0')



h[100].sum tensor(55.4395, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.7967, device='cuda:0')



h[200].sum tensor(-2.3539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(11.1784, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0019, 0.0105,  ..., 0.0000, 0.0096, 0.0024],
        [0.0000, 0.0019, 0.0106,  ..., 0.0000, 0.0097, 0.0024],
        [0.0000, 0.0019, 0.0106,  ..., 0.0000, 0.0097, 0.0024],
        ...,
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0098, 0.0024],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0098, 0.0024],
        [0.0000, 0.0020, 0.0108,  ..., 0.0000, 0.0098, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68082.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0025, 0.0000, 0.0000,  ..., 0.0670, 0.0000, 0.0377],
        [0.0025, 0.0000, 0.0000,  ..., 0.0673, 0.0000, 0.0382],
        [0.0024, 0.0000, 0.0000,  ..., 0.0672, 0.0000, 0.0396],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.0388],
        [0.0026, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.0387],
        [0.0026, 0.0000, 0.0000,  ..., 0.0684, 0.0000, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(656767.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1232.5052, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.3844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14387.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-680.4446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1330.1614, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-420.1960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5095],
        [-2.3772],
        [-2.1653],
        ...,
        [-2.4108],
        [-2.4054],
        [-2.4034]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246702.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0221],
        [1.0232],
        [1.0225],
        ...,
        [1.0010],
        [1.0004],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369245.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 1700 loss: tensor(454.4190, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0222],
        [1.0233],
        [1.0226],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369252.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        ...,
        [-0.0043,  0.0133,  0.0157,  ..., -0.0010,  0.0086,  0.0038],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3029.0024, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.6822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.3867, device='cuda:0')



h[100].sum tensor(50.0305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.1461, device='cuda:0')



h[200].sum tensor(-2.0946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.0799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.9084e-03, 1.0542e-02,  ..., 0.0000e+00, 9.7193e-03,
         2.3495e-03],
        [0.0000e+00, 1.9165e-03, 1.0587e-02,  ..., 0.0000e+00, 9.7602e-03,
         2.3594e-03],
        [0.0000e+00, 1.9189e-03, 1.0600e-02,  ..., 0.0000e+00, 9.7728e-03,
         2.3624e-03],
        ...,
        [0.0000e+00, 4.2365e-02, 5.2229e-02,  ..., 1.3171e-05, 2.9453e-02,
         1.2545e-02],
        [0.0000e+00, 1.5256e-02, 2.4413e-02,  ..., 0.0000e+00, 1.6350e-02,
         5.7388e-03],
        [0.0000e+00, 1.9470e-03, 1.0755e-02,  ..., 0.0000e+00, 9.9159e-03,
         2.3970e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65543.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0043, 0.0000, 0.0000,  ..., 0.0686, 0.0000, 0.0494],
        [0.0090, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0808],
        [0.0147, 0.0000, 0.0000,  ..., 0.0750, 0.0000, 0.1203],
        ...,
        [0.0412, 0.0000, 0.0000,  ..., 0.0934, 0.0000, 0.2591],
        [0.0217, 0.0000, 0.0000,  ..., 0.0809, 0.0000, 0.1514],
        [0.0083, 0.0000, 0.0000,  ..., 0.0724, 0.0000, 0.0733]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(655321.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1139.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.8784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14341.7432, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-651.6420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1375.2909, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-413.2599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8424],
        [-0.3727],
        [ 0.0056],
        ...,
        [ 0.0271],
        [-0.6243],
        [-1.4282]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-243718.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0222],
        [1.0233],
        [1.0226],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369252.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0222],
        [1.0233],
        [1.0226],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369252.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2654.2476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3223, device='cuda:0')



h[100].sum tensor(37.0893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3666, device='cuda:0')



h[200].sum tensor(-1.4175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8991, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0121, 0.0210,  ..., 0.0000, 0.0146, 0.0049],
        [0.0000, 0.0019, 0.0106,  ..., 0.0000, 0.0098, 0.0024],
        [0.0000, 0.0019, 0.0106,  ..., 0.0000, 0.0098, 0.0024],
        ...,
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0099, 0.0024],
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0099, 0.0024],
        [0.0000, 0.0019, 0.0108,  ..., 0.0000, 0.0099, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53295.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0094, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0932],
        [0.0044, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0540],
        [0.0030, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0429],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0388],
        [0.0024, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0387],
        [0.0024, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0387]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(584596.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(703.2124, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(278.5887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13117.8086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-498.3348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1437.6975, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.7500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8616],
        [-1.3488],
        [-1.5821],
        ...,
        [-2.4246],
        [-2.4191],
        [-2.4171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276714.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0222],
        [1.0233],
        [1.0226],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369252.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0234],
        [1.0227],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369259.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045,  0.0139,  0.0164,  ..., -0.0009,  0.0089,  0.0039],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [-0.0128,  0.0387,  0.0418,  ...,  0.0004,  0.0209,  0.0102],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2898.2485, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.7855, device='cuda:0')



h[100].sum tensor(44.9454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.3862, device='cuda:0')



h[200].sum tensor(-1.8356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.9087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0131, 0.0220,  ..., 0.0000, 0.0152, 0.0051],
        [0.0000, 0.0773, 0.0879,  ..., 0.0004, 0.0463, 0.0212],
        [0.0000, 0.0395, 0.0491,  ..., 0.0000, 0.0280, 0.0118],
        ...,
        [0.0000, 0.0020, 0.0107,  ..., 0.0000, 0.0100, 0.0024],
        [0.0000, 0.0020, 0.0107,  ..., 0.0000, 0.0100, 0.0024],
        [0.0000, 0.0020, 0.0107,  ..., 0.0000, 0.0100, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61554.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0335, 0.0000, 0.0000,  ..., 0.0873, 0.0000, 0.2245],
        [0.0448, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.2919],
        [0.0446, 0.0000, 0.0000,  ..., 0.0948, 0.0000, 0.2868],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0389],
        [0.0023, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0389],
        [0.0023, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(626934.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(837.2175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.7834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13850.6807, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-603.2969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1374.9202, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.2976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3762],
        [ 0.3844],
        [ 0.3875],
        ...,
        [-2.4328],
        [-2.4273],
        [-2.4253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-234624.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0234],
        [1.0227],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369259.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0223],
        [1.0234],
        [1.0227],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369267.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0186,  0.0211,  ..., -0.0007,  0.0111,  0.0051],
        [-0.0038,  0.0119,  0.0143,  ..., -0.0010,  0.0079,  0.0034],
        [-0.0030,  0.0094,  0.0117,  ..., -0.0012,  0.0067,  0.0028],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2636.3276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4738, device='cuda:0')



h[100].sum tensor(35.5611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.7925, device='cuda:0')



h[200].sum tensor(-1.3389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5170, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0592, 0.0693,  ..., 0.0000, 0.0375, 0.0167],
        [0.0000, 0.0540, 0.0639,  ..., 0.0000, 0.0350, 0.0154],
        [0.0000, 0.0211, 0.0302,  ..., 0.0000, 0.0191, 0.0071],
        ...,
        [0.0000, 0.0020, 0.0107,  ..., 0.0000, 0.0100, 0.0024],
        [0.0000, 0.0020, 0.0107,  ..., 0.0000, 0.0100, 0.0024],
        [0.0000, 0.0020, 0.0107,  ..., 0.0000, 0.0100, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53150.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0274, 0.0000, 0.0000,  ..., 0.0819, 0.0000, 0.2153],
        [0.0227, 0.0000, 0.0000,  ..., 0.0793, 0.0000, 0.1852],
        [0.0130, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.1230],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0389],
        [0.0024, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0389],
        [0.0024, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0389]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(587543.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(671.2490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(278.5855, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13166.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-497.8826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1431.6672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.8672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2579],
        [-0.1462],
        [-0.7621],
        ...,
        [-2.4341],
        [-2.4290],
        [-2.4276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-269142.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0223],
        [1.0234],
        [1.0227],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369267.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0224],
        [1.0235],
        [1.0228],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369274.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0016,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2890.1294, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.0116, device='cuda:0')



h[100].sum tensor(43.8858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.8627, device='cuda:0')



h[200].sum tensor(-1.7555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0020, 0.0105,  ..., 0.0000, 0.0098, 0.0023],
        [0.0000, 0.0020, 0.0105,  ..., 0.0000, 0.0099, 0.0023],
        [0.0000, 0.0418, 0.0514,  ..., 0.0000, 0.0291, 0.0123],
        ...,
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0024],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0024],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60596.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0048, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0527],
        [0.0118, 0.0000, 0.0000,  ..., 0.0735, 0.0000, 0.0941],
        [0.0347, 0.0000, 0.0000,  ..., 0.0879, 0.0000, 0.2214],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0390],
        [0.0024, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0390],
        [0.0024, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622137.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(877.7952, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(314.3564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13783.4717, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-590.8543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1360.2900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.9642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2254],
        [-0.5597],
        [-0.0311],
        ...,
        [-2.4379],
        [-2.4325],
        [-2.4304]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-242872.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0224],
        [1.0235],
        [1.0228],
        ...,
        [1.0010],
        [1.0003],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369274.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0225],
        [1.0236],
        [1.0228],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369282.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0031,  0.0100,  0.0123,  ..., -0.0012,  0.0070,  0.0029],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2674.6602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8331, device='cuda:0')



h[100].sum tensor(36.4262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.0356, device='cuda:0')



h[200].sum tensor(-1.3509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6788, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0336, 0.0429,  ..., 0.0000, 0.0251, 0.0102],
        [0.0000, 0.0267, 0.0358,  ..., 0.0000, 0.0218, 0.0085],
        [0.0000, 0.0021, 0.0106,  ..., 0.0000, 0.0099, 0.0023],
        ...,
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55649.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0184, 0.0000, 0.0000,  ..., 0.0753, 0.0000, 0.1645],
        [0.0149, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.1368],
        [0.0068, 0.0000, 0.0000,  ..., 0.0698, 0.0000, 0.0790],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0390],
        [0.0024, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0390],
        [0.0024, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606886.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(886.4813, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.1546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13503.2100, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-527.8340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1372.6903, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.1768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0320],
        [-0.2722],
        [-0.6652],
        ...,
        [-2.4437],
        [-2.4384],
        [-2.4363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-267848., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0225],
        [1.0236],
        [1.0228],
        ...,
        [1.0009],
        [1.0003],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369282.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0236],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369290.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2931.7949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.6116, device='cuda:0')



h[100].sum tensor(45.3529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.2685, device='cuda:0')



h[200].sum tensor(-1.7766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0139, 0.0226,  ..., 0.0000, 0.0155, 0.0053],
        [0.0000, 0.0210, 0.0299,  ..., 0.0000, 0.0190, 0.0071],
        [0.0000, 0.0281, 0.0372,  ..., 0.0000, 0.0224, 0.0088],
        ...,
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0100, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61303.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0119, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.1226],
        [0.0141, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.1306],
        [0.0166, 0.0000, 0.0000,  ..., 0.0749, 0.0000, 0.1405],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0391],
        [0.0025, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0390],
        [0.0025, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0390]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(629471.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1045.9490, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.1495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13885.4590, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-596.2347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1338.6920, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-384.1820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0853],
        [-0.0209],
        [-0.1818],
        ...,
        [-2.4524],
        [-2.4470],
        [-2.4449]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253922.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0236],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369290.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0226],
        [1.0236],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369298., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.6642e-03,  1.1607e-02,  1.3947e-02,  ..., -1.1068e-03,
          7.7429e-03,  3.3555e-03],
        [ 0.0000e+00,  5.2694e-04,  2.5766e-03,  ..., -1.6765e-03,
          2.3968e-03,  5.6682e-04],
        [-1.1291e-02,  3.4668e-02,  3.7615e-02,  ...,  7.8994e-05,
          1.8871e-02,  9.1600e-03],
        ...,
        [ 0.0000e+00,  5.2694e-04,  2.5766e-03,  ..., -1.6765e-03,
          2.3968e-03,  5.6682e-04],
        [ 0.0000e+00,  5.2694e-04,  2.5766e-03,  ..., -1.6765e-03,
          2.3968e-03,  5.6682e-04],
        [ 0.0000e+00,  5.2694e-04,  2.5766e-03,  ..., -1.6765e-03,
          2.3968e-03,  5.6682e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2757.0015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0373, device='cuda:0')



h[100].sum tensor(39.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8503, device='cuda:0')



h[200].sum tensor(-1.4531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2210, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.1388e-02, 1.9990e-02,  ..., 0.0000e+00, 1.4233e-02,
         4.6371e-03],
        [0.0000e+00, 4.8456e-02, 5.8067e-02,  ..., 8.0906e-05, 3.2156e-02,
         1.3974e-02],
        [0.0000e+00, 3.0795e-02, 3.9953e-02,  ..., 0.0000e+00, 2.3645e-02,
         9.5315e-03],
        ...,
        [0.0000e+00, 2.1948e-03, 1.0732e-02,  ..., 0.0000e+00, 9.9832e-03,
         2.3609e-03],
        [0.0000e+00, 2.1943e-03, 1.0729e-02,  ..., 0.0000e+00, 9.9808e-03,
         2.3603e-03],
        [0.0000e+00, 2.1932e-03, 1.0724e-02,  ..., 0.0000e+00, 9.9760e-03,
         2.3592e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55650.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0171, 0.0000, 0.0000,  ..., 0.0744, 0.0000, 0.1408],
        [0.0293, 0.0000, 0.0000,  ..., 0.0826, 0.0000, 0.1916],
        [0.0306, 0.0000, 0.0000,  ..., 0.0841, 0.0000, 0.1889],
        ...,
        [0.0027, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0391],
        [0.0027, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0391],
        [0.0027, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0391]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598586.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(813.5570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.6764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13369.6846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-522.3568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1335.2498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.3376, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1045],
        [ 0.1549],
        [ 0.0479],
        ...,
        [-2.4603],
        [-2.4550],
        [-2.4528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-253913.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0226],
        [1.0236],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369298., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0227],
        [1.0237],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369305.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0035,  0.0111,  0.0135,  ..., -0.0011,  0.0075,  0.0032],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2759.7854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0923, device='cuda:0')



h[100].sum tensor(40.3754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.8875, device='cuda:0')



h[200].sum tensor(-1.4455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.2458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0251, 0.0341,  ..., 0.0000, 0.0208, 0.0081],
        [0.0000, 0.0130, 0.0217,  ..., 0.0000, 0.0150, 0.0050],
        [0.0000, 0.0022, 0.0106,  ..., 0.0000, 0.0098, 0.0023],
        ...,
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0100, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56734.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0268, 0.0000, 0.0000,  ..., 0.0803, 0.0000, 0.1861],
        [0.0123, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.1055],
        [0.0047, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0591],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.0392],
        [0.0028, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.0392],
        [0.0028, 0.0000, 0.0000,  ..., 0.0694, 0.0000, 0.0392]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(606969.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(857.5249, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(300.3282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13512.0928, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-532.2471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1301.1282, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.0702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0420],
        [-0.5233],
        [-1.0835],
        ...,
        [-2.4721],
        [-2.4667],
        [-2.4645]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246212.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0227],
        [1.0237],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369305.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0228],
        [1.0237],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369312.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032,  0.0103,  0.0126,  ..., -0.0012,  0.0071,  0.0030],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2677.9473, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.5988, device='cuda:0')



h[100].sum tensor(38.6116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8771, device='cuda:0')



h[200].sum tensor(-1.3060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0263, 0.0354,  ..., 0.0000, 0.0214, 0.0084],
        [0.0000, 0.0121, 0.0209,  ..., 0.0000, 0.0146, 0.0048],
        [0.0000, 0.0021, 0.0106,  ..., 0.0000, 0.0098, 0.0023],
        ...,
        [0.0000, 0.0022, 0.0108,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0022, 0.0108,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0022, 0.0108,  ..., 0.0000, 0.0099, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55075.4297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0196, 0.0000, 0.0000,  ..., 0.0764, 0.0000, 0.1539],
        [0.0127, 0.0000, 0.0000,  ..., 0.0734, 0.0000, 0.1079],
        [0.0088, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0803],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0702, 0.0000, 0.0392],
        [0.0028, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0392],
        [0.0028, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0392]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(601720.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(771.1035, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.8332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13412.3711, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-506.8131, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1276.8081, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-364.5171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0542],
        [-0.1483],
        [-0.2655],
        ...,
        [-2.4909],
        [-2.4858],
        [-2.4838]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-245823.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0228],
        [1.0237],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369312.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(446.8787, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0228],
        [1.0238],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369319.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0130,  0.0400,  0.0431,  ...,  0.0003,  0.0214,  0.0105],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [-0.0081,  0.0250,  0.0278,  ..., -0.0005,  0.0142,  0.0067],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3033.3398, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.9302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.6347, device='cuda:0')



h[100].sum tensor(51.7190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.6373, device='cuda:0')



h[200].sum tensor(-1.9107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.7413, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1122, 0.1237,  ..., 0.0006, 0.0629, 0.0299],
        [0.0000, 0.1330, 0.1451,  ..., 0.0009, 0.0729, 0.0352],
        [0.0000, 0.0325, 0.0419,  ..., 0.0000, 0.0245, 0.0099],
        ...,
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63448.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1134, 0.0000, 0.0000,  ..., 0.1341, 0.0000, 0.5850],
        [0.0887, 0.0000, 0.0000,  ..., 0.1192, 0.0000, 0.4759],
        [0.0474, 0.0000, 0.0000,  ..., 0.0942, 0.0000, 0.2877],
        ...,
        [0.0028, 0.0000, 0.0000,  ..., 0.0711, 0.0000, 0.0392],
        [0.0028, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0392],
        [0.0028, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0392]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(638527.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1113.1937, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.8955, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13945.3848, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-607.5959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1272.7893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.2426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1157],
        [ 0.1942],
        [ 0.2716],
        ...,
        [-2.5180],
        [-2.5124],
        [-2.5101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-274504.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0228],
        [1.0238],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369319.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0239],
        [1.0230],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369326.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [-0.0074,  0.0231,  0.0258,  ..., -0.0006,  0.0133,  0.0062],
        [-0.0074,  0.0231,  0.0258,  ..., -0.0006,  0.0133,  0.0062],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2832.5471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.8225, device='cuda:0')



h[100].sum tensor(45.4006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.0581, device='cuda:0')



h[200].sum tensor(-1.5596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0248, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0251, 0.0342,  ..., 0.0000, 0.0209, 0.0080],
        [0.0000, 0.0441, 0.0538,  ..., 0.0000, 0.0301, 0.0128],
        [0.0000, 0.1319, 0.1439,  ..., 0.0009, 0.0724, 0.0349],
        ...,
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57388.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0218, 0.0000, 0.0000,  ..., 0.0806, 0.0000, 0.1398],
        [0.0392, 0.0000, 0.0000,  ..., 0.0909, 0.0000, 0.2284],
        [0.0704, 0.0000, 0.0000,  ..., 0.1094, 0.0000, 0.3806],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0713, 0.0000, 0.0394],
        [0.0026, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0394],
        [0.0026, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0394]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(611363.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(792.5043, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(307.7593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13496.5566, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-529.5936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1255.2634, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-371.4651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6589],
        [-0.2278],
        [ 0.1810],
        ...,
        [-2.5283],
        [-2.5227],
        [-2.5203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255318.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0239],
        [1.0230],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369326.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0229],
        [1.0239],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369333.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0151,  0.0175,  ..., -0.0010,  0.0094,  0.0042],
        [-0.0158,  0.0487,  0.0520,  ...,  0.0007,  0.0256,  0.0127],
        [-0.0074,  0.0230,  0.0257,  ..., -0.0006,  0.0132,  0.0062],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2630.4604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9545, device='cuda:0')



h[100].sum tensor(38.9661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.4412, device='cuda:0')



h[200].sum tensor(-1.2143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2832, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1482, 0.1607,  ..., 0.0012, 0.0803, 0.0391],
        [0.0000, 0.1113, 0.1228,  ..., 0.0004, 0.0625, 0.0298],
        [0.0000, 0.1135, 0.1250,  ..., 0.0007, 0.0636, 0.0303],
        ...,
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53720.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0752, 0.0000, 0.0000,  ..., 0.1116, 0.0000, 0.4157],
        [0.0772, 0.0000, 0.0000,  ..., 0.1128, 0.0000, 0.4322],
        [0.0765, 0.0000, 0.0000,  ..., 0.1124, 0.0000, 0.4287],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0395],
        [0.0025, 0.0000, 0.0000,  ..., 0.0715, 0.0000, 0.0395],
        [0.0025, 0.0000, 0.0000,  ..., 0.0715, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598373.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(730.6681, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.4448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13228.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-482.4583, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1282.2010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-353.6429, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3114],
        [ 0.3145],
        [ 0.2921],
        ...,
        [-2.5414],
        [-2.5358],
        [-2.5334]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279830.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0229],
        [1.0239],
        [1.0229],
        ...,
        [1.0009],
        [1.0002],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369333.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0230],
        [1.0239],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369341., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3160.6714, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.2875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.8938, device='cuda:0')



h[100].sum tensor(56.7531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.1658, device='cuda:0')



h[200].sum tensor(-2.0693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.7585, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0125, 0.0213,  ..., 0.0000, 0.0149, 0.0048],
        [0.0000, 0.0020, 0.0106,  ..., 0.0000, 0.0099, 0.0022],
        [0.0000, 0.0425, 0.0522,  ..., 0.0000, 0.0294, 0.0124],
        ...,
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67235.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0180, 0.0000, 0.0000,  ..., 0.0776, 0.0000, 0.1426],
        [0.0254, 0.0000, 0.0000,  ..., 0.0829, 0.0000, 0.1691],
        [0.0580, 0.0000, 0.0000,  ..., 0.1025, 0.0000, 0.3264],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0395],
        [0.0024, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0395],
        [0.0024, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(666823.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1219.1028, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(354.5603, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14390.0010, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-650.7742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1239.1732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.6058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3334],
        [ 0.2657],
        [ 0.1998],
        ...,
        [-2.5533],
        [-2.5476],
        [-2.5452]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273340.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0230],
        [1.0239],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369341., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0239],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369348.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0176,  0.0202,  ..., -0.0009,  0.0107,  0.0049],
        [-0.0027,  0.0088,  0.0111,  ..., -0.0013,  0.0064,  0.0026],
        [-0.0089,  0.0276,  0.0304,  ..., -0.0004,  0.0155,  0.0074],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0005]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2907.1807, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.0389, device='cuda:0')



h[100].sum tensor(47.7718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.8811, device='cuda:0')



h[200].sum tensor(-1.6335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.5725, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0591, 0.0691,  ..., 0.0000, 0.0373, 0.0166],
        [0.0000, 0.0758, 0.0864,  ..., 0.0000, 0.0454, 0.0209],
        [0.0000, 0.0556, 0.0656,  ..., 0.0000, 0.0357, 0.0158],
        ...,
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0023],
        [0.0000, 0.0021, 0.0108,  ..., 0.0000, 0.0100, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59223.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0415, 0.0000, 0.0000,  ..., 0.0897, 0.0000, 0.2894],
        [0.0451, 0.0000, 0.0000,  ..., 0.0923, 0.0000, 0.3041],
        [0.0423, 0.0000, 0.0000,  ..., 0.0905, 0.0000, 0.2926],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0395],
        [0.0025, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0395],
        [0.0025, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0394]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617276.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(850.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.9141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13581.4355, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-550.1687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1279.2476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-375.0779, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4525],
        [ 0.4029],
        [ 0.3340],
        ...,
        [-2.5657],
        [-2.5604],
        [-2.5581]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283725.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0239],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369348.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369355.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.8880e-03,  2.4671e-02,  2.7390e-02,  ..., -5.0834e-04,
          1.4060e-02,  6.6731e-03],
        [-1.1233e-02,  3.4916e-02,  3.7911e-02,  ...,  8.5003e-06,
          1.8999e-02,  9.2638e-03],
        [-9.8556e-03,  3.0698e-02,  3.3579e-02,  ..., -2.0432e-04,
          1.6965e-02,  8.1970e-03],
        ...,
        [ 0.0000e+00,  5.0929e-04,  2.5795e-03,  ..., -1.7272e-03,
          2.4122e-03,  5.6377e-04],
        [ 0.0000e+00,  5.0929e-04,  2.5795e-03,  ..., -1.7272e-03,
          2.4122e-03,  5.6377e-04],
        [ 0.0000e+00,  5.0929e-04,  2.5795e-03,  ..., -1.7272e-03,
          2.4122e-03,  5.6377e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2999.3540, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.5975, device='cuda:0')



h[100].sum tensor(50.1173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.9356, device='cuda:0')



h[200].sum tensor(-1.7567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.2743, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 9.9571e-02, 1.1064e-01,  ..., 8.6482e-06, 5.6840e-02,
         2.6952e-02],
        [0.0000e+00, 1.1996e-01, 1.3161e-01,  ..., 0.0000e+00, 6.6704e-02,
         3.2114e-02],
        [0.0000e+00, 1.2532e-01, 1.3712e-01,  ..., 8.7042e-06, 6.9300e-02,
         3.3472e-02],
        ...,
        [0.0000e+00, 2.1231e-03, 1.0753e-02,  ..., 0.0000e+00, 1.0056e-02,
         2.3502e-03],
        [0.0000e+00, 2.1226e-03, 1.0751e-02,  ..., 0.0000e+00, 1.0054e-02,
         2.3497e-03],
        [0.0000e+00, 2.1214e-03, 1.0745e-02,  ..., 0.0000e+00, 1.0048e-02,
         2.3484e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60239.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0608, 0.0000, 0.0000,  ..., 0.1022, 0.0000, 0.3629],
        [0.0745, 0.0000, 0.0000,  ..., 0.1106, 0.0000, 0.4287],
        [0.0791, 0.0000, 0.0000,  ..., 0.1137, 0.0000, 0.4485],
        ...,
        [0.0031, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0450],
        [0.0034, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0477],
        [0.0031, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0449]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(620460.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(889.8215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(321.9010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13657.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.8201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1297.3459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-378.6142, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3482],
        [ 0.3692],
        [ 0.3533],
        ...,
        [-2.3596],
        [-2.3132],
        [-2.3130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-288693.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369355.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369362.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0161,  0.0500,  0.0534,  ...,  0.0008,  0.0263,  0.0131],
        [-0.0062,  0.0197,  0.0222,  ..., -0.0008,  0.0117,  0.0054],
        [-0.0086,  0.0270,  0.0298,  ..., -0.0004,  0.0152,  0.0073],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2857.6479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.5051, device='cuda:0')



h[100].sum tensor(44.6446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.8434, device='cuda:0')



h[200].sum tensor(-1.4963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8819, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 9.9051e-02, 1.0997e-01,  ..., 5.1746e-05, 5.6584e-02,
         2.6893e-02],
        [0.0000e+00, 1.2152e-01, 1.3307e-01,  ..., 7.8106e-04, 6.7449e-02,
         3.2586e-02],
        [0.0000e+00, 1.1056e-01, 1.2183e-01,  ..., 0.0000e+00, 6.2180e-02,
         2.9816e-02],
        ...,
        [0.0000e+00, 2.1957e-03, 1.0692e-02,  ..., 0.0000e+00, 1.0086e-02,
         2.4253e-03],
        [0.0000e+00, 2.1952e-03, 1.0689e-02,  ..., 0.0000e+00, 1.0084e-02,
         2.4248e-03],
        [0.0000e+00, 2.1940e-03, 1.0684e-02,  ..., 0.0000e+00, 1.0079e-02,
         2.4235e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57894.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0779, 0.0000, 0.0000,  ..., 0.1131, 0.0000, 0.4275],
        [0.0759, 0.0000, 0.0000,  ..., 0.1119, 0.0000, 0.4226],
        [0.0667, 0.0000, 0.0000,  ..., 0.1059, 0.0000, 0.3886],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0396],
        [0.0025, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0396],
        [0.0025, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0396]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614389.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(830.0721, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.7602, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13603.4316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-534.3553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1291.9696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-369.7609, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4358],
        [ 0.4510],
        [ 0.4588],
        ...,
        [-2.5729],
        [-2.5673],
        [-2.5650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273354.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369362.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0231],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369369.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0088,  0.0276,  0.0303,  ..., -0.0004,  0.0155,  0.0074],
        [-0.0073,  0.0229,  0.0255,  ..., -0.0006,  0.0132,  0.0062],
        [-0.0087,  0.0272,  0.0300,  ..., -0.0004,  0.0153,  0.0074],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2739.3042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2388, device='cuda:0')



h[100].sum tensor(40.0166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3101, device='cuda:0')



h[200].sum tensor(-1.2868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0842, 0.0946,  ..., 0.0000, 0.0494, 0.0232],
        [0.0000, 0.1091, 0.1202,  ..., 0.0000, 0.0615, 0.0295],
        [0.0000, 0.1223, 0.1338,  ..., 0.0008, 0.0678, 0.0328],
        ...,
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0101, 0.0025],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0101, 0.0025],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0101, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54502.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0823, 0.0000, 0.0000,  ..., 0.1146, 0.0000, 0.4682],
        [0.1000, 0.0000, 0.0000,  ..., 0.1257, 0.0000, 0.5494],
        [0.1042, 0.0000, 0.0000,  ..., 0.1285, 0.0000, 0.5650],
        ...,
        [0.0025, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0397],
        [0.0025, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0397],
        [0.0025, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0397]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(598748.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(654.7550, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(298.5504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13390.1836, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-492.5342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1324.8361, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-360.1540, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1480],
        [ 0.1293],
        [ 0.1276],
        ...,
        [-2.5831],
        [-2.5775],
        [-2.5750]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261875.9531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0231],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369369.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369375.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032,  0.0104,  0.0127,  ..., -0.0012,  0.0072,  0.0031],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0024,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3387.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-17.3943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-27.6645, device='cuda:0')



h[100].sum tensor(61.4291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-18.7169, device='cuda:0')



h[200].sum tensor(-2.3027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(12.4563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0245, 0.0333,  ..., 0.0000, 0.0207, 0.0081],
        [0.0000, 0.0123, 0.0209,  ..., 0.0000, 0.0149, 0.0050],
        [0.0000, 0.0022, 0.0105,  ..., 0.0000, 0.0100, 0.0024],
        ...,
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0102, 0.0025],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0102, 0.0025],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0102, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(75516.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0159, 0.0000, 0.0000,  ..., 0.0756, 0.0000, 0.1440],
        [0.0090, 0.0000, 0.0000,  ..., 0.0726, 0.0000, 0.0953],
        [0.0044, 0.0000, 0.0000,  ..., 0.0707, 0.0000, 0.0607],
        ...,
        [0.0024, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0397],
        [0.0036, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.0461],
        [0.0075, 0.0000, 0.0000,  ..., 0.0742, 0.0000, 0.0673]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(718447.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1520.2832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(392.2899, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15389.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-756.5613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1318.7595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-436.7306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1657],
        [-0.7337],
        [-1.3190],
        ...,
        [-2.5307],
        [-2.3818],
        [-2.0657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-254404.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0240],
        [1.0229],
        ...,
        [1.0008],
        [1.0001],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369375.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0239],
        [1.0229],
        ...,
        [1.0007],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369381.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2748.3276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4566, device='cuda:0')



h[100].sum tensor(39.6565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.4574, device='cuda:0')



h[200].sum tensor(-1.2918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0105,  ..., 0.0000, 0.0100, 0.0024],
        [0.0000, 0.0021, 0.0105,  ..., 0.0000, 0.0101, 0.0024],
        [0.0000, 0.0021, 0.0105,  ..., 0.0000, 0.0101, 0.0024],
        ...,
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0103, 0.0025],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0103, 0.0025],
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0103, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54803.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0022, 0.0000, 0.0000,  ..., 0.0702, 0.0000, 0.0384],
        [0.0022, 0.0000, 0.0000,  ..., 0.0705, 0.0000, 0.0385],
        [0.0022, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0387],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0396],
        [0.0023, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0396],
        [0.0023, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(602300.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(623.3955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(293.5019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13464.3154, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-498.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1440.6829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.0605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8501],
        [-2.8689],
        [-2.8744],
        ...,
        [-2.6217],
        [-2.6160],
        [-2.6136]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-286314.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0239],
        [1.0229],
        ...,
        [1.0007],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369381.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(479.0318, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0232],
        [1.0240],
        [1.0230],
        ...,
        [1.0007],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369387.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        [-0.0025,  0.0083,  0.0105,  ..., -0.0014,  0.0062,  0.0026],
        [-0.0025,  0.0083,  0.0105,  ..., -0.0014,  0.0062,  0.0026],
        ...,
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0026,  ..., -0.0017,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2739.5190, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.3044, device='cuda:0')



h[100].sum tensor(38.7212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3544, device='cuda:0')



h[200].sum tensor(-1.2663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0165, 0.0252,  ..., 0.0000, 0.0170, 0.0060],
        [0.0000, 0.0165, 0.0253,  ..., 0.0000, 0.0171, 0.0061],
        [0.0000, 0.0166, 0.0254,  ..., 0.0000, 0.0172, 0.0061],
        ...,
        [0.0000, 0.0022, 0.0107,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0021, 0.0107,  ..., 0.0000, 0.0103, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55155.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0000, 0.0000,  ..., 0.0726, 0.0000, 0.1045],
        [0.0062, 0.0000, 0.0000,  ..., 0.0711, 0.0000, 0.0949],
        [0.0057, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0920],
        ...,
        [0.0021, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0396],
        [0.0021, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0395],
        [0.0021, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(609006.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(676.9474, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(292.5973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13570.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-506.0852, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1488.9072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-359.3169, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0067],
        [-1.4821],
        [-1.8741],
        ...,
        [-2.6388],
        [-2.6328],
        [-2.6302]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-312864., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0232],
        [1.0240],
        [1.0230],
        ...,
        [1.0007],
        [1.0000],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369387.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0240],
        [1.0230],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369394.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0005,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0005,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0005,  0.0025,  ..., -0.0017,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3004.7717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.6731, device='cuda:0')



h[100].sum tensor(46.4073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.3102, device='cuda:0')



h[200].sum tensor(-1.6345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.8581, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0022, 0.0104,  ..., 0.0000, 0.0102, 0.0024],
        [0.0000, 0.0022, 0.0104,  ..., 0.0000, 0.0102, 0.0025],
        [0.0000, 0.0022, 0.0104,  ..., 0.0000, 0.0102, 0.0025],
        ...,
        [0.0000, 0.0023, 0.0106,  ..., 0.0000, 0.0104, 0.0025],
        [0.0000, 0.0023, 0.0106,  ..., 0.0000, 0.0104, 0.0025],
        [0.0000, 0.0023, 0.0106,  ..., 0.0000, 0.0104, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62630.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0123, 0.0000, 0.0000,  ..., 0.0755, 0.0000, 0.1024],
        [0.0114, 0.0000, 0.0000,  ..., 0.0753, 0.0000, 0.0964],
        [0.0100, 0.0000, 0.0000,  ..., 0.0747, 0.0000, 0.0860],
        ...,
        [0.0017, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0400],
        [0.0017, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0399],
        [0.0017, 0.0000, 0.0000,  ..., 0.0709, 0.0000, 0.0399]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647395.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(860.4211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(331.9313, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14264.2461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-603.8585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1407.3940, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.3160, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0251],
        [-0.0797],
        [-0.1918],
        ...,
        [-2.4156],
        [-2.3964],
        [-2.3146]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-275040.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0240],
        [1.0230],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369394.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0233],
        [1.0240],
        [1.0230],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369401.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0024,  0.0080,  0.0101,  ..., -0.0014,  0.0061,  0.0025],
        [-0.0035,  0.0115,  0.0138,  ..., -0.0012,  0.0078,  0.0034],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3224.1245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.8190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.1317, device='cuda:0')



h[100].sum tensor(52.7132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.6502, device='cuda:0')



h[200].sum tensor(-1.9269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.4154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0641, 0.0737,  ..., 0.0000, 0.0400, 0.0182],
        [0.0000, 0.0252, 0.0338,  ..., 0.0000, 0.0213, 0.0083],
        [0.0000, 0.0135, 0.0218,  ..., 0.0000, 0.0157, 0.0054],
        ...,
        [0.0000, 0.0024, 0.0105,  ..., 0.0000, 0.0105, 0.0026],
        [0.0000, 0.0024, 0.0105,  ..., 0.0000, 0.0105, 0.0026],
        [0.0000, 0.0024, 0.0105,  ..., 0.0000, 0.0105, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68233.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0304, 0.0000, 0.0000,  ..., 0.0839, 0.0000, 0.2356],
        [0.0196, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.1660],
        [0.0103, 0.0000, 0.0000,  ..., 0.0735, 0.0000, 0.1023],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0702, 0.0000, 0.0459],
        [0.0022, 0.0000, 0.0000,  ..., 0.0703, 0.0000, 0.0487],
        [0.0020, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0459]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(672550.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(899.3907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(363.4795, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14731.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-677.5135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1327.3314, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.3602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3209],
        [-0.0478],
        [-0.6096],
        ...,
        [-2.3893],
        [-2.3485],
        [-2.3810]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-240927.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0233],
        [1.0240],
        [1.0230],
        ...,
        [1.0007],
        [1.0000],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369401.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0234],
        [1.0240],
        [1.0230],
        ...,
        [1.0006],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369407.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [-0.0044,  0.0141,  0.0164,  ..., -0.0011,  0.0090,  0.0041],
        [-0.0077,  0.0245,  0.0270,  ..., -0.0005,  0.0140,  0.0067],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2911.3435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.3354, device='cuda:0')



h[100].sum tensor(41.3704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.7286, device='cuda:0')



h[200].sum tensor(-1.4152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8055, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0275, 0.0359,  ..., 0.0000, 0.0224, 0.0090],
        [0.0000, 0.0467, 0.0557,  ..., 0.0000, 0.0317, 0.0139],
        [0.0000, 0.0791, 0.0890,  ..., 0.0005, 0.0473, 0.0221],
        ...,
        [0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0105, 0.0026],
        [0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0105, 0.0026],
        [0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0105, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57742.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0203, 0.0000, 0.0000,  ..., 0.0779, 0.0000, 0.1633],
        [0.0362, 0.0000, 0.0000,  ..., 0.0871, 0.0000, 0.2586],
        [0.0538, 0.0000, 0.0000,  ..., 0.0973, 0.0000, 0.3582],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0406],
        [0.0012, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0406],
        [0.0012, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(615804., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(544.2701, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(318.1264, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13750.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-549.4025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1377.5430, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.6576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3771],
        [ 0.1480],
        [ 0.4178],
        ...,
        [-2.6048],
        [-2.5992],
        [-2.5968]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-264738.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0234],
        [1.0240],
        [1.0230],
        ...,
        [1.0006],
        [0.9999],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369407.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0235],
        [1.0240],
        [1.0231],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369414., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0200,  0.0224,  ..., -0.0008,  0.0119,  0.0056],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [-0.0049,  0.0157,  0.0180,  ..., -0.0010,  0.0098,  0.0045],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2675.3459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.6486, device='cuda:0')



h[100].sum tensor(32.7099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.5576, device='cuda:0')



h[200].sum tensor(-1.0357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.6952, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0337, 0.0422,  ..., 0.0000, 0.0253, 0.0106],
        [0.0000, 0.0703, 0.0799,  ..., 0.0000, 0.0431, 0.0199],
        [0.0000, 0.0980, 0.1082,  ..., 0.0002, 0.0564, 0.0269],
        ...,
        [0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0105, 0.0027],
        [0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0105, 0.0027],
        [0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0105, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52653.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0363, 0.0000, 0.0000,  ..., 0.0854, 0.0000, 0.2779],
        [0.0633, 0.0000, 0.0000,  ..., 0.1024, 0.0000, 0.4073],
        [0.0967, 0.0000, 0.0000,  ..., 0.1237, 0.0000, 0.5553],
        ...,
        [0.0011, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0407],
        [0.0011, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0407],
        [0.0011, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(596588.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(375.9377, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.0470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13497.9121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-488.1939, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1411.7424, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-342.0732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3991],
        [ 0.3542],
        [ 0.2903],
        ...,
        [-2.6079],
        [-2.6024],
        [-2.6000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258068.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0235],
        [1.0240],
        [1.0231],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369414., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0236],
        [1.0241],
        [1.0231],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369420.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [-0.0045,  0.0146,  0.0168,  ..., -0.0010,  0.0092,  0.0042],
        [-0.0030,  0.0101,  0.0122,  ..., -0.0013,  0.0071,  0.0031],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2785.5796, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7544, device='cuda:0')



h[100].sum tensor(35.9744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.9824, device='cuda:0')



h[200].sum tensor(-1.1882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0167, 0.0248,  ..., 0.0000, 0.0171, 0.0064],
        [0.0000, 0.0239, 0.0321,  ..., 0.0000, 0.0206, 0.0082],
        [0.0000, 0.0864, 0.0963,  ..., 0.0000, 0.0508, 0.0241],
        ...,
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0105, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53486.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0132, 0.0000, 0.0000,  ..., 0.0732, 0.0000, 0.1224],
        [0.0236, 0.0000, 0.0000,  ..., 0.0788, 0.0000, 0.1899],
        [0.0446, 0.0000, 0.0000,  ..., 0.0908, 0.0000, 0.3075],
        ...,
        [0.0012, 0.0000, 0.0000,  ..., 0.0687, 0.0000, 0.0404],
        [0.0012, 0.0000, 0.0000,  ..., 0.0687, 0.0000, 0.0404],
        [0.0012, 0.0000, 0.0000,  ..., 0.0687, 0.0000, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(595843., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(362.3578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(295.5758, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13574.8604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-498.3777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1442.9102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-347.1425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0470],
        [ 0.2122],
        [ 0.3228],
        ...,
        [-2.6226],
        [-2.6168],
        [-2.6118]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-260248.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0236],
        [1.0241],
        [1.0231],
        ...,
        [1.0006],
        [0.9999],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369420.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0237],
        [1.0241],
        [1.0231],
        ...,
        [1.0005],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369426.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2769.0801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.2456, device='cuda:0')



h[100].sum tensor(35.3196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.6381, device='cuda:0')



h[200].sum tensor(-1.1570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.4143, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0025, 0.0102,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0025, 0.0103,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0025, 0.0103,  ..., 0.0000, 0.0103, 0.0028],
        ...,
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0105, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56621.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0013, 0.0000, 0.0000,  ..., 0.0675, 0.0000, 0.0390],
        [0.0017, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0422],
        [0.0055, 0.0000, 0.0000,  ..., 0.0700, 0.0000, 0.0646],
        ...,
        [0.0013, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0402],
        [0.0013, 0.0000, 0.0000,  ..., 0.0689, 0.0000, 0.0402],
        [0.0013, 0.0000, 0.0000,  ..., 0.0688, 0.0000, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621052.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(473.7475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(306.5562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14125.8252, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-536.1378, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1460.5654, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.4999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8492],
        [-1.5478],
        [-1.0262],
        ...,
        [-2.6402],
        [-2.6350],
        [-2.6328]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248762.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0237],
        [1.0241],
        [1.0231],
        ...,
        [1.0005],
        [0.9998],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369426.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0238],
        [1.0241],
        [1.0232],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369432.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2911.5947, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.2695, device='cuda:0')



h[100].sum tensor(40.0804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.6840, device='cuda:0')



h[200].sum tensor(-1.3599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.7758, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0028],
        ...,
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0105, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(58805.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0016, 0.0000, 0.0000,  ..., 0.0676, 0.0000, 0.0390],
        [0.0016, 0.0000, 0.0000,  ..., 0.0679, 0.0000, 0.0391],
        [0.0016, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0393],
        ...,
        [0.0016, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0402],
        [0.0016, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0402],
        [0.0016, 0.0000, 0.0000,  ..., 0.0690, 0.0000, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627114.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(633.7958, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.4133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14240.6475, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-561.1267, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1429.4121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.9144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7837],
        [-2.7480],
        [-2.6673],
        ...,
        [-2.6560],
        [-2.6502],
        [-2.6477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-261110.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0238],
        [1.0241],
        [1.0232],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369432.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0239],
        [1.0241],
        [1.0232],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369438.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2791.6636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7975, device='cuda:0')



h[100].sum tensor(36.3107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.0115, device='cuda:0')



h[200].sum tensor(-1.1784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6628, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0102, 0.0028],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0028],
        ...,
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0028],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0028],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55796.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0019, 0.0000, 0.0000,  ..., 0.0677, 0.0000, 0.0390],
        [0.0019, 0.0000, 0.0000,  ..., 0.0680, 0.0000, 0.0392],
        [0.0019, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0393],
        ...,
        [0.0020, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0402],
        [0.0020, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0402],
        [0.0020, 0.0000, 0.0000,  ..., 0.0691, 0.0000, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612549.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(518.8048, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.8314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14058.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-520.6002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1432.1932, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.1451, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6308],
        [-2.7442],
        [-2.8342],
        ...,
        [-2.6648],
        [-2.6587],
        [-2.6560]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-246810.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0239],
        [1.0241],
        [1.0232],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369438.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0240],
        [1.0241],
        [1.0232],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369444.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2846.3496, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8476, device='cuda:0')



h[100].sum tensor(38.4795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.7219, device='cuda:0')



h[200].sum tensor(-1.2502, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.1356, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0102, 0.0027],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0102, 0.0027],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0102, 0.0027],
        ...,
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0028],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0028],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57863.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0023, 0.0000, 0.0000,  ..., 0.0678, 0.0000, 0.0392],
        [0.0023, 0.0000, 0.0000,  ..., 0.0681, 0.0000, 0.0393],
        [0.0023, 0.0000, 0.0000,  ..., 0.0682, 0.0000, 0.0394],
        ...,
        [0.0023, 0.0000, 0.0000,  ..., 0.0693, 0.0000, 0.0404],
        [0.0023, 0.0000, 0.0000,  ..., 0.0693, 0.0000, 0.0404],
        [0.0023, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0403]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627962.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(727.6818, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(313.3324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14283.7021, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-544.5406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1437.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-368.2069, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6644],
        [-2.7947],
        [-2.8662],
        ...,
        [-2.6756],
        [-2.6698],
        [-2.6674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-255251.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0240],
        [1.0241],
        [1.0232],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369444.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(460.0402, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0241],
        [1.0242],
        [1.0233],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369450.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [-0.0033,  0.0110,  0.0132,  ..., -0.0012,  0.0075,  0.0033],
        [-0.0025,  0.0084,  0.0105,  ..., -0.0014,  0.0062,  0.0026],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0017,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2689.6650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.9325, device='cuda:0')



h[100].sum tensor(34.1118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7497, device='cuda:0')



h[200].sum tensor(-1.0271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8230, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0131, 0.0211,  ..., 0.0000, 0.0153, 0.0053],
        [0.0000, 0.0257, 0.0340,  ..., 0.0000, 0.0214, 0.0085],
        [0.0000, 0.0820, 0.0918,  ..., 0.0000, 0.0485, 0.0228],
        ...,
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0027],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0027],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54474.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0119, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.1100],
        [0.0240, 0.0000, 0.0000,  ..., 0.0782, 0.0000, 0.1844],
        [0.0471, 0.0000, 0.0000,  ..., 0.0908, 0.0000, 0.3102],
        ...,
        [0.0026, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0406],
        [0.0026, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0406],
        [0.0026, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612521.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(766.9317, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(299.9136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13973.4102, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-500.3998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1413.4349, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-351.2017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1124],
        [ 0.1585],
        [ 0.3520],
        ...,
        [-2.6873],
        [-2.6814],
        [-2.6789]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-285483.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0241],
        [1.0242],
        [1.0233],
        ...,
        [1.0005],
        [0.9998],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369450.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0242],
        [1.0242],
        [1.0233],
        ...,
        [1.0004],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369456.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0032,  0.0107,  0.0129,  ..., -0.0012,  0.0073,  0.0032],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3105.7104, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.9701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.9446, device='cuda:0')



h[100].sum tensor(48.0556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.1704, device='cuda:0')



h[200].sum tensor(-1.6189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.4306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0335, 0.0420,  ..., 0.0000, 0.0251, 0.0104],
        [0.0000, 0.0581, 0.0673,  ..., 0.0000, 0.0370, 0.0167],
        [0.0000, 0.0486, 0.0576,  ..., 0.0000, 0.0324, 0.0143],
        ...,
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0026],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0026],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0104, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63149.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0339, 0.0000, 0.0000,  ..., 0.0831, 0.0000, 0.2413],
        [0.0461, 0.0000, 0.0000,  ..., 0.0907, 0.0000, 0.2960],
        [0.0496, 0.0000, 0.0000,  ..., 0.0933, 0.0000, 0.3057],
        ...,
        [0.0029, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0407],
        [0.0029, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0407],
        [0.0029, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(652997.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1030.8503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(340.0876, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14667.8926, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-606.3182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1351.0771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-390.2222, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4433],
        [ 0.4199],
        [ 0.3978],
        ...,
        [-2.7015],
        [-2.6957],
        [-2.6933]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248348.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0242],
        [1.0242],
        [1.0233],
        ...,
        [1.0004],
        [0.9997],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369456.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0243],
        [1.0243],
        [1.0233],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369462.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0093,  0.0299,  0.0326,  ..., -0.0003,  0.0166,  0.0081],
        [-0.0133,  0.0424,  0.0454,  ...,  0.0003,  0.0226,  0.0112],
        [-0.0095,  0.0307,  0.0334,  ..., -0.0002,  0.0170,  0.0082],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3125.8755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.1516, device='cuda:0')



h[100].sum tensor(48.9488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.3105, device='cuda:0')



h[200].sum tensor(-1.6463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.5238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1609, 0.1729,  ..., 0.0011, 0.0865, 0.0427],
        [0.0000, 0.1409, 0.1524,  ..., 0.0004, 0.0769, 0.0377],
        [0.0000, 0.1174, 0.1283,  ..., 0.0004, 0.0656, 0.0317],
        ...,
        [0.0000, 0.0026, 0.0106,  ..., 0.0000, 0.0104, 0.0025],
        [0.0000, 0.0026, 0.0106,  ..., 0.0000, 0.0104, 0.0025],
        [0.0000, 0.0026, 0.0106,  ..., 0.0000, 0.0104, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(65150.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1189, 0.0000, 0.0000,  ..., 0.1326, 0.0000, 0.6420],
        [0.1104, 0.0000, 0.0000,  ..., 0.1277, 0.0000, 0.6042],
        [0.0971, 0.0000, 0.0000,  ..., 0.1199, 0.0000, 0.5463],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0708, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0707, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0707, 0.0000, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(660084.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1127.9153, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.3777, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14741.4805, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-629.3339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1332.3450, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-400.4204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1833],
        [ 0.1813],
        [ 0.1832],
        ...,
        [-2.5535],
        [-2.5012],
        [-2.4986]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-248563.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0243],
        [1.0243],
        [1.0233],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369462.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0243],
        [1.0244],
        [1.0234],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369468.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3059.4116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.0936, device='cuda:0')



h[100].sum tensor(47.3054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.5947, device='cuda:0')



h[200].sum tensor(-1.5582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.0474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0101, 0.0024],
        [0.0000, 0.0025, 0.0105,  ..., 0.0000, 0.0102, 0.0024],
        [0.0000, 0.0025, 0.0105,  ..., 0.0000, 0.0102, 0.0024],
        ...,
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0025, 0.0106,  ..., 0.0000, 0.0104, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62827.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0158, 0.0000, 0.0000,  ..., 0.0757, 0.0000, 0.1192],
        [0.0170, 0.0000, 0.0000,  ..., 0.0767, 0.0000, 0.1244],
        [0.0188, 0.0000, 0.0000,  ..., 0.0777, 0.0000, 0.1340],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0406],
        [0.0037, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0406],
        [0.0037, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(647096.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1015.1398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(334.3190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14539.4033, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-597.0382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1322.1417, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-399.6660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1212],
        [ 0.1347],
        [ 0.1591],
        ...,
        [-2.7451],
        [-2.7391],
        [-2.7366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-231672.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0243],
        [1.0244],
        [1.0234],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369468.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0243],
        [1.0244],
        [1.0234],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369468.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2628.3071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.4096, device='cuda:0')



h[100].sum tensor(33.3027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.3959, device='cuda:0')



h[200].sum tensor(-0.9564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.5876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0025, 0.0104,  ..., 0.0000, 0.0101, 0.0024],
        [0.0000, 0.0025, 0.0105,  ..., 0.0000, 0.0102, 0.0024],
        [0.0000, 0.0146, 0.0229,  ..., 0.0000, 0.0160, 0.0054],
        ...,
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0025, 0.0106,  ..., 0.0000, 0.0104, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50977.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0039, 0.0000, 0.0000,  ..., 0.0703, 0.0000, 0.0421],
        [0.0062, 0.0000, 0.0000,  ..., 0.0715, 0.0000, 0.0582],
        [0.0129, 0.0000, 0.0000,  ..., 0.0744, 0.0000, 0.1052],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0406],
        [0.0037, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0406],
        [0.0037, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(592725.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(862.6429, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(279.6530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13476.3066, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-449.9623, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1453.3073, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-340.4444, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2865],
        [-1.7735],
        [-1.0875],
        ...,
        [-2.7451],
        [-2.7391],
        [-2.7366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313906.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0243],
        [1.0244],
        [1.0234],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369468.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0244],
        [1.0234],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369474.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [-0.0089,  0.0288,  0.0315,  ..., -0.0003,  0.0161,  0.0077],
        ...,
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2847.0061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.6621, device='cuda:0')



h[100].sum tensor(40.9040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.2731, device='cuda:0')



h[200].sum tensor(-1.2653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.5024, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0024, 0.0105,  ..., 0.0000, 0.0101, 0.0023],
        [0.0000, 0.0313, 0.0402,  ..., 0.0000, 0.0241, 0.0096],
        [0.0000, 0.0261, 0.0348,  ..., 0.0000, 0.0216, 0.0083],
        ...,
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0104, 0.0023],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56943.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0090, 0.0000, 0.0000,  ..., 0.0737, 0.0000, 0.0670],
        [0.0203, 0.0000, 0.0000,  ..., 0.0798, 0.0000, 0.1292],
        [0.0244, 0.0000, 0.0000,  ..., 0.0817, 0.0000, 0.1561],
        ...,
        [0.0040, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.0405],
        [0.0040, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.0405],
        [0.0040, 0.0000, 0.0000,  ..., 0.0725, 0.0000, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(621902.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1047.5865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(304.6280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13982.2910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-521.0132, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1412.4309, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.7982, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1317],
        [-1.4685],
        [-0.8132],
        ...,
        [-2.7637],
        [-2.7577],
        [-2.7557]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-294602.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0244],
        [1.0234],
        ...,
        [1.0004],
        [0.9997],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369474.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0244],
        [1.0245],
        [1.0233],
        ...,
        [1.0003],
        [0.9996],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369481.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [-0.0030,  0.0103,  0.0125,  ..., -0.0013,  0.0071,  0.0030],
        [-0.0027,  0.0093,  0.0115,  ..., -0.0013,  0.0067,  0.0028],
        ...,
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2741.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7152, device='cuda:0')



h[100].sum tensor(37.4352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.9558, device='cuda:0')



h[200].sum tensor(-1.1108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0123, 0.0206,  ..., 0.0000, 0.0148, 0.0048],
        [0.0000, 0.0194, 0.0280,  ..., 0.0000, 0.0183, 0.0066],
        [0.0000, 0.0458, 0.0550,  ..., 0.0000, 0.0310, 0.0133],
        ...,
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55297.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0157, 0.0000, 0.0000,  ..., 0.0754, 0.0000, 0.1262],
        [0.0208, 0.0000, 0.0000,  ..., 0.0771, 0.0000, 0.1684],
        [0.0263, 0.0000, 0.0000,  ..., 0.0790, 0.0000, 0.2116],
        ...,
        [0.0044, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0402],
        [0.0044, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0402],
        [0.0044, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(618557.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1141.9327, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.6793, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13894.2871, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-497.7071, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1436.0692, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-363.7162, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0642],
        [ 0.2065],
        [ 0.3550],
        ...,
        [-2.7855],
        [-2.7794],
        [-2.7769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313419.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0244],
        [1.0245],
        [1.0233],
        ...,
        [1.0003],
        [0.9996],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369481.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0245],
        [1.0246],
        [1.0233],
        ...,
        [1.0003],
        [0.9996],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369487.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0124,  0.0399,  0.0429,  ...,  0.0002,  0.0214,  0.0106],
        [-0.0167,  0.0536,  0.0570,  ...,  0.0009,  0.0280,  0.0141],
        [-0.0129,  0.0414,  0.0445,  ...,  0.0003,  0.0221,  0.0110],
        ...,
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3005.1741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.2349, device='cuda:0')



h[100].sum tensor(45.9271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-13.0137, device='cuda:0')



h[200].sum tensor(-1.4534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.6608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1841, 0.1970,  ..., 0.0021, 0.0975, 0.0485],
        [0.0000, 0.2073, 0.2208,  ..., 0.0031, 0.1087, 0.0544],
        [0.0000, 0.2038, 0.2172,  ..., 0.0034, 0.1070, 0.0535],
        ...,
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0025, 0.0107,  ..., 0.0000, 0.0103, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62181.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1362, 0.0000, 0.0000,  ..., 0.1416, 0.0000, 0.6998],
        [0.1531, 0.0000, 0.0000,  ..., 0.1516, 0.0000, 0.7742],
        [0.1501, 0.0000, 0.0000,  ..., 0.1500, 0.0000, 0.7620],
        ...,
        [0.0045, 0.0000, 0.0000,  ..., 0.0731, 0.0000, 0.0402],
        [0.0045, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0402],
        [0.0045, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0402]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(654182., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1395.2224, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(326.3671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14502.4170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-581.4463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1392.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.2166, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1134],
        [ 0.0993],
        [ 0.0844],
        ...,
        [-2.7929],
        [-2.7867],
        [-2.7841]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-289249.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0245],
        [1.0246],
        [1.0233],
        ...,
        [1.0003],
        [0.9996],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369487.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0245],
        [1.0246],
        [1.0233],
        ...,
        [1.0002],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369494.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0026,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2805.5154, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5499, device='cuda:0')



h[100].sum tensor(39.1334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5206, device='cuda:0')



h[200].sum tensor(-1.1551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0016, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0163, 0.0246,  ..., 0.0000, 0.0167, 0.0058],
        [0.0000, 0.0095, 0.0176,  ..., 0.0000, 0.0135, 0.0040],
        [0.0000, 0.0126, 0.0208,  ..., 0.0000, 0.0150, 0.0048],
        ...,
        [0.0000, 0.0026, 0.0107,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0026, 0.0107,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0026, 0.0107,  ..., 0.0000, 0.0103, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56254.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0087, 0.0000, 0.0000,  ..., 0.0701, 0.0000, 0.1004],
        [0.0079, 0.0000, 0.0000,  ..., 0.0704, 0.0000, 0.0913],
        [0.0090, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.1013],
        ...,
        [0.0045, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0404],
        [0.0045, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0404],
        [0.0045, 0.0000, 0.0000,  ..., 0.0726, 0.0000, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(622785.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1133.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.1717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13977.1025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-506.4613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1361.2264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.4039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0602],
        [-0.9047],
        [-0.7190],
        ...,
        [-2.7905],
        [-2.7846],
        [-2.7822]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-287027.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0245],
        [1.0246],
        [1.0233],
        ...,
        [1.0002],
        [0.9995],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369494.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0245],
        [1.0247],
        [1.0233],
        ...,
        [1.0002],
        [0.9995],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369501.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0053,  0.0177,  0.0200,  ..., -0.0009,  0.0107,  0.0049],
        [-0.0037,  0.0126,  0.0148,  ..., -0.0012,  0.0082,  0.0036],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2911.4600, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9434, device='cuda:0')



h[100].sum tensor(42.1238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.4633, device='cuda:0')



h[200].sum tensor(-1.2645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.6290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0711, 0.0806,  ..., 0.0000, 0.0431, 0.0198],
        [0.0000, 0.0301, 0.0385,  ..., 0.0000, 0.0233, 0.0093],
        [0.0000, 0.0149, 0.0230,  ..., 0.0000, 0.0160, 0.0054],
        ...,
        [0.0000, 0.0027, 0.0106,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0027, 0.0106,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0027, 0.0106,  ..., 0.0000, 0.0103, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57791.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0392, 0.0000, 0.0000,  ..., 0.0855, 0.0000, 0.2535],
        [0.0262, 0.0000, 0.0000,  ..., 0.0791, 0.0000, 0.1864],
        [0.0168, 0.0000, 0.0000,  ..., 0.0748, 0.0000, 0.1339],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.0408],
        [0.0043, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.0408],
        [0.0043, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627191.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1109.4164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(311.9114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14072.6123, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-525.6768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1344.1997, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.6801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3593],
        [ 0.2130],
        [-0.0669],
        ...,
        [-2.7844],
        [-2.7785],
        [-2.7760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-262871.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0245],
        [1.0247],
        [1.0233],
        ...,
        [1.0002],
        [0.9995],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369501.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(420.9384, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0248],
        [1.0234],
        ...,
        [1.0002],
        [0.9995],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369508.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2809.3933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.7854, device='cuda:0')



h[100].sum tensor(38.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.0033, device='cuda:0')



h[200].sum tensor(-1.1045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.6573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0101, 0.0023],
        [0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0101, 0.0023],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0102, 0.0023],
        ...,
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0103, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55391.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0042, 0.0000, 0.0000,  ..., 0.0703, 0.0000, 0.0395],
        [0.0042, 0.0000, 0.0000,  ..., 0.0705, 0.0000, 0.0396],
        [0.0045, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0428],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0408],
        [0.0043, 0.0000, 0.0000,  ..., 0.0718, 0.0000, 0.0408],
        [0.0043, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(617948.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1093.1582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(301.1336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13918.3584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-493.8293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1356.9261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.9314, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8558],
        [-2.6953],
        [-2.4071],
        ...,
        [-2.5827],
        [-2.7195],
        [-2.7593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-276370.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0248],
        [1.0234],
        ...,
        [1.0002],
        [0.9995],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369508.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0246],
        [1.0248],
        [1.0234],
        ...,
        [1.0002],
        [0.9995],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369514.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3173.8638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.3768, device='cuda:0')



h[100].sum tensor(50.2157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-14.4629, device='cuda:0')



h[200].sum tensor(-1.5637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(9.6252, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0101, 0.0024],
        [0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0101, 0.0024],
        [0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0102, 0.0024],
        ...,
        [0.0000, 0.0028, 0.0105,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0028, 0.0105,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0028, 0.0105,  ..., 0.0000, 0.0103, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64689.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0095, 0.0000, 0.0000,  ..., 0.0708, 0.0000, 0.0888],
        [0.0058, 0.0000, 0.0000,  ..., 0.0696, 0.0000, 0.0659],
        [0.0048, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.0577],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0407],
        [0.0043, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0407],
        [0.0043, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0407]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(663040.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1322.9146, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(343.7151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14748.9268, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-607.6512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1306.9847, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.2462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3862],
        [-0.8467],
        [-1.2075],
        ...,
        [-2.7944],
        [-2.7818],
        [-2.7486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-238105.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0246],
        [1.0248],
        [1.0234],
        ...,
        [1.0002],
        [0.9995],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369514.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0247],
        [1.0249],
        [1.0235],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369519.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0061,  0.0203,  0.0227,  ..., -0.0008,  0.0119,  0.0056],
        [-0.0037,  0.0124,  0.0146,  ..., -0.0012,  0.0081,  0.0036],
        [-0.0082,  0.0268,  0.0294,  ..., -0.0004,  0.0151,  0.0073],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3290.2876, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.3137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-23.0396, device='cuda:0')



h[100].sum tensor(53.6279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.5878, device='cuda:0')



h[200].sum tensor(-1.7058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.3739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0511, 0.0599,  ..., 0.0000, 0.0334, 0.0147],
        [0.0000, 0.0678, 0.0771,  ..., 0.0000, 0.0415, 0.0190],
        [0.0000, 0.0367, 0.0452,  ..., 0.0000, 0.0266, 0.0110],
        ...,
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66435.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0324, 0.0000, 0.0000,  ..., 0.0807, 0.0000, 0.2405],
        [0.0393, 0.0000, 0.0000,  ..., 0.0856, 0.0000, 0.2622],
        [0.0339, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.2269],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0723, 0.0000, 0.0405],
        [0.0043, 0.0000, 0.0000,  ..., 0.0723, 0.0000, 0.0405],
        [0.0043, 0.0000, 0.0000,  ..., 0.0723, 0.0000, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(668404.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1366.3799, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(348.7992, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14766.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-630.4885, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1354.7732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-410.8010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4581],
        [ 0.5340],
        [ 0.5382],
        ...,
        [-2.7963],
        [-2.7813],
        [-2.7747]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-273218.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0247],
        [1.0249],
        [1.0235],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369519.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0250],
        [1.0235],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369525.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0029,  0.0099,  0.0120,  ..., -0.0013,  0.0070,  0.0029],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2768.5383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9732, device='cuda:0')



h[100].sum tensor(36.6724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.4538, device='cuda:0')



h[200].sum tensor(-1.0174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.2916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0347, 0.0432,  ..., 0.0000, 0.0256, 0.0105],
        [0.0000, 0.0121, 0.0201,  ..., 0.0000, 0.0148, 0.0048],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0102, 0.0023],
        ...,
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0104, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54923.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0320, 0.0000, 0.0000,  ..., 0.0836, 0.0000, 0.2167],
        [0.0178, 0.0000, 0.0000,  ..., 0.0769, 0.0000, 0.1343],
        [0.0087, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0768],
        ...,
        [0.0041, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0404],
        [0.0041, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0404],
        [0.0041, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0403]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(619234.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(873.1766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(294.4061, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13952.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-488.1856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1455.4484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-372.9821, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2108],
        [-0.0959],
        [-0.5454],
        ...,
        [-2.8372],
        [-2.8311],
        [-2.8285]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-258186.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0250],
        [1.0235],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369525.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0248],
        [1.0250],
        [1.0236],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369530.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [-0.0056,  0.0187,  0.0211,  ..., -0.0008,  0.0112,  0.0052],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2706.2144, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-12.8993, device='cuda:0')



h[100].sum tensor(34.6127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-8.7273, device='cuda:0')



h[200].sum tensor(-0.9313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.8081, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0023],
        [0.0000, 0.0287, 0.0371,  ..., 0.0000, 0.0229, 0.0090],
        [0.0000, 0.0271, 0.0355,  ..., 0.0000, 0.0221, 0.0085],
        ...,
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0105, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52959.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0119, 0.0000, 0.0000,  ..., 0.0748, 0.0000, 0.0971],
        [0.0161, 0.0000, 0.0000,  ..., 0.0762, 0.0000, 0.1352],
        [0.0189, 0.0000, 0.0000,  ..., 0.0767, 0.0000, 0.1653],
        ...,
        [0.0038, 0.0000, 0.0000,  ..., 0.0731, 0.0000, 0.0405],
        [0.0038, 0.0000, 0.0000,  ..., 0.0731, 0.0000, 0.0405],
        [0.0038, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0404]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(612005.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(923.6904, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(284.6571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13729.4521, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-466.0448, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1530.9800, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-357.9089, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2367],
        [-0.1505],
        [ 0.0725],
        ...,
        [-2.8363],
        [-2.8400],
        [-2.8388]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-316371.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0248],
        [1.0250],
        [1.0236],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369530.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0249],
        [1.0251],
        [1.0236],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369535.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2891.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.2393, device='cuda:0')



h[100].sum tensor(40.3708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.9870, device='cuda:0')



h[200].sum tensor(-1.1550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0102,  ..., 0.0000, 0.0104, 0.0023],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0104, 0.0023],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0104, 0.0023],
        ...,
        [0.0000, 0.0026, 0.0105,  ..., 0.0000, 0.0106, 0.0023],
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0106, 0.0023],
        [0.0000, 0.0026, 0.0104,  ..., 0.0000, 0.0106, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56517.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0041, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0463],
        [0.0038, 0.0000, 0.0000,  ..., 0.0715, 0.0000, 0.0468],
        [0.0042, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0511],
        ...,
        [0.0043, 0.0000, 0.0000,  ..., 0.0734, 0.0000, 0.0451],
        [0.0036, 0.0000, 0.0000,  ..., 0.0731, 0.0000, 0.0406],
        [0.0036, 0.0000, 0.0000,  ..., 0.0730, 0.0000, 0.0406]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(627126.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(976.9941, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(302.3234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13982.4404, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-511.9412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1517.6914, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-370.9471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7595],
        [-1.7177],
        [-1.4644],
        ...,
        [-2.6944],
        [-2.7961],
        [-2.8373]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-309432.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0249],
        [1.0251],
        [1.0236],
        ...,
        [1.0001],
        [0.9994],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369535.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0251],
        [1.0253],
        [1.0237],
        ...,
        [1.0000],
        [0.9994],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369541.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3003.5835, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.9825, device='cuda:0')



h[100].sum tensor(43.6004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.1664, device='cuda:0')



h[200].sum tensor(-1.2826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.0969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0102,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0026, 0.0102,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0026, 0.0102,  ..., 0.0000, 0.0105, 0.0023],
        ...,
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0023],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0023],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59839.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0000, 0.0000,  ..., 0.0710, 0.0000, 0.0437],
        [0.0032, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0410],
        [0.0032, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0398],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0033, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0033, 0.0000, 0.0000,  ..., 0.0728, 0.0000, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(640275.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(900.7064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(320.8141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14238.2998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-555.4416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1491.0415, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.5573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8569],
        [-2.2748],
        [-2.5563],
        ...,
        [-2.8622],
        [-2.8563],
        [-2.8537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279632.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0251],
        [1.0253],
        [1.0237],
        ...,
        [1.0000],
        [0.9994],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369541.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0252],
        [1.0254],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369546.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2849.0210, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2789, device='cuda:0')



h[100].sum tensor(38.9833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.3372, device='cuda:0')



h[200].sum tensor(-1.0778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.8795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0027, 0.0101,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0027, 0.0102,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0027, 0.0102,  ..., 0.0000, 0.0105, 0.0023],
        ...,
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0024],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0024],
        [0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0107, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55753.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0396],
        [0.0031, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0397],
        [0.0031, 0.0000, 0.0000,  ..., 0.0717, 0.0000, 0.0398],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(623848.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(732.8370, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(303.6602, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14001.7197, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-502.6219, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1513.0837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.4662, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0466],
        [-2.9637],
        [-2.8025],
        ...,
        [-2.8718],
        [-2.8658],
        [-2.8633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279306.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0252],
        [1.0254],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369546.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0252],
        [1.0254],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369546.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [-0.0059,  0.0197,  0.0220,  ..., -0.0008,  0.0117,  0.0054],
        [-0.0063,  0.0209,  0.0232,  ..., -0.0007,  0.0123,  0.0058],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3020.6189, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.3436, device='cuda:0')



h[100].sum tensor(44.4180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-12.4107, device='cuda:0')



h[200].sum tensor(-1.2932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(8.2595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0221, 0.0300,  ..., 0.0000, 0.0198, 0.0073],
        [0.0000, 0.0394, 0.0478,  ..., 0.0000, 0.0282, 0.0117],
        [0.0000, 0.1120, 0.1222,  ..., 0.0002, 0.0631, 0.0303],
        ...,
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0024],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0024],
        [0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0107, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60237.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0168, 0.0000, 0.0000,  ..., 0.0785, 0.0000, 0.1252],
        [0.0296, 0.0000, 0.0000,  ..., 0.0856, 0.0000, 0.2029],
        [0.0537, 0.0000, 0.0000,  ..., 0.0991, 0.0000, 0.3367],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(648892.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(910.5562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(324.3533, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14436.9326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-558.7710, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1468.0756, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-391.6010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0282],
        [-0.5164],
        [ 0.0158],
        ...,
        [-2.8718],
        [-2.8658],
        [-2.8633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-265596.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0252],
        [1.0254],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369546.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0252],
        [1.0254],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369546.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0026,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3270.0300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.7986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-22.7634, device='cuda:0')



h[100].sum tensor(52.3172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-15.4010, device='cuda:0')



h[200].sum tensor(-1.6063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.2495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0027, 0.0101,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0027, 0.0102,  ..., 0.0000, 0.0105, 0.0023],
        [0.0000, 0.0027, 0.0102,  ..., 0.0000, 0.0105, 0.0023],
        ...,
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0024],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0107, 0.0024],
        [0.0000, 0.0027, 0.0103,  ..., 0.0000, 0.0107, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68091.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0031, 0.0000, 0.0000,  ..., 0.0714, 0.0000, 0.0396],
        [0.0032, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0423],
        [0.0039, 0.0000, 0.0000,  ..., 0.0716, 0.0000, 0.0536],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408],
        [0.0032, 0.0000, 0.0000,  ..., 0.0729, 0.0000, 0.0408]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(684959.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1016.4307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(360.7669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15056.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-656.6803, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1429.2672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-428.3751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9754],
        [-1.8045],
        [-1.3948],
        ...,
        [-2.8630],
        [-2.8615],
        [-2.8603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-237615.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0252],
        [1.0254],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369546.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(404.0984, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0253],
        [1.0255],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369552.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2957.9043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.3850, device='cuda:0')



h[100].sum tensor(43.9176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.7621, device='cuda:0')



h[200].sum tensor(-1.2186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.8278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0102,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0026, 0.0102,  ..., 0.0000, 0.0104, 0.0024],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0104, 0.0024],
        ...,
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0106, 0.0024],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0106, 0.0024],
        [0.0000, 0.0027, 0.0104,  ..., 0.0000, 0.0106, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61104.0117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.0000,  ..., 0.0721, 0.0000, 0.0388],
        [0.0034, 0.0000, 0.0000,  ..., 0.0723, 0.0000, 0.0389],
        [0.0034, 0.0000, 0.0000,  ..., 0.0724, 0.0000, 0.0390],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.0400],
        [0.0035, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.0400],
        [0.0034, 0.0000, 0.0000,  ..., 0.0736, 0.0000, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(661525.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1174.0686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(323.7068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14667.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-562.9284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1494.0089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.9254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5411],
        [-2.6837],
        [-2.7648],
        ...,
        [-2.9003],
        [-2.8943],
        [-2.8917]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-321527.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0253],
        [1.0255],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369552.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0254],
        [1.0255],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369557.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        ...,
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0006,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2983.9822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6886, device='cuda:0')



h[100].sum tensor(45.9893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.9675, device='cuda:0')



h[200].sum tensor(-1.2456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.9645, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0103, 0.0024],
        [0.0000, 0.0026, 0.0103,  ..., 0.0000, 0.0104, 0.0025],
        ...,
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0105, 0.0025],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0105, 0.0025],
        [0.0000, 0.0027, 0.0105,  ..., 0.0000, 0.0105, 0.0025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59588.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0033, 0.0000, 0.0000,  ..., 0.0720, 0.0000, 0.0400],
        [0.0037, 0.0000, 0.0000,  ..., 0.0719, 0.0000, 0.0488],
        [0.0063, 0.0000, 0.0000,  ..., 0.0727, 0.0000, 0.0713],
        ...,
        [0.0036, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.0394],
        [0.0036, 0.0000, 0.0000,  ..., 0.0739, 0.0000, 0.0394],
        [0.0036, 0.0000, 0.0000,  ..., 0.0738, 0.0000, 0.0394]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(639684.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(820.7336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(315.7446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14540.0449, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-537.1098, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1410.0591, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-394.3664, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2917],
        [-1.8822],
        [-1.3540],
        ...,
        [-2.9165],
        [-2.9105],
        [-2.9079]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-279043.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0254],
        [1.0255],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369557.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0256],
        [1.0256],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369562.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2832e-03,  2.1050e-02,  2.3416e-02,  ..., -7.7651e-04,
          1.2313e-02,  5.8531e-03],
        [-1.1540e-02,  3.8111e-02,  4.0920e-02,  ...,  6.2027e-05,
          2.0515e-02,  1.0235e-02],
        [-1.3958e-02,  4.5958e-02,  4.8972e-02,  ...,  4.4772e-04,
          2.4288e-02,  1.2251e-02],
        ...,
        [ 0.0000e+00,  6.5898e-04,  2.4945e-03,  ..., -1.7787e-03,
          2.5107e-03,  6.1564e-04],
        [ 0.0000e+00,  6.5898e-04,  2.4945e-03,  ..., -1.7787e-03,
          2.5107e-03,  6.1564e-04],
        [ 0.0000e+00,  6.5898e-04,  2.4945e-03,  ..., -1.7787e-03,
          2.5107e-03,  6.1564e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2868.3499, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.6582, device='cuda:0')



h[100].sum tensor(43.7751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-10.5938, device='cuda:0')



h[200].sum tensor(-1.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.0503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1367, 0.1477,  ..., 0.0009, 0.0747, 0.0369],
        [0.0000, 0.1807, 0.1928,  ..., 0.0022, 0.0959, 0.0482],
        [0.0000, 0.1801, 0.1923,  ..., 0.0014, 0.0956, 0.0481],
        ...,
        [0.0000, 0.0028, 0.0104,  ..., 0.0000, 0.0105, 0.0026],
        [0.0000, 0.0028, 0.0104,  ..., 0.0000, 0.0105, 0.0026],
        [0.0000, 0.0028, 0.0104,  ..., 0.0000, 0.0105, 0.0026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57676.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1069, 0.0000, 0.0000,  ..., 0.1232, 0.0000, 0.5938],
        [0.1254, 0.0000, 0.0000,  ..., 0.1338, 0.0000, 0.6770],
        [0.1316, 0.0000, 0.0000,  ..., 0.1374, 0.0000, 0.7049],
        ...,
        [0.0036, 0.0000, 0.0000,  ..., 0.0733, 0.0000, 0.0393],
        [0.0036, 0.0000, 0.0000,  ..., 0.0733, 0.0000, 0.0393],
        [0.0036, 0.0000, 0.0000,  ..., 0.0733, 0.0000, 0.0393]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(635131.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(880.3063, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(310.9317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14583.7793, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-509.2603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1355.3289, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.2924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2944],
        [ 0.2654],
        [ 0.2553],
        ...,
        [-2.9129],
        [-2.9066],
        [-2.9038]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-283987.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0256],
        [1.0256],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369562.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0257],
        [1.0257],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369567.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0054,  0.0184,  0.0206,  ..., -0.0009,  0.0110,  0.0052],
        [-0.0044,  0.0150,  0.0172,  ..., -0.0011,  0.0094,  0.0043],
        [-0.0100,  0.0333,  0.0360,  ..., -0.0002,  0.0182,  0.0090],
        ...,
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006],
        [ 0.0000,  0.0007,  0.0025,  ..., -0.0018,  0.0025,  0.0006]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2573.4836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.2918, device='cuda:0')



h[100].sum tensor(36.0711, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-6.9631, device='cuda:0')



h[200].sum tensor(-0.7054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(4.6340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0322, 0.0402,  ..., 0.0000, 0.0244, 0.0102],
        [0.0000, 0.0961, 0.1058,  ..., 0.0000, 0.0551, 0.0266],
        [0.0000, 0.0997, 0.1095,  ..., 0.0000, 0.0568, 0.0275],
        ...,
        [0.0000, 0.0029, 0.0103,  ..., 0.0000, 0.0105, 0.0027],
        [0.0000, 0.0029, 0.0103,  ..., 0.0000, 0.0105, 0.0027],
        [0.0000, 0.0029, 0.0103,  ..., 0.0000, 0.0105, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49485.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0258, 0.0000, 0.0000,  ..., 0.0803, 0.0000, 0.1738],
        [0.0470, 0.0000, 0.0000,  ..., 0.0907, 0.0000, 0.2870],
        [0.0551, 0.0000, 0.0000,  ..., 0.0947, 0.0000, 0.3302],
        ...,
        [0.0035, 0.0000, 0.0000,  ..., 0.0722, 0.0000, 0.0396],
        [0.0035, 0.0000, 0.0000,  ..., 0.0722, 0.0000, 0.0396],
        [0.0035, 0.0000, 0.0000,  ..., 0.0722, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(593194.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(701.4706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(281.0742, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13951.6621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-403.9596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1335.2676, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-336.9730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8204],
        [-0.1168],
        [ 0.3143],
        ...,
        [-2.8985],
        [-2.8926],
        [-2.8902]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-313065.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0257],
        [1.0257],
        [1.0238],
        ...,
        [1.0000],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369567.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0258],
        [1.0238],
        ...,
        [0.9999],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369572.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0028,  0.0100,  0.0119,  ..., -0.0013,  0.0069,  0.0030],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [-0.0028,  0.0100,  0.0119,  ..., -0.0013,  0.0069,  0.0030],
        ...,
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2832.2971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.6100, device='cuda:0')



h[100].sum tensor(45.7822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-9.8846, device='cuda:0')



h[200].sum tensor(-0.9985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.5783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0179,  ..., 0.0000, 0.0140, 0.0047],
        [0.0000, 0.0374, 0.0453,  ..., 0.0000, 0.0268, 0.0116],
        [0.0000, 0.0107, 0.0180,  ..., 0.0000, 0.0140, 0.0047],
        ...,
        [0.0000, 0.0030, 0.0102,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0030, 0.0102,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0030, 0.0102,  ..., 0.0000, 0.0105, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55288.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0065, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.0768],
        [0.0091, 0.0000, 0.0000,  ..., 0.0698, 0.0000, 0.1045],
        [0.0065, 0.0000, 0.0000,  ..., 0.0698, 0.0000, 0.0772],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0398],
        [0.0032, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0398],
        [0.0032, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(614614.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(747.9131, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(316.5504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14506.6846, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-472.0602, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1224.5758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.3752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0319],
        [-2.0827],
        [-2.2883],
        ...,
        [-2.8851],
        [-2.8793],
        [-2.8769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-272421., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0258],
        [1.0238],
        ...,
        [0.9999],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369572.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0258],
        [1.0258],
        [1.0238],
        ...,
        [0.9999],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369572.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [-0.0039,  0.0134,  0.0155,  ..., -0.0012,  0.0086,  0.0039],
        [-0.0145,  0.0480,  0.0510,  ...,  0.0005,  0.0252,  0.0128],
        ...,
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(4039.3093, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-21.5770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-36.2154, device='cuda:0')



h[100].sum tensor(83.7409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-24.5022, device='cuda:0')



h[200].sum tensor(-2.4530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.3065, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0372, 0.0452,  ..., 0.0000, 0.0267, 0.0115],
        [0.0000, 0.1040, 0.1138,  ..., 0.0005, 0.0589, 0.0287],
        [0.0000, 0.1383, 0.1490,  ..., 0.0009, 0.0753, 0.0376],
        ...,
        [0.0000, 0.0030, 0.0102,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0030, 0.0102,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0030, 0.0102,  ..., 0.0000, 0.0105, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(87087.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0531, 0.0000, 0.0000,  ..., 0.0928, 0.0000, 0.3084],
        [0.0993, 0.0000, 0.0000,  ..., 0.1163, 0.0000, 0.5331],
        [0.1369, 0.0000, 0.0000,  ..., 0.1353, 0.0000, 0.7152],
        ...,
        [0.0032, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0398],
        [0.0032, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0398],
        [0.0032, 0.0000, 0.0000,  ..., 0.0712, 0.0000, 0.0398]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(777707.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1908.4335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(462.9420, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17233.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-868.8972, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1127.4504, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-482.4410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0408],
        [ 0.0500],
        [ 0.0248],
        ...,
        [-2.8851],
        [-2.8793],
        [-2.8769]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-249911.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0258],
        [1.0258],
        [1.0238],
        ...,
        [0.9999],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369572.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0259],
        [1.0259],
        [1.0238],
        ...,
        [0.9999],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369576.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        ...,
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2671.2400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-11.6853, device='cuda:0')



h[100].sum tensor(42.3231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-7.9059, device='cuda:0')



h[200].sum tensor(-0.7926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(5.2615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0099,  ..., 0.0000, 0.0103, 0.0027],
        [0.0000, 0.0030, 0.0099,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0030, 0.0100,  ..., 0.0000, 0.0103, 0.0028],
        ...,
        [0.0000, 0.0031, 0.0101,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0031, 0.0101,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0031, 0.0101,  ..., 0.0000, 0.0105, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51896.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0030, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0389],
        [0.0030, 0.0000, 0.0000,  ..., 0.0694, 0.0000, 0.0390],
        [0.0038, 0.0000, 0.0000,  ..., 0.0698, 0.0000, 0.0437],
        ...,
        [0.0031, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0401],
        [0.0031, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0401],
        [0.0031, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(600361.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(754.2669, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(306.6970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14309.8779, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-427.0446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1230.5544, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-339.2962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8636],
        [-2.6547],
        [-2.2859],
        ...,
        [-2.8795],
        [-2.8737],
        [-2.8714]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-293967.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0259],
        [1.0259],
        [1.0238],
        ...,
        [0.9999],
        [0.9993],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369576.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0260],
        [1.0260],
        [1.0238],
        ...,
        [0.9999],
        [0.9992],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369580.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        ...,
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2927.3701, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3485, device='cuda:0')



h[100].sum tensor(51.5433, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.0609, device='cuda:0')



h[200].sum tensor(-1.0959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(7.3612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0099,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0031, 0.0099,  ..., 0.0000, 0.0103, 0.0028],
        [0.0000, 0.0031, 0.0099,  ..., 0.0000, 0.0103, 0.0028],
        ...,
        [0.0000, 0.0031, 0.0101,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0031, 0.0101,  ..., 0.0000, 0.0105, 0.0028],
        [0.0000, 0.0031, 0.0101,  ..., 0.0000, 0.0105, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57831.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0032, 0.0000, 0.0000,  ..., 0.0692, 0.0000, 0.0388],
        [0.0032, 0.0000, 0.0000,  ..., 0.0694, 0.0000, 0.0389],
        [0.0032, 0.0000, 0.0000,  ..., 0.0695, 0.0000, 0.0391],
        ...,
        [0.0033, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0400],
        [0.0033, 0.0000, 0.0000,  ..., 0.0706, 0.0000, 0.0400],
        [0.0033, 0.0000, 0.0000,  ..., 0.0705, 0.0000, 0.0400]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(624281.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(817.5952, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(335.5748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14899.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-497.2729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1164.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-371.6197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8854],
        [-2.9211],
        [-2.8956],
        ...,
        [-2.8897],
        [-2.8839],
        [-2.8815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-244628.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0260],
        [1.0260],
        [1.0238],
        ...,
        [0.9999],
        [0.9992],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369580.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0261],
        [1.0261],
        [1.0238],
        ...,
        [0.9999],
        [0.9992],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(369584.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0151,  0.0501,  0.0532,  ...,  0.0006,  0.0262,  0.0134],
        [-0.0100,  0.0334,  0.0359,  ..., -0.0002,  0.0182,  0.0091],
        [-0.0052,  0.0177,  0.0199,  ..., -0.0010,  0.0107,  0.0051],
        ...,
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007],
        [ 0.0000,  0.0007,  0.0024,  ..., -0.0018,  0.0025,  0.0007]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3388.5525, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-14.7188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-24.2884, device='cuda:0')



h[100].sum tensor(66.9184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-16.4327, device='cuda:0')



h[200].sum tensor(-1.6496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(10.9362, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.1528, 0.1637,  ..., 0.0008, 0.0822, 0.0415],
        [0.0000, 0.1127, 0.1226,  ..., 0.0006, 0.0629, 0.0311],
        [0.0000, 0.0643, 0.0730,  ..., 0.0000, 0.0397, 0.0186],
        ...,
        [0.0000, 0.0031, 0.0102,  ..., 0.0000, 0.0104, 0.0028],
        [0.0000, 0.0031, 0.0102,  ..., 0.0000, 0.0104, 0.0028],
        [0.0000, 0.0031, 0.0101,  ..., 0.0000, 0.0104, 0.0028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(70504.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.1036, 0.0000, 0.0000,  ..., 0.1150, 0.0000, 0.5619],
        [0.0869, 0.0000, 0.0000,  ..., 0.1066, 0.0000, 0.4868],
        [0.0659, 0.0000, 0.0000,  ..., 0.0961, 0.0000, 0.3894],
        ...,
        [0.0037, 0.0000, 0.0000,  ..., 0.0713, 0.0000, 0.0396],
        [0.0037, 0.0000, 0.0000,  ..., 0.0713, 0.0000, 0.0396],
        [0.0037, 0.0000, 0.0000,  ..., 0.0713, 0.0000, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(689530.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1314.7437, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(389.6919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16132.6113, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-653.5168, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1136.9089, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-423.8980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3883],
        [ 0.4182],
        [ 0.4545],
        ...,
        [-2.9186],
        [-2.9126],
        [-2.9102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(-236002.8906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0261],
        [1.0261],
        [1.0238],
        ...,
        [0.9999],
        [0.9992],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(369584.7500, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:35.309556
evaluation loss: 521.0322265625
epoch: 0 mean loss: 490.60595703125
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 138, in <module>
    writer.add_hparams({'Lr': LrVal, 'weight_decay': weight_decay_val, 'BatchSize':BatchSize},\
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 313, in add_hparams
    w_hp.add_scalar(k, v)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py", line 354, in add_scalar
    summary = scalar(
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/utils/tensorboard/summary.py", line 250, in scalar
    assert scalar.squeeze().ndim == 0, "scalar should be 0D"
AssertionError: scalar should be 0D

real	1m3.499s
user	0m45.656s
sys	0m16.329s
