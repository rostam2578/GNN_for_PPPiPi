0: gpu033.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-6d3c827b-c4d8-2ca2-4892-99cb918e77c7)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Jul  6 00:52:07 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |
| N/A   34C    P0    43W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b813f825910> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m32.412s
user	0m2.880s
sys	0m2.280s
[00:52:40] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[-1.4373],
        [ 2.7789],
        [ 1.6236],
        ...,
        [-1.1553],
        [-1.2738],
        [ 1.0489]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-87.7319, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (2000, 6796) (2000, 6796)
sum 189931 265535
shape torch.Size([2000, 6796]) torch.Size([2000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-1.3305e-01, -1.4074e-02, -2.5679e-02, -2.2594e-02,  1.1070e-01,
         -1.3175e-01, -1.1737e-01,  1.3448e-01,  4.8590e-02, -3.7385e-02,
          1.1415e-01,  1.2938e-01, -1.5321e-03,  9.2711e-02, -5.0303e-02,
         -1.4778e-01,  8.5119e-03,  8.5105e-02, -3.6557e-02,  6.5958e-03,
          3.1901e-02, -1.9473e-02, -8.4900e-02, -5.7682e-02,  3.1592e-02,
         -9.4657e-02, -1.2756e-01,  2.1374e-02, -5.1316e-02,  1.4592e-01,
         -8.1379e-02,  9.0489e-02, -1.2524e-01,  1.7933e-02, -1.3411e-01,
         -4.3661e-02,  1.4212e-01,  2.1101e-02,  4.2854e-02,  6.3570e-02,
         -1.4912e-01,  1.2747e-01, -1.4389e-01,  1.0485e-01,  1.3564e-01,
          4.1327e-02,  1.1098e-01, -1.0943e-01, -1.4422e-01, -1.0887e-01,
         -6.3398e-02, -1.1030e-01,  3.2196e-02,  1.1258e-01,  1.3461e-01,
         -7.8570e-02, -6.4242e-02, -1.1927e-01,  1.0007e-01,  4.5868e-02,
          1.1610e-01, -1.0971e-01,  1.1373e-01, -5.7723e-02, -7.0640e-02,
         -2.6538e-02,  1.2414e-01,  9.3708e-02,  1.0577e-01, -5.0866e-02,
         -6.4143e-02, -1.2052e-01,  9.3087e-02, -1.3453e-01, -5.7331e-02,
          1.5147e-01,  9.4389e-02,  1.2935e-01,  1.1098e-01, -1.0710e-01,
         -2.4487e-02, -4.0818e-02,  9.8906e-02, -1.1003e-01,  1.4984e-01,
          1.3338e-01,  6.8447e-02, -2.5558e-02, -3.4516e-02,  3.2232e-02,
         -1.3225e-01,  6.4813e-03,  2.7760e-02, -1.3430e-01, -9.6020e-02,
         -4.8855e-02,  8.6496e-02, -3.7573e-02,  4.7450e-02,  2.7948e-02,
         -1.2574e-01, -3.5766e-02, -8.5023e-02,  6.6778e-02, -3.6458e-02,
         -5.2364e-02,  2.1109e-02,  7.1006e-02,  2.8703e-02, -1.4142e-04,
         -7.7885e-02, -1.1676e-01,  2.3594e-02, -8.7531e-02, -1.7559e-02,
          3.9449e-02, -1.6564e-02,  1.0653e-01, -1.3889e-01,  1.4967e-01,
          3.2682e-02, -1.3607e-01,  9.3834e-02, -2.9454e-02,  1.0281e-01,
         -1.1987e-01, -6.5180e-02,  1.2284e-01,  1.4587e-01, -1.4508e-01,
          6.9429e-02,  9.6024e-02,  6.8678e-02, -5.6293e-02,  1.8231e-02,
         -8.1100e-02,  6.7183e-02, -1.0579e-01, -4.0967e-02,  1.4319e-01,
         -1.4974e-01,  3.6580e-02,  1.4431e-02,  2.7461e-02,  7.8623e-02,
          1.3312e-01,  1.1768e-01,  9.4526e-02, -6.3869e-02, -2.8680e-02,
          7.9233e-02, -1.2739e-01,  2.8218e-02, -9.2921e-02, -1.0294e-01,
          6.5225e-03,  5.7138e-02,  1.3199e-01,  9.3442e-02,  1.2317e-01,
         -1.3022e-01,  6.4828e-02, -1.0016e-02,  1.1186e-01,  1.2423e-01,
          2.4973e-02,  4.5941e-02, -1.9466e-02,  5.3263e-02, -6.9702e-03,
          1.2830e-01, -6.3246e-02, -1.0610e-01,  1.2176e-01,  5.6518e-03,
          1.5127e-01,  6.6974e-02,  1.1947e-01, -1.4084e-02,  1.4838e-01,
          6.1727e-02, -6.0509e-02,  9.8658e-02, -1.0833e-01, -1.4942e-02,
          1.4547e-01, -9.5649e-02, -6.8941e-03,  8.5007e-03,  1.7738e-02,
         -9.2776e-02, -1.3424e-01, -6.7018e-02,  3.3228e-02, -4.9316e-03,
          1.4183e-01, -9.2179e-02,  5.7496e-02,  1.6406e-02,  1.4065e-01,
         -5.9949e-02, -6.2568e-02, -8.8071e-02,  1.1553e-01, -1.1479e-01,
         -5.3267e-02,  5.9051e-02, -1.3911e-01, -1.1670e-01,  1.4739e-01,
          7.6638e-02, -1.2206e-01, -3.8069e-02,  4.5506e-02,  1.7965e-03,
          7.1747e-02, -1.0167e-02,  1.1629e-01,  1.5142e-01,  1.1722e-01,
         -2.1067e-02,  8.4124e-02, -8.2450e-02,  8.6083e-02,  9.0706e-02,
         -9.9612e-02,  9.4944e-03,  1.3457e-01,  8.2560e-02, -7.7872e-02,
         -1.0480e-01, -1.0168e-01, -3.9002e-03,  6.1939e-02, -1.1422e-01,
         -1.0176e-01, -2.5755e-03, -1.5241e-01, -1.5276e-01, -1.4462e-02,
          4.4459e-02,  1.0874e-01, -7.5657e-02,  2.2231e-02,  7.3281e-02,
         -1.3676e-01,  1.2034e-02,  2.8863e-02, -7.8683e-02,  3.3847e-02,
          3.1426e-02,  4.2511e-02, -6.9854e-02,  6.4745e-02, -7.9351e-02,
          3.5184e-02]], device='cuda:0') 
 Parameter containing:
tensor([[-1.3305e-01, -1.4074e-02, -2.5679e-02, -2.2594e-02,  1.1070e-01,
         -1.3175e-01, -1.1737e-01,  1.3448e-01,  4.8590e-02, -3.7385e-02,
          1.1415e-01,  1.2938e-01, -1.5321e-03,  9.2711e-02, -5.0303e-02,
         -1.4778e-01,  8.5119e-03,  8.5105e-02, -3.6557e-02,  6.5958e-03,
          3.1901e-02, -1.9473e-02, -8.4900e-02, -5.7682e-02,  3.1592e-02,
         -9.4657e-02, -1.2756e-01,  2.1374e-02, -5.1316e-02,  1.4592e-01,
         -8.1379e-02,  9.0489e-02, -1.2524e-01,  1.7933e-02, -1.3411e-01,
         -4.3661e-02,  1.4212e-01,  2.1101e-02,  4.2854e-02,  6.3570e-02,
         -1.4912e-01,  1.2747e-01, -1.4389e-01,  1.0485e-01,  1.3564e-01,
          4.1327e-02,  1.1098e-01, -1.0943e-01, -1.4422e-01, -1.0887e-01,
         -6.3398e-02, -1.1030e-01,  3.2196e-02,  1.1258e-01,  1.3461e-01,
         -7.8570e-02, -6.4242e-02, -1.1927e-01,  1.0007e-01,  4.5868e-02,
          1.1610e-01, -1.0971e-01,  1.1373e-01, -5.7723e-02, -7.0640e-02,
         -2.6538e-02,  1.2414e-01,  9.3708e-02,  1.0577e-01, -5.0866e-02,
         -6.4143e-02, -1.2052e-01,  9.3087e-02, -1.3453e-01, -5.7331e-02,
          1.5147e-01,  9.4389e-02,  1.2935e-01,  1.1098e-01, -1.0710e-01,
         -2.4487e-02, -4.0818e-02,  9.8906e-02, -1.1003e-01,  1.4984e-01,
          1.3338e-01,  6.8447e-02, -2.5558e-02, -3.4516e-02,  3.2232e-02,
         -1.3225e-01,  6.4813e-03,  2.7760e-02, -1.3430e-01, -9.6020e-02,
         -4.8855e-02,  8.6496e-02, -3.7573e-02,  4.7450e-02,  2.7948e-02,
         -1.2574e-01, -3.5766e-02, -8.5023e-02,  6.6778e-02, -3.6458e-02,
         -5.2364e-02,  2.1109e-02,  7.1006e-02,  2.8703e-02, -1.4142e-04,
         -7.7885e-02, -1.1676e-01,  2.3594e-02, -8.7531e-02, -1.7559e-02,
          3.9449e-02, -1.6564e-02,  1.0653e-01, -1.3889e-01,  1.4967e-01,
          3.2682e-02, -1.3607e-01,  9.3834e-02, -2.9454e-02,  1.0281e-01,
         -1.1987e-01, -6.5180e-02,  1.2284e-01,  1.4587e-01, -1.4508e-01,
          6.9429e-02,  9.6024e-02,  6.8678e-02, -5.6293e-02,  1.8231e-02,
         -8.1100e-02,  6.7183e-02, -1.0579e-01, -4.0967e-02,  1.4319e-01,
         -1.4974e-01,  3.6580e-02,  1.4431e-02,  2.7461e-02,  7.8623e-02,
          1.3312e-01,  1.1768e-01,  9.4526e-02, -6.3869e-02, -2.8680e-02,
          7.9233e-02, -1.2739e-01,  2.8218e-02, -9.2921e-02, -1.0294e-01,
          6.5225e-03,  5.7138e-02,  1.3199e-01,  9.3442e-02,  1.2317e-01,
         -1.3022e-01,  6.4828e-02, -1.0016e-02,  1.1186e-01,  1.2423e-01,
          2.4973e-02,  4.5941e-02, -1.9466e-02,  5.3263e-02, -6.9702e-03,
          1.2830e-01, -6.3246e-02, -1.0610e-01,  1.2176e-01,  5.6518e-03,
          1.5127e-01,  6.6974e-02,  1.1947e-01, -1.4084e-02,  1.4838e-01,
          6.1727e-02, -6.0509e-02,  9.8658e-02, -1.0833e-01, -1.4942e-02,
          1.4547e-01, -9.5649e-02, -6.8941e-03,  8.5007e-03,  1.7738e-02,
         -9.2776e-02, -1.3424e-01, -6.7018e-02,  3.3228e-02, -4.9316e-03,
          1.4183e-01, -9.2179e-02,  5.7496e-02,  1.6406e-02,  1.4065e-01,
         -5.9949e-02, -6.2568e-02, -8.8071e-02,  1.1553e-01, -1.1479e-01,
         -5.3267e-02,  5.9051e-02, -1.3911e-01, -1.1670e-01,  1.4739e-01,
          7.6638e-02, -1.2206e-01, -3.8069e-02,  4.5506e-02,  1.7965e-03,
          7.1747e-02, -1.0167e-02,  1.1629e-01,  1.5142e-01,  1.1722e-01,
         -2.1067e-02,  8.4124e-02, -8.2450e-02,  8.6083e-02,  9.0706e-02,
         -9.9612e-02,  9.4944e-03,  1.3457e-01,  8.2560e-02, -7.7872e-02,
         -1.0480e-01, -1.0168e-01, -3.9002e-03,  6.1939e-02, -1.1422e-01,
         -1.0176e-01, -2.5755e-03, -1.5241e-01, -1.5276e-01, -1.4462e-02,
          4.4459e-02,  1.0874e-01, -7.5657e-02,  2.2231e-02,  7.3281e-02,
         -1.3676e-01,  1.2034e-02,  2.8863e-02, -7.8683e-02,  3.3847e-02,
          3.1426e-02,  4.2511e-02, -6.9854e-02,  6.4745e-02, -7.9351e-02,
          3.5184e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.1052,  0.1224, -0.0516,  ..., -0.0097, -0.0886,  0.0580],
        [ 0.0667,  0.0356,  0.0212,  ...,  0.1194,  0.0833,  0.0129],
        [-0.1060, -0.0545,  0.0559,  ...,  0.0495,  0.0170,  0.0838],
        ...,
        [ 0.0676, -0.0549,  0.0350,  ..., -0.1056, -0.0152, -0.0213],
        [-0.0968, -0.0889,  0.0475,  ..., -0.0161, -0.0680,  0.0605],
        [ 0.0106, -0.0523,  0.1082,  ..., -0.0337,  0.1198, -0.0800]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1052,  0.1224, -0.0516,  ..., -0.0097, -0.0886,  0.0580],
        [ 0.0667,  0.0356,  0.0212,  ...,  0.1194,  0.0833,  0.0129],
        [-0.1060, -0.0545,  0.0559,  ...,  0.0495,  0.0170,  0.0838],
        ...,
        [ 0.0676, -0.0549,  0.0350,  ..., -0.1056, -0.0152, -0.0213],
        [-0.0968, -0.0889,  0.0475,  ..., -0.0161, -0.0680,  0.0605],
        [ 0.0106, -0.0523,  0.1082,  ..., -0.0337,  0.1198, -0.0800]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1375, -0.1727,  0.1422,  ...,  0.1047,  0.0507, -0.0664],
        [-0.1383,  0.1428,  0.0406,  ...,  0.1234, -0.1618,  0.1461],
        [-0.1512, -0.0809, -0.1661,  ..., -0.1550, -0.1667,  0.1372],
        ...,
        [-0.0741,  0.1328, -0.0259,  ...,  0.1078, -0.1210,  0.1210],
        [-0.0572,  0.1541,  0.0850,  ..., -0.1574,  0.0518,  0.1383],
        [-0.1350,  0.0052,  0.0915,  ..., -0.1317,  0.0970, -0.1557]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1375, -0.1727,  0.1422,  ...,  0.1047,  0.0507, -0.0664],
        [-0.1383,  0.1428,  0.0406,  ...,  0.1234, -0.1618,  0.1461],
        [-0.1512, -0.0809, -0.1661,  ..., -0.1550, -0.1667,  0.1372],
        ...,
        [-0.0741,  0.1328, -0.0259,  ...,  0.1078, -0.1210,  0.1210],
        [-0.0572,  0.1541,  0.0850,  ..., -0.1574,  0.0518,  0.1383],
        [-0.1350,  0.0052,  0.0915,  ..., -0.1317,  0.0970, -0.1557]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1358,  0.1232, -0.1190,  ...,  0.2447,  0.1033,  0.1880],
        [-0.0287,  0.1615,  0.1954,  ..., -0.1331,  0.0407,  0.0390],
        [ 0.1636, -0.0613,  0.0732,  ...,  0.1627,  0.1161, -0.2259],
        ...,
        [-0.1756, -0.2064, -0.0280,  ..., -0.1139,  0.0523,  0.2472],
        [ 0.0609,  0.0704,  0.2000,  ..., -0.1746,  0.0292, -0.0515],
        [-0.0043, -0.0106, -0.0889,  ..., -0.1696,  0.0943,  0.1780]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1358,  0.1232, -0.1190,  ...,  0.2447,  0.1033,  0.1880],
        [-0.0287,  0.1615,  0.1954,  ..., -0.1331,  0.0407,  0.0390],
        [ 0.1636, -0.0613,  0.0732,  ...,  0.1627,  0.1161, -0.2259],
        ...,
        [-0.1756, -0.2064, -0.0280,  ..., -0.1139,  0.0523,  0.2472],
        [ 0.0609,  0.0704,  0.2000,  ..., -0.1746,  0.0292, -0.0515],
        [-0.0043, -0.0106, -0.0889,  ..., -0.1696,  0.0943,  0.1780]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2445],
        [ 0.4235],
        [ 0.0409],
        [-0.2203],
        [ 0.0729],
        [-0.1826],
        [-0.0373],
        [ 0.3503],
        [-0.0467],
        [-0.2881],
        [-0.2927],
        [-0.4226],
        [-0.4119],
        [-0.1354],
        [ 0.3792],
        [ 0.3979],
        [-0.1819],
        [-0.3487],
        [ 0.0830],
        [-0.0881],
        [ 0.2509],
        [-0.1545],
        [ 0.1099],
        [-0.3904],
        [ 0.1312],
        [ 0.2640],
        [-0.0967],
        [-0.0936],
        [-0.1948],
        [ 0.2238],
        [-0.3389],
        [ 0.1058]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2445],
        [ 0.4235],
        [ 0.0409],
        [-0.2203],
        [ 0.0729],
        [-0.1826],
        [-0.0373],
        [ 0.3503],
        [-0.0467],
        [-0.2881],
        [-0.2927],
        [-0.4226],
        [-0.4119],
        [-0.1354],
        [ 0.3792],
        [ 0.3979],
        [-0.1819],
        [-0.3487],
        [ 0.0830],
        [-0.0881],
        [ 0.2509],
        [-0.1545],
        [ 0.1099],
        [-0.3904],
        [ 0.1312],
        [ 0.2640],
        [-0.0967],
        [-0.0936],
        [-0.1948],
        [ 0.2238],
        [-0.3389],
        [ 0.1058]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0430,  0.1024, -0.1224, -0.0934,  0.0554,  0.0041,  0.0034,  0.1525,
          0.1467,  0.0371, -0.1426,  0.0257, -0.0249,  0.1081,  0.0687,  0.0940,
          0.0652,  0.0145, -0.1498, -0.0230, -0.0756,  0.0327,  0.0348,  0.0568,
          0.1274,  0.1163,  0.0052,  0.1299,  0.1347,  0.0441, -0.0496,  0.0192,
          0.1189, -0.1300,  0.0884,  0.0628, -0.0081,  0.1242,  0.0846, -0.0968,
          0.0069, -0.1419, -0.1406,  0.1025,  0.0573,  0.0484, -0.0162,  0.1239,
         -0.0219,  0.0875,  0.0971, -0.0068,  0.0896,  0.0504,  0.0779,  0.0243,
          0.0850, -0.0591, -0.1067, -0.0012,  0.0542,  0.0100, -0.0847, -0.0281,
          0.1503,  0.0159, -0.0391,  0.1497, -0.1060, -0.0752, -0.0052,  0.1039,
          0.0430, -0.0808,  0.0393, -0.1103, -0.0410, -0.0554, -0.0793, -0.1454,
         -0.0241, -0.1397, -0.0166,  0.0082, -0.0356,  0.0341,  0.0829, -0.0620,
          0.0245, -0.0149,  0.0489,  0.1451, -0.0351,  0.1511, -0.0456, -0.1266,
          0.1477, -0.0168,  0.1495, -0.1151,  0.0961,  0.1108, -0.1325,  0.0246,
          0.1003, -0.1111, -0.0870,  0.0820, -0.0548, -0.0856,  0.0234,  0.0614,
          0.1518,  0.0786,  0.0462,  0.1074,  0.1377, -0.0637,  0.0166, -0.1329,
          0.0733, -0.0413,  0.0830,  0.0652, -0.0590,  0.0955, -0.1009, -0.0922,
         -0.0025, -0.1485,  0.1033,  0.0490,  0.0563,  0.1301,  0.1511, -0.0914,
         -0.0125,  0.1418,  0.1314,  0.1249, -0.0918, -0.0074, -0.0841, -0.1396,
         -0.0766,  0.0073, -0.0814, -0.0819, -0.1326,  0.0246,  0.1280, -0.0569,
          0.0847, -0.0411,  0.0301, -0.0216, -0.0722,  0.0044,  0.1402,  0.0363,
          0.0096,  0.0047, -0.0083, -0.0379,  0.0924, -0.1015, -0.1201,  0.1441,
          0.0320,  0.0325, -0.1392,  0.0306, -0.0082,  0.0284, -0.0508, -0.0715,
          0.1224,  0.0990,  0.0843,  0.1167, -0.0666,  0.0426,  0.0138,  0.0445,
          0.0160,  0.0233, -0.1005, -0.1339,  0.0322,  0.1221, -0.0990,  0.1210,
          0.0701, -0.0096,  0.0845,  0.0275, -0.0743,  0.0225, -0.1043, -0.1122,
          0.1012,  0.1201, -0.0868, -0.0461, -0.0066, -0.1078, -0.0915, -0.0307,
         -0.0429, -0.1397,  0.1054, -0.0886, -0.1119, -0.0014, -0.0689, -0.0573,
          0.0225, -0.0836,  0.0550,  0.1502, -0.1491, -0.1471, -0.0363, -0.1395,
         -0.0816,  0.0237,  0.0691,  0.0232,  0.1160, -0.0535,  0.1487, -0.1102,
          0.0230,  0.0649,  0.0135, -0.1348,  0.0105, -0.1305,  0.0619,  0.0878,
         -0.0853, -0.0241, -0.0092, -0.1432,  0.1158,  0.0915,  0.0979, -0.1327,
          0.0012, -0.0561,  0.0058,  0.0926, -0.0502, -0.0473,  0.1457, -0.1262]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0430,  0.1024, -0.1224, -0.0934,  0.0554,  0.0041,  0.0034,  0.1525,
          0.1467,  0.0371, -0.1426,  0.0257, -0.0249,  0.1081,  0.0687,  0.0940,
          0.0652,  0.0145, -0.1498, -0.0230, -0.0756,  0.0327,  0.0348,  0.0568,
          0.1274,  0.1163,  0.0052,  0.1299,  0.1347,  0.0441, -0.0496,  0.0192,
          0.1189, -0.1300,  0.0884,  0.0628, -0.0081,  0.1242,  0.0846, -0.0968,
          0.0069, -0.1419, -0.1406,  0.1025,  0.0573,  0.0484, -0.0162,  0.1239,
         -0.0219,  0.0875,  0.0971, -0.0068,  0.0896,  0.0504,  0.0779,  0.0243,
          0.0850, -0.0591, -0.1067, -0.0012,  0.0542,  0.0100, -0.0847, -0.0281,
          0.1503,  0.0159, -0.0391,  0.1497, -0.1060, -0.0752, -0.0052,  0.1039,
          0.0430, -0.0808,  0.0393, -0.1103, -0.0410, -0.0554, -0.0793, -0.1454,
         -0.0241, -0.1397, -0.0166,  0.0082, -0.0356,  0.0341,  0.0829, -0.0620,
          0.0245, -0.0149,  0.0489,  0.1451, -0.0351,  0.1511, -0.0456, -0.1266,
          0.1477, -0.0168,  0.1495, -0.1151,  0.0961,  0.1108, -0.1325,  0.0246,
          0.1003, -0.1111, -0.0870,  0.0820, -0.0548, -0.0856,  0.0234,  0.0614,
          0.1518,  0.0786,  0.0462,  0.1074,  0.1377, -0.0637,  0.0166, -0.1329,
          0.0733, -0.0413,  0.0830,  0.0652, -0.0590,  0.0955, -0.1009, -0.0922,
         -0.0025, -0.1485,  0.1033,  0.0490,  0.0563,  0.1301,  0.1511, -0.0914,
         -0.0125,  0.1418,  0.1314,  0.1249, -0.0918, -0.0074, -0.0841, -0.1396,
         -0.0766,  0.0073, -0.0814, -0.0819, -0.1326,  0.0246,  0.1280, -0.0569,
          0.0847, -0.0411,  0.0301, -0.0216, -0.0722,  0.0044,  0.1402,  0.0363,
          0.0096,  0.0047, -0.0083, -0.0379,  0.0924, -0.1015, -0.1201,  0.1441,
          0.0320,  0.0325, -0.1392,  0.0306, -0.0082,  0.0284, -0.0508, -0.0715,
          0.1224,  0.0990,  0.0843,  0.1167, -0.0666,  0.0426,  0.0138,  0.0445,
          0.0160,  0.0233, -0.1005, -0.1339,  0.0322,  0.1221, -0.0990,  0.1210,
          0.0701, -0.0096,  0.0845,  0.0275, -0.0743,  0.0225, -0.1043, -0.1122,
          0.1012,  0.1201, -0.0868, -0.0461, -0.0066, -0.1078, -0.0915, -0.0307,
         -0.0429, -0.1397,  0.1054, -0.0886, -0.1119, -0.0014, -0.0689, -0.0573,
          0.0225, -0.0836,  0.0550,  0.1502, -0.1491, -0.1471, -0.0363, -0.1395,
         -0.0816,  0.0237,  0.0691,  0.0232,  0.1160, -0.0535,  0.1487, -0.1102,
          0.0230,  0.0649,  0.0135, -0.1348,  0.0105, -0.1305,  0.0619,  0.0878,
         -0.0853, -0.0241, -0.0092, -0.1432,  0.1158,  0.0915,  0.0979, -0.1327,
          0.0012, -0.0561,  0.0058,  0.0926, -0.0502, -0.0473,  0.1457, -0.1262]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0009, -0.0647,  0.0380,  ...,  0.0121, -0.0927, -0.0893],
        [ 0.0233, -0.0648, -0.0572,  ..., -0.0850, -0.0281,  0.1245],
        [ 0.0144, -0.0473,  0.0219,  ..., -0.0136,  0.0818,  0.1238],
        ...,
        [-0.0788,  0.0570, -0.0573,  ...,  0.0669, -0.0153,  0.0730],
        [ 0.1240, -0.0124, -0.1033,  ...,  0.0404, -0.1177, -0.0681],
        [ 0.0058, -0.0741, -0.0661,  ..., -0.0160, -0.0507,  0.0825]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0009, -0.0647,  0.0380,  ...,  0.0121, -0.0927, -0.0893],
        [ 0.0233, -0.0648, -0.0572,  ..., -0.0850, -0.0281,  0.1245],
        [ 0.0144, -0.0473,  0.0219,  ..., -0.0136,  0.0818,  0.1238],
        ...,
        [-0.0788,  0.0570, -0.0573,  ...,  0.0669, -0.0153,  0.0730],
        [ 0.1240, -0.0124, -0.1033,  ...,  0.0404, -0.1177, -0.0681],
        [ 0.0058, -0.0741, -0.0661,  ..., -0.0160, -0.0507,  0.0825]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0752,  0.0319,  0.0409,  ..., -0.0507, -0.0228,  0.1520],
        [-0.1616, -0.1524,  0.0750,  ...,  0.1454,  0.0840,  0.0391],
        [ 0.1565, -0.0975,  0.1730,  ..., -0.1276, -0.1129, -0.1027],
        ...,
        [-0.1734,  0.0115,  0.0526,  ...,  0.0257, -0.0749, -0.0476],
        [-0.1385, -0.1025,  0.0112,  ..., -0.0622, -0.1738,  0.0052],
        [-0.1690, -0.1488,  0.1763,  ..., -0.1493, -0.1733,  0.1118]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0752,  0.0319,  0.0409,  ..., -0.0507, -0.0228,  0.1520],
        [-0.1616, -0.1524,  0.0750,  ...,  0.1454,  0.0840,  0.0391],
        [ 0.1565, -0.0975,  0.1730,  ..., -0.1276, -0.1129, -0.1027],
        ...,
        [-0.1734,  0.0115,  0.0526,  ...,  0.0257, -0.0749, -0.0476],
        [-0.1385, -0.1025,  0.0112,  ..., -0.0622, -0.1738,  0.0052],
        [-0.1690, -0.1488,  0.1763,  ..., -0.1493, -0.1733,  0.1118]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.1987, -0.1137, -0.1623,  ..., -0.1059,  0.1965,  0.0043],
        [ 0.2186,  0.1727,  0.1866,  ..., -0.1974,  0.1145,  0.0078],
        [-0.1529,  0.1058,  0.0442,  ...,  0.1621, -0.0740,  0.0158],
        ...,
        [-0.1309, -0.1357, -0.0737,  ..., -0.1805,  0.0031,  0.2112],
        [ 0.2177, -0.1864, -0.1470,  ...,  0.1200,  0.1723, -0.0742],
        [-0.0552,  0.1476, -0.0505,  ..., -0.0196, -0.1187,  0.1892]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1987, -0.1137, -0.1623,  ..., -0.1059,  0.1965,  0.0043],
        [ 0.2186,  0.1727,  0.1866,  ..., -0.1974,  0.1145,  0.0078],
        [-0.1529,  0.1058,  0.0442,  ...,  0.1621, -0.0740,  0.0158],
        ...,
        [-0.1309, -0.1357, -0.0737,  ..., -0.1805,  0.0031,  0.2112],
        [ 0.2177, -0.1864, -0.1470,  ...,  0.1200,  0.1723, -0.0742],
        [-0.0552,  0.1476, -0.0505,  ..., -0.0196, -0.1187,  0.1892]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-2.2100e-01],
        [-8.5083e-02],
        [-2.0594e-01],
        [-6.5600e-03],
        [-4.9876e-02],
        [-9.0374e-02],
        [ 4.1072e-01],
        [ 1.8807e-04],
        [-3.1870e-01],
        [-3.9244e-01],
        [ 4.9077e-02],
        [ 7.1913e-02],
        [ 2.7748e-02],
        [-9.4628e-02],
        [-9.6543e-02],
        [ 4.1022e-01],
        [ 1.7828e-01],
        [ 9.8701e-03],
        [-3.3727e-01],
        [ 3.5068e-01],
        [-5.2054e-02],
        [-6.5104e-02],
        [-1.4235e-01],
        [-3.1387e-01],
        [ 3.8063e-01],
        [-1.4678e-01],
        [ 2.9344e-01],
        [-3.5622e-01],
        [-4.1851e-02],
        [-1.6811e-01],
        [ 7.9885e-02],
        [ 2.9308e-01]], device='cuda:0') 
 Parameter containing:
tensor([[-2.2100e-01],
        [-8.5083e-02],
        [-2.0594e-01],
        [-6.5600e-03],
        [-4.9876e-02],
        [-9.0374e-02],
        [ 4.1072e-01],
        [ 1.8807e-04],
        [-3.1870e-01],
        [-3.9244e-01],
        [ 4.9077e-02],
        [ 7.1913e-02],
        [ 2.7748e-02],
        [-9.4628e-02],
        [-9.6543e-02],
        [ 4.1022e-01],
        [ 1.7828e-01],
        [ 9.8701e-03],
        [-3.3727e-01],
        [ 3.5068e-01],
        [-5.2054e-02],
        [-6.5104e-02],
        [-1.4235e-01],
        [-3.1387e-01],
        [ 3.8063e-01],
        [-1.4678e-01],
        [ 2.9344e-01],
        [-3.5622e-01],
        [-4.1851e-02],
        [-1.6811e-01],
        [ 7.9885e-02],
        [ 2.9308e-01]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(173.1525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0008,  0.0059,  0.0010,  ...,  0.0062, -0.0080, -0.0032],
        [-0.0027,  0.0194,  0.0034,  ...,  0.0206, -0.0264, -0.0105],
        [-0.0018,  0.0129,  0.0023,  ...,  0.0137, -0.0175, -0.0070],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(-493.5149, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-2.3630, device='cuda:0')



h[100].sum tensor(8.7182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0575, device='cuda:0')



h[200].sum tensor(-23.9098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-24.8404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0445, 0.0078,  ..., 0.0473, 0.0000, 0.0000],
        [0.0000, 0.0392, 0.0069,  ..., 0.0416, 0.0000, 0.0000],
        [0.0000, 0.0539, 0.0095,  ..., 0.0572, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(17177.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0044, 0.1706,  ..., 0.0000, 0.0000, 0.1057],
        [0.0000, 0.0043, 0.1673,  ..., 0.0000, 0.0000, 0.1036],
        [0.0000, 0.0043, 0.1681,  ..., 0.0000, 0.0000, 0.1042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(103081.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-5.6192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-94.9774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-0.0250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0192],
        [-0.0185],
        [-0.0166],
        ...,
        [-0.0001],
        [-0.0001],
        [-0.0002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(-989.2848, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[-0.0192],
        [-0.0185],
        [-0.0166],
        ...,
        [-0.0001],
        [-0.0001],
        [-0.0002]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2988],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.7637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0007, -0.0024, -0.0027,  ...,  0.0073,  0.0040,  0.0049],
        [-0.0023, -0.0080, -0.0089,  ...,  0.0242,  0.0132,  0.0161],
        [-0.0015, -0.0053, -0.0059,  ...,  0.0161,  0.0087,  0.0107],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(-40.0889, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.0045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-3.1169, device='cuda:0')



h[100].sum tensor(-10.7905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-11.1944, device='cuda:0')



h[200].sum tensor(-32.2666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-33.4744, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0556, 0.0302, 0.0370],
        [0.0000, 0.0000, 0.0000,  ..., 0.0490, 0.0266, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0673, 0.0366, 0.0447],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(26710.5957, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1545, 0.0000,  ..., 0.0000, 0.0000, 0.1062],
        [0.0000, 0.1515, 0.0000,  ..., 0.0000, 0.0000, 0.1041],
        [0.0000, 0.1522, 0.0000,  ..., 0.0000, 0.0000, 0.1046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(176370.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-274.6424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4946.2930, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(331.7064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1147.7076, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(76.9671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1508e-01],
        [-3.9957e-01],
        [-3.5883e-01],
        ...,
        [-1.3923e-05],
        [-2.2990e-04],
        [-1.5408e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(-29104.9961, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[-0.0192],
        [-0.0185],
        [-0.0166],
        ...,
        [-0.0001],
        [-0.0001],
        [-0.0002]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0.     0.     0.2988 ... 0.     0.     0.    ]



load_model False 
TraEvN 1999 
BatchSize 15 
EpochNum 10 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.1411,  0.0223, -0.0212, -0.0469, -0.1003, -0.1150, -0.1478, -0.1031,
         -0.1119, -0.0542,  0.0330,  0.0343, -0.0294,  0.1243, -0.1227,  0.1418,
          0.1031,  0.0073,  0.1316, -0.0097, -0.0681,  0.0631,  0.0157,  0.0569,
         -0.0382, -0.1167, -0.1034,  0.1392,  0.0979, -0.0325, -0.1251,  0.0701,
          0.1181,  0.0370,  0.1247,  0.1497, -0.0003, -0.1402,  0.1356, -0.0678,
          0.0159, -0.0554,  0.1335, -0.1497,  0.0391,  0.0916,  0.0755,  0.0753,
         -0.1061, -0.0508, -0.1022, -0.0450, -0.0512,  0.0032,  0.0408, -0.1261,
         -0.0461,  0.1201, -0.0588,  0.0262,  0.1207, -0.1028,  0.0072, -0.0962,
          0.1455,  0.0873,  0.1235,  0.1045,  0.1428, -0.0634,  0.1394, -0.0414,
         -0.0705, -0.1265,  0.0824,  0.1465,  0.1028, -0.0823, -0.0481, -0.1149,
          0.1294, -0.0599, -0.0510, -0.1093,  0.0582,  0.0888,  0.0242, -0.0260,
          0.0202, -0.1424, -0.0169, -0.1246, -0.0713,  0.0153, -0.0546, -0.1305,
         -0.0920,  0.1373,  0.1102, -0.0675, -0.1191, -0.0576, -0.0808, -0.0536,
         -0.0245, -0.0607, -0.0540,  0.1379, -0.1342,  0.1083, -0.0671,  0.0014,
          0.0584,  0.0719,  0.0915,  0.0484,  0.0775,  0.0613, -0.1086,  0.1416,
          0.1160,  0.0461,  0.1267,  0.0155, -0.0903, -0.1469, -0.0637, -0.1362,
          0.1235, -0.0987, -0.0786,  0.1448, -0.0123,  0.0692,  0.1399,  0.1167,
         -0.0075,  0.0094, -0.0490,  0.0172,  0.1472, -0.1055, -0.1178,  0.1453,
         -0.0769,  0.1261, -0.0457,  0.0120, -0.1024, -0.1200, -0.1422, -0.0589,
         -0.0672, -0.1427,  0.0658, -0.1296,  0.0905, -0.0767,  0.1155, -0.0927,
         -0.0878, -0.0378, -0.0255,  0.1101,  0.1206, -0.0886,  0.0600,  0.0260,
         -0.1202, -0.0310, -0.0812,  0.0930,  0.1380,  0.0543, -0.0412,  0.1322,
         -0.0496, -0.0672, -0.0189, -0.0766, -0.0492, -0.0964, -0.0983, -0.0919,
         -0.0388, -0.1120, -0.1171, -0.0396,  0.1417, -0.1245,  0.1009, -0.1135,
          0.0773, -0.0759,  0.1406, -0.0843, -0.0277,  0.0886,  0.0483, -0.1315,
         -0.0444, -0.0538, -0.1408, -0.1189, -0.1350, -0.0362,  0.0045,  0.0553,
         -0.0096,  0.0703,  0.0557, -0.0615, -0.1159,  0.0635, -0.1419, -0.0293,
          0.0078, -0.1453,  0.1274, -0.1064, -0.1348,  0.1049,  0.0029,  0.1054,
          0.1215, -0.1196,  0.0281, -0.1129,  0.1099, -0.0468, -0.0519, -0.1021,
          0.0219,  0.0929,  0.1477, -0.1279,  0.0345,  0.0012, -0.1248, -0.0666,
         -0.0118, -0.1079,  0.1025, -0.1499, -0.1234,  0.0909, -0.0087, -0.1012,
         -0.0214,  0.0809, -0.0880, -0.0768, -0.0775,  0.0454, -0.1185, -0.1009]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1048, -0.0876, -0.0241,  ..., -0.0406, -0.0833, -0.0097],
        [ 0.0272,  0.1037,  0.1078,  ...,  0.0024,  0.0105, -0.0423],
        [ 0.0054,  0.0111,  0.0050,  ...,  0.1116,  0.0893,  0.0622],
        ...,
        [-0.0013,  0.1062, -0.0817,  ...,  0.1072,  0.0572, -0.1228],
        [ 0.0565,  0.0205, -0.0079,  ..., -0.0390, -0.0301, -0.0074],
        [ 0.0559,  0.1025,  0.0181,  ..., -0.0610,  0.0958, -0.0740]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0730, -0.0768,  0.1002,  ..., -0.1168, -0.0544, -0.0731],
        [ 0.0713, -0.1161,  0.0790,  ..., -0.0094,  0.1284,  0.1232],
        [ 0.1478,  0.0177,  0.0309,  ..., -0.1763,  0.1002,  0.1243],
        ...,
        [ 0.1333,  0.0210,  0.0784,  ..., -0.0098,  0.0248,  0.0219],
        [-0.0962, -0.0473,  0.1331,  ..., -0.1543,  0.1037,  0.1107],
        [ 0.0802, -0.1547, -0.0099,  ...,  0.1056,  0.0181, -0.0111]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0323, -0.0901, -0.0104,  ..., -0.1964, -0.0923,  0.1478],
        [-0.2125, -0.1154,  0.1111,  ...,  0.2489,  0.1154,  0.2459],
        [-0.2434,  0.0610, -0.0623,  ...,  0.0539,  0.2187,  0.2465],
        ...,
        [ 0.0788,  0.1780,  0.0952,  ..., -0.0195, -0.0189,  0.0173],
        [-0.0538, -0.0761, -0.1460,  ..., -0.1808,  0.0603,  0.2127],
        [-0.0984,  0.0576,  0.0136,  ..., -0.0747, -0.1108,  0.0121]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1899],
        [ 0.1578],
        [-0.0750],
        [-0.2226],
        [-0.4059],
        [-0.0840],
        [-0.0232],
        [-0.3862],
        [-0.1976],
        [ 0.0067],
        [-0.3911],
        [-0.1460],
        [ 0.1515],
        [ 0.0567],
        [-0.3714],
        [ 0.1479],
        [ 0.0440],
        [ 0.3762],
        [-0.0262],
        [-0.1062],
        [ 0.1735],
        [ 0.3786],
        [ 0.2661],
        [-0.3378],
        [-0.3164],
        [ 0.2817],
        [ 0.2026],
        [ 0.3520],
        [-0.2933],
        [ 0.0703],
        [ 0.1284],
        [-0.1193]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.1411,  0.0223, -0.0212, -0.0469, -0.1003, -0.1150, -0.1478, -0.1031,
         -0.1119, -0.0542,  0.0330,  0.0343, -0.0294,  0.1243, -0.1227,  0.1418,
          0.1031,  0.0073,  0.1316, -0.0097, -0.0681,  0.0631,  0.0157,  0.0569,
         -0.0382, -0.1167, -0.1034,  0.1392,  0.0979, -0.0325, -0.1251,  0.0701,
          0.1181,  0.0370,  0.1247,  0.1497, -0.0003, -0.1402,  0.1356, -0.0678,
          0.0159, -0.0554,  0.1335, -0.1497,  0.0391,  0.0916,  0.0755,  0.0753,
         -0.1061, -0.0508, -0.1022, -0.0450, -0.0512,  0.0032,  0.0408, -0.1261,
         -0.0461,  0.1201, -0.0588,  0.0262,  0.1207, -0.1028,  0.0072, -0.0962,
          0.1455,  0.0873,  0.1235,  0.1045,  0.1428, -0.0634,  0.1394, -0.0414,
         -0.0705, -0.1265,  0.0824,  0.1465,  0.1028, -0.0823, -0.0481, -0.1149,
          0.1294, -0.0599, -0.0510, -0.1093,  0.0582,  0.0888,  0.0242, -0.0260,
          0.0202, -0.1424, -0.0169, -0.1246, -0.0713,  0.0153, -0.0546, -0.1305,
         -0.0920,  0.1373,  0.1102, -0.0675, -0.1191, -0.0576, -0.0808, -0.0536,
         -0.0245, -0.0607, -0.0540,  0.1379, -0.1342,  0.1083, -0.0671,  0.0014,
          0.0584,  0.0719,  0.0915,  0.0484,  0.0775,  0.0613, -0.1086,  0.1416,
          0.1160,  0.0461,  0.1267,  0.0155, -0.0903, -0.1469, -0.0637, -0.1362,
          0.1235, -0.0987, -0.0786,  0.1448, -0.0123,  0.0692,  0.1399,  0.1167,
         -0.0075,  0.0094, -0.0490,  0.0172,  0.1472, -0.1055, -0.1178,  0.1453,
         -0.0769,  0.1261, -0.0457,  0.0120, -0.1024, -0.1200, -0.1422, -0.0589,
         -0.0672, -0.1427,  0.0658, -0.1296,  0.0905, -0.0767,  0.1155, -0.0927,
         -0.0878, -0.0378, -0.0255,  0.1101,  0.1206, -0.0886,  0.0600,  0.0260,
         -0.1202, -0.0310, -0.0812,  0.0930,  0.1380,  0.0543, -0.0412,  0.1322,
         -0.0496, -0.0672, -0.0189, -0.0766, -0.0492, -0.0964, -0.0983, -0.0919,
         -0.0388, -0.1120, -0.1171, -0.0396,  0.1417, -0.1245,  0.1009, -0.1135,
          0.0773, -0.0759,  0.1406, -0.0843, -0.0277,  0.0886,  0.0483, -0.1315,
         -0.0444, -0.0538, -0.1408, -0.1189, -0.1350, -0.0362,  0.0045,  0.0553,
         -0.0096,  0.0703,  0.0557, -0.0615, -0.1159,  0.0635, -0.1419, -0.0293,
          0.0078, -0.1453,  0.1274, -0.1064, -0.1348,  0.1049,  0.0029,  0.1054,
          0.1215, -0.1196,  0.0281, -0.1129,  0.1099, -0.0468, -0.0519, -0.1021,
          0.0219,  0.0929,  0.1477, -0.1279,  0.0345,  0.0012, -0.1248, -0.0666,
         -0.0118, -0.1079,  0.1025, -0.1499, -0.1234,  0.0909, -0.0087, -0.1012,
         -0.0214,  0.0809, -0.0880, -0.0768, -0.0775,  0.0454, -0.1185, -0.1009]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.1048, -0.0876, -0.0241,  ..., -0.0406, -0.0833, -0.0097],
        [ 0.0272,  0.1037,  0.1078,  ...,  0.0024,  0.0105, -0.0423],
        [ 0.0054,  0.0111,  0.0050,  ...,  0.1116,  0.0893,  0.0622],
        ...,
        [-0.0013,  0.1062, -0.0817,  ...,  0.1072,  0.0572, -0.1228],
        [ 0.0565,  0.0205, -0.0079,  ..., -0.0390, -0.0301, -0.0074],
        [ 0.0559,  0.1025,  0.0181,  ..., -0.0610,  0.0958, -0.0740]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0730, -0.0768,  0.1002,  ..., -0.1168, -0.0544, -0.0731],
        [ 0.0713, -0.1161,  0.0790,  ..., -0.0094,  0.1284,  0.1232],
        [ 0.1478,  0.0177,  0.0309,  ..., -0.1763,  0.1002,  0.1243],
        ...,
        [ 0.1333,  0.0210,  0.0784,  ..., -0.0098,  0.0248,  0.0219],
        [-0.0962, -0.0473,  0.1331,  ..., -0.1543,  0.1037,  0.1107],
        [ 0.0802, -0.1547, -0.0099,  ...,  0.1056,  0.0181, -0.0111]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0323, -0.0901, -0.0104,  ..., -0.1964, -0.0923,  0.1478],
        [-0.2125, -0.1154,  0.1111,  ...,  0.2489,  0.1154,  0.2459],
        [-0.2434,  0.0610, -0.0623,  ...,  0.0539,  0.2187,  0.2465],
        ...,
        [ 0.0788,  0.1780,  0.0952,  ..., -0.0195, -0.0189,  0.0173],
        [-0.0538, -0.0761, -0.1460,  ..., -0.1808,  0.0603,  0.2127],
        [-0.0984,  0.0576,  0.0136,  ..., -0.0747, -0.1108,  0.0121]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1899],
        [ 0.1578],
        [-0.0750],
        [-0.2226],
        [-0.4059],
        [-0.0840],
        [-0.0232],
        [-0.3862],
        [-0.1976],
        [ 0.0067],
        [-0.3911],
        [-0.1460],
        [ 0.1515],
        [ 0.0567],
        [-0.3714],
        [ 0.1479],
        [ 0.0440],
        [ 0.3762],
        [-0.0262],
        [-0.1062],
        [ 0.1735],
        [ 0.3786],
        [ 0.2661],
        [-0.3378],
        [-0.3164],
        [ 0.2817],
        [ 0.2026],
        [ 0.3520],
        [-0.2933],
        [ 0.0703],
        [ 0.1284],
        [-0.1193]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1550.9999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3489.4995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(213.0822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.6589, device='cuda:0')



h[100].sum tensor(-179.7460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-63.4217, device='cuda:0')



h[200].sum tensor(-67.0284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-189.6485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0246, 0.0039, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(154922.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0247, 0.0000,  ..., 0.0401, 0.0000, 0.1445],
        [0.0000, 0.0105, 0.0000,  ..., 0.0171, 0.0000, 0.0614],
        [0.0000, 0.0071, 0.0000,  ..., 0.0116, 0.0000, 0.0418],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(597375.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1300.8960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(396.2349, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-901.7745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[9.7464e-02],
        [8.5566e-02],
        [8.3919e-02],
        ...,
        [4.0994e-07],
        [5.3860e-08],
        [0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(36021.3398, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1622.8817, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3656.6553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(223.4206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.4773, device='cuda:0')



h[100].sum tensor(-188.4423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-66.3611, device='cuda:0')



h[200].sum tensor(-70.1725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-198.4378, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0131, 0.0021, 0.0000,  ..., 0.0042, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159092.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0134, 0.0000,  ..., 0.0217, 0.0000, 0.0783],
        [0.0000, 0.0036, 0.0000,  ..., 0.0059, 0.0000, 0.0213],
        [0.0000, 0.0015, 0.0000,  ..., 0.0024, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0094, 0.0000, 0.0339],
        [0.0000, 0.0098, 0.0000,  ..., 0.0159, 0.0000, 0.0573],
        [0.0000, 0.0144, 0.0000,  ..., 0.0234, 0.0000, 0.0842]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(607527., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1335.1057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(406.8127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-927.3312, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0453],
        [0.0364],
        [0.0384],
        ...,
        [0.0321],
        [0.0417],
        [0.0512]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(36636.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9999],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097680.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1691.9197, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0149,  0.0023, -0.0022,  ...,  0.0048, -0.0125, -0.0106],
        [ 0.0331,  0.0052, -0.0049,  ...,  0.0106, -0.0278, -0.0236],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3800.2832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(232.3329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.2633, device='cuda:0')



h[100].sum tensor(-195.9334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-69.1841, device='cuda:0')



h[200].sum tensor(-72.8593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-206.8795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1322, 0.0208, 0.0000,  ..., 0.0424, 0.0000, 0.0000],
        [0.0666, 0.0104, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0748, 0.0117, 0.0000,  ..., 0.0240, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167985.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0918, 0.0000,  ..., 0.1491, 0.0000, 0.5368],
        [0.0000, 0.0701, 0.0000,  ..., 0.1139, 0.0000, 0.4099],
        [0.0000, 0.0646, 0.0000,  ..., 0.1049, 0.0000, 0.3776],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(640346.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1408.8931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(429.4626, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-980.5306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.2775],
        [0.2400],
        [0.2094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(38015.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9998],
        [0.9998],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097570.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097461.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1482.7794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 6.3881e-05,  6.3881e-05,  0.0000e+00,  ...,  6.3881e-05,
          0.0000e+00,  0.0000e+00],
        [ 2.4161e-02,  3.8560e-03, -3.5808e-03,  ...,  7.8037e-03,
         -2.0203e-02, -1.7190e-02],
        [ 6.3881e-05,  6.3881e-05,  0.0000e+00,  ...,  6.3881e-05,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 6.3881e-05,  6.3881e-05,  0.0000e+00,  ...,  6.3881e-05,
          0.0000e+00,  0.0000e+00],
        [ 6.3881e-05,  6.3881e-05,  0.0000e+00,  ...,  6.3881e-05,
          0.0000e+00,  0.0000e+00],
        [ 6.3881e-05,  6.3881e-05,  0.0000e+00,  ...,  6.3881e-05,
          0.0000e+00,  0.0000e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3276.2476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(210.9851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.8822, device='cuda:0')



h[100].sum tensor(-172.2155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-60.6321, device='cuda:0')



h[200].sum tensor(-63.9491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-181.3068, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0988, 0.0158, 0.0000,  ..., 0.0319, 0.0000, 0.0000],
        [0.0199, 0.0034, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0243, 0.0040, 0.0000,  ..., 0.0080, 0.0000, 0.0000],
        ...,
        [0.0003, 0.0003, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0003, 0.0003, 0.0000,  ..., 0.0003, 0.0000, 0.0000],
        [0.0093, 0.0017, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(152064.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.7451e-02, 0.0000e+00,  ..., 7.7832e-02, 0.0000e+00,
         2.7100e-01],
        [0.0000e+00, 2.3970e-02, 0.0000e+00,  ..., 3.9707e-02, 0.0000e+00,
         1.3732e-01],
        [0.0000e+00, 1.5835e-02, 0.0000e+00,  ..., 2.6467e-02, 0.0000e+00,
         9.1004e-02],
        ...,
        [0.0000e+00, 4.8502e-04, 1.8009e-04,  ..., 1.5168e-03, 0.0000e+00,
         3.5797e-03],
        [0.0000e+00, 4.8375e-03, 1.1554e-04,  ..., 8.6140e-03, 0.0000e+00,
         2.8370e-02],
        [0.0000e+00, 1.4235e-02, 0.0000e+00,  ..., 2.3956e-02, 0.0000e+00,
         8.1903e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(590971.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1370.6000, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(376.1673, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-874.5972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0658e-02],
        [ 2.7208e-05],
        [-8.5997e-03],
        ...,
        [-5.6513e-03],
        [-5.9088e-03],
        [-3.3143e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-16105.1777, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9997],
        [0.9997],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097461.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097351.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2637],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1714.7555, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0001,  0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0094,  0.0016, -0.0014,  ...,  0.0031, -0.0078, -0.0066],
        [ 0.0001,  0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        ...,
        [ 0.0001,  0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0001,  0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000],
        [ 0.0001,  0.0001,  0.0000,  ...,  0.0001,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3733.7900, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(248.3714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.5233, device='cuda:0')



h[100].sum tensor(-198.9698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-70.1179, device='cuda:0')



h[200].sum tensor(-73.7791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-209.6717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0476, 0.0079, 0.0000,  ..., 0.0156, 0.0000, 0.0000],
        [0.0147, 0.0027, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0098, 0.0019, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(176956.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0230, 0.0000,  ..., 0.0393, 0.0000, 0.1301],
        [0.0000, 0.0184, 0.0000,  ..., 0.0315, 0.0000, 0.1039],
        [0.0000, 0.0226, 0.0000,  ..., 0.0383, 0.0000, 0.1274],
        ...,
        [0.0000, 0.0002, 0.0004,  ..., 0.0016, 0.0000, 0.0023],
        [0.0000, 0.0002, 0.0004,  ..., 0.0016, 0.0000, 0.0023],
        [0.0000, 0.0002, 0.0004,  ..., 0.0016, 0.0000, 0.0023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(687698., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1659.9102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(428.7035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1010.3087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0543],
        [-0.0557],
        [-0.0561],
        ...,
        [-0.0051],
        [-0.0051],
        [-0.0050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-45820.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9996],
        [0.9996],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097351.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097242.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1718.7347, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3674.2480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(252.7244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.5686, device='cuda:0')



h[100].sum tensor(-198.6092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-70.2806, device='cuda:0')



h[200].sum tensor(-73.5407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-210.1583, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0006, 0.0006, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        ...,
        [0.0006, 0.0006, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0006, 0.0006, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(180257.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0006,  ..., 0.0022, 0.0000, 0.0032],
        [0.0000, 0.0004, 0.0004,  ..., 0.0024, 0.0000, 0.0040],
        [0.0000, 0.0007, 0.0003,  ..., 0.0031, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0002, 0.0006,  ..., 0.0022, 0.0000, 0.0032],
        [0.0000, 0.0002, 0.0006,  ..., 0.0022, 0.0000, 0.0032],
        [0.0000, 0.0002, 0.0006,  ..., 0.0022, 0.0000, 0.0032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(702563.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1754.1462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(428.0033, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1019.7914, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0230],
        [-0.0297],
        [-0.0402],
        ...,
        [-0.0057],
        [-0.0057],
        [-0.0057]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-63401.8398, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9995],
        [0.9995],
        ...,
        [0.9995],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097242.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097133.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1779.0912, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        ...,
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000],
        [0.0002, 0.0002, 0.0000,  ..., 0.0002, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3774.6794, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(266.2090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.2558, device='cuda:0')



h[100].sum tensor(-206.4449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-72.7486, device='cuda:0')



h[200].sum tensor(-76.3331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-217.5383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0008, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0008, 0.0008, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0008, 0.0008, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        ...,
        [0.0008, 0.0008, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0008, 0.0008, 0.0000,  ..., 0.0008, 0.0000, 0.0000],
        [0.0008, 0.0008, 0.0000,  ..., 0.0008, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(190072.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0041, 0.0002,  ..., 0.0091, 0.0000, 0.0246],
        [0.0000, 0.0009, 0.0004,  ..., 0.0038, 0.0000, 0.0075],
        [0.0000, 0.0010, 0.0006,  ..., 0.0039, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0003, 0.0007,  ..., 0.0027, 0.0000, 0.0039],
        [0.0000, 0.0003, 0.0007,  ..., 0.0027, 0.0000, 0.0039],
        [0.0000, 0.0003, 0.0007,  ..., 0.0027, 0.0000, 0.0039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(752840.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1896.7760, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(444.6173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1070.3020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0712],
        [-0.0585],
        [-0.0515],
        ...,
        [-0.0072],
        [-0.0071],
        [-0.0071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-82733.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9994],
        [0.9994],
        ...,
        [0.9994],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097133.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097024., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5615],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1370.9707, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0002,  0.0002,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0358,  0.0059, -0.0052,  ...,  0.0117, -0.0297, -0.0252],
        [ 0.0160,  0.0027, -0.0023,  ...,  0.0053, -0.0131, -0.0112],
        ...,
        [ 0.0002,  0.0002,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0002,  0.0002,  0.0000,  ...,  0.0002,  0.0000,  0.0000],
        [ 0.0002,  0.0002,  0.0000,  ...,  0.0002,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2823.4705, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(212.6083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.6092, device='cuda:0')



h[100].sum tensor(-158.4377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-56.0602, device='cuda:0')



h[200].sum tensor(-58.4987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-167.6354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0887, 0.0148, 0.0000,  ..., 0.0292, 0.0000, 0.0000],
        [0.0865, 0.0145, 0.0000,  ..., 0.0285, 0.0000, 0.0000],
        [0.1266, 0.0208, 0.0000,  ..., 0.0414, 0.0000, 0.0000],
        ...,
        [0.0009, 0.0009, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0009, 0.0009, 0.0000,  ..., 0.0009, 0.0000, 0.0000],
        [0.0009, 0.0009, 0.0000,  ..., 0.0009, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(149209.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.5585e-02, 0.0000e+00,  ..., 1.2497e-01, 0.0000e+00,
         4.0535e-01],
        [0.0000e+00, 8.4876e-02, 0.0000e+00,  ..., 1.4006e-01, 0.0000e+00,
         4.5485e-01],
        [0.0000e+00, 9.1591e-02, 0.0000e+00,  ..., 1.5116e-01, 0.0000e+00,
         4.9073e-01],
        ...,
        [0.0000e+00, 3.4192e-04, 8.3141e-04,  ..., 3.1449e-03, 0.0000e+00,
         4.5870e-03],
        [0.0000e+00, 3.4192e-04, 8.3141e-04,  ..., 3.1449e-03, 0.0000e+00,
         4.5870e-03],
        [0.0000e+00, 3.4192e-04, 8.3141e-04,  ..., 3.1449e-03, 0.0000e+00,
         4.5870e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(596928.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1597.0981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(335.2291, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-820.6579, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2388],
        [-0.2452],
        [-0.2454],
        ...,
        [-0.0088],
        [-0.0086],
        [-0.0085]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-82298.2109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9993],
        [0.9993],
        ...,
        [0.9993],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097024., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096914.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2593],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1649.0205, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0094,  0.0017, -0.0013,  ...,  0.0032, -0.0076, -0.0065],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3399.0481, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(254.8293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.7749, device='cuda:0')



h[100].sum tensor(-191.1270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-67.4299, device='cuda:0')



h[200].sum tensor(-70.4672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-201.6340, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0343, 0.0063, 0.0000,  ..., 0.0117, 0.0000, 0.0000],
        [0.0085, 0.0022, 0.0000,  ..., 0.0034, 0.0000, 0.0000],
        [0.0102, 0.0025, 0.0000,  ..., 0.0040, 0.0000, 0.0000],
        ...,
        [0.0010, 0.0010, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0010, 0.0010, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0010, 0.0010, 0.0000,  ..., 0.0010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(178454.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0160, 0.0000,  ..., 0.0304, 0.0000, 0.0880],
        [0.0000, 0.0099, 0.0000,  ..., 0.0198, 0.0000, 0.0555],
        [0.0000, 0.0109, 0.0000,  ..., 0.0214, 0.0000, 0.0609],
        ...,
        [0.0000, 0.0004, 0.0009,  ..., 0.0036, 0.0000, 0.0052],
        [0.0000, 0.0004, 0.0009,  ..., 0.0036, 0.0000, 0.0052],
        [0.0000, 0.0004, 0.0009,  ..., 0.0036, 0.0000, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(708827., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1893.1046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(402.4944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-988.6898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1597],
        [-0.1620],
        [-0.1688],
        ...,
        [-0.0097],
        [-0.0097],
        [-0.0098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-100569.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9992],
        [0.9992],
        ...,
        [0.9992],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096914.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096805.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3391],
        [0.3093],
        [0.3354],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1398.6345, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0198,  0.0034, -0.0028,  ...,  0.0066, -0.0163, -0.0138],
        [ 0.0421,  0.0069, -0.0060,  ...,  0.0138, -0.0348, -0.0296],
        [ 0.0398,  0.0066, -0.0057,  ...,  0.0130, -0.0329, -0.0280],
        ...,
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2797.4353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(221.6546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9241, device='cuda:0')



h[100].sum tensor(-160.9522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-57.1914, device='cuda:0')



h[200].sum tensor(-59.2567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-171.0180, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1326, 0.0220, 0.0000,  ..., 0.0435, 0.0000, 0.0000],
        [0.1483, 0.0245, 0.0000,  ..., 0.0486, 0.0000, 0.0000],
        [0.1702, 0.0280, 0.0000,  ..., 0.0556, 0.0000, 0.0000],
        ...,
        [0.0011, 0.0011, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0011, 0.0011, 0.0000,  ..., 0.0011, 0.0000, 0.0000],
        [0.0011, 0.0011, 0.0000,  ..., 0.0011, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156295.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.0513e-01, 0.0000e+00,  ..., 1.7329e-01, 0.0000e+00,
         5.5160e-01],
        [0.0000e+00, 1.2305e-01, 0.0000e+00,  ..., 2.0160e-01, 0.0000e+00,
         6.4472e-01],
        [0.0000e+00, 1.2941e-01, 0.0000e+00,  ..., 2.1162e-01, 0.0000e+00,
         6.7781e-01],
        ...,
        [0.0000e+00, 4.4297e-04, 1.0258e-03,  ..., 3.9143e-03, 0.0000e+00,
         5.6612e-03],
        [0.0000e+00, 4.4297e-04, 1.0258e-03,  ..., 3.9143e-03, 0.0000e+00,
         5.6612e-03],
        [0.0000e+00, 4.4297e-04, 1.0258e-03,  ..., 3.9143e-03, 0.0000e+00,
         5.6612e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(642374., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1738.2448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(342.8655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-848.7584, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2849],
        [-0.2996],
        [-0.2999],
        ...,
        [-0.0166],
        [-0.0124],
        [-0.0109]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-96685.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9991],
        [0.9991],
        ...,
        [0.9991],
        [0.9991],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096805.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096696.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1335.2832, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0360,  0.0060, -0.0051,  ...,  0.0118, -0.0297, -0.0252],
        [ 0.0372,  0.0062, -0.0053,  ...,  0.0122, -0.0307, -0.0261],
        [ 0.0358,  0.0059, -0.0051,  ...,  0.0117, -0.0295, -0.0251],
        ...,
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2636.4834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(216.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2029, device='cuda:0')



h[100].sum tensor(-154.0832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.6009, device='cuda:0')



h[200].sum tensor(-56.6460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-163.2717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1540, 0.0255, 0.0000,  ..., 0.0505, 0.0000, 0.0000],
        [0.1303, 0.0217, 0.0000,  ..., 0.0429, 0.0000, 0.0000],
        [0.1336, 0.0223, 0.0000,  ..., 0.0439, 0.0000, 0.0000],
        ...,
        [0.0012, 0.0012, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0012, 0.0012, 0.0000,  ..., 0.0012, 0.0000, 0.0000],
        [0.0012, 0.0012, 0.0000,  ..., 0.0012, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(148651.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1963e-01, 0.0000e+00,  ..., 1.9605e-01, 0.0000e+00,
         6.2157e-01],
        [0.0000e+00, 1.1342e-01, 0.0000e+00,  ..., 1.8628e-01, 0.0000e+00,
         5.8959e-01],
        [0.0000e+00, 1.0631e-01, 0.0000e+00,  ..., 1.7504e-01, 0.0000e+00,
         5.5300e-01],
        ...,
        [0.0000e+00, 4.8762e-04, 1.1065e-03,  ..., 4.2391e-03, 0.0000e+00,
         6.1093e-03],
        [0.0000e+00, 4.8762e-04, 1.1065e-03,  ..., 4.2391e-03, 0.0000e+00,
         6.1093e-03],
        [0.0000e+00, 4.8762e-04, 1.1065e-03,  ..., 4.2391e-03, 0.0000e+00,
         6.1093e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(607120., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1707.0269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(319.2249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-798.2859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2411],
        [-0.2377],
        [-0.2239],
        ...,
        [-0.0115],
        [-0.0115],
        [-0.0114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-98761.4844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9990],
        [0.9990],
        ...,
        [0.9990],
        [0.9990],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096696.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 150 loss: tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096587., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3284],
        [0.2822],
        [0.2483]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.2028, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        [ 0.0003,  0.0003,  0.0000,  ...,  0.0003,  0.0000,  0.0000],
        ...,
        [ 0.0231,  0.0040, -0.0033,  ...,  0.0077, -0.0189, -0.0161],
        [ 0.0207,  0.0036, -0.0029,  ...,  0.0069, -0.0169, -0.0144],
        [ 0.0205,  0.0035, -0.0029,  ...,  0.0068, -0.0168, -0.0143]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2854.8018, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(234.5028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.5796, device='cuda:0')



h[100].sum tensor(-167.6059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-59.5454, device='cuda:0')



h[200].sum tensor(-61.5284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-178.0572, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0013, 0.0013, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0013, 0.0013, 0.0000,  ..., 0.0013, 0.0000, 0.0000],
        [0.0351, 0.0067, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0708, 0.0124, 0.0000,  ..., 0.0237, 0.0000, 0.0000],
        [0.0771, 0.0134, 0.0000,  ..., 0.0258, 0.0000, 0.0000],
        [0.0739, 0.0129, 0.0000,  ..., 0.0247, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160995.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0022, 0.0009,  ..., 0.0072, 0.0000, 0.0150],
        [0.0000, 0.0090, 0.0006,  ..., 0.0182, 0.0000, 0.0498],
        [0.0000, 0.0272, 0.0000,  ..., 0.0478, 0.0000, 0.1436],
        ...,
        [0.0000, 0.0462, 0.0000,  ..., 0.0795, 0.0000, 0.2411],
        [0.0000, 0.0528, 0.0000,  ..., 0.0901, 0.0000, 0.2749],
        [0.0000, 0.0545, 0.0000,  ..., 0.0929, 0.0000, 0.2836]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(654957.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1845.7679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(345.6816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-867.9326, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0660],
        [-0.0940],
        [-0.1274],
        ...,
        [-0.1225],
        [-0.1339],
        [-0.1412]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-109277.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9989],
        [0.9989],
        ...,
        [0.9989],
        [0.9989],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096587., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096477.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1593.0643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0190,  0.0033, -0.0026,  ...,  0.0064, -0.0155, -0.0131],
        [ 0.0078,  0.0015, -0.0011,  ...,  0.0027, -0.0062, -0.0052],
        [ 0.0116,  0.0021, -0.0016,  ...,  0.0040, -0.0093, -0.0079],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3117.7229, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(255.8235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1378, device='cuda:0')



h[100].sum tensor(-183.6340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-65.1418, device='cuda:0')



h[200].sum tensor(-67.3147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-194.7919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0322, 0.0063, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        [0.0719, 0.0126, 0.0000,  ..., 0.0241, 0.0000, 0.0000],
        [0.0322, 0.0063, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        ...,
        [0.0014, 0.0014, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0014, 0.0014, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0014, 0.0014, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(175411.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0318, 0.0000,  ..., 0.0573, 0.0000, 0.1668],
        [0.0000, 0.0417, 0.0000,  ..., 0.0733, 0.0000, 0.2174],
        [0.0000, 0.0289, 0.0000,  ..., 0.0521, 0.0000, 0.1519],
        ...,
        [0.0000, 0.0006, 0.0012,  ..., 0.0048, 0.0000, 0.0069],
        [0.0000, 0.0006, 0.0012,  ..., 0.0048, 0.0000, 0.0069],
        [0.0000, 0.0006, 0.0012,  ..., 0.0048, 0.0000, 0.0069]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(715409.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1999.3721, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(378.0308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-950.1465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2179],
        [-0.2057],
        [-0.1865],
        ...,
        [-0.0131],
        [-0.0130],
        [-0.0130]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-119705.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9988],
        [0.9988],
        ...,
        [0.9988],
        [0.9988],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096477.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096368.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3066],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1456.1750, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        ...,
        [ 0.0119,  0.0022, -0.0016,  ...,  0.0041, -0.0096, -0.0081],
        [ 0.0112,  0.0021, -0.0015,  ...,  0.0039, -0.0090, -0.0076],
        [ 0.0219,  0.0038, -0.0030,  ...,  0.0073, -0.0178, -0.0152]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2804.2644, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(239.1936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.5793, device='cuda:0')



h[100].sum tensor(-168.1629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-59.5443, device='cuda:0')



h[200].sum tensor(-61.5540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-178.0538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0031, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0015, 0.0015, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0015, 0.0015, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        ...,
        [0.0659, 0.0117, 0.0000,  ..., 0.0222, 0.0000, 0.0000],
        [0.0524, 0.0096, 0.0000,  ..., 0.0179, 0.0000, 0.0000],
        [0.0471, 0.0087, 0.0000,  ..., 0.0162, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165420.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0122, 0.0000,  ..., 0.0248, 0.0000, 0.0663],
        [0.0000, 0.0040, 0.0007,  ..., 0.0108, 0.0000, 0.0244],
        [0.0000, 0.0011, 0.0010,  ..., 0.0059, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0525, 0.0000,  ..., 0.0908, 0.0000, 0.2705],
        [0.0000, 0.0510, 0.0000,  ..., 0.0887, 0.0000, 0.2633],
        [0.0000, 0.0499, 0.0000,  ..., 0.0870, 0.0000, 0.2579]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(680787.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1938.2375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(349.9042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-886.8534, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1315],
        [-0.0991],
        [-0.0728],
        ...,
        [-0.1883],
        [-0.1852],
        [-0.1805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-119304.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9987],
        [0.9987],
        ...,
        [0.9987],
        [0.9987],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096368.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096259.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.3259],
        [0.4558],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1543.3264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0224,  0.0039, -0.0031,  ...,  0.0075, -0.0182, -0.0155],
        [ 0.0262,  0.0045, -0.0036,  ...,  0.0087, -0.0214, -0.0182],
        [ 0.0119,  0.0022, -0.0016,  ...,  0.0041, -0.0095, -0.0081],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2949.0996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(252.4319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.5715, device='cuda:0')



h[100].sum tensor(-177.7293, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-63.1080, device='cuda:0')



h[200].sum tensor(-64.9609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-188.7102, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0846, 0.0148, 0.0000,  ..., 0.0283, 0.0000, 0.0000],
        [0.0749, 0.0132, 0.0000,  ..., 0.0252, 0.0000, 0.0000],
        [0.0791, 0.0139, 0.0000,  ..., 0.0266, 0.0000, 0.0000],
        ...,
        [0.0015, 0.0015, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0015, 0.0015, 0.0000,  ..., 0.0015, 0.0000, 0.0000],
        [0.0015, 0.0015, 0.0000,  ..., 0.0015, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(171883.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0646, 0.0000,  ..., 0.1099, 0.0000, 0.3298],
        [0.0000, 0.0549, 0.0000,  ..., 0.0947, 0.0000, 0.2815],
        [0.0000, 0.0508, 0.0000,  ..., 0.0883, 0.0000, 0.2608],
        ...,
        [0.0000, 0.0006, 0.0014,  ..., 0.0052, 0.0000, 0.0075],
        [0.0000, 0.0006, 0.0014,  ..., 0.0052, 0.0000, 0.0075],
        [0.0000, 0.0006, 0.0014,  ..., 0.0052, 0.0000, 0.0075]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(708836.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2012.2981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(364.4768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-919.1724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2659],
        [-0.2541],
        [-0.2459],
        ...,
        [-0.0144],
        [-0.0144],
        [-0.0143]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-121039.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9986],
        [0.9986],
        ...,
        [0.9986],
        [0.9986],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096259.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096149.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2942],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2856]], device='cuda:0') 
g.ndata[nfet].sum tensor(1889.9858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0432,  0.0072, -0.0060,  ...,  0.0142, -0.0354, -0.0301],
        [ 0.0081,  0.0016, -0.0011,  ...,  0.0029, -0.0064, -0.0054],
        [ 0.0108,  0.0021, -0.0015,  ...,  0.0038, -0.0086, -0.0073],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0185,  0.0033, -0.0025,  ...,  0.0063, -0.0150, -0.0128],
        [ 0.0084,  0.0017, -0.0011,  ...,  0.0030, -0.0067, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3633.7510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(301.6168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-21.5184, device='cuda:0')



h[100].sum tensor(-217.3022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-77.2832, device='cuda:0')



h[200].sum tensor(-79.3092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-231.0980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0837, 0.0147, 0.0000,  ..., 0.0281, 0.0000, 0.0000],
        [0.1091, 0.0187, 0.0000,  ..., 0.0363, 0.0000, 0.0000],
        [0.0317, 0.0064, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        ...,
        [0.0326, 0.0065, 0.0000,  ..., 0.0116, 0.0000, 0.0000],
        [0.0372, 0.0073, 0.0000,  ..., 0.0131, 0.0000, 0.0000],
        [0.0592, 0.0108, 0.0000,  ..., 0.0202, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(209242.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0895, 0.0000,  ..., 0.1493, 0.0000, 0.4527],
        [0.0000, 0.0800, 0.0000,  ..., 0.1343, 0.0000, 0.4054],
        [0.0000, 0.0460, 0.0000,  ..., 0.0797, 0.0000, 0.2351],
        ...,
        [0.0000, 0.0300, 0.0000,  ..., 0.0543, 0.0000, 0.1552],
        [0.0000, 0.0364, 0.0000,  ..., 0.0652, 0.0000, 0.1874],
        [0.0000, 0.0382, 0.0000,  ..., 0.0681, 0.0000, 0.1966]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(853606.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2363.6577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(452.9026, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1143.9579, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3969],
        [-0.3744],
        [-0.3414],
        ...,
        [-0.1399],
        [-0.1514],
        [-0.1430]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-149334.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096149.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096149.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1488.8586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0082,  0.0016, -0.0011,  ...,  0.0029, -0.0064, -0.0055],
        [ 0.0082,  0.0016, -0.0011,  ...,  0.0029, -0.0064, -0.0055],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2801.4727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(246.0984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9514, device='cuda:0')



h[100].sum tensor(-171.1225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-60.8807, device='cuda:0')



h[200].sum tensor(-62.4549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-182.0502, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0333, 0.0066, 0.0000,  ..., 0.0118, 0.0000, 0.0000],
        [0.0156, 0.0038, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0156, 0.0038, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0016, 0.0016, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0016, 0.0016, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0016, 0.0016, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168242.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0244, 0.0000,  ..., 0.0471, 0.0000, 0.1278],
        [0.0000, 0.0199, 0.0000,  ..., 0.0393, 0.0000, 0.1053],
        [0.0000, 0.0235, 0.0000,  ..., 0.0448, 0.0000, 0.1231],
        ...,
        [0.0000, 0.0007, 0.0014,  ..., 0.0054, 0.0000, 0.0077],
        [0.0000, 0.0007, 0.0014,  ..., 0.0054, 0.0000, 0.0077],
        [0.0000, 0.0007, 0.0014,  ..., 0.0054, 0.0000, 0.0077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(692354.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2001.8496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.8605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-894.6449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1964],
        [-0.2184],
        [-0.2484],
        ...,
        [-0.0152],
        [-0.0152],
        [-0.0153]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-119933.5469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9985],
        [0.9985],
        ...,
        [0.9985],
        [0.9985],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096149.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1096040.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3247],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1485.5027, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0470,  0.0078, -0.0065,  ...,  0.0155, -0.0386, -0.0328],
        [ 0.0160,  0.0029, -0.0022,  ...,  0.0054, -0.0129, -0.0109],
        [ 0.0587,  0.0097, -0.0081,  ...,  0.0192, -0.0482, -0.0409],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2754.5171, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(246.2231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.9132, device='cuda:0')



h[100].sum tensor(-170.0057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-60.7435, device='cuda:0')



h[200].sum tensor(-61.9566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-181.6398, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0934, 0.0163, 0.0000,  ..., 0.0312, 0.0000, 0.0000],
        [0.1991, 0.0331, 0.0000,  ..., 0.0654, 0.0000, 0.0000],
        [0.1851, 0.0309, 0.0000,  ..., 0.0609, 0.0000, 0.0000],
        ...,
        [0.0016, 0.0016, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0016, 0.0016, 0.0000,  ..., 0.0016, 0.0000, 0.0000],
        [0.0016, 0.0016, 0.0000,  ..., 0.0016, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165794.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1062e-01, 0.0000e+00,  ..., 1.8311e-01, 0.0000e+00,
         5.5582e-01],
        [0.0000e+00, 1.6002e-01, 0.0000e+00,  ..., 2.6032e-01, 0.0000e+00,
         8.0076e-01],
        [0.0000e+00, 1.7712e-01, 0.0000e+00,  ..., 2.8705e-01, 0.0000e+00,
         8.8563e-01],
        ...,
        [0.0000e+00, 6.9049e-04, 1.4390e-03,  ..., 5.6203e-03, 0.0000e+00,
         7.9790e-03],
        [0.0000e+00, 6.9049e-04, 1.4390e-03,  ..., 5.6203e-03, 0.0000e+00,
         7.9790e-03],
        [0.0000e+00, 6.9049e-04, 1.4390e-03,  ..., 5.6203e-03, 0.0000e+00,
         7.9790e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(688893., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2003.0197, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(343.4965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-879.5760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4339],
        [-0.4846],
        [-0.5193],
        ...,
        [-0.0155],
        [-0.0155],
        [-0.0154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-127572.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9984],
        [0.9984],
        ...,
        [0.9984],
        [0.9984],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1096040.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095931.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1342.4327, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0077,  0.0016, -0.0010,  ...,  0.0028, -0.0060, -0.0051],
        [ 0.0252,  0.0044, -0.0034,  ...,  0.0084, -0.0205, -0.0174],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2434.5500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(227.6926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2843, device='cuda:0')



h[100].sum tensor(-153.5021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.8932, device='cuda:0')



h[200].sum tensor(-55.8600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-164.1460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0149, 0.0038, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0455, 0.0087, 0.0000,  ..., 0.0158, 0.0000, 0.0000],
        [0.0681, 0.0123, 0.0000,  ..., 0.0231, 0.0000, 0.0000],
        ...,
        [0.0017, 0.0017, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0017, 0.0017, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0017, 0.0017, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156090.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0222, 0.0000,  ..., 0.0423, 0.0000, 0.1155],
        [0.0000, 0.0423, 0.0000,  ..., 0.0750, 0.0000, 0.2154],
        [0.0000, 0.0578, 0.0000,  ..., 0.1003, 0.0000, 0.2926],
        ...,
        [0.0000, 0.0007, 0.0015,  ..., 0.0058, 0.0000, 0.0082],
        [0.0000, 0.0007, 0.0015,  ..., 0.0058, 0.0000, 0.0082],
        [0.0000, 0.0007, 0.0015,  ..., 0.0058, 0.0000, 0.0082]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(660873.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1931.3212, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(318.3458, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-816.2365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2133],
        [-0.2577],
        [-0.2875],
        ...,
        [-0.0160],
        [-0.0159],
        [-0.0159]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-122082.7109, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9983],
        [0.9983],
        ...,
        [0.9983],
        [0.9983],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095931.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095821.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1387.2144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0089,  0.0018, -0.0012,  ...,  0.0032, -0.0070, -0.0059],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2507.9270, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(235.5123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.7941, device='cuda:0')



h[100].sum tensor(-158.9750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-56.7244, device='cuda:0')



h[200].sum tensor(-57.7667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-169.6216, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0017, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0171, 0.0042, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0231, 0.0051, 0.0000,  ..., 0.0086, 0.0000, 0.0000],
        ...,
        [0.0017, 0.0017, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0017, 0.0017, 0.0000,  ..., 0.0017, 0.0000, 0.0000],
        [0.0017, 0.0017, 0.0000,  ..., 0.0017, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(160859.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0064, 0.0003,  ..., 0.0162, 0.0000, 0.0371],
        [0.0000, 0.0164, 0.0000,  ..., 0.0333, 0.0000, 0.0867],
        [0.0000, 0.0233, 0.0000,  ..., 0.0453, 0.0000, 0.1212],
        ...,
        [0.0000, 0.0007, 0.0015,  ..., 0.0059, 0.0000, 0.0084],
        [0.0000, 0.0012, 0.0012,  ..., 0.0067, 0.0000, 0.0107],
        [0.0000, 0.0062, 0.0008,  ..., 0.0150, 0.0000, 0.0354]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(679478.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1989.4203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(328.1768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-843.1288, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1385],
        [-0.1811],
        [-0.2239],
        ...,
        [-0.0275],
        [-0.0446],
        [-0.0767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-126461.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9982],
        [0.9982],
        ...,
        [0.9982],
        [0.9982],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095821.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095712.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6367],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1321.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0099,  0.0020, -0.0013,  ...,  0.0035, -0.0078, -0.0066],
        [ 0.0404,  0.0068, -0.0055,  ...,  0.0133, -0.0329, -0.0280],
        [ 0.0165,  0.0030, -0.0022,  ...,  0.0056, -0.0132, -0.0112],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2348.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(227.2880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0495, device='cuda:0')



h[100].sum tensor(-151.2222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.0502, device='cuda:0')



h[200].sum tensor(-54.8686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-161.6251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1378, 0.0235, 0.0000,  ..., 0.0457, 0.0000, 0.0000],
        [0.0945, 0.0166, 0.0000,  ..., 0.0317, 0.0000, 0.0000],
        [0.1110, 0.0192, 0.0000,  ..., 0.0371, 0.0000, 0.0000],
        ...,
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156518.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1018, 0.0000,  ..., 0.1689, 0.0000, 0.5064],
        [0.0000, 0.1079, 0.0000,  ..., 0.1791, 0.0000, 0.5366],
        [0.0000, 0.1154, 0.0000,  ..., 0.1911, 0.0000, 0.5735],
        ...,
        [0.0000, 0.0008, 0.0015,  ..., 0.0061, 0.0000, 0.0086],
        [0.0000, 0.0008, 0.0015,  ..., 0.0061, 0.0000, 0.0086],
        [0.0000, 0.0008, 0.0015,  ..., 0.0061, 0.0000, 0.0086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(671580.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1967.5009, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(315.3813, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-816.1353, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4861],
        [-0.5400],
        [-0.5760],
        ...,
        [-0.0180],
        [-0.0179],
        [-0.0177]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-129579., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9981],
        [0.9981],
        ...,
        [0.9981],
        [0.9981],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095712.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 300 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095603.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4812],
        [0.3899],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1453.2106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0527,  0.0088, -0.0071,  ...,  0.0173, -0.0430, -0.0365],
        [ 0.0485,  0.0081, -0.0066,  ...,  0.0160, -0.0396, -0.0336],
        [ 0.0389,  0.0066, -0.0052,  ...,  0.0129, -0.0317, -0.0269],
        ...,
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000],
        [ 0.0004,  0.0004,  0.0000,  ...,  0.0004,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2591.4727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(246.6496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.5455, device='cuda:0')



h[100].sum tensor(-166.3961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-59.4231, device='cuda:0')



h[200].sum tensor(-60.2852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-177.6913, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.2082, 0.0347, 0.0000,  ..., 0.0685, 0.0000, 0.0000],
        [0.1940, 0.0325, 0.0000,  ..., 0.0639, 0.0000, 0.0000],
        [0.1644, 0.0278, 0.0000,  ..., 0.0543, 0.0000, 0.0000],
        ...,
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166415.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9608e-01, 0.0000e+00,  ..., 3.1651e-01, 0.0000e+00,
         9.6558e-01],
        [0.0000e+00, 1.8651e-01, 0.0000e+00,  ..., 3.0163e-01, 0.0000e+00,
         9.1886e-01],
        [0.0000e+00, 1.6978e-01, 0.0000e+00,  ..., 2.7567e-01, 0.0000e+00,
         8.3722e-01],
        ...,
        [0.0000e+00, 7.7913e-04, 1.5667e-03,  ..., 6.1818e-03, 0.0000e+00,
         8.7220e-03],
        [0.0000e+00, 7.7913e-04, 1.5667e-03,  ..., 6.1818e-03, 0.0000e+00,
         8.7220e-03],
        [0.0000e+00, 7.7913e-04, 1.5667e-03,  ..., 6.1818e-03, 0.0000e+00,
         8.7220e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(696457.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2065.3174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(338.6514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-872.9636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9085],
        [-0.9043],
        [-0.8893],
        ...,
        [-0.0172],
        [-0.0172],
        [-0.0171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-131109.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9980],
        [0.9980],
        ...,
        [0.9980],
        [0.9980],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095603.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095493.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1412.7876, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0203,  0.0036, -0.0027,  ...,  0.0069, -0.0164, -0.0139],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2488.1636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(242.0460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0853, device='cuda:0')



h[100].sum tensor(-161.7836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-57.7701, device='cuda:0')



h[200].sum tensor(-58.5274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-172.7486, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0293, 0.0062, 0.0000,  ..., 0.0107, 0.0000, 0.0000],
        [0.0274, 0.0059, 0.0000,  ..., 0.0101, 0.0000, 0.0000],
        ...,
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167659.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0004,  ..., 0.0187, 0.0000, 0.0439],
        [0.0000, 0.0215, 0.0000,  ..., 0.0418, 0.0000, 0.1113],
        [0.0000, 0.0271, 0.0000,  ..., 0.0517, 0.0000, 0.1392],
        ...,
        [0.0000, 0.0008, 0.0016,  ..., 0.0063, 0.0000, 0.0089],
        [0.0000, 0.0008, 0.0016,  ..., 0.0063, 0.0000, 0.0089],
        [0.0000, 0.0008, 0.0016,  ..., 0.0063, 0.0000, 0.0089]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(720185.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2089.8635, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(339.3340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-880.1077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1529],
        [-0.2029],
        [-0.2522],
        ...,
        [-0.0176],
        [-0.0175],
        [-0.0175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-140724.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9979],
        [0.9979],
        ...,
        [0.9979],
        [0.9979],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095493.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095384.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1312.7942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2265.7139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(228.9186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.9468, device='cuda:0')



h[100].sum tensor(-150.1937, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-53.6813, device='cuda:0')



h[200].sum tensor(-54.2540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-160.5219, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        ...,
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0018, 0.0018, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(156624.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0016,  ..., 0.0064, 0.0000, 0.0091],
        [0.0000, 0.0014, 0.0008,  ..., 0.0076, 0.0000, 0.0123],
        [0.0000, 0.0025, 0.0004,  ..., 0.0096, 0.0000, 0.0175],
        ...,
        [0.0000, 0.0008, 0.0016,  ..., 0.0064, 0.0000, 0.0090],
        [0.0000, 0.0008, 0.0016,  ..., 0.0064, 0.0000, 0.0090],
        [0.0000, 0.0008, 0.0016,  ..., 0.0064, 0.0000, 0.0090]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(672704.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2004.4875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(311.3757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-812.0503, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0720],
        [-0.0962],
        [-0.1295],
        ...,
        [-0.0179],
        [-0.0178],
        [-0.0178]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-131146.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9978],
        [0.9978],
        ...,
        [0.9978],
        [0.9978],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095384.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095275.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1619.2588, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2830.6699, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(271.0061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.4361, device='cuda:0')



h[100].sum tensor(-184.2712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-66.2129, device='cuda:0')



h[200].sum tensor(-66.4649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-197.9948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        ...,
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(183508.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0047, 0.0006,  ..., 0.0131, 0.0000, 0.0281],
        [0.0000, 0.0018, 0.0006,  ..., 0.0082, 0.0000, 0.0139],
        [0.0000, 0.0097, 0.0004,  ..., 0.0212, 0.0000, 0.0527],
        ...,
        [0.0000, 0.0008, 0.0016,  ..., 0.0065, 0.0000, 0.0091],
        [0.0000, 0.0008, 0.0016,  ..., 0.0065, 0.0000, 0.0091],
        [0.0000, 0.0008, 0.0016,  ..., 0.0065, 0.0000, 0.0091]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(769322.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2248.5857, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(376.5514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-972.4971, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1635],
        [-0.1491],
        [-0.1701],
        ...,
        [-0.0206],
        [-0.0209],
        [-0.0212]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-147818.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095275.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095275.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2998],
        [0.4436],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1800.2104, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0259,  0.0045, -0.0034,  ...,  0.0087, -0.0209, -0.0177],
        [ 0.0517,  0.0087, -0.0069,  ...,  0.0170, -0.0421, -0.0357],
        [ 0.0654,  0.0109, -0.0087,  ...,  0.0215, -0.0534, -0.0453],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3201.6848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(297.1612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.4963, device='cuda:0')



h[100].sum tensor(-205.8682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-73.6122, device='cuda:0')



h[200].sum tensor(-74.2548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-220.1207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1257, 0.0217, 0.0000,  ..., 0.0419, 0.0000, 0.0000],
        [0.1748, 0.0295, 0.0000,  ..., 0.0578, 0.0000, 0.0000],
        [0.2202, 0.0368, 0.0000,  ..., 0.0724, 0.0000, 0.0000],
        ...,
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(203651.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.3380e-01, 0.0000e+00,  ..., 2.1986e-01, 0.0000e+00,
         6.5657e-01],
        [0.0000e+00, 1.7022e-01, 0.0000e+00,  ..., 2.7637e-01, 0.0000e+00,
         8.3301e-01],
        [0.0000e+00, 2.0081e-01, 0.0000e+00,  ..., 3.2388e-01, 0.0000e+00,
         9.8125e-01],
        ...,
        [0.0000e+00, 8.2839e-04, 1.6319e-03,  ..., 6.4835e-03, 0.0000e+00,
         9.1169e-03],
        [0.0000e+00, 8.2839e-04, 1.6319e-03,  ..., 6.4835e-03, 0.0000e+00,
         9.1169e-03],
        [0.0000e+00, 8.2839e-04, 1.6319e-03,  ..., 6.4835e-03, 0.0000e+00,
         9.1169e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(848977.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2428.1218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(425.3861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1095.8064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7678],
        [-0.8457],
        [-0.9115],
        ...,
        [-0.0182],
        [-0.0181],
        [-0.0181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-164101., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9977],
        [0.9977],
        ...,
        [0.9977],
        [0.9977],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095275.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095165.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2812],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1397.5367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0104,  0.0021, -0.0013,  ...,  0.0037, -0.0082, -0.0069],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2384.5505, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(241.8097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.9116, device='cuda:0')



h[100].sum tensor(-159.5275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-57.1465, device='cuda:0')



h[200].sum tensor(-57.4545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-170.8838, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0453, 0.0088, 0.0000,  ..., 0.0159, 0.0000, 0.0000],
        [0.0100, 0.0032, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0269, 0.0059, 0.0000,  ..., 0.0100, 0.0000, 0.0000],
        ...,
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165882.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0282, 0.0000,  ..., 0.0538, 0.0000, 0.1437],
        [0.0000, 0.0222, 0.0000,  ..., 0.0437, 0.0000, 0.1143],
        [0.0000, 0.0277, 0.0000,  ..., 0.0532, 0.0000, 0.1414],
        ...,
        [0.0000, 0.0008, 0.0016,  ..., 0.0066, 0.0000, 0.0092],
        [0.0000, 0.0008, 0.0016,  ..., 0.0066, 0.0000, 0.0092],
        [0.0000, 0.0008, 0.0016,  ..., 0.0066, 0.0000, 0.0092]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(714748.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2105.5789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(331.3374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-866.1797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2784],
        [-0.2879],
        [-0.2938],
        ...,
        [-0.0185],
        [-0.0184],
        [-0.0184]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-141891.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9976],
        [0.9976],
        ...,
        [0.9976],
        [0.9976],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095165.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1095056.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1149.7751, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0084,  0.0017, -0.0011,  ...,  0.0030, -0.0065, -0.0055],
        [ 0.0084,  0.0017, -0.0011,  ...,  0.0030, -0.0065, -0.0055],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1877.4338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(207.7445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.0908, device='cuda:0')



h[100].sum tensor(-130.8591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-47.0153, device='cuda:0')



h[200].sum tensor(-47.0591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-140.5887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0231, 0.0053, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        [0.0300, 0.0064, 0.0000,  ..., 0.0110, 0.0000, 0.0000],
        [0.0231, 0.0053, 0.0000,  ..., 0.0088, 0.0000, 0.0000],
        ...,
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(139732.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0203, 0.0000,  ..., 0.0406, 0.0000, 0.1048],
        [0.0000, 0.0259, 0.0000,  ..., 0.0504, 0.0000, 0.1324],
        [0.0000, 0.0235, 0.0000,  ..., 0.0465, 0.0000, 0.1206],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0066, 0.0000, 0.0093],
        [0.0000, 0.0009, 0.0017,  ..., 0.0066, 0.0000, 0.0093],
        [0.0000, 0.0009, 0.0017,  ..., 0.0066, 0.0000, 0.0093]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(609928.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1874.9880, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(268.3967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-701.9874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1937],
        [-0.2154],
        [-0.2202],
        ...,
        [-0.0187],
        [-0.0186],
        [-0.0186]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-119177.7344, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9975],
        [0.9975],
        ...,
        [0.9975],
        [0.9975],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1095056.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094947., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4390],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1494.5598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0160,  0.0030, -0.0021,  ...,  0.0055, -0.0127, -0.0108],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2522.9785, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(256.1176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0163, device='cuda:0')



h[100].sum tensor(-170.1720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-61.1139, device='cuda:0')



h[200].sum tensor(-61.1053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-182.7473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0274, 0.0060, 0.0000,  ..., 0.0102, 0.0000, 0.0000],
        [0.0146, 0.0040, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0582, 0.0109, 0.0000,  ..., 0.0201, 0.0000, 0.0000],
        ...,
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000],
        [0.0019, 0.0019, 0.0000,  ..., 0.0019, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(174220.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0209, 0.0000,  ..., 0.0413, 0.0000, 0.1075],
        [0.0000, 0.0178, 0.0000,  ..., 0.0358, 0.0000, 0.0923],
        [0.0000, 0.0282, 0.0000,  ..., 0.0531, 0.0000, 0.1431],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0067, 0.0000, 0.0094],
        [0.0000, 0.0009, 0.0017,  ..., 0.0067, 0.0000, 0.0094],
        [0.0000, 0.0009, 0.0017,  ..., 0.0067, 0.0000, 0.0094]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(739238.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2186.1821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.2163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-909.9929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2143],
        [-0.2147],
        [-0.2221],
        ...,
        [-0.0190],
        [-0.0189],
        [-0.0189]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-141936.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9974],
        [0.9974],
        ...,
        [0.9974],
        [0.9974],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094947., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094837.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1396.1572, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084,  0.0018, -0.0010,  ...,  0.0030, -0.0065, -0.0055],
        [ 0.0084,  0.0018, -0.0010,  ...,  0.0030, -0.0065, -0.0055],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2304.9272, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(242.4726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8959, device='cuda:0')



h[100].sum tensor(-158.4232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-57.0901, device='cuda:0')



h[200].sum tensor(-56.8014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-170.7151, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0163, 0.0042, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0163, 0.0042, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0358, 0.0074, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(161426.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0136, 0.0000,  ..., 0.0297, 0.0000, 0.0719],
        [0.0000, 0.0169, 0.0000,  ..., 0.0352, 0.0000, 0.0879],
        [0.0000, 0.0256, 0.0000,  ..., 0.0491, 0.0000, 0.1301],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0068, 0.0000, 0.0095],
        [0.0000, 0.0009, 0.0017,  ..., 0.0068, 0.0000, 0.0095],
        [0.0000, 0.0009, 0.0017,  ..., 0.0068, 0.0000, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(686372.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2087.6785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(318.6737, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-835.7028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1261],
        [-0.1411],
        [-0.1645],
        ...,
        [-0.0192],
        [-0.0191],
        [-0.0191]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-137162.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9973],
        [0.9973],
        ...,
        [0.9973],
        [0.9973],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094837.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094728.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.2874],
        [0.4065],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1443.0061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0179,  0.0033, -0.0023,  ...,  0.0061, -0.0143, -0.0121],
        [ 0.0346,  0.0059, -0.0045,  ...,  0.0115, -0.0279, -0.0236],
        [ 0.0460,  0.0078, -0.0060,  ...,  0.0152, -0.0372, -0.0316],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2372.0264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(249.3938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.4293, device='cuda:0')



h[100].sum tensor(-163.6356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-59.0058, device='cuda:0')



h[200].sum tensor(-58.5822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-176.4436, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1304, 0.0225, 0.0000,  ..., 0.0435, 0.0000, 0.0000],
        [0.1245, 0.0216, 0.0000,  ..., 0.0416, 0.0000, 0.0000],
        [0.1443, 0.0247, 0.0000,  ..., 0.0480, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168015.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1432e-01, 0.0000e+00,  ..., 1.8981e-01, 0.0000e+00,
         5.5758e-01],
        [0.0000e+00, 1.1330e-01, 0.0000e+00,  ..., 1.8832e-01, 0.0000e+00,
         5.5270e-01],
        [0.0000e+00, 1.1130e-01, 0.0000e+00,  ..., 1.8519e-01, 0.0000e+00,
         5.4309e-01],
        ...,
        [0.0000e+00, 3.3100e-03, 3.7537e-04,  ..., 1.1372e-02, 0.0000e+00,
         2.1477e-02],
        [0.0000e+00, 1.6945e-03, 8.5059e-04,  ..., 8.3444e-03, 0.0000e+00,
         1.3537e-02],
        [0.0000e+00, 8.8674e-04, 1.7012e-03,  ..., 6.8307e-03, 0.0000e+00,
         9.5668e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(715081.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2142.2007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(336.1566, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-869.4751, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4794],
        [-0.4813],
        [-0.4599],
        ...,
        [-0.0536],
        [-0.0509],
        [-0.0614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-137943.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9972],
        [0.9972],
        ...,
        [0.9972],
        [0.9972],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094728.3750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 450 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094619., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6274],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1336.2196, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0181,  0.0033, -0.0023,  ...,  0.0062, -0.0144, -0.0122],
        [ 0.0308,  0.0053, -0.0039,  ...,  0.0103, -0.0248, -0.0210],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2166.5244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(236.2909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2135, device='cuda:0')



h[100].sum tensor(-152.4220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.6392, device='cuda:0')



h[200].sum tensor(-54.4856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-163.3862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1211, 0.0210, 0.0000,  ..., 0.0405, 0.0000, 0.0000],
        [0.0657, 0.0122, 0.0000,  ..., 0.0226, 0.0000, 0.0000],
        [0.0457, 0.0090, 0.0000,  ..., 0.0161, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(158419.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0755, 0.0000,  ..., 0.1287, 0.0000, 0.3704],
        [0.0000, 0.0610, 0.0000,  ..., 0.1055, 0.0000, 0.3003],
        [0.0000, 0.0420, 0.0000,  ..., 0.0747, 0.0000, 0.2087],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0069, 0.0000, 0.0096],
        [0.0000, 0.0009, 0.0017,  ..., 0.0069, 0.0000, 0.0096],
        [0.0000, 0.0009, 0.0017,  ..., 0.0069, 0.0000, 0.0096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(678548.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2062.8157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(311.6260, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-810.3551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3694],
        [-0.3697],
        [-0.3509],
        ...,
        [-0.0195],
        [-0.0195],
        [-0.0194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-133237.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9971],
        [0.9971],
        ...,
        [0.9971],
        [0.9971],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094619., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094509.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2769],
        [0.3801],
        [0.2729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1503.1597, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0237,  0.0042, -0.0030,  ...,  0.0080, -0.0190, -0.0161],
        [ 0.0405,  0.0069, -0.0052,  ...,  0.0134, -0.0327, -0.0277],
        [ 0.0443,  0.0075, -0.0057,  ...,  0.0147, -0.0358, -0.0303],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2444.7803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(258.8016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.1142, device='cuda:0')



h[100].sum tensor(-170.4812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-61.4655, device='cuda:0')



h[200].sum tensor(-60.8494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-183.7988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0953, 0.0169, 0.0000,  ..., 0.0322, 0.0000, 0.0000],
        [0.1299, 0.0225, 0.0000,  ..., 0.0433, 0.0000, 0.0000],
        [0.1479, 0.0253, 0.0000,  ..., 0.0492, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(179597.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0752, 0.0000,  ..., 0.1283, 0.0000, 0.3685],
        [0.0000, 0.0960, 0.0000,  ..., 0.1608, 0.0000, 0.4684],
        [0.0000, 0.1065, 0.0000,  ..., 0.1773, 0.0000, 0.5185],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0069, 0.0000, 0.0097],
        [0.0000, 0.0009, 0.0017,  ..., 0.0069, 0.0000, 0.0097],
        [0.0000, 0.0009, 0.0017,  ..., 0.0069, 0.0000, 0.0097]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(780315.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2258.0598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(362.6212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-939.9090, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3381],
        [-0.3495],
        [-0.3456],
        ...,
        [-0.0197],
        [-0.0196],
        [-0.0196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-151536.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9970],
        [0.9970],
        ...,
        [0.9970],
        [0.9970],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094509.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094400.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4739],
        [0.5479],
        [0.5757],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1301.2494, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0554,  0.0093, -0.0071,  ...,  0.0183, -0.0448, -0.0380],
        [ 0.0627,  0.0105, -0.0080,  ...,  0.0206, -0.0508, -0.0430],
        [ 0.0447,  0.0076, -0.0057,  ...,  0.0148, -0.0361, -0.0306],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0182,  0.0033, -0.0023,  ...,  0.0062, -0.0144, -0.0122]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2057.8059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(231.9100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.8154, device='cuda:0')



h[100].sum tensor(-148.0281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-53.2092, device='cuda:0')



h[200].sum tensor(-52.7556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-159.1103, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1590, 0.0271, 0.0000,  ..., 0.0528, 0.0000, 0.0000],
        [0.1830, 0.0310, 0.0000,  ..., 0.0605, 0.0000, 0.0000],
        [0.1912, 0.0323, 0.0000,  ..., 0.0632, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0348, 0.0073, 0.0000,  ..., 0.0126, 0.0000, 0.0000],
        [0.0586, 0.0111, 0.0000,  ..., 0.0203, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159535.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1781e-01, 0.0000e+00,  ..., 1.9398e-01, 0.0000e+00,
         5.7176e-01],
        [0.0000e+00, 1.3705e-01, 0.0000e+00,  ..., 2.2450e-01, 0.0000e+00,
         6.6408e-01],
        [0.0000e+00, 1.4057e-01, 0.0000e+00,  ..., 2.3024e-01, 0.0000e+00,
         6.8109e-01],
        ...,
        [0.0000e+00, 1.6771e-02, 3.8083e-04,  ..., 3.3072e-02, 0.0000e+00,
         8.6289e-02],
        [0.0000e+00, 4.1868e-02, 0.0000e+00,  ..., 7.4056e-02, 0.0000e+00,
         2.0730e-01],
        [0.0000e+00, 6.1009e-02, 0.0000e+00,  ..., 1.0523e-01, 0.0000e+00,
         2.9952e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(696971.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2082.6907, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(312.9872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-815.9720, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3868],
        [-0.4357],
        [-0.4591],
        ...,
        [-0.1476],
        [-0.2073],
        [-0.2458]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-137700.6094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9969],
        [0.9969],
        ...,
        [0.9969],
        [0.9969],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094400.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094291., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1323.7964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0201,  0.0036, -0.0025,  ...,  0.0069, -0.0160, -0.0136],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2070.1401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(234.6133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.0721, device='cuda:0')



h[100].sum tensor(-149.8812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.1312, device='cuda:0')



h[200].sum tensor(-53.3352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-161.8672, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0292, 0.0064, 0.0000,  ..., 0.0108, 0.0000, 0.0000],
        [0.0335, 0.0071, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(161228.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0092, 0.0004,  ..., 0.0212, 0.0000, 0.0501],
        [0.0000, 0.0253, 0.0000,  ..., 0.0484, 0.0000, 0.1283],
        [0.0000, 0.0358, 0.0000,  ..., 0.0663, 0.0000, 0.1791],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0070, 0.0000, 0.0098],
        [0.0000, 0.0009, 0.0017,  ..., 0.0070, 0.0000, 0.0098],
        [0.0000, 0.0009, 0.0017,  ..., 0.0070, 0.0000, 0.0098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(702900.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2099.3804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(317.5563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-824.2692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2094],
        [-0.2777],
        [-0.3498],
        ...,
        [-0.0201],
        [-0.0200],
        [-0.0199]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-137843.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9968],
        [0.9968],
        ...,
        [0.9968],
        [0.9968],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094291., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094181.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1213.2301, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0122,  0.0024, -0.0015,  ...,  0.0043, -0.0095, -0.0081],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1855.2957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(220.0915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.8132, device='cuda:0')



h[100].sum tensor(-137.6535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-49.6100, device='cuda:0')



h[200].sum tensor(-48.9098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-148.3477, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0485, 0.0095, 0.0000,  ..., 0.0171, 0.0000, 0.0000],
        [0.0433, 0.0086, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0410, 0.0083, 0.0000,  ..., 0.0146, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0159, 0.0042, 0.0000,  ..., 0.0065, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(148042.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0671, 0.0000,  ..., 0.1148, 0.0000, 0.3282],
        [0.0000, 0.0618, 0.0000,  ..., 0.1069, 0.0000, 0.3034],
        [0.0000, 0.0552, 0.0000,  ..., 0.0961, 0.0000, 0.2715],
        ...,
        [0.0000, 0.0016, 0.0014,  ..., 0.0082, 0.0000, 0.0133],
        [0.0000, 0.0060, 0.0009,  ..., 0.0154, 0.0000, 0.0343],
        [0.0000, 0.0199, 0.0000,  ..., 0.0383, 0.0000, 0.1014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(644903.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1986.2523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(284.9885, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-743.5256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4617],
        [-0.4734],
        [-0.4716],
        ...,
        [-0.0480],
        [-0.0820],
        [-0.1305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-126989.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9967],
        [0.9967],
        ...,
        [0.9967],
        [0.9967],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094181.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1094072.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1765.4762, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0183,  0.0034, -0.0023,  ...,  0.0063, -0.0144, -0.0122],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2826.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(295.9072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-20.1008, device='cuda:0')



h[100].sum tensor(-199.3407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-72.1919, device='cuda:0')



h[200].sum tensor(-70.7205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-215.8736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0197, 0.0049, 0.0000,  ..., 0.0078, 0.0000, 0.0000],
        [0.0165, 0.0044, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0663, 0.0123, 0.0000,  ..., 0.0228, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(199923.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0162, 0.0000,  ..., 0.0332, 0.0000, 0.0840],
        [0.0000, 0.0180, 0.0000,  ..., 0.0361, 0.0000, 0.0928],
        [0.0000, 0.0287, 0.0000,  ..., 0.0535, 0.0000, 0.1440],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0099],
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0099],
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0099]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(837193.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2456.0447, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(409.2805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1062.5676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2045],
        [-0.1890],
        [-0.1809],
        ...,
        [-0.0203],
        [-0.0202],
        [-0.0202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-167387.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9966],
        [0.9966],
        ...,
        [0.9966],
        [0.9966],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1094072.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093962.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6191],
        [0.3860],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1227.8320, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0224,  0.0040, -0.0028,  ...,  0.0076, -0.0178, -0.0151],
        [ 0.0323,  0.0056, -0.0040,  ...,  0.0108, -0.0259, -0.0219],
        [ 0.0804,  0.0133, -0.0101,  ...,  0.0263, -0.0649, -0.0550],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1841.1538, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(222.2453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9795, device='cuda:0')



h[100].sum tensor(-138.8066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-50.2071, device='cuda:0')



h[200].sum tensor(-49.1698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-150.1331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0759, 0.0139, 0.0000,  ..., 0.0259, 0.0000, 0.0000],
        [0.1797, 0.0305, 0.0000,  ..., 0.0595, 0.0000, 0.0000],
        [0.2070, 0.0349, 0.0000,  ..., 0.0683, 0.0000, 0.0000],
        ...,
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000],
        [0.0020, 0.0020, 0.0000,  ..., 0.0020, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(147711.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1028, 0.0000,  ..., 0.1714, 0.0000, 0.4984],
        [0.0000, 0.1471, 0.0000,  ..., 0.2404, 0.0000, 0.7097],
        [0.0000, 0.1778, 0.0000,  ..., 0.2881, 0.0000, 0.8565],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0099],
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0099],
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0099]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(644344.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1995.9559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(282.3328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-743.8570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6769],
        [-0.7295],
        [-0.7753],
        ...,
        [-0.0204],
        [-0.0203],
        [-0.0203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-129348.6172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9965],
        [0.9965],
        ...,
        [0.9965],
        [0.9965],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093962.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093853.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1411.9010, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0125,  0.0024, -0.0015,  ...,  0.0044, -0.0097, -0.0082],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2145.3120, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(247.4265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.0752, device='cuda:0')



h[100].sum tensor(-159.0832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-57.7339, device='cuda:0')



h[200].sum tensor(-56.2666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-172.6402, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0036, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0140, 0.0040, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(167852.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0259, 0.0000, 0.0626],
        [0.0000, 0.0098, 0.0000,  ..., 0.0226, 0.0000, 0.0530],
        [0.0000, 0.0045, 0.0004,  ..., 0.0137, 0.0000, 0.0275],
        ...,
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0100],
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0100],
        [0.0000, 0.0009, 0.0017,  ..., 0.0071, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(729284.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2179.8091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(331.1725, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-867.5047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1938],
        [-0.2042],
        [-0.2239],
        ...,
        [-0.0210],
        [-0.0208],
        [-0.0207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-147122.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9964],
        [0.9964],
        ...,
        [0.9964],
        [0.9964],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093853.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093744.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3721],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.9250, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094,  0.0019, -0.0011,  ...,  0.0034, -0.0072, -0.0061],
        [ 0.0229,  0.0041, -0.0028,  ...,  0.0078, -0.0182, -0.0154],
        [ 0.0318,  0.0055, -0.0039,  ...,  0.0106, -0.0254, -0.0215],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2230.9604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(255.8894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7928, device='cuda:0')



h[100].sum tensor(-165.7027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-60.3110, device='cuda:0')



h[200].sum tensor(-58.5185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-180.3464, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0619, 0.0116, 0.0000,  ..., 0.0214, 0.0000, 0.0000],
        [0.0742, 0.0136, 0.0000,  ..., 0.0254, 0.0000, 0.0000],
        [0.1273, 0.0221, 0.0000,  ..., 0.0425, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(173326.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0653, 0.0000,  ..., 0.1144, 0.0000, 0.3199],
        [0.0000, 0.0851, 0.0000,  ..., 0.1450, 0.0000, 0.4140],
        [0.0000, 0.1146, 0.0000,  ..., 0.1906, 0.0000, 0.5544],
        ...,
        [0.0000, 0.0009, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0009, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0009, 0.0018,  ..., 0.0072, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(744715.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2228.9878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(344.1238, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-899.4167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4874],
        [-0.5629],
        [-0.6366],
        ...,
        [-0.0210],
        [-0.0209],
        [-0.0207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-149601.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9963],
        [0.9963],
        ...,
        [0.9963],
        [0.9963],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093744.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093634.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1190.6757, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0237,  0.0042, -0.0029,  ...,  0.0080, -0.0188, -0.0159],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0189,  0.0035, -0.0023,  ...,  0.0065, -0.0149, -0.0126],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1712.1765, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(216.8612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.5564, device='cuda:0')



h[100].sum tensor(-133.6221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-48.6878, device='cuda:0')



h[200].sum tensor(-47.1170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-145.5898, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0209, 0.0051, 0.0000,  ..., 0.0082, 0.0000, 0.0000],
        [0.0582, 0.0111, 0.0000,  ..., 0.0202, 0.0000, 0.0000],
        [0.0572, 0.0109, 0.0000,  ..., 0.0199, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(144220.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0410, 0.0000,  ..., 0.0733, 0.0000, 0.2027],
        [0.0000, 0.0621, 0.0000,  ..., 0.1074, 0.0000, 0.3037],
        [0.0000, 0.0748, 0.0000,  ..., 0.1278, 0.0000, 0.3645],
        ...,
        [0.0000, 0.0009, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0009, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0009, 0.0018,  ..., 0.0072, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(631749.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1959.1208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(275.6373, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-714.0394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5414],
        [-0.5559],
        [-0.5717],
        ...,
        [-0.0207],
        [-0.0206],
        [-0.0205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-121400.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9962],
        [0.9962],
        ...,
        [0.9962],
        [0.9962],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093634.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 600 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093525.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1496.4321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0082,  0.0018, -0.0010,  ...,  0.0030, -0.0062, -0.0053],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2229.6343, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(259.4266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.0376, device='cuda:0')



h[100].sum tensor(-168.0438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-61.1904, device='cuda:0')



h[200].sum tensor(-59.1638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-182.9762, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0383, 0.0079, 0.0000,  ..., 0.0138, 0.0000, 0.0000],
        [0.0232, 0.0055, 0.0000,  ..., 0.0089, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(180214.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0357, 0.0000,  ..., 0.0663, 0.0000, 0.1777],
        [0.0000, 0.0254, 0.0000,  ..., 0.0492, 0.0000, 0.1284],
        [0.0000, 0.0113, 0.0004,  ..., 0.0256, 0.0000, 0.0607],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(784580.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2285.9478, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(362.3394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-935.6399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2557],
        [-0.2384],
        [-0.2143],
        ...,
        [-0.0207],
        [-0.0206],
        [-0.0206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-150219.6406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093525.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093525.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.7329],
        [0.6099],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1474.6163, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0534,  0.0090, -0.0065,  ...,  0.0176, -0.0429, -0.0363],
        [ 0.0588,  0.0099, -0.0072,  ...,  0.0194, -0.0472, -0.0400],
        [ 0.0646,  0.0108, -0.0079,  ...,  0.0212, -0.0519, -0.0439],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2192.8547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(256.5080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.7892, device='cuda:0')



h[100].sum tensor(-165.6675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-60.2984, device='cuda:0')



h[200].sum tensor(-58.3272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-180.3087, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1519, 0.0261, 0.0000,  ..., 0.0505, 0.0000, 0.0000],
        [0.2081, 0.0351, 0.0000,  ..., 0.0687, 0.0000, 0.0000],
        [0.1835, 0.0311, 0.0000,  ..., 0.0607, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168792.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1433, 0.0000,  ..., 0.2350, 0.0000, 0.6904],
        [0.0000, 0.1579, 0.0000,  ..., 0.2573, 0.0000, 0.7598],
        [0.0000, 0.1457, 0.0000,  ..., 0.2379, 0.0000, 0.7018],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0100],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(719492.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2196.8652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(331.9098, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-873.1130, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5835],
        [-0.5848],
        [-0.5671],
        ...,
        [-0.0207],
        [-0.0206],
        [-0.0206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-146782.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9961],
        [0.9961],
        ...,
        [0.9961],
        [0.9961],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093525.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093416., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1224.2810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1746.9146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(222.6464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9390, device='cuda:0')



h[100].sum tensor(-137.8682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-50.0619, device='cuda:0')



h[200].sum tensor(-48.4653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-149.6989, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0215, 0.0052, 0.0000,  ..., 0.0084, 0.0000, 0.0000],
        [0.0615, 0.0116, 0.0000,  ..., 0.0213, 0.0000, 0.0000],
        ...,
        [0.0303, 0.0066, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(150067.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0167, 0.0004,  ..., 0.0329, 0.0000, 0.0858],
        [0.0000, 0.0388, 0.0000,  ..., 0.0686, 0.0000, 0.1915],
        [0.0000, 0.0712, 0.0000,  ..., 0.1202, 0.0000, 0.3461],
        ...,
        [0.0000, 0.0360, 0.0000,  ..., 0.0654, 0.0000, 0.1783],
        [0.0000, 0.0153, 0.0004,  ..., 0.0311, 0.0000, 0.0788],
        [0.0000, 0.0045, 0.0009,  ..., 0.0133, 0.0000, 0.0272]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(657793., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2026.0029, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(287.1182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-755.2859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2561],
        [-0.3360],
        [-0.4150],
        ...,
        [-0.2056],
        [-0.1485],
        [-0.0946]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-132687.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9960],
        [0.9960],
        ...,
        [0.9960],
        [0.9960],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093416., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093306.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1338.7021, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0336,  0.0058, -0.0040,  ...,  0.0112, -0.0267, -0.0226],
        [ 0.0176,  0.0033, -0.0021,  ...,  0.0061, -0.0138, -0.0117],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1922.7634, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(238.3468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2418, device='cuda:0')



h[100].sum tensor(-150.4110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.7407, device='cuda:0')



h[200].sum tensor(-52.7933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-163.6898, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1428, 0.0246, 0.0000,  ..., 0.0476, 0.0000, 0.0000],
        [0.0610, 0.0115, 0.0000,  ..., 0.0211, 0.0000, 0.0000],
        [0.0564, 0.0108, 0.0000,  ..., 0.0197, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159219.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1131, 0.0000,  ..., 0.1873, 0.0000, 0.5459],
        [0.0000, 0.0774, 0.0000,  ..., 0.1312, 0.0000, 0.3758],
        [0.0000, 0.0659, 0.0000,  ..., 0.1138, 0.0000, 0.3216],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(689347.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2108.3284, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(309.5393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-810.2930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6147],
        [-0.5536],
        [-0.5067],
        ...,
        [-0.0209],
        [-0.0208],
        [-0.0208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-137679.2656, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9959],
        [0.9959],
        ...,
        [0.9959],
        [0.9959],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093306.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093197.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4824],
        [0.3977],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1508.2367, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0393,  0.0067, -0.0047,  ...,  0.0131, -0.0313, -0.0265],
        [ 0.0367,  0.0063, -0.0044,  ...,  0.0122, -0.0292, -0.0247],
        [ 0.0539,  0.0091, -0.0065,  ...,  0.0178, -0.0431, -0.0365],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2189.8848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(261.6904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.1720, device='cuda:0')



h[100].sum tensor(-169.1481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-61.6731, device='cuda:0')



h[200].sum tensor(-59.2785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-184.4196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0986, 0.0175, 0.0000,  ..., 0.0333, 0.0000, 0.0000],
        [0.1958, 0.0331, 0.0000,  ..., 0.0647, 0.0000, 0.0000],
        [0.1871, 0.0317, 0.0000,  ..., 0.0619, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(175609.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1097, 0.0000,  ..., 0.1828, 0.0000, 0.5297],
        [0.0000, 0.1499, 0.0000,  ..., 0.2451, 0.0000, 0.7208],
        [0.0000, 0.1564, 0.0000,  ..., 0.2552, 0.0000, 0.7516],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(746759.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2250.9902, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(350.3938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-907.5104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6982],
        [-0.7446],
        [-0.7795],
        ...,
        [-0.0239],
        [-0.0322],
        [-0.0499]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-144583.9844, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093197.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093197.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1222.4298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1706.5261, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(222.4211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9180, device='cuda:0')



h[100].sum tensor(-137.2574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-49.9862, device='cuda:0')



h[200].sum tensor(-48.1023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-149.4726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(151063.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0009, 0.0018,  ..., 0.0073, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0072, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(666249., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2039.9771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(288.8195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-761.6297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1608],
        [-0.1522],
        [-0.1437],
        ...,
        [-0.0209],
        [-0.0209],
        [-0.0208]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-135162.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9958],
        [0.9958],
        ...,
        [0.9958],
        [0.9958],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093197.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093087.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1565.6714, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0079,  0.0017, -0.0009,  ...,  0.0029, -0.0059, -0.0050],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2255.3228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(268.8719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.8259, device='cuda:0')



h[100].sum tensor(-174.7466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-64.0217, device='cuda:0')



h[200].sum tensor(-61.1461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-191.4424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0154, 0.0042, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0302, 0.0066, 0.0000,  ..., 0.0112, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(180339.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0105, 0.0000,  ..., 0.0243, 0.0000, 0.0563],
        [0.0000, 0.0190, 0.0000,  ..., 0.0389, 0.0000, 0.0973],
        [0.0000, 0.0306, 0.0000,  ..., 0.0585, 0.0000, 0.1532],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(766549., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2298.2695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(360.4854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-938.2688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3038],
        [-0.3411],
        [-0.3895],
        ...,
        [-0.0210],
        [-0.0210],
        [-0.0209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-153747.8594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093087.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1093087.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1270.6741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0136,  0.0026, -0.0016,  ...,  0.0047, -0.0105, -0.0089],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1764.7157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(228.6951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.4673, device='cuda:0')



h[100].sum tensor(-142.1473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-51.9590, device='cuda:0')



h[200].sum tensor(-49.7392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-155.3717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0274, 0.0061, 0.0000,  ..., 0.0103, 0.0000, 0.0000],
        [0.0151, 0.0042, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(154194.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0047, 0.0009,  ..., 0.0134, 0.0000, 0.0281],
        [0.0000, 0.0099, 0.0004,  ..., 0.0219, 0.0000, 0.0529],
        ...,
        [0.0000, 0.0232, 0.0000,  ..., 0.0446, 0.0000, 0.1170],
        [0.0000, 0.0134, 0.0000,  ..., 0.0282, 0.0000, 0.0700],
        [0.0000, 0.0046, 0.0009,  ..., 0.0134, 0.0000, 0.0275]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(676913.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2067.2085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(296.7269, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-779.5580, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0545],
        [-0.0785],
        [-0.1078],
        ...,
        [-0.1098],
        [-0.0835],
        [-0.0576]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-138281.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9957],
        [0.9957],
        ...,
        [0.9957],
        [0.9957],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1093087.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092978.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1392.1897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1947.5168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(245.3394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.8508, device='cuda:0')



h[100].sum tensor(-155.4432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-56.9279, device='cuda:0')



h[200].sum tensor(-54.3076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-170.2300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0419, 0.0085, 0.0000,  ..., 0.0150, 0.0000, 0.0000],
        [0.0558, 0.0107, 0.0000,  ..., 0.0195, 0.0000, 0.0000],
        [0.0417, 0.0084, 0.0000,  ..., 0.0149, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164660.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0690, 0.0000,  ..., 0.1168, 0.0000, 0.3350],
        [0.0000, 0.0728, 0.0000,  ..., 0.1231, 0.0000, 0.3534],
        [0.0000, 0.0610, 0.0000,  ..., 0.1046, 0.0000, 0.2972],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(718289.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2164.4216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(321.6243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-844.7598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7967],
        [-0.7578],
        [-0.7090],
        ...,
        [-0.0211],
        [-0.0210],
        [-0.0210]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-145929.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9956],
        [0.9956],
        ...,
        [0.9956],
        [0.9956],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092978.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092869.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1225.6698, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1660.2590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(223.1190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.9549, device='cuda:0')



h[100].sum tensor(-137.2426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-50.1187, device='cuda:0')



h[200].sum tensor(-47.8746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-149.8688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(145938.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0008,  ..., 0.0092, 0.0000, 0.0144],
        [0.0000, 0.0026, 0.0004,  ..., 0.0106, 0.0000, 0.0185],
        [0.0000, 0.0059, 0.0004,  ..., 0.0164, 0.0000, 0.0344],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(632496.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1996.1067, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(276.7734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-728.6845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1165],
        [-0.1362],
        [-0.1633],
        ...,
        [-0.0214],
        [-0.0215],
        [-0.0219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-126194.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9955],
        [0.9955],
        ...,
        [0.9955],
        [0.9955],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092869.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 750 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092759.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4792],
        [0.0000],
        [0.7246],
        ...,
        [0.0000],
        [0.2690],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1421.2837, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0341,  0.0059, -0.0040,  ...,  0.0114, -0.0270, -0.0229],
        [ 0.0641,  0.0107, -0.0076,  ...,  0.0211, -0.0512, -0.0433],
        [ 0.0476,  0.0081, -0.0056,  ...,  0.0157, -0.0379, -0.0320],
        ...,
        [ 0.0291,  0.0051, -0.0034,  ...,  0.0098, -0.0230, -0.0195],
        [ 0.0105,  0.0021, -0.0012,  ...,  0.0038, -0.0080, -0.0068],
        [ 0.0100,  0.0021, -0.0011,  ...,  0.0036, -0.0077, -0.0065]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1967.4021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(250.3897, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.1820, device='cuda:0')



h[100].sum tensor(-159.1323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-58.1175, device='cuda:0')



h[200].sum tensor(-55.4244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-173.7874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.2134, 0.0359, 0.0000,  ..., 0.0704, 0.0000, 0.0000],
        [0.1801, 0.0306, 0.0000,  ..., 0.0597, 0.0000, 0.0000],
        [0.2050, 0.0346, 0.0000,  ..., 0.0677, 0.0000, 0.0000],
        ...,
        [0.0513, 0.0100, 0.0000,  ..., 0.0180, 0.0000, 0.0000],
        [0.0710, 0.0131, 0.0000,  ..., 0.0244, 0.0000, 0.0000],
        [0.0279, 0.0062, 0.0000,  ..., 0.0104, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(166695.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1832, 0.0000,  ..., 0.2966, 0.0000, 0.8778],
        [0.0000, 0.1768, 0.0000,  ..., 0.2866, 0.0000, 0.8476],
        [0.0000, 0.1749, 0.0000,  ..., 0.2833, 0.0000, 0.8383],
        ...,
        [0.0000, 0.0598, 0.0000,  ..., 0.1049, 0.0000, 0.2922],
        [0.0000, 0.0511, 0.0000,  ..., 0.0904, 0.0000, 0.2505],
        [0.0000, 0.0277, 0.0000,  ..., 0.0523, 0.0000, 0.1386]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(714299.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2184.0676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(326.5553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-856.4540, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7451],
        [-0.7635],
        [-0.7696],
        ...,
        [-0.2791],
        [-0.2256],
        [-0.1632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-143752.8281, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9954],
        [0.9954],
        ...,
        [0.9954],
        [0.9954],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092759.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092650.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.4954],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1704.0856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0330,  0.0057, -0.0038,  ...,  0.0110, -0.0261, -0.0221],
        [ 0.0378,  0.0065, -0.0044,  ...,  0.0126, -0.0300, -0.0254],
        [ 0.0180,  0.0033, -0.0021,  ...,  0.0062, -0.0141, -0.0119],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2408.7397, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(289.4456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.4019, device='cuda:0')



h[100].sum tensor(-190.5191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-69.6815, device='cuda:0')



h[200].sum tensor(-66.2531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-208.3670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1634, 0.0279, 0.0000,  ..., 0.0543, 0.0000, 0.0000],
        [0.1462, 0.0252, 0.0000,  ..., 0.0487, 0.0000, 0.0000],
        [0.1448, 0.0250, 0.0000,  ..., 0.0483, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(204921.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1265, 0.0000,  ..., 0.2082, 0.0000, 0.6084],
        [0.0000, 0.1298, 0.0000,  ..., 0.2138, 0.0000, 0.6243],
        [0.0000, 0.1289, 0.0000,  ..., 0.2128, 0.0000, 0.6204],
        ...,
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0018,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(889766., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2517.0220, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(421.2869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1085.0186, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6049],
        [-0.6137],
        [-0.6002],
        ...,
        [-0.0226],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-174297.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9953],
        [0.9953],
        ...,
        [0.9953],
        [0.9953],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092650.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092541., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1432.2850, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1936.5845, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(251.2259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.3073, device='cuda:0')



h[100].sum tensor(-159.4261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-58.5674, device='cuda:0')



h[200].sum tensor(-55.3543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-175.1326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(169190.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0018,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0018,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0018,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(732024.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2204.0911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(332.7139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-869.4290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0386],
        [-0.0455],
        [-0.0563],
        ...,
        [-0.0213],
        [-0.0212],
        [-0.0211]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-148279.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9952],
        [0.9952],
        ...,
        [0.9952],
        [0.9952],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092541., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092431.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1364.7399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0092,  0.0019, -0.0010,  ...,  0.0033, -0.0070, -0.0059],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1814.1354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(242.4307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5382, device='cuda:0')



h[100].sum tensor(-152.1428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-55.8054, device='cuda:0')



h[200].sum tensor(-52.7432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-166.8736, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0334, 0.0071, 0.0000,  ..., 0.0122, 0.0000, 0.0000],
        [0.0178, 0.0046, 0.0000,  ..., 0.0072, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(162005.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0327, 0.0000,  ..., 0.0620, 0.0000, 0.1628],
        [0.0000, 0.0178, 0.0000,  ..., 0.0370, 0.0000, 0.0913],
        [0.0000, 0.0066, 0.0004,  ..., 0.0175, 0.0000, 0.0375],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(703446.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2150.2913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.8826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-830.4444, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3263],
        [-0.2580],
        [-0.1947],
        ...,
        [-0.0214],
        [-0.0213],
        [-0.0212]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-145843.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9951],
        [0.9951],
        ...,
        [0.9951],
        [0.9951],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092431.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092322.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3013],
        [0.7891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1263.7090, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0284,  0.0050, -0.0032,  ...,  0.0096, -0.0224, -0.0189],
        [ 0.0112,  0.0022, -0.0012,  ...,  0.0040, -0.0086, -0.0072],
        [ 0.0284,  0.0050, -0.0032,  ...,  0.0096, -0.0224, -0.0189],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1641.4650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(229.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-14.3880, device='cuda:0')



h[100].sum tensor(-141.2357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-51.6742, device='cuda:0')



h[200].sum tensor(-48.8856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-154.5200, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0633, 0.0119, 0.0000,  ..., 0.0219, 0.0000, 0.0000],
        [0.1116, 0.0196, 0.0000,  ..., 0.0375, 0.0000, 0.0000],
        [0.0354, 0.0074, 0.0000,  ..., 0.0129, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(153718.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0528, 0.0000,  ..., 0.0942, 0.0000, 0.2588],
        [0.0000, 0.0631, 0.0000,  ..., 0.1097, 0.0000, 0.3074],
        [0.0000, 0.0424, 0.0000,  ..., 0.0763, 0.0000, 0.2087],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(672386.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2067.8123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(295.1256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-774.6101, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3949],
        [-0.3967],
        [-0.3774],
        ...,
        [-0.0223],
        [-0.0224],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-136300.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9950],
        [0.9950],
        ...,
        [0.9950],
        [0.9950],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092322.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092213., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3115],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1597.2953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0115,  0.0023, -0.0013,  ...,  0.0041, -0.0088, -0.0075],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2149.8611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(275.1374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-18.1860, device='cuda:0')



h[100].sum tensor(-178.1546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-65.3148, device='cuda:0')



h[200].sum tensor(-61.5680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-195.3093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0489, 0.0096, 0.0000,  ..., 0.0172, 0.0000, 0.0000],
        [0.0250, 0.0058, 0.0000,  ..., 0.0095, 0.0000, 0.0000],
        [0.0200, 0.0050, 0.0000,  ..., 0.0079, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(185541.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0298, 0.0000,  ..., 0.0577, 0.0000, 0.1494],
        [0.0000, 0.0243, 0.0000,  ..., 0.0483, 0.0000, 0.1230],
        [0.0000, 0.0186, 0.0000,  ..., 0.0387, 0.0000, 0.0955],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(785808.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2352.0630, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(372.8994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-968.6940, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2774],
        [-0.2566],
        [-0.2296],
        ...,
        [-0.0214],
        [-0.0213],
        [-0.0214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-152416.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9949],
        [0.9949],
        ...,
        [0.9949],
        [0.9949],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092213., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1092103.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1696.7344, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0135,  0.0026, -0.0015,  ...,  0.0047, -0.0104, -0.0088],
        [ 0.0098,  0.0020, -0.0011,  ...,  0.0035, -0.0074, -0.0063],
        [ 0.0098,  0.0020, -0.0011,  ...,  0.0035, -0.0074, -0.0063],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-2278.2698, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(288.4141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-19.3182, device='cuda:0')



h[100].sum tensor(-188.6549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-69.3810, device='cuda:0')



h[200].sum tensor(-65.0947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-207.4682, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0735, 0.0135, 0.0000,  ..., 0.0252, 0.0000, 0.0000],
        [0.0621, 0.0117, 0.0000,  ..., 0.0215, 0.0000, 0.0000],
        [0.0304, 0.0066, 0.0000,  ..., 0.0113, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(200059.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0697, 0.0000,  ..., 0.1213, 0.0000, 0.3393],
        [0.0000, 0.0558, 0.0000,  ..., 0.0990, 0.0000, 0.2729],
        [0.0000, 0.0358, 0.0000,  ..., 0.0663, 0.0000, 0.1773],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(862111.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2481.5334, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(408.2769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-1057.5082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4064],
        [-0.3526],
        [-0.2908],
        ...,
        [-0.0214],
        [-0.0213],
        [-0.0213]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-171084.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9948],
        [0.9948],
        ...,
        [0.9948],
        [0.9948],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1092103.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091994.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1337.9180, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0101,  0.0021, -0.0011,  ...,  0.0036, -0.0076, -0.0064],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1702.5200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(239.2730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.2329, device='cuda:0')



h[100].sum tensor(-148.9311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.7086, device='cuda:0')



h[200].sum tensor(-51.3075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-163.5939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0116, 0.0036, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0481, 0.0095, 0.0000,  ..., 0.0170, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(163642.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0174, 0.0000,  ..., 0.0361, 0.0000, 0.0894],
        [0.0000, 0.0270, 0.0000,  ..., 0.0517, 0.0000, 0.1355],
        [0.0000, 0.0474, 0.0000,  ..., 0.0845, 0.0000, 0.2328],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(716663.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2158.4275, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(319.9092, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-835.0425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3517],
        [-0.3711],
        [-0.3913],
        ...,
        [-0.0215],
        [-0.0215],
        [-0.0216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-141508.4219, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9947],
        [0.9947],
        ...,
        [0.9947],
        [0.9947],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091994.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091885., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2612],
        [0.3628],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1356.6160, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0195,  0.0036, -0.0022,  ...,  0.0067, -0.0151, -0.0128],
        [ 0.0205,  0.0037, -0.0023,  ...,  0.0070, -0.0160, -0.0135],
        [ 0.0246,  0.0044, -0.0027,  ...,  0.0083, -0.0193, -0.0163],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1706.4071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(241.2789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4457, device='cuda:0')



h[100].sum tensor(-150.3860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-55.4732, device='cuda:0')



h[200].sum tensor(-51.7274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-165.8802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0432, 0.0087, 0.0000,  ..., 0.0154, 0.0000, 0.0000],
        [0.0824, 0.0150, 0.0000,  ..., 0.0281, 0.0000, 0.0000],
        [0.0919, 0.0165, 0.0000,  ..., 0.0311, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(162380.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0466, 0.0000,  ..., 0.0838, 0.0000, 0.2288],
        [0.0000, 0.0587, 0.0000,  ..., 0.1032, 0.0000, 0.2866],
        [0.0000, 0.0610, 0.0000,  ..., 0.1069, 0.0000, 0.2976],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(704019., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2149.0400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(316.2419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-828.2311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2815],
        [-0.2948],
        [-0.3042],
        ...,
        [-0.0215],
        [-0.0214],
        [-0.0214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-140291.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9946],
        [0.9946],
        ...,
        [0.9946],
        [0.9946],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091885., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091775.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1362.3661, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1703.1545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(242.6661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.5112, device='cuda:0')



h[100].sum tensor(-151.3440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-55.7083, device='cuda:0')



h[200].sum tensor(-51.9750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-166.5833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(164340.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0043, 0.0004,  ..., 0.0133, 0.0000, 0.0265],
        [0.0000, 0.0020, 0.0006,  ..., 0.0093, 0.0000, 0.0154],
        [0.0000, 0.0038, 0.0006,  ..., 0.0126, 0.0000, 0.0240],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(716209.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2166.8123, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(320.1259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-840.3251, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2665],
        [-0.2177],
        [-0.1935],
        ...,
        [-0.0215],
        [-0.0214],
        [-0.0214]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-146416.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9945],
        [0.9945],
        ...,
        [0.9945],
        [0.9945],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091775.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 900 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091666.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1417.5375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1767.3860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(250.2316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-16.1394, device='cuda:0')



h[100].sum tensor(-157.2592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-57.9643, device='cuda:0')



h[200].sum tensor(-53.9213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-173.3294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(168871.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0013, 0.0009,  ..., 0.0084, 0.0000, 0.0124],
        [0.0000, 0.0011, 0.0014,  ..., 0.0077, 0.0000, 0.0110],
        [0.0000, 0.0010, 0.0015,  ..., 0.0076, 0.0000, 0.0108],
        ...,
        [0.0000, 0.0156, 0.0004,  ..., 0.0318, 0.0000, 0.0803],
        [0.0000, 0.0044, 0.0009,  ..., 0.0133, 0.0000, 0.0267],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(730953.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2199.8867, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(333.3531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-863.2850, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1029],
        [-0.0922],
        [-0.0924],
        ...,
        [-0.1537],
        [-0.0956],
        [-0.0544]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-142618., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9944],
        [0.9944],
        ...,
        [0.9944],
        [0.9944],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091666.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091556.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1331.2137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1612.9998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(237.7886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.1565, device='cuda:0')



h[100].sum tensor(-147.1273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-54.4345, device='cuda:0')



h[200].sum tensor(-50.3676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-162.7741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(159174.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0051, 0.0004,  ..., 0.0150, 0.0000, 0.0307],
        [0.0000, 0.0018, 0.0007,  ..., 0.0091, 0.0000, 0.0146],
        [0.0000, 0.0010, 0.0015,  ..., 0.0076, 0.0000, 0.0108],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(694287.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2123.4294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(307.3521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-809.6417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1769],
        [-0.1556],
        [-0.1521],
        ...,
        [-0.0217],
        [-0.0216],
        [-0.0216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-142476.2031, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9943],
        [0.9943],
        ...,
        [0.9943],
        [0.9943],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091556.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091447.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5181],
        [0.5444],
        [0.5391],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1375.4944, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0349,  0.0060, -0.0038,  ...,  0.0117, -0.0274, -0.0231],
        [ 0.0680,  0.0113, -0.0075,  ...,  0.0224, -0.0538, -0.0454],
        [ 0.0684,  0.0114, -0.0076,  ...,  0.0225, -0.0541, -0.0457],
        ...,
        [ 0.0184,  0.0034, -0.0020,  ...,  0.0063, -0.0142, -0.0120],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1666.7003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(244.4534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.6607, device='cuda:0')



h[100].sum tensor(-152.3185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-56.2452, device='cuda:0')



h[200].sum tensor(-52.0624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-168.1886, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.1572, 0.0270, 0.0000,  ..., 0.0523, 0.0000, 0.0000],
        [0.2020, 0.0341, 0.0000,  ..., 0.0668, 0.0000, 0.0000],
        [0.2599, 0.0434, 0.0000,  ..., 0.0855, 0.0000, 0.0000],
        ...,
        [0.0393, 0.0081, 0.0000,  ..., 0.0142, 0.0000, 0.0000],
        [0.0336, 0.0072, 0.0000,  ..., 0.0123, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(165208.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1308, 0.0000,  ..., 0.2148, 0.0000, 0.6279],
        [0.0000, 0.1722, 0.0000,  ..., 0.2793, 0.0000, 0.8239],
        [0.0000, 0.2047, 0.0000,  ..., 0.3299, 0.0000, 0.9782],
        ...,
        [0.0000, 0.0546, 0.0000,  ..., 0.0959, 0.0000, 0.2665],
        [0.0000, 0.0461, 0.0000,  ..., 0.0819, 0.0000, 0.2261],
        [0.0000, 0.0301, 0.0000,  ..., 0.0554, 0.0000, 0.1494]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(719600.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2174.6785, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(323.0629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-844.8884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6955],
        [-0.7382],
        [-0.7580],
        ...,
        [-0.3281],
        [-0.3102],
        [-0.2839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-145050.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9942],
        [0.9942],
        ...,
        [0.9942],
        [0.9942],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091447.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091338.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1206.6663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0123,  0.0024, -0.0013,  ...,  0.0043, -0.0093, -0.0079]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1401.8420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(221.3649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-13.7385, device='cuda:0')



h[100].sum tensor(-133.6930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-49.3416, device='cuda:0')



h[200].sum tensor(-45.6238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-147.5451, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0138, 0.0040, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0232, 0.0055, 0.0000,  ..., 0.0089, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(148127.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0023, 0.0004,  ..., 0.0101, 0.0000, 0.0169],
        [0.0000, 0.0012, 0.0012,  ..., 0.0079, 0.0000, 0.0116],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0041, 0.0009,  ..., 0.0128, 0.0000, 0.0254],
        [0.0000, 0.0118, 0.0000,  ..., 0.0257, 0.0000, 0.0620],
        [0.0000, 0.0204, 0.0000,  ..., 0.0403, 0.0000, 0.1033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(654224.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2020.3707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(281.4359, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-739.1385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1380],
        [-0.1206],
        [-0.1059],
        ...,
        [-0.0560],
        [-0.0812],
        [-0.1114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-131066.0078, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9941],
        [0.9941],
        ...,
        [0.9941],
        [0.9941],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091338.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091228.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5796],
        [0.6226],
        [0.0000],
        ...,
        [0.3176],
        [0.3633],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1523.2872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0575,  0.0097, -0.0063,  ...,  0.0190, -0.0453, -0.0382],
        [ 0.0547,  0.0092, -0.0060,  ...,  0.0181, -0.0431, -0.0364],
        [ 0.0777,  0.0129, -0.0085,  ...,  0.0255, -0.0614, -0.0518],
        ...,
        [ 0.0230,  0.0041, -0.0025,  ...,  0.0078, -0.0178, -0.0150],
        [ 0.0118,  0.0023, -0.0012,  ...,  0.0042, -0.0089, -0.0075],
        [ 0.0134,  0.0026, -0.0014,  ...,  0.0047, -0.0102, -0.0086]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1841.4330, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(264.4982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-17.3434, device='cuda:0')



h[100].sum tensor(-168.0477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-62.2886, device='cuda:0')



h[200].sum tensor(-57.2567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-186.2599, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.2256, 0.0379, 0.0000,  ..., 0.0744, 0.0000, 0.0000],
        [0.2828, 0.0471, 0.0000,  ..., 0.0929, 0.0000, 0.0000],
        [0.1627, 0.0278, 0.0000,  ..., 0.0541, 0.0000, 0.0000],
        ...,
        [0.0702, 0.0130, 0.0000,  ..., 0.0241, 0.0000, 0.0000],
        [0.0672, 0.0125, 0.0000,  ..., 0.0232, 0.0000, 0.0000],
        [0.0325, 0.0070, 0.0000,  ..., 0.0119, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(178187.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1919, 0.0000,  ..., 0.3096, 0.0000, 0.9172],
        [0.0000, 0.2126, 0.0000,  ..., 0.3418, 0.0000, 1.0149],
        [0.0000, 0.1649, 0.0000,  ..., 0.2679, 0.0000, 0.7895],
        ...,
        [0.0000, 0.0675, 0.0000,  ..., 0.1178, 0.0000, 0.3286],
        [0.0000, 0.0641, 0.0000,  ..., 0.1122, 0.0000, 0.3121],
        [0.0000, 0.0535, 0.0000,  ..., 0.0953, 0.0000, 0.2620]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(767439.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2294.2539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(353.1947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-926.1993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7727],
        [-0.7622],
        [-0.6951],
        ...,
        [-0.3353],
        [-0.3216],
        [-0.3066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-156841.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9940],
        [0.9940],
        ...,
        [0.9940],
        [0.9940],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091228.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091119.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5317],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(935.2174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0076,  0.0017, -0.0008,  ...,  0.0028, -0.0057, -0.0048],
        [ 0.0265,  0.0047, -0.0028,  ...,  0.0089, -0.0206, -0.0174],
        [ 0.0109,  0.0022, -0.0011,  ...,  0.0039, -0.0083, -0.0070],
        ...,
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000],
        [ 0.0005,  0.0005,  0.0000,  ...,  0.0005,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-983.3643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(184.5261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-10.6479, device='cuda:0')



h[100].sum tensor(-104.0027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-38.2419, device='cuda:0')



h[200].sum tensor(-35.3792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-114.3537, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0973, 0.0174, 0.0000,  ..., 0.0329, 0.0000, 0.0000],
        [0.0478, 0.0094, 0.0000,  ..., 0.0169, 0.0000, 0.0000],
        [0.0487, 0.0096, 0.0000,  ..., 0.0172, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(123223.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0593, 0.0000,  ..., 0.1042, 0.0000, 0.2894],
        [0.0000, 0.0514, 0.0000,  ..., 0.0924, 0.0000, 0.2519],
        [0.0000, 0.0485, 0.0000,  ..., 0.0875, 0.0000, 0.2383],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(563756.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1781.7687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(223.9889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-577.5067, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3612],
        [-0.3675],
        [-0.3648],
        ...,
        [-0.0217],
        [-0.0216],
        [-0.0216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-107307.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9939],
        [0.9939],
        ...,
        [0.9939],
        [0.9939],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091119.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1091010.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(380.4504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34900.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248526.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.6338, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8464, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0145, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0318],
        [-0.0321],
        [-0.0326],
        ...,
        [-0.0217],
        [-0.0216],
        [-0.0216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33241.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9938],
        [0.9938],
        ...,
        [0.9938],
        [0.9938],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1091010.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090900.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(380.5540, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.3647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34906.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248506.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.7988, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8491, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0179, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0319],
        [-0.0322],
        [-0.0326],
        ...,
        [-0.0217],
        [-0.0216],
        [-0.0216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33282.8672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9937],
        [0.9937],
        ...,
        [0.9937],
        [0.9937],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090900.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090791.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(380.6476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.3781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34911.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248480.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.9399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0319],
        [-0.0322],
        [-0.0327],
        ...,
        [-0.0218],
        [-0.0217],
        [-0.0216]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33323.1758, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9936],
        [0.9936],
        ...,
        [0.9936],
        [0.9936],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090791.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090682.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(380.7327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.3902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34916.1211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248448.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.0587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0320],
        [-0.0322],
        [-0.0327],
        ...,
        [-0.0218],
        [-0.0217],
        [-0.0217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33362.4297, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9935],
        [0.9935],
        ...,
        [0.9935],
        [0.9935],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090682.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 1050 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090572.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(380.8099, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34919.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248411.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8550, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0320],
        [-0.0323],
        [-0.0327],
        ...,
        [-0.0218],
        [-0.0217],
        [-0.0217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33400.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9934],
        [0.9934],
        ...,
        [0.9934],
        [0.9934],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090572.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090463.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(380.8800, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34922.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248368.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.2375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8563, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0320],
        [-0.0323],
        [-0.0328],
        ...,
        [-0.0218],
        [-0.0217],
        [-0.0217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33438.0234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9933],
        [0.9933],
        ...,
        [0.9933],
        [0.9933],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090463.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090354.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(380.9438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34924.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248322.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.3015, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8573, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0285, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0320],
        [-0.0323],
        [-0.0328],
        ...,
        [-0.0219],
        [-0.0218],
        [-0.0217]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33474.5273, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9932],
        [0.9932],
        ...,
        [0.9932],
        [0.9932],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090354.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090244.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.0010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34926.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248271.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.3506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0295, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0321],
        [-0.0324],
        [-0.0328],
        ...,
        [-0.0219],
        [-0.0218],
        [-0.0218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33510.2578, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9931],
        [0.9931],
        ...,
        [0.9931],
        [0.9931],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090244.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090135.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.0532, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34927.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248217.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.3860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0302, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0321],
        [-0.0324],
        [-0.0329],
        ...,
        [-0.0219],
        [-0.0218],
        [-0.0218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33545.2852, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9930],
        [0.9930],
        ...,
        [0.9930],
        [0.9930],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090135.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1090026.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.1008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34928.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248160.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.4090, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0321],
        [-0.0324],
        [-0.0329],
        ...,
        [-0.0219],
        [-0.0218],
        [-0.0218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33579.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9929],
        [0.9929],
        ...,
        [0.9929],
        [0.9929],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1090026.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089917., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.1434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34929.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248099.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.4207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0322],
        [-0.0324],
        [-0.0329],
        ...,
        [-0.0219],
        [-0.0219],
        [-0.0218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33613.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9928],
        [0.9928],
        ...,
        [0.9928],
        [0.9928],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089917., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089807.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.1824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34929.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(248036.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.4224, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0322],
        [-0.0325],
        [-0.0329],
        ...,
        [-0.0220],
        [-0.0219],
        [-0.0218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33646.6641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9927],
        [0.9927],
        ...,
        [0.9927],
        [0.9927],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089807.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089698.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.2177, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34929.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247971.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.4148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8592, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0322],
        [-0.0325],
        [-0.0330],
        ...,
        [-0.0220],
        [-0.0219],
        [-0.0219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33679.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9926],
        [0.9926],
        ...,
        [0.9926],
        [0.9926],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089698.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089589., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.2497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34928.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247903.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.3988, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8589, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0322],
        [-0.0325],
        [-0.0330],
        ...,
        [-0.0220],
        [-0.0219],
        [-0.0219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33711.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9925],
        [0.9925],
        ...,
        [0.9925],
        [0.9925],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089589., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 1200 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089479.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.2788, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34927.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247834.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.3755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0323],
        [-0.0326],
        [-0.0330],
        ...,
        [-0.0220],
        [-0.0219],
        [-0.0219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33743.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9924],
        [0.9924],
        ...,
        [0.9924],
        [0.9924],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089479.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089370.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34926.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247762.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.3448, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0323],
        [-0.0326],
        [-0.0331],
        ...,
        [-0.0220],
        [-0.0220],
        [-0.0219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33774.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9923],
        [0.9923],
        ...,
        [0.9923],
        [0.9923],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089370.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089261.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.3285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34925.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247689.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.3083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0323],
        [-0.0326],
        [-0.0331],
        ...,
        [-0.0221],
        [-0.0220],
        [-0.0219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33805.7422, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9922],
        [0.9922],
        ...,
        [0.9922],
        [0.9922],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089261.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089151.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.3502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34923.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247615.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.2661, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8567, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0324],
        [-0.0326],
        [-0.0331],
        ...,
        [-0.0221],
        [-0.0220],
        [-0.0220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33836.3984, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9921],
        [0.9921],
        ...,
        [0.9921],
        [0.9921],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089151.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1089042.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.3697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34921.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247539.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.2184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0324],
        [-0.0327],
        [-0.0331],
        ...,
        [-0.0221],
        [-0.0220],
        [-0.0220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33866.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9920],
        [0.9920],
        ...,
        [0.9920],
        [0.9920],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1089042.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088933.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.3872, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34920.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247462., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.1665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0257, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0324],
        [-0.0327],
        [-0.0332],
        ...,
        [-0.0221],
        [-0.0220],
        [-0.0220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33896.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9919],
        [0.9919],
        ...,
        [0.9919],
        [0.9919],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088933.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088824., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4033, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34917.9883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247383.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.1102, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8541, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0324],
        [-0.0327],
        [-0.0332],
        ...,
        [-0.0221],
        [-0.0220],
        [-0.0220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33926.4883, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9918],
        [0.9918],
        ...,
        [0.9918],
        [0.9918],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088824., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088714.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34915.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247304.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-963.0500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0325],
        [-0.0327],
        [-0.0332],
        ...,
        [-0.0222],
        [-0.0221],
        [-0.0220]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33955.9492, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9917],
        [0.9917],
        ...,
        [0.9917],
        [0.9917],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088714.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088605.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4310, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34913.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247224.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.9866, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0325],
        [-0.0328],
        [-0.0332],
        ...,
        [-0.0222],
        [-0.0221],
        [-0.0221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-33985.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9916],
        [0.9916],
        ...,
        [0.9916],
        [0.9916],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088605.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088496.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4430, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34911.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247143.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.9199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8510, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0325],
        [-0.0328],
        [-0.0333],
        ...,
        [-0.0222],
        [-0.0221],
        [-0.0221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34014.1406, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9915],
        [0.9915],
        ...,
        [0.9915],
        [0.9915],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088496.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 1350 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088387., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4537, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34908.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(247061.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.8506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8498, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0191, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0325],
        [-0.0328],
        [-0.0333],
        ...,
        [-0.0222],
        [-0.0221],
        [-0.0221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34042.8789, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9914],
        [0.9914],
        ...,
        [0.9914],
        [0.9914],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088387., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088277.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4636, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34905.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246979.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.7784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0326],
        [-0.0328],
        [-0.0333],
        ...,
        [-0.0222],
        [-0.0221],
        [-0.0221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34071.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9913],
        [0.9913],
        ...,
        [0.9913],
        [0.9913],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088277.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088168.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34903.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246896.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.7040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0326],
        [-0.0329],
        [-0.0333],
        ...,
        [-0.0222],
        [-0.0222],
        [-0.0221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34099.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9912],
        [0.9912],
        [0.9912],
        ...,
        [0.9912],
        [0.9912],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088168.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1088059.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4804, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34900.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246813., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.6273, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8461, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0326],
        [-0.0329],
        [-0.0334],
        ...,
        [-0.0223],
        [-0.0222],
        [-0.0221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34127.7695, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9911],
        [0.9911],
        [0.9911],
        ...,
        [0.9911],
        [0.9911],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1088059.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087950., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34897.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0017,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246729.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.5488, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8448, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0327],
        [-0.0329],
        [-0.0334],
        ...,
        [-0.0223],
        [-0.0222],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34155.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9910],
        [0.9910],
        [0.9910],
        ...,
        [0.9910],
        [0.9910],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087950., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087840.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.4942, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34894.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246644.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.4686, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0327],
        [-0.0330],
        [-0.0334],
        ...,
        [-0.0223],
        [-0.0222],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34183.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9909],
        [0.9909],
        [0.9909],
        ...,
        [0.9909],
        [0.9909],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087840.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087731.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.4997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34891.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246559.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.3868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0327],
        [-0.0330],
        [-0.0335],
        ...,
        [-0.0223],
        [-0.0222],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34210.8633, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9908],
        [0.9908],
        [0.9908],
        ...,
        [0.9908],
        [0.9908],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087731.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087622.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34888.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246474.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.3038, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0327],
        [-0.0330],
        [-0.0335],
        ...,
        [-0.0223],
        [-0.0222],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34238.1914, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9907],
        [0.9907],
        [0.9907],
        ...,
        [0.9907],
        [0.9907],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087622.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087513., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5106, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34885.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246389.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.2194, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0328],
        [-0.0330],
        [-0.0335],
        ...,
        [-0.0223],
        [-0.0223],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34265.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9906],
        [0.9906],
        [0.9906],
        ...,
        [0.9906],
        [0.9906],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087513., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087403.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5149, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34882.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246303.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.1339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0328],
        [-0.0331],
        [-0.0335],
        ...,
        [-0.0224],
        [-0.0223],
        [-0.0222]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34292.3164, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9905],
        [0.9905],
        [0.9905],
        ...,
        [0.9905],
        [0.9905],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087403.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 1500 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087294.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34879.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246217.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-962.0474, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0025, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0328],
        [-0.0331],
        [-0.0336],
        ...,
        [-0.0224],
        [-0.0223],
        [-0.0223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34319.1328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9904],
        [0.9904],
        [0.9904],
        ...,
        [0.9904],
        [0.9904],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087294.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087185.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5226, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34876.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246131.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.9597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8351, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-20.0006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0328],
        [-0.0331],
        [-0.0336],
        ...,
        [-0.0224],
        [-0.0223],
        [-0.0223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34346.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9903],
        [0.9903],
        [0.9903],
        ...,
        [0.9903],
        [0.9903],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087185.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1087076.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34873.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(246045.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.8713, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8336, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9987, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0329],
        [-0.0331],
        [-0.0336],
        ...,
        [-0.0224],
        [-0.0223],
        [-0.0223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34373.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9902],
        [0.9902],
        [0.9902],
        ...,
        [0.9902],
        [0.9902],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1087076.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086967., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5287, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34869.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245959.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.7820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9969, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0329],
        [-0.0332],
        [-0.0336],
        ...,
        [-0.0224],
        [-0.0223],
        [-0.0223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34401.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9901],
        [0.9901],
        [0.9901],
        ...,
        [0.9901],
        [0.9901],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086967., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086857.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5315, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34866.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245873.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.6918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9950, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0329],
        [-0.0332],
        [-0.0337],
        ...,
        [-0.0224],
        [-0.0223],
        [-0.0223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34428.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9900],
        [0.9900],
        [0.9900],
        ...,
        [0.9900],
        [0.9900],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086857.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086748.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34863.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245786.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.6016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9931, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0329],
        [-0.0332],
        [-0.0337],
        ...,
        [-0.0225],
        [-0.0224],
        [-0.0223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34455.4453, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9899],
        [0.9899],
        [0.9899],
        ...,
        [0.9899],
        [0.9899],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086748.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086639.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5363, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34859.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245700.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.5105, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0330],
        [-0.0332],
        [-0.0337],
        ...,
        [-0.0225],
        [-0.0224],
        [-0.0223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34482.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9898],
        [0.9898],
        [0.9898],
        ...,
        [0.9898],
        [0.9898],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086639.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086530.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34856.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245613.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.4187, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9893, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0330],
        [-0.0333],
        [-0.0337],
        ...,
        [-0.0225],
        [-0.0224],
        [-0.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34509.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9897],
        [0.9897],
        [0.9897],
        ...,
        [0.9897],
        [0.9897],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086530.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086421., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5401, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34853.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245526.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.3265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0330],
        [-0.0333],
        [-0.0338],
        ...,
        [-0.0225],
        [-0.0224],
        [-0.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34535.6172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9896],
        [0.9896],
        [0.9896],
        ...,
        [0.9896],
        [0.9896],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086421., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086311.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5416, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34849.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245439.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.2340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8231, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0330],
        [-0.0333],
        [-0.0338],
        ...,
        [-0.0225],
        [-0.0224],
        [-0.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34562.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9895],
        [0.9895],
        [0.9895],
        ...,
        [0.9895],
        [0.9895],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086311.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 1650 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086202.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34846.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245352.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.1409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9836, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0330],
        [-0.0333],
        [-0.0338],
        ...,
        [-0.0225],
        [-0.0224],
        [-0.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34588.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9894],
        [0.9894],
        [0.9894],
        ...,
        [0.9894],
        [0.9894],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086202.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1086093.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5446, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34843.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245265.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-961.0479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0331],
        [-0.0334],
        [-0.0338],
        ...,
        [-0.0225],
        [-0.0224],
        [-0.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34614.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9893],
        [0.9893],
        [0.9893],
        ...,
        [0.9893],
        [0.9893],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1086093.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085984.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5458, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34839.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245178.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.9542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8185, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0331],
        [-0.0334],
        [-0.0339],
        ...,
        [-0.0226],
        [-0.0225],
        [-0.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34640.6328, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9892],
        [0.9892],
        [0.9892],
        ...,
        [0.9892],
        [0.9892],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085984.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085875.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34836.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245091.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.8602, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9778, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0331],
        [-0.0334],
        [-0.0339],
        ...,
        [-0.0226],
        [-0.0225],
        [-0.0224]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34666.5859, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9891],
        [0.9891],
        [0.9891],
        ...,
        [0.9891],
        [0.9891],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085875.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085766., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34833.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(245004.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.7660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0331],
        [-0.0334],
        [-0.0339],
        ...,
        [-0.0226],
        [-0.0225],
        [-0.0225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34692.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9890],
        [0.9890],
        [0.9890],
        ...,
        [0.9890],
        [0.9890],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085766., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085656.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34829.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244917.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.6718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9739, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0332],
        [-0.0334],
        [-0.0339],
        ...,
        [-0.0226],
        [-0.0225],
        [-0.0225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34718.1836, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9889],
        [0.9889],
        [0.9889],
        ...,
        [0.9889],
        [0.9889],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085656.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085547.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34826.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244829.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.5770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9719, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0332],
        [-0.0335],
        [-0.0340],
        ...,
        [-0.0226],
        [-0.0225],
        [-0.0225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34743.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9888],
        [0.9888],
        [0.9888],
        ...,
        [0.9888],
        [0.9888],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085547.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085438.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34822.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244742.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.4824, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9699, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0332],
        [-0.0335],
        [-0.0340],
        ...,
        [-0.0226],
        [-0.0225],
        [-0.0225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34769.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9887],
        [0.9887],
        [0.9887],
        ...,
        [0.9887],
        [0.9887],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085438.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085329.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5511, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34819.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244654.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.3873, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0332],
        [-0.0335],
        [-0.0340],
        ...,
        [-0.0226],
        [-0.0225],
        [-0.0225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34794.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9886],
        [0.9886],
        [0.9886],
        ...,
        [0.9886],
        [0.9886],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085329.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085220.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5515, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34815.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244567.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.2921, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0333],
        [-0.0335],
        [-0.0340],
        ...,
        [-0.0226],
        [-0.0226],
        [-0.0225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34820.1016, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9885],
        [0.9885],
        [0.9885],
        ...,
        [0.9885],
        [0.9885],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085220.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 1800 loss: tensor(0., device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085111.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5520, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34812.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244479.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.1970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8059, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0333],
        [-0.0336],
        [-0.0340],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34845.3359, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9884],
        [0.9884],
        [0.9884],
        ...,
        [0.9884],
        [0.9884],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085111.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085002.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5528, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34808.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244392.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.1017, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8044, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0333],
        [-0.0336],
        [-0.0341],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34870.4648, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085002.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1085002.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5528, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34808.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244392.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.1017, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8044, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9620, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0333],
        [-0.0336],
        [-0.0341],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34870.4648, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9883],
        [0.9883],
        [0.9883],
        ...,
        [0.9883],
        [0.9883],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1085002.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084893., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34805.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244304.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-960.0062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0333],
        [-0.0336],
        [-0.0341],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34895.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9882],
        [0.9882],
        [0.9882],
        ...,
        [0.9882],
        [0.9882],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084893., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084783.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34801.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244217.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.8012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0333],
        [-0.0336],
        [-0.0341],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34920.4414, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9881],
        [0.9881],
        [0.9881],
        ...,
        [0.9881],
        [0.9881],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084783.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084674.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5540, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34798.5117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244129.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.8147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.7997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0334],
        [-0.0336],
        [-0.0341],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34945.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084674.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084674.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5540, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34798.5117, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244129.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.8147, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.7997, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0334],
        [-0.0336],
        [-0.0341],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34945.2891, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9880],
        [0.9880],
        [0.9880],
        ...,
        [0.9880],
        [0.9880],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084674.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084565.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34795.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(244042.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.7191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.7981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9541, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0334],
        [-0.0337],
        [-0.0342],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34970.0469, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9879],
        [0.9879],
        [0.9879],
        ...,
        [0.9879],
        [0.9879],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084565.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084456.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5546, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34791.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(243954.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.6234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.7965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0334],
        [-0.0337],
        [-0.0342],
        ...,
        [-0.0227],
        [-0.0226],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-34994.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9878],
        [0.9878],
        [0.9878],
        ...,
        [0.9878],
        [0.9878],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084456.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084347.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34788.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(243867.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.5273, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.7949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0334],
        [-0.0337],
        [-0.0342],
        ...,
        [-0.0227],
        [-0.0227],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-35019.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084347.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084347.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34788.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(243867.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.5273, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.7949, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9501, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0334],
        [-0.0337],
        [-0.0342],
        ...,
        [-0.0227],
        [-0.0227],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-35019.3047, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9877],
        [0.9877],
        [0.9877],
        ...,
        [0.9877],
        [0.9877],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084347.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(0., device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9876],
        [0.9876],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1084238.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(0., device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        ...,
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000],
        [0.0005, 0.0005, 0.0000,  ..., 0.0005, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(381.5549, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(54.5074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-0., device='cuda:0')



h[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-0., device='cuda:0')



h[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-0., device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        ...,
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0021, 0.0021, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(34784.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0073, 0.0000, 0.0102],
        [0.0000, 0.0010, 0.0016,  ..., 0.0074, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101],
        [0.0000, 0.0010, 0.0015,  ..., 0.0073, 0.0000, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(243779.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-959.4315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(15.7933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-19.9481, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0335],
        [-0.0337],
        [-0.0342],
        ...,
        [-0.0228],
        [-0.0227],
        [-0.0226]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-35043.8008, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9876],
        [0.9876],
        [0.9876],
        ...,
        [0.9876],
        [0.9876],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1084238.2500, device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./Training.py", line 76, in <module>
    featbatch = TraTen[i : i + BatchSize].reshape(BatchSize * 6796, 1)
RuntimeError: shape '[101940, 1]' is invalid for input of size 33980

real	0m47.145s
user	0m12.324s
sys	0m9.768s
